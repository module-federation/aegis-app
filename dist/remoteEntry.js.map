{"version":3,"sources":["webpack://microlib-example/./node_modules/asap/asap.js","webpack://microlib-example/./node_modules/asap/raw.js","webpack://microlib-example/./node_modules/axios-proxy-fix/index.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/adapters/http.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/adapters/xhr.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/axios.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/cancel/Cancel.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/cancel/CancelToken.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/cancel/isCancel.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/Axios.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/InterceptorManager.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/createError.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/dispatchRequest.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/enhanceError.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/settle.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/core/transformData.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/defaults.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/bind.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/btoa.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/buildURL.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/combineURLs.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/cookies.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/isAbsoluteURL.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/isURLSameOrigin.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/normalizeHeaderName.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/parseHeaders.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/helpers/spread.js","webpack://microlib-example/./node_modules/axios-proxy-fix/lib/utils.js","webpack://microlib-example/./node_modules/axios-retry/index.js","webpack://microlib-example/./node_modules/axios-retry/lib/index.js","webpack://microlib-example/./node_modules/axios/index.js","webpack://microlib-example/./node_modules/axios/lib/adapters/http.js","webpack://microlib-example/./node_modules/axios/lib/adapters/xhr.js","webpack://microlib-example/./node_modules/axios/lib/axios.js","webpack://microlib-example/./node_modules/axios/lib/cancel/Cancel.js","webpack://microlib-example/./node_modules/axios/lib/cancel/CancelToken.js","webpack://microlib-example/./node_modules/axios/lib/cancel/isCancel.js","webpack://microlib-example/./node_modules/axios/lib/core/Axios.js","webpack://microlib-example/./node_modules/axios/lib/core/InterceptorManager.js","webpack://microlib-example/./node_modules/axios/lib/core/buildFullPath.js","webpack://microlib-example/./node_modules/axios/lib/core/createError.js","webpack://microlib-example/./node_modules/axios/lib/core/dispatchRequest.js","webpack://microlib-example/./node_modules/axios/lib/core/enhanceError.js","webpack://microlib-example/./node_modules/axios/lib/core/mergeConfig.js","webpack://microlib-example/./node_modules/axios/lib/core/settle.js","webpack://microlib-example/./node_modules/axios/lib/core/transformData.js","webpack://microlib-example/./node_modules/axios/lib/defaults.js","webpack://microlib-example/./node_modules/axios/lib/helpers/bind.js","webpack://microlib-example/./node_modules/axios/lib/helpers/buildURL.js","webpack://microlib-example/./node_modules/axios/lib/helpers/combineURLs.js","webpack://microlib-example/./node_modules/axios/lib/helpers/cookies.js","webpack://microlib-example/./node_modules/axios/lib/helpers/isAbsoluteURL.js","webpack://microlib-example/./node_modules/axios/lib/helpers/isAxiosError.js","webpack://microlib-example/./node_modules/axios/lib/helpers/isURLSameOrigin.js","webpack://microlib-example/./node_modules/axios/lib/helpers/normalizeHeaderName.js","webpack://microlib-example/./node_modules/axios/lib/helpers/parseHeaders.js","webpack://microlib-example/./node_modules/axios/lib/helpers/spread.js","webpack://microlib-example/./node_modules/axios/lib/utils.js","webpack://microlib-example/./node_modules/debug/node_modules/ms/index.js","webpack://microlib-example/./node_modules/debug/src/browser.js","webpack://microlib-example/./node_modules/debug/src/debug.js","webpack://microlib-example/./node_modules/debug/src/index.js","webpack://microlib-example/./node_modules/debug/src/node.js","webpack://microlib-example/./node_modules/follow-redirects/debug.js","webpack://microlib-example/./node_modules/follow-redirects/index.js","webpack://microlib-example/./node_modules/is-buffer/index.js","webpack://microlib-example/./node_modules/is-retry-allowed/index.js","webpack://microlib-example/./node_modules/kafkajs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/admin/index.js","webpack://microlib-example/./node_modules/kafkajs/src/admin/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/index.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/awsIam.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/index.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/oauthBearer.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/plain.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram256.js","webpack://microlib-example/./node_modules/kafkajs/src/broker/saslAuthenticator/scram512.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/brokerPool.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/connectionBuilder.js","webpack://microlib-example/./node_modules/kafkajs/src/cluster/index.js","webpack://microlib-example/./node_modules/kafkajs/src/constants.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assignerProtocol.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assigners/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/assigners/roundRobinAssigner/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/barrier.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/batch.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/consumerGroup.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/filterAbortedMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/index.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/initializeConsumerOffsets.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/offsetManager/isInvalidOffset.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/runner.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/seekOffsets.js","webpack://microlib-example/./node_modules/kafkajs/src/consumer/subscriptionState.js","webpack://microlib-example/./node_modules/kafkajs/src/env.js","webpack://microlib-example/./node_modules/kafkajs/src/errors.js","webpack://microlib-example/./node_modules/kafkajs/src/index.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/emitter.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/event.js","webpack://microlib-example/./node_modules/kafkajs/src/instrumentation/eventType.js","webpack://microlib-example/./node_modules/kafkajs/src/loggers/console.js","webpack://microlib-example/./node_modules/kafkajs/src/loggers/index.js","webpack://microlib-example/./node_modules/kafkajs/src/network/connection.js","webpack://microlib-example/./node_modules/kafkajs/src/network/connectionStatus.js","webpack://microlib-example/./node_modules/kafkajs/src/network/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/network/requestQueue/index.js","webpack://microlib-example/./node_modules/kafkajs/src/network/requestQueue/socketRequest.js","webpack://microlib-example/./node_modules/kafkajs/src/network/socket.js","webpack://microlib-example/./node_modules/kafkajs/src/network/socketFactory.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/createTopicData.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/transactionStateMachine.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/eosManager/transactionStates.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/groupMessagesPerPartition.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/instrumentationEvents.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/messageProducer.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/murmur2.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/partitioner.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/default/randomBytes.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/defaultJava/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/defaultJava/murmur2.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/partitioners/index.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/responseSerializer.js","webpack://microlib-example/./node_modules/kafkajs/src/producer/sendMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/coordinatorTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/crc32.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/encoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/error.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/isolationLevel.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/compression/gzip.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/compression/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v1/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/message/v1/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/messageSet/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/messageSet/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/crc32C.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/v0/decoder.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/recordBatch/v0/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiKeys.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v10/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v10/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v11/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v11/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/decodeMessages.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v6/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v6/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v7/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v7/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v8/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v8/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v9/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/fetch/v9/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/metadata/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v2/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v2/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v3/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v3/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v4/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v4/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v5/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v5/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v6/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v6/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v7/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/produce/v7/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/resourceTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/awsIam/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/plain/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/request.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/response.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/sasl/scram/index.js","webpack://microlib-example/./node_modules/kafkajs/src/protocol/timestampTypes.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/defaults.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/defaults.test.js","webpack://microlib-example/./node_modules/kafkajs/src/retry/index.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/arrayDiff.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/bufferedAsyncIterator.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/concurrency.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/flatten.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/lock.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/long.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/shuffle.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/sleep.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/swapObject.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/waitFor.js","webpack://microlib-example/./node_modules/kafkajs/src/utils/websiteUrl.js","webpack://microlib-example/./node_modules/promise/index.js","webpack://microlib-example/./node_modules/promise/lib/core.js","webpack://microlib-example/./node_modules/promise/lib/done.js","webpack://microlib-example/./node_modules/promise/lib/es6-extensions.js","webpack://microlib-example/./node_modules/promise/lib/finally.js","webpack://microlib-example/./node_modules/promise/lib/index.js","webpack://microlib-example/./node_modules/promise/lib/node-extensions.js","webpack://microlib-example/./node_modules/promise/lib/synchronous.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/index.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/AgentSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/BaseUrlSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/Batch.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/ClientBuilder.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/CustomHeaderSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/Errors.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/HttpSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/InputData.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/LicenseSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/Request.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/Response.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/SharedCredentials.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/SigningSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/StaticCredentials.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/StatusCodeSender.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/international_street/Candidate.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/international_street/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/international_street/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Suggestion.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Suggestion.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_extract/Address.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_extract/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_extract/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_extract/Result.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_reverse_geo/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_street/Candidate.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_street/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_street/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Client.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Lookup.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Result.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/util/apiToSDKKeyMap.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/util/buildClients.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/util/buildInputData.js","webpack://microlib-example/./node_modules/smartystreets-javascript-sdk/src/util/sendBatch.js","webpack://microlib-example/webpack/container-entry","webpack://microlib-example/external \"assert\"","webpack://microlib-example/external \"crypto\"","webpack://microlib-example/external \"dns/promises\"","webpack://microlib-example/external \"domain\"","webpack://microlib-example/external \"events\"","webpack://microlib-example/external \"fs\"","webpack://microlib-example/external \"http\"","webpack://microlib-example/external \"https\"","webpack://microlib-example/external \"net\"","webpack://microlib-example/external \"stream\"","webpack://microlib-example/external \"tls\"","webpack://microlib-example/external \"tty\"","webpack://microlib-example/external \"url\"","webpack://microlib-example/external \"util\"","webpack://microlib-example/external \"zlib\"","webpack://microlib-example/webpack/bootstrap","webpack://microlib-example/webpack/runtime/compat get default export","webpack://microlib-example/webpack/runtime/define property getters","webpack://microlib-example/webpack/runtime/ensure chunk","webpack://microlib-example/webpack/runtime/get javascript chunk filename","webpack://microlib-example/webpack/runtime/hasOwnProperty shorthand","webpack://microlib-example/webpack/runtime/make namespace object","webpack://microlib-example/webpack/runtime/publicPath","webpack://microlib-example/webpack/runtime/sharing","webpack://microlib-example/webpack/runtime/consumes","webpack://microlib-example/webpack/runtime/readFile chunk loading","webpack://microlib-example/webpack/startup"],"names":[],"mappings":";;;;;;;;;;;;;;AAAa;;AAEb,cAAc,mBAAO,CAAC,yCAAO;AAC7B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,MAAM;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;AC/Da;;AAEb,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gEAAgE,kBAAkB;AAClF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,mBAAO,CAAC,sBAAQ;AACrC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpGA,sGAAuC,C;;;;;;;;;;;;;;ACA1B;;AAEb,YAAY,mBAAO,CAAC,+DAAY;AAChC,aAAa,mBAAO,CAAC,2EAAkB;AACvC,eAAe,mBAAO,CAAC,qFAAuB;AAC9C,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,iBAAiB,4FAAgC;AACjD,kBAAkB,6FAAiC;AACnD,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;AACzB,UAAU,mBAAO,CAAC,yEAAsB;AACxC,kBAAkB,mBAAO,CAAC,mFAAqB;AAC/C,mBAAmB,mBAAO,CAAC,qFAAsB;;AAEjD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACzOa;;AAEb,YAAY,mBAAO,CAAC,+DAAY;AAChC,aAAa,mBAAO,CAAC,2EAAkB;AACvC,eAAe,mBAAO,CAAC,qFAAuB;AAC9C,mBAAmB,mBAAO,CAAC,6FAA2B;AACtD,sBAAsB,mBAAO,CAAC,mGAA8B;AAC5D,kBAAkB,mBAAO,CAAC,mFAAqB;AAC/C,yFAAyF,mBAAO,CAAC,6EAAmB;;AAEpH;AACA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,QAAQ,KAA+B;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,oBAAoB,mBAAO,CAAC,mFAAsB;;AAElD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACnLa;;AAEb,YAAY,mBAAO,CAAC,4DAAS;AAC7B,WAAW,mBAAO,CAAC,0EAAgB;AACnC,YAAY,mBAAO,CAAC,sEAAc;AAClC,eAAe,mBAAO,CAAC,kEAAY;;AAEnC;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,4EAAiB;AACxC,oBAAoB,mBAAO,CAAC,sFAAsB;AAClD,iBAAiB,mBAAO,CAAC,gFAAmB;;AAE5C;AACA;AACA;AACA;AACA,eAAe,mBAAO,CAAC,8EAAkB;;AAEzC;;AAEA;AACA,sBAAsB;;;;;;;;;;;;;;;ACnDT;;AAEb;AACA;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;;;;;;;;;;;;;;AClBa;;AAEb,aAAa,mBAAO,CAAC,qEAAU;;AAE/B;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;ACxDa;;AAEb;AACA;AACA;;;;;;;;;;;;;;;ACJa;;AAEb,eAAe,mBAAO,CAAC,qEAAe;AACtC,YAAY,mBAAO,CAAC,+DAAY;AAChC,yBAAyB,mBAAO,CAAC,2FAAsB;AACvD,sBAAsB,mBAAO,CAAC,qFAAmB;;AAEjD;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,iDAAiD,gBAAgB;AACjE;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA,KAAK;AACL;AACA,CAAC;;AAED;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA,CAAC;;AAED;;;;;;;;;;;;;;;AC9Ea;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;;;;;;;;;;;;;;ACnDa;;AAEb,mBAAmB,mBAAO,CAAC,+EAAgB;;AAE3C;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACjBa;;AAEb,YAAY,mBAAO,CAAC,+DAAY;AAChC,oBAAoB,mBAAO,CAAC,iFAAiB;AAC7C,eAAe,mBAAO,CAAC,iFAAoB;AAC3C,eAAe,mBAAO,CAAC,mEAAa;AACpC,oBAAoB,mBAAO,CAAC,+FAA4B;AACxD,kBAAkB,mBAAO,CAAC,2FAA0B;;AAEpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,+BAA+B;AAC/B,uCAAuC;AACvC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACrFa;;AAEb;AACA;AACA;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACpBa;;AAEb,kBAAkB,mBAAO,CAAC,6EAAe;;AAEzC;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACzBa;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,MAAM;AACjB,WAAW,eAAe;AAC1B,aAAa,EAAE;AACf;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACnBa;;AAEb,YAAY,mBAAO,CAAC,4DAAS;AAC7B,0BAA0B,mBAAO,CAAC,wGAA+B;;AAEjE;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc,mBAAO,CAAC,0EAAgB;AACtC,GAAG;AACH;AACA,cAAc,mBAAO,CAAC,4EAAiB;AACvC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wEAAwE;AACxE;AACA;AACA;AACA,uDAAuD;AACvD;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,OAAO,YAAY;AACnB;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA,CAAC;;AAED;;;;;;;;;;;;;;;AC3Fa;;AAEb;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;ACnCa;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;;ACnEa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,wCAAwC;AACxC,OAAO;;AAEP;AACA,0DAA0D,wBAAwB;AAClF;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,gCAAgC;AAChC,6BAA6B,aAAa,EAAE;AAC5C;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB;AAChB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACnEa;;AAEb,YAAY,mBAAO,CAAC,6DAAU;;AAE9B;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACXa;;AAEb,YAAY,mBAAO,CAAC,+DAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,eAAe;;AAEhC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA,WAAW,SAAS;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC1Ba;;AAEb,WAAW,mBAAO,CAAC,0EAAgB;AACnC,eAAe,mBAAO,CAAC,oDAAW;;AAElC;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,OAAO;AAC1C;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,SAAS,GAAG,SAAS;AAC5C,2BAA2B;AAC3B;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA,uCAAuC,OAAO;AAC9C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC9SA,0GAA+C,C;;;;;;;;;;;;;;;;;;;;;;ACAlC;;AAEb,8CAA6C;AAC7C;AACA,CAAC,EAAC;AACF,sBAAsB;AACtB,wBAAwB;AACxB,0BAA0B;AAC1B,gCAAgC;AAChC,yCAAyC;AACzC,wBAAwB;AACxB,eAAe;;AAEf,sBAAsB,mBAAO,CAAC,kEAAkB;;AAEhD;;AAEA,sCAAsC,uCAAuC,gBAAgB;;AAE7F;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA,uCAAuC;AACvC;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;;AAEA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;;AAEA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB;AACA;AACA;;AAEA;AACA,8CAA8C;AAC9C;AACA;;AAEA;AACA;AACA,YAAY,mBAAmB;AAC/B,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,mBAAmB;AAC/B,YAAY,iBAAiB;AAC7B,YAAY;AACZ;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY,mBAAmB;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB,aAAa;AACnC;AACA;AACA;AACA,mBAAmB;AACnB,MAAM;AACN;AACA;AACA,sBAAsB,0CAA0C;AAChE;AACA;AACA,sBAAsB;AACtB;AACA,KAAK;AACL;AACA;AACA,gCAAgC,gCAAgC;AAChE,uBAAuB,aAAa;AACpC;AACA;AACA;AACA,mBAAmB;AACnB,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,sBAAsB;AACtB;AACA,MAAM;AACN;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA,WAAW,SAAS;AACpB;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iC;;;;;;;;;;;;;AC1PA,4FAAuC,C;;;;;;;;;;;;;;ACA1B;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,aAAa,mBAAO,CAAC,iEAAkB;AACvC,oBAAoB,mBAAO,CAAC,6EAAuB;AACnD,eAAe,mBAAO,CAAC,2EAAuB;AAC9C,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,iBAAiB,4FAAgC;AACjD,kBAAkB,6FAAiC;AACnD,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;AACzB,UAAU,mBAAO,CAAC,+DAAsB;AACxC,kBAAkB,mBAAO,CAAC,yEAAqB;AAC/C,mBAAmB,mBAAO,CAAC,2EAAsB;;AAEjD;;AAEA;AACA;AACA,WAAW,uBAAuB;AAClC,WAAW,iBAAiB;AAC5B,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mDAAmD;AAClE;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;AC9Sa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,aAAa,mBAAO,CAAC,iEAAkB;AACvC,cAAc,mBAAO,CAAC,yEAAsB;AAC5C,eAAe,mBAAO,CAAC,2EAAuB;AAC9C,oBAAoB,mBAAO,CAAC,6EAAuB;AACnD,mBAAmB,mBAAO,CAAC,mFAA2B;AACtD,sBAAsB,mBAAO,CAAC,yFAA8B;AAC5D,kBAAkB,mBAAO,CAAC,yEAAqB;;AAE/C;AACA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;AClLa;;AAEb,YAAY,mBAAO,CAAC,kDAAS;AAC7B,WAAW,mBAAO,CAAC,gEAAgB;AACnC,YAAY,mBAAO,CAAC,4DAAc;AAClC,kBAAkB,mBAAO,CAAC,wEAAoB;AAC9C,eAAe,mBAAO,CAAC,wDAAY;;AAEnC;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,kEAAiB;AACxC,oBAAoB,mBAAO,CAAC,4EAAsB;AAClD,iBAAiB,mBAAO,CAAC,sEAAmB;;AAE5C;AACA;AACA;AACA;AACA,eAAe,mBAAO,CAAC,oEAAkB;;AAEzC;AACA,qBAAqB,mBAAO,CAAC,gFAAwB;;AAErD;;AAEA;AACA,sBAAsB;;;;;;;;;;;;;;;ACvDT;;AAEb;AACA;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;;;;;;;;;;;;;;AClBa;;AAEb,aAAa,mBAAO,CAAC,2DAAU;;AAE/B;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;ACxDa;;AAEb;AACA;AACA;;;;;;;;;;;;;;;ACJa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,eAAe,mBAAO,CAAC,yEAAqB;AAC5C,yBAAyB,mBAAO,CAAC,iFAAsB;AACvD,sBAAsB,mBAAO,CAAC,2EAAmB;AACjD,kBAAkB,mBAAO,CAAC,mEAAe;;AAEzC;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA,yBAAyB;AACzB,KAAK;AACL;AACA,CAAC;;AAED;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA,CAAC;;AAED;;;;;;;;;;;;;;;AC9Fa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;;;;;;;;;;;;;;ACnDa;;AAEb,oBAAoB,mBAAO,CAAC,mFAA0B;AACtD,kBAAkB,mBAAO,CAAC,+EAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACnBa;;AAEb,mBAAmB,mBAAO,CAAC,qEAAgB;;AAE3C;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACjBa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,oBAAoB,mBAAO,CAAC,uEAAiB;AAC7C,eAAe,mBAAO,CAAC,uEAAoB;AAC3C,eAAe,mBAAO,CAAC,yDAAa;;AAEpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,+BAA+B;AAC/B,uCAAuC;AACvC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;;AC9Ea;;AAEb;AACA;AACA;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACzCa;;AAEb,YAAY,mBAAO,CAAC,mDAAU;;AAE9B;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,2BAA2B;AAC3B,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;;;;;;;;;;;;;;;ACtFa;;AAEb,kBAAkB,mBAAO,CAAC,mEAAe;;AAEzC;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACxBa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,MAAM;AACjB,WAAW,eAAe;AAC1B,aAAa,EAAE;AACf;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACnBa;;AAEb,YAAY,mBAAO,CAAC,kDAAS;AAC7B,0BAA0B,mBAAO,CAAC,8FAA+B;;AAEjE;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc,mBAAO,CAAC,gEAAgB;AACtC,GAAG;AACH;AACA,cAAc,mBAAO,CAAC,kEAAiB;AACvC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wEAAwE;AACxE;AACA;AACA;AACA,uDAAuD;AACvD;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,OAAO,YAAY;AACnB;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA,CAAC;;AAED;;;;;;;;;;;;;;;ACjGa;;AAEb;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;;;;;;;;;;;;;ACrEa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0CAA0C;AAC1C,SAAS;;AAET;AACA,4DAA4D,wBAAwB;AACpF;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,kCAAkC;AAClC,+BAA+B,aAAa,EAAE;AAC9C;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb;AACA;AACA;AACA,WAAW,EAAE;AACb,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB;AAChB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACnEa;;AAEb,YAAY,mBAAO,CAAC,mDAAU;;AAE9B;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACXa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,eAAe;;AAEhC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA,WAAW,SAAS;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC1Ba;;AAEb,WAAW,mBAAO,CAAC,gEAAgB;;AAEnC;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,OAAO;AAC1C;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,SAAS,GAAG,SAAS;AAC5C,2BAA2B;AAC3B;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,4BAA4B;AAC5B,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;;AAEA,uCAAuC,OAAO;AAC9C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC9VA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;ACvJA;AACA;AACA;AACA;AACA;;AAEA,UAAU,wFAAmC;AAC7C,WAAW;AACX,kBAAkB;AAClB,YAAY;AACZ,YAAY;AACZ,iBAAiB;AACjB,eAAe;AACf;AACA;AACA;;AAEA;AACA;AACA;;AAEA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA,GAAG;AACH;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,MAAM,qBAAqB;AAC3B;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;;;;;;;;ACvLA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd,eAAe;AACf,cAAc;AACd,eAAe;AACf,iGAAgC;;AAEhC;AACA;AACA;;AAEA,aAAa;AACb,aAAa;;AAEb;AACA;AACA;AACA;AACA;;AAEA,kBAAkB;;AAElB;AACA;AACA;;AAEA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;;AAEA;AACA;AACA,cAAc;AACd;;AAEA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;;AAEA,EAAE,aAAa;AACf,EAAE,aAAa;;AAEf;AACA;;AAEA,iBAAiB,SAAS;AAC1B,4BAA4B;AAC5B;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA,yCAAyC,SAAS;AAClD;AACA;AACA;AACA;AACA,yCAAyC,SAAS;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,MAAM;AACjB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzMA;AACA;AACA;AACA;;AAEA;AACA,EAAE,+FAAwC;AAC1C,CAAC;AACD,EAAE,yFAAqC;AACvC;;;;;;;;;;;;;;;;ACTA;AACA;AACA;;AAEA,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;;AAEzB;AACA;AACA;AACA;AACA;;AAEA,UAAU,wFAAmC;AAC7C,YAAY;AACZ,WAAW;AACX,kBAAkB;AAClB,YAAY;AACZ,YAAY;AACZ,iBAAiB;;AAEjB;AACA;AACA;;AAEA,cAAc;;AAEd;AACA;AACA;AACA;AACA;;AAEA,mBAAmB;AACnB;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,2CAA2C,yBAAyB;;AAEpE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC,IAAI;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,6BAA6B;AAC7B;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,sCAAsC;;AAEtC;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,cAAI;AAC3B,2CAA2C,mBAAmB;AAC9D;AACA;;AAEA;AACA;AACA,gBAAgB,mBAAO,CAAC,gBAAK;AAC7B;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,iBAAiB,iBAAiB;AAClC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACvPA;AACA;AACA;AACA,UAAU,mBAAO,CAAC,gDAAO;AACzB;AACA;AACA,uBAAuB;AACvB;AACA;;;;;;;;;;;;;;ACRA,UAAU,mBAAO,CAAC,gBAAK;AACvB;AACA,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,eAAe,oDAA0B;AACzC,aAAa,mBAAO,CAAC,sBAAQ;AAC7B,YAAY,mBAAO,CAAC,yDAAS;;AAE7B;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mCAAmC,iCAAiC;AACpE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,sBAAsB,uCAAuC,EAAE;AAC/D,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC;AACtC;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA,iBAAiB;;AAEjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,2BAA2B;AAClD,mBAAmB;;;;;;;;;;;;;;ACjfnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACpBa;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC7DA,cAAc,mBAAO,CAAC,kDAAO;AAC7B,2BAA2B,mBAAO,CAAC,wFAA0B;AAC7D,yBAAyB,mBAAO,CAAC,gGAAiC;AAClE,qBAAqB,mBAAO,CAAC,8FAA6B;AAC1D,oBAAoB,mBAAO,CAAC,4GAAoC;AAChE,sBAAsB,mBAAO,CAAC,0FAA8B;AAC5D,OAAO,SAAS,GAAG,mBAAO,CAAC,kEAAe;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,uBAAuB,mBAAO,CAAC,iEAAa;AAC5C,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,+CAA+C,GAAG,mBAAO,CAAC,0FAAyB;AAC1F,OAAO,SAAS,GAAG,mBAAO,CAAC,+DAAY;AACvC,OAAO,qDAAqD,GAAG,mBAAO,CAAC,uDAAW;AAClF,uBAAuB,mBAAO,CAAC,uFAA2B;;AAE1D,OAAO,sBAAsB;;AAE7B;;AAEA,OAAO,eAAe;AACtB;AACA;AACA,8BAA8B,IAAI;AAClC;;AAEA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,cAAc;AACzB;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,6BAA6B;AACxC,WAAW,qCAAqC;AAChD,WAAW,mCAAmC;AAC9C,WAAW,8BAA8B;AACzC;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,uDAAuD;AACtF;AACA,iEAAiE,OAAO;AACxE;;AAEA,wBAAwB,QAAQ;AAChC;AACA;AACA;AACA;;AAEA,4CAA4C,QAAQ;AACpD;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,gCAAgC;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,mCAAmC,yCAAyC;AAC5E;AACA,2EAA2E,gBAAgB;AAC3F;AACA;AACA;AACA;;AAEA,iCAAiC,QAAQ;AACzC;AACA;AACA;AACA;;AAEA,qDAAqD,QAAQ;AAC7D;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,uCAAuC,yCAAyC;AAChF,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,+BAA+B,kBAAkB;AACjD;AACA,iEAAiE,OAAO;AACxE;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,kBAAkB;;AAErD;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA,eAAe,6BAA6B;AAC5C,eAAe,4BAA4B;AAC3C,oCAAoC,oBAAoB;AACxD;AACA;AACA;AACA,oCAAoC,0BAA0B;AAC9D;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,2BAA2B;;AAE1E;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,6BAA6B;;AAE5C;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,4BAA4B;;AAE3C,mCAAmC,oBAAoB;AACvD;AACA;AACA;AACA;AACA,sCAAsC,2BAA2B;AACjE;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,+BAA+B,iBAAiB;AAChD;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA,4DAA4D,UAAU;AACtE,4DAA4D,YAAY;;AAExE,WAAW,YAAY;AACvB;AACA,gBAAgB,uCAAuC;AACvD,KAAK;;AAEL;AACA;AACA,aAAa,aAAa;AAC1B,yBAAyB,8BAA8B;AACvD;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,mCAAmC;AAClE;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,qCAAqC,0BAA0B;AAC/D,KAAK;;AAEL,uBAAuB,+CAA+C;AACtE;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,6BAA6B,6BAA6B;AAC1D;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL,8BAA8B,6BAA6B;AAC3D;;AAEA;AACA;AACA,6EAA6E,kBAAkB;AAC/F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,uBAAuB,QAAQ;;AAE/B;AACA,uBAAuB,qBAAqB;AAC5C;AACA,KAAK;AACL;;AAEA;AACA,aAAa,2BAA2B;AACxC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,aAAa;AAC7B,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B;AACA,kCAAkC,6BAA6B;AAC/D;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA,wCAAwC,YAAY,IAAI,+BAA+B;AACvF;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,uDAAuD,6BAA6B;AACpF;AACA,OAAO;AACP;AACA,qDAAqD,0CAA0C;AAC/F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,sBAAsB;AACnC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,aAAa;AAC7B,gBAAgB,OAAO;AACvB,gBAAgB,2BAA2B;AAC3C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,+BAA+B,0BAA0B;AACzD;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA,aAAa,gBAAgB;AAC7B;AACA,0CAA0C,cAAc,IAAI,+BAA+B;AAC3F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0CAA0C,mCAAmC;AAC7E;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,oDAAoD,0CAA0C;AAC9F;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA;AACA,WAAW,SAAS;;AAEpB;AACA;AACA;AACA;AACA,gEAAgE,MAAM;AACtE;;AAEA;AACA;AACA,WAAW;AACX,sDAAsD,MAAM,IAAI,UAAU;AAC1E;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA,qCAAqC,cAAc,KAAK;AACxD;AACA;AACA;AACA,8DAA8D,MAAM;AACpE;AACA,OAAO;AACP;;AAEA,6CAA6C,SAAS;;AAEtD;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,cAAc;AAC9B,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,WAAW,0CAA0C,2BAA2B,aAAa;AAC7F,gCAAgC,qBAAqB;AACrD;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,iBAAiB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA;AACA,+CAA+C,SAAS;AACxD;AACA;AACA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA,eAAe,OAAO;AACtB,gBAAgB,wBAAwB;AACxC;AACA,cAAc;AACd;AACA;AACA;AACA;AACA,gEAAgE,UAAU;AAC1E;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,kDAAkD,uBAAuB;AACzE;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,8CAA8C;AAC9C;AACA;AACA,OAAO,IAAI;AACX;;AAEA;AACA,sCAAsC,wBAAwB;AAC9D;AACA,eAAe,SAAS,mDAAmD,WAAW;AACtF;AACA,OAAO;AACP;;AAEA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,MAAM;AACrB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,mEAAmE,SAAS;AAC5E;;AAEA;;AAEA;AACA,kEAAkE,+BAA+B;AACjG;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB,UAAU;AAC9B,0BAA0B,4BAA4B;AACtD,sBAAsB;AACtB,aAAa;AACb;AACA,mBAAmB,YAAY;;AAE/B,sCAAsC,UAAU;;AAEhD;;AAEA,oCAAoC,UAAU;;AAE9C;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACt7BA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,aAAa,mBAAO,CAAC,+DAAe;AACpC,aAAa,mBAAO,CAAC,+DAAe;AACpC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,yGAAiC;AACxE,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAsB;AAC3D,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,gBAAgB,mBAAO,CAAC,6FAA8B;AACtD,0BAA0B,mBAAO,CAAC,yFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;;AAE1C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;AACV,WAAW,WAAW;AACtB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,4BAA4B,qBAAqB,GAAG,qBAAqB;;AAEzE;AACA;AACA,wCAAwC,mBAAmB;AAC3D,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,kBAAkB;AAC7B;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,2DAA2D,4BAA4B;AACvF;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,YAAY;AACZ,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,8EAA8E;AAC9F;AACA;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE,yCAAyC,uBAAuB;AAChE;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA,qCAAqC;AACrC;AACA,iCAAiC;AACjC;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,kBAAkB;AAC/B,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA,sEAAsE,oBAAoB;AAC1F;AACA,8BAA8B,mBAAmB;AACjD,OAAO;AACP;AACA,KAAK;;AAEL;;AAEA;AACA;AACA,yBAAyB,mBAAmB;AAC5C;;AAEA;AACA;AACA,SAAS;AACT,gCAAgC,iCAAiC;AACjE;;AAEA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,uCAAuC;AAC1D;AACA,uDAAuD,uCAAuC;AAC9F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,gBAAgB;AAC7B,eAAe;AACf;AACA,8BAA8B,2BAA2B;AACzD;AACA;AACA,6DAA6D,2BAA2B;AACxF;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,sCAAsC,mCAAmC,2BAA2B,GAAG;AACvG,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,oBAAoB;AACxC;AACA,wDAAwD,oBAAoB;AAC5E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uBAAuB;AACpC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,eAAe;AACf;AACA,qBAAqB,oCAAoC;AACzD;AACA;AACA,mBAAmB,oCAAoC;AACvD;;AAEA;AACA;AACA;AACA,sDAAsD,4BAA4B;AAClF,0BAA0B,0CAA0C;AACpE,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,sBAAsB,8DAA8D;AACpF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,qBAAqB,kBAAkB;AACvC;AACA,yDAAyD,kBAAkB;AAC3E;;AAEA;AACA;AACA,aAAa,MAAM;AACnB,eAAe;AACf;AACA,wBAAwB,WAAW;AACnC;AACA,4DAA4D,WAAW;AACvE;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,sBAAsB,+CAA+C;AACrE;AACA,0DAA0D,gCAAgC;AAC1F;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,0BAA0B,wDAAwD;AAClF;AACA;AACA,wBAAwB,yCAAyC;AACjE;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA;AACA,eAAe;AACf;AACA,sBAAsB,yBAAyB;AAC/C;AACA,0DAA0D,kBAAkB;AAC5E;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,yBAAyB,qCAAqC;AAC9D;AACA,6DAA6D,6BAA6B;AAC1F;;AAEA;AACA;AACA,aAAa,sBAAsB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,sBAAsB,kCAAkC;AACxD;AACA,0DAA0D,0BAA0B;AACpF;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,wBAAwB,sCAAsC;AAC9D;AACA,4DAA4D,sCAAsC;AAClG;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,4BAA4B,qDAAqD;AACjF;AACA;AACA;AACA;AACA;AACA,0BAA0B,qDAAqD;AAC/E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,yBAAyB,sDAAsD;AAC/E;AACA;AACA,uBAAuB,sDAAsD;AAC7E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,oBAAoB;AACjC,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,oBAAoB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,6BAA6B;AAC7C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe;AACf;AACA,yBAAyB,8DAA8D;AACvF;AACA;AACA,uBAAuB,8DAA8D;AACrF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,gBAAgB,gEAAgE;AAChF;AACA;AACA,cAAc,gEAAgE;AAC9E;AACA;;AAEA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3xBA,eAAe,mBAAO,CAAC,4FAA4B;AACnD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,2DAA2D,SAAS;AACpE,mCAAmC,oBAAoB;AACvD,mEAAmE,SAAS;AAC5E,KAAK;AACL;AACA,+CAA+C,UAAU;AACzD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,OAAO,mBAAmB,GAAG,mBAAO,CAAC,sFAAyB;AAC9D,gBAAgB,mBAAO,CAAC,gGAAiC;AACzD,2BAA2B,mBAAO,CAAC,6EAAS;AAC5C,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,4BAA4B,mBAAO,CAAC,+EAAU;AAC9C,iCAAiC,mBAAO,CAAC,yFAAe;AACxD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qEAAqE,YAAY;AACjF;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qCAAqC,wCAAwC;AAC7E;AACA,eAAe,2BAA2B;AAC1C;AACA,uCAAuC,8BAA8B;AACrE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,eAAe,+BAA+B;AAC9C;AACA;AACA;;AAEA,2CAA2C,wCAAwC;AACnF;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,mBAAO,CAAC,sGAAiC;AAC7D,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;;AAEA,WAAW,sBAAsB;;AAEjC;;AAEA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,+DAA+D,SAAS;AACxE,mCAAmC,oBAAoB;AACvD,uEAAuE,SAAS;AAChF,KAAK;AACL;AACA,mDAAmD,UAAU;AAC7D;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,yDAAyD,SAAS;AAClE,mCAAmC,oBAAoB;AACvD,iEAAiE,SAAS;AAC1E,KAAK;AACL;AACA,6CAA6C,UAAU;AACvD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA,eAAe,mBAAO,CAAC,sBAAQ;AAC/B,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,2DAA2D,GAAG,mBAAO,CAAC,0DAAc;;AAE3F;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,WAAW;AACxB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,iBAAiB;AAC9B;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gCAAgC,WAAW;;AAE3C;AACA;;AAEA;AACA,WAAW,SAAS;AACpB,WAAW,mBAAmB;AAC9B,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,kDAAkD,YAAY;AAC9D;;AAEA;AACA,4DAA4D,SAAS;AACrE;;AAEA,kDAAkD,SAAS;AAC3D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,2BAA2B,OAAO,eAAe,SAAS;AAC1D,KAAK;AACL,0DAA0D,OAAO,WAAW,UAAU;AACtF,wCAAwC,SAAS;AACjD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,kCAAkC,WAAW,EAAE,wBAAwB;AACvE,gDAAgD,qBAAqB;AACrE;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA,WAAW,gBAAgB;;AAE3B;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA,WAAW,OAAO,gCAAgC,WAAW,4BAA4B,cAAc;AACvG;AACA;;AAEA;AACA;AACA,4BAA4B,yBAAyB,KAAK,YAAY;AACtE,gDAAgD,eAAe;AAC/D;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,uBAAuB,KAAK,kBAAkB;AAC9D;;AAEA;AACA;AACA;AACA;AACA;AACA,gBAAgB,qBAAqB,KAAK,OAAO;AACjD;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrUA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,eAAe,mBAAO,CAAC,6DAAW;AAClC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,kBAAkB,mBAAO,CAAC,yEAAoB;AAC9C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,uDAAW;;AAErD,OAAO,uBAAuB;AAC9B,wCAAwC,mBAAmB;AAC3D;AACA;AACA;;AAEA;AACA;AACA,aAAa,kBAAkB;AAC/B,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,wCAAwC;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,sFAAsF,UAAU;AAChG,aAAa;AACb;AACA,SAAS;AACT,wCAAwC,wBAAwB;AAChE;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA,WAAW,iCAAiC;;AAE5C;AACA;AACA;AACA;;AAEA;;AAEA;AACA,iCAAiC,2BAA2B;AAC5D;;AAEA;AACA,0DAA0D,mBAAmB;AAC7E;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,mBAAmB;AACnF;AACA,eAAe;AACf,aAAa;AACb,WAAW;AACX;AACA;;AAEA,2DAA2D,SAAS,QAAQ,OAAO;AACnF;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;;AAEA;AACA,gDAAgD,OAAO;AACvD;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS,iCAAiC,EAAE;AACzD,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4CAA4C,SAAS;AACrD;AACA,+BAA+B,iBAAiB;AAChD,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA,oCAAoC,4BAA4B;AAChE;;AAEA;AACA;AACA;AACA,sCAAsC,SAAS;AAC/C,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX,0EAA0E,wBAAwB;AAClG;;AAEA;AACA,8BAA8B,wCAAwC;AACtE;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AChVA,mBAAmB,mBAAO,CAAC,+EAAuB;AAClD,OAAO,mDAAmD,GAAG,mBAAO,CAAC,uDAAW;;AAEhF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,sDAAsD,UAAU;AAChE;AACA,8BAA8B,kBAAkB,iBAAiB,QAAQ;AACzE;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,mBAAmB,KAAK;AAC3C;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACjFA,mBAAmB,mBAAO,CAAC,sEAAc;AACzC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,0BAA0B,mBAAO,CAAC,oFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;AACvB,0BAA0B,mBAAO,CAAC,6FAA8B;;AAEhE,OAAO,OAAO;;AAEd,2BAA2B,oBAAoB;AAC/C;AACA;AACA,CAAC;;AAED;AACA,WAAW,cAAc;AACzB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,IAAI;AACf,WAAW,4BAA4B;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,kBAAkB;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B,kCAAkC,aAAa;AAC/C;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,kBAAkB,cAAc,KAAK;AACrC;AACA;AACA;AACA,kDAAkD,SAAS;AAC3D,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc;AACd;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;AACA,+CAA+C,SAAS;AACxD,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,WAAW;;AAEtB;AACA;AACA;;AAEA,0CAA0C,gCAAgC;;AAE1E;AACA;AACA,qCAAqC,sBAAsB;AAC3D;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe,0CAA0C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA,WAAW,WAAW;AACtB;AACA,4EAA4E,QAAQ;AACpF;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,eAAe,OAAO;AACtB;AACA,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA,aAAa,SAAS;AACtB;AACA,cAAc;AACd,KAAK,IAAI;AACT;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,8BAA8B,qDAAqD;AACnF;AACA;AACA,eAAe,cAAc;AAC7B;AACA;AACA,SAAS;AACT,sCAAsC,6BAA6B;AACnE,OAAO;AACP;AACA;AACA;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;;AAEA;AACA,oEAAoE;AACpE;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,sCAAsC,2BAA2B;AACjE,oEAAoE,iBAAiB;AACrF;AACA;AACA,oEAAoE,2BAA2B;AAC/F;AACA;AACA;AACA,WAAW;AACX;AACA,SAAS;AACT;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA;;AAEA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,iBAAiB,gBAAgB;AACjC;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;AACA,gDAAgD,eAAe;AAC/D;AACA;AACA;AACA,eAAe,eAAe;AAC9B;AACA;AACA;AACA;AACA,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,YAAY;AACzB,cAAc;AACd;;AAEA;AACA;AACA,aAAa,kDAAkD;AAC/D;AACA;AACA;AACA;AACA;AACA,oEAAoE,gBAAgB;;AAEpF,oCAAoC;;AAEpC;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,4CAA4C,SAAS;AACrD;;AAEA,aAAa,0BAA0B;AACvC;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA,KAAK;;AAEL;AACA;AACA,wEAAwE;;AAExE;AACA;AACA,kDAAkD,oBAAoB;AACtE;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,UAAU;AAC9B;AACA,kDAAkD;AAClD;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB,eAAe;AACf;AACA,yBAAyB,oCAAoC;AAC7D,oDAAoD,UAAU;;AAE9D;AACA;AACA;AACA;;;;;;;;;;;;;;ACnfA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,gBAAgB,mBAAO,CAAC,2EAAqB;;AAE7C;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,8CAA8C;AACxD;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,qBAAqB;AAClC;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,kDAAkD;AAC5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA,gFAAgF;AAChF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,mBAAmB,mBAAO,CAAC,uGAAsB;;AAEjD;AACA;AACA;;;;;;;;;;;;;;ACJA,OAAO,mCAAmC,GAAG,mBAAO,CAAC,uFAAwB;AAC7E,gBAAgB,mBAAO,CAAC,2EAAwB;;AAEhD;AACA;AACA,WAAW,QAAQ;AACnB,aAAa;AACb;AACA,mBAAmB,UAAU;AAC7B;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,MAAM;AACnB,gCAAgC,oDAAoD;AACpF,aAAa,MAAM;AACnB,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;AACA,gBAAgB,kBAAkB;AAClC;AACA,wCAAwC,WAAW;AACnD;;AAEA;AACA;AACA,0CAA0C,2CAA2C;AACrF,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL,GAAG;;AAEH,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClFD;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,UAAU;AACV;;;;;;;;;;;;;;ACbA,aAAa,mBAAO,CAAC,+DAAe;AACpC,8BAA8B,mBAAO,CAAC,6FAAyB;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,gCAAgC;;AAE3C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yDAAyD,kBAAkB;AAC3E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvGA,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,cAAc,mBAAO,CAAC,iEAAgB;AACtC,8BAA8B,mBAAO,CAAC,iGAAgC;AACtE,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,kBAAkB,mBAAO,CAAC,yEAAoB;;AAE9C,sBAAsB,mBAAO,CAAC,mFAAiB;AAC/C,cAAc,mBAAO,CAAC,6DAAS;AAC/B,oBAAoB,mBAAO,CAAC,yEAAe;AAC3C,0BAA0B,mBAAO,CAAC,qFAAqB;AACvD;AACA,WAAW,qBAAqB;AAChC,CAAC,GAAG,mBAAO,CAAC,6FAAyB;AACrC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAoB;AACzD;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;;AAEvB,OAAO,OAAO;;AAEd;AACA;AACA;AACA;AACA,0BAA0B,eAAe;AACzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,eAAe,8BAA8B;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,sBAAsB,sBAAsB;AAC3D;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,4CAA4C;;AAEvD,gEAAgE,UAAU;;AAE1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,oBAAoB;AAC/B;AACA,yCAAyC,oBAAoB;AAC7D;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,mDAAmD,0CAA0C;AAC7F,6CAA6C,OAAO;;AAEpD;AACA;AACA,6CAA6C,cAAc;AAC3D;AACA;;AAEA;AACA,0CAA0C,oCAAoC;;AAE9E;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,WAAW,mBAAmB;AAC9B;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gDAAgD,QAAQ;AACxD;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,oBAAoB;AACjD;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,eAAe,mBAAmB;AAClC,oCAAoC,mBAAmB;AACvD;;AAEA,iBAAiB,2BAA2B;AAC5C,sCAAsC,2BAA2B;AACjE;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,QAAQ,2BAA2B;AACnC;AACA;;AAEA;AACA,8CAA8C,uBAAuB;AACrE;AACA,KAAK;AACL;AACA;;AAEA;AACA,+CAA+C,uBAAuB;AACtE;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,mBAAmB,WAAW;AAC9B,WAAW,kCAAkC;AAC7C;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,gEAAgE;AAC7E,kBAAkB,mBAAmB,4BAA4B,mBAAmB,qBAAqB,mBAAmB,GAAG,IAAI;AACnI;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA,mEAAmE,aAAa;AAChF;AACA,kBAAkB,aAAa;AAC/B,eAAe,QAAQ;;AAEvB;AACA,sEAAsE,iBAAiB;AACvF;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;;AAEA;AACA,yBAAyB,wBAAwB,kBAAkB,oBAAoB,UAAU,cAAc;AAC/G;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA,wCAAwC,0CAA0C;AAClF;AACA;;AAEA;AACA,sDAAsD,SAAS;AAC/D,eAAe,YAAY;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET,oDAAoD,wBAAwB;AAC5E,kEAAkE,QAAQ;AAC1E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,kCAAkC;AACvD;AACA,uBAAuB,sCAAsC;AAC7D;AACA;AACA,oEAAoE,qBAAqB;AACzF;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,YAAY;AAC9B;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA,aAAa;AACb,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;;AAEA;AACA,0BAA0B,UAAU;AACpC;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,iCAAiC,6BAA6B;AAC9D;;AAEA;AACA,2BAA2B,UAAU;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,gBAAgB,4BAA4B;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA;AACA;AACA;AACA,eAAe,yCAAyC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA,cAAc;AACd,KAAK,IAAI;AACT;AACA;;;;;;;;;;;;;;ACzqBA,aAAa,mBAAO,CAAC,+DAAe;AACpC;;AAEA,wBAAwB,MAAM;AAC9B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW,UAAU;AACrB,WAAW,cAAc;AACzB,aAAa,UAAU;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,WAAW;AACtB,WAAW,YAAY;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA,mBAAmB,gCAAgC;AACnD;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;;AAEA,WAAW,4BAA4B;;AAEvC;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC/DA,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,uEAAmB;AACxD,sBAAsB,mBAAO,CAAC,6EAAiB;AAC/C,eAAe,mBAAO,CAAC,+DAAU;AACjC,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,OAAO,aAAa,GAAG,mBAAO,CAAC,2EAAa;AAC5C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE,wBAAwB,mBAAO,CAAC,yFAA4B;;AAE5D,OAAO,eAAe;AACtB,OAAO,mCAAmC;;AAE1C;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,OAAO;AAClB,WAAW,mCAAmC;AAC9C,WAAW,6BAA6B;AACxC,WAAW,0CAA0C;AACrD,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD,WAAW,OAAO;AAClB;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA,oBAAoB,2BAA2B;AAC/C;;AAEA;AACA;AACA;;AAEA;AACA;AACA,qCAAqC,kBAAkB,uCAAuC,eAAe;AAC7G;AACA;;AAEA,gCAAgC,0CAA0C;AAC1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,0CAA0C;AACvD;AACA;AACA;AACA;;AAEA,aAAa,6CAA6C;AAC1D;AACA;AACA;AACA,2DAA2D,UAAU;AACrE;AACA;AACA,KAAK;AACL;;AAEA,aAAa,uCAAuC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,UAAU;AACxC,KAAK;AACL;;AAEA,aAAa,4CAA4C;AACzD,4BAA4B,+BAA+B;AAC3D;AACA;AACA;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,yBAAyB,MAAM,IAAI,aAAa;AAChD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mBAAmB;AAClC;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;;AAEA;AACA,mBAAmB;AACnB;;AAEA;AACA;;AAEA,aAAa,sCAAsC;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA,mFAAmF,UAAU;AAC7F;AACA;;AAEA;AACA;AACA;AACA,KAAK;;AAEL;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA,6BAA6B,OAAO,IAAI,UAAU;AAClD;AACA;AACA;AACA,OAAO;;AAEP;AACA,8BAA8B,6BAA6B;AAC3D;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;;AAEX;AACA;AACA,qDAAqD,UAAU;AAC/D;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;;AAEA;AACA;;AAEA,aAAa,qCAAqC;AAClD;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,yEAAyE;AAC3F;AACA;AACA;AACA,iBAAiB,4CAA4C;AAC7D;AACA,8DAA8D,MAAM;AACpE;;AAEA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,yFAAyF,OAAO;AAChG;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0EAA0E,SAAS;AACnF;AACA;;AAEA;;AAEA,2BAA2B,4CAA4C;;AAEvE,gBAAgB;AAChB,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA,aAAa,uCAAuC;AACpD,iBAAiB,2BAA2B;AAC5C;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA,yDAAyD,UAAU;AACnE;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,qFAAqF,OAAO;AAC5F;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kDAAkD;AAC1E;;AAEA,aAAa,gDAAgD;AAC7D;AACA,4DAA4D,UAAU;AACtE;AACA;AACA,aAAa,SAAS,qCAAqC,sBAAsB;AACjF;AACA,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,0CAA0C;AAC5D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,wFAAwF,0BAA0B;AAClH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY;AACZ;AACA,iBAAiB,0CAA0C;AAC3D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,yFAAyF,0BAA0B;AACnH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChgBA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,wEAAqB;AAC7C,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,kCAAkC,mBAAO,CAAC,mHAA6B;AACvE;AACA,WAAW,iBAAiB;AAC5B,CAAC,GAAG,mBAAO,CAAC,8FAA0B;;AAEtC,OAAO,eAAe;AACtB,yEAAyE,YAAY,EAAE,KAAK;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,eAAe,mBAAmB;AAClC;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,iBAAiB,2BAA2B;AAC5C;AACA;AACA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,0BAA0B,mBAAmB;AAC7C,WAAW,kCAAkC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mCAAmC;AAC3D,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,cAAc,2BAA2B;AACzC;AACA;AACA;;AAEA,WAAW,kCAAkC;AAC7C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,oBAAoB;AAC5C,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,eAAe;AAC/B;AACA,eAAe,OAAO;AACtB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,8BAA8B,aAAa;AAC3C;AACA;AACA;AACA,KAAK;AACL,sCAAsC,oBAAoB;AAC1D;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,YAAY;AACZ;;AAEA,kCAAkC;AAClC,WAAW,kCAAkC;AAC7C,WAAW,4CAA4C;;AAEvD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,oBAAoB;AAC3C;AACA,iBAAiB,oBAAoB,kBAAkB,sBAAsB;AAC7E;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,UAAU;AACrB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,YAAY;AAC1C,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,6BAA6B;AACxC;AACA;AACA,KAAK;;AAEL,uDAAuD,oBAAoB;AAC3E;AACA;AACA;AACA;AACA,sBAAsB,SAAS;AAC/B,mBAAmB,YAAY,aAAa,YAAY;AACxD,SAAS;AACT;AACA;AACA;;AAEA,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;AAChD;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AClXA,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,OAAO,eAAe;;AAEtB,+BAA+B,oBAAoB,kBAAkB,sBAAsB;AAC3F,2BAA2B,oBAAoB;AAC/C,eAAe,+CAA+C,GAAG;;AAEjE;AACA,uEAAuE;AACvE,iEAAiE;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,GAAG;AACH;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;;AAEvC;;;;;;;;;;;;;;ACFA,qBAAqB,mBAAO,CAAC,sBAAQ;AACrC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,yBAAyB,mBAAO,CAAC,6EAAsB;AACvD,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;AAC5C,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC;AACA,WAAW,yEAAyE;AACpF,CAAC,GAAG,mBAAO,CAAC,6FAAyB;;AAErC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,6BAA6B;AAC1C,aAAa,0BAA0B;AACvC,aAAa,qCAAqC;AAClD,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,mEAAmE;AAChF,aAAa,qEAAqE;AAClF,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC,aAAa,mCAAmC;AAChD,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,4BAA4B,oBAAoB,OAAO,iCAAiC,KAAK;;AAE7F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA,WAAW,mBAAmB;;AAE9B;AACA,6DAA6D,mBAAmB;AAChF;AACA;;AAEA;AACA,gCAAgC,4BAA4B;AAC5D,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;;AAEA,wCAAwC,2CAA2C;AACnF,0CAA0C,mCAAmC;AAC7E;AACA;AACA;;AAEA;AACA,WAAW,mBAAmB;AAC9B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C,4CAA4C;AACxF,SAAS;AACT;AACA,8CAA8C,mCAAmC;AACjF,SAAS;AACT;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,yDAAyD,mBAAmB;AAC5E,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,+CAA+C;AACvF;AACA;;AAEA;AACA;;AAEA,oDAAoD;;AAEpD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA,WAAW,gCAAgC;AAC3C,2CAA2C,6CAA6C;;AAExF;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,kDAAkD,mCAAmC;AACrF,aAAa;AACb;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wCAAwC,mCAAmC;AAC3E;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;ACzjBA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C,gBAAgB,wBAAwB,oBAAoB;AAC5D,OAAO;AACP;AACA;AACA;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA;AACA,aAAa,oBAAoB;AACjC;AACA;AACA,OAAO;AACP,gBAAgB,aAAa;AAC7B;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1HA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD;AACA,kBAAkB,mBAAmB,KAAK;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc,mBAAmB;AACjC;AACA;AACA;;AAEA;AACA;AACA,cAAc,yBAAyB;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,mBAAmB;AACrC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,WAAW;AAC7B;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,wBAAwB;AAC1C;AACA,oBAAoB,UAAU,iBAAiB,QAAQ;AACvD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,eAAe,KAAK;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,aAAa,KAAK;AACpC,cAAc,YAAY,KAAK,GAAG,KAAK,GAAG;AAC1C;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,4DAA4D,KAAK;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,QAAQ,KAAK;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,2BAA2B,KAAK;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,kBAAkB,KAAK;AACzC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3MA;AACA;AACA,WAAW,OAAO;AAClB,CAAC,GAAG,mBAAO,CAAC,8DAAW;;AAEvB,oCAAoC,mBAAO,CAAC,wFAA2B;AACvE,sBAAsB,mBAAO,CAAC,wEAAmB;AACjD,gBAAgB,mBAAO,CAAC,8DAAW;AACnC,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,oBAAoB,mBAAO,CAAC,0DAAS;AACrC,wBAAwB,mBAAO,CAAC,wFAA2B;AAC3D,6BAA6B,mBAAO,CAAC,oFAAyB;;AAE9D;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,yCAAyC,8BAA8B;AACvE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,SAAS,QAAQ,KAAK;AACtB;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrLA,qBAAqB,mBAAO,CAAC,sBAAQ;AACrC,6BAA6B,mBAAO,CAAC,oEAAS;AAC9C,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;;AAE5C;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,oDAAoD,mBAAmB;AACvE;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,eAAe,SAAS;AACxB;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACtBA,yCAAyC,UAAU,GAAG,KAAK;;;;;;;;;;;;;;ACA3D,OAAO,mBAAmB,GAAG,mBAAO,CAAC,4DAAS;;AAE9C,yBAAyB,+BAA+B;AACxD,iCAAiC,UAAU;AAC3C;AACA,mBAAmB,eAAe;AAClC,kBAAkB,OAAO,EAAE,YAAY;AACvC,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpBA,OAAO,SAAS;;AAEhB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA,uBAAuB,kCAAkC,KAAK;AAC9D;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnEA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,qBAAqB,mBAAO,CAAC,8DAAU;AACvC,sBAAsB,mBAAO,CAAC,2EAAqB;AACnD,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,OAAO,uDAAuD,GAAG,mBAAO,CAAC,uDAAW;AACpF,OAAO,mBAAmB,GAAG,mBAAO,CAAC,6DAAc;AACnD,eAAe,mBAAO,CAAC,iDAAQ;AAC/B,qBAAqB,mBAAO,CAAC,gFAAgB;AAC7C,OAAO,sCAAsC,GAAG,mBAAO,CAAC,kFAAoB;;AAE5E,sBAAsB,8BAA8B;AACpD,KAAK,QAAQ,QAAQ,OAAO,aAAa,WAAW;;AAEpD;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,WAAW,OAAO;AAClB;AACA,WAAW,4BAA4B;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd,GAAG;AACH;AACA;AACA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C;;AAEA;AACA;AACA;;AAEA;AACA,gCAAgC,gBAAgB;AAChD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA,6CAA6C;AAC7C;AACA,sBAAsB,0CAA0C;AAChE;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sEAAsE,UAAU;AAChF,qBAAqB,UAAU,GAAG,UAAU;AAC5C;AACA,SAAS;;AAET,sCAAsC,iBAAiB;AACvD;AACA;;AAEA;AACA;;AAEA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C,SAAS;;AAET;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA,2DAA2D,UAAU;AACrE,uBAAuB,UAAU,GAAG,UAAU;AAC9C,WAAW;AACX;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,gBAAgB,gDAAgD;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,yBAAyB,UAAU,GAAG,UAAU;AAChD,aAAa;AACb;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe,cAAc;AAC7B;AACA,cAAc,oEAAoE;AAClF;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;;AAEA,kDAAkD,mCAAmC;AACrF,aAAa,8BAA8B;AAC3C,+BAA+B,qBAAqB;AACpD;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,yBAAyB;;AAEzB;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX,SAAS;AACT;AACA;AACA,OAAO;AACP;;AAEA,WAAW,sCAAsC;;AAEjD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA,kCAAkC,mBAAmB;AACrD;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,gDAAgD,qCAAqC;AACrF,OAAO;;AAEP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,UAAU,GAAG,UAAU;AAC1C,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7bA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACPA,qBAAqB,mBAAO,CAAC,sBAAQ;AACrC,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,eAAe,mBAAO,CAAC,6FAA0B;;AAEjD;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,4BAA4B;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,aAAa;AAC7B,gBAAgB,QAAQ;AACxB,gBAAgB,SAAS;AACzB,gBAAgB,OAAO;AACvB;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,+BAA+B,yBAAyB;AACxD;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,kBAAkB,+BAA+B;AACjD;AACA;AACA;;AAEA;AACA,+BAA+B,gBAAgB;AAC/C,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;;;;;;;;;;;;;AC5SA,OAAO,uDAAuD,GAAG,mBAAO,CAAC,0DAAc;AACvF,eAAe,mBAAO,CAAC,6FAA0B;;AAEjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,aAAa;AAC3B,cAAc,QAAQ;AACtB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,aAAa;AAC1B,aAAa,QAAQ;AACrB,aAAa,SAAS;AACtB,aAAa,4BAA4B;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,8BAA8B;AACzC,2BAA2B,QAAQ,QAAQ,OAAO,aAAa,WAAW;AAC1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4DAA4D,YAAY;AACxE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,gBAAgB;AAC7B;AACA;AACA;AACA,KAAK;;AAEL,WAAW,6EAA6E;;AAExF;AACA;AACA,mBAAmB,sCAAsC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;;AAEA;;AAEA;AACA,8CAA8C,QAAQ,MAAM,gBAAgB;AAC5E;AACA;AACA;;;;;;;;;;;;;;ACpKA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,gCAAgC,6BAA6B;;AAE7D;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACnBA;;AAEA;AACA,cAAc,mBAAO,CAAC,gBAAK;AAC3B,cAAc,mBAAO,CAAC,gBAAK;;AAE3B,WAAW,6BAA6B;AACxC;AACA,mCAAmC,+BAA+B;AAClE,qBAAqB,aAAa;;AAElC;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACfA;AACA;AACA,MAAM,gEAAgE;AACtE;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACXA,oBAAoB,mBAAO,CAAC,8DAAa;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,0BAA0B,mBAAO,CAAC,gGAAiC;AACnE,2BAA2B,mBAAO,CAAC,4GAA2B;AAC9D,eAAe,mBAAO,CAAC,sBAAQ;;AAE/B,eAAe,mBAAO,CAAC,gGAAqB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yEAAyE;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2CAA2C,SAAS;AACpD,kCAAkC,KAAK;AACvC;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa;;AAEb;AACA;AACA;AACA;;AAEA,6DAA6D,4BAA4B;AACzF,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,mBAAmB;AACnB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,4BAA4B,MAAM,GAAG,UAAU,sBAAsB,SAAS;AAC9E;AACA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,YAAY;AAC7B;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,SAAS;AAC7B,oBAAoB,OAAO;AAC3B;AACA;AACA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;;AAEA,+BAA+B,YAAY;AAC3C;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,2CAA2C,qDAAqD;AAChG;;AAEA,yBAAyB,oBAAoB;AAC7C;AACA;AACA,WAAW;AACX,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,oBAAoB;AACrC,mBAAmB;AACnB;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,6BAA6B;AACjD;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,OAAO;AAC3B;AACA,yBAAyB,0BAA0B;AACnD;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;AACA,uBAAuB,oDAAoD;AAC3E,yBAAyB,4CAA4C;AACrE,mCAAmC,oCAAoC;AACvE,oBAAoB,oCAAoC;AACxD,eAAe,oCAAoC;AACnD,cAAc,oCAAoC;AAClD;AACA;;AAEA;AACA;;;;;;;;;;;;;;AChZA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,eAAe,mBAAO,CAAC,gGAAqB;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8CAA8C;AACjE;;AAEA,kCAAkC,qCAAqC;AACvE;AACA,gFAAgF,OAAO;AACvF;;AAEA;AACA;;AAEA;AACA;AACA,uDAAuD,OAAO,cAAc,aAAa;AACzF;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,SAAS;AACxB,eAAe,aAAa;AAC5B;AACA;AACA;AACA;AACA;AACA,OAAO,IAAI;;AAEX,cAAc;AACd,KAAK;AACL;AACA;AACA;AACA;AACA,mDAAmD,aAAa,OAAO,MAAM;;AAE7E;AACA;AACA,6DAA6D,aAAa,OAAO,MAAM;AACvF;AACA;;AAEA,uCAAuC,gCAAgC;AACvE;AACA,KAAK;;AAEL;AACA;AACA,KAAK;AACL,GAAG;;AAEH;AACA;;;;;;;;;;;;;;AC9EA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,mBAAmB,kDAAkD;AACrE;AACA;AACA;;AAEA;AACA,mCAAmC,oCAAoC;AACvE;AACA,kCAAkC,qCAAqC;AACvE,GAAG,IAAI;AACP;;;;;;;;;;;;;;ACVA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;AACnE,OAAO,qBAAqB,GAAG,mBAAO,CAAC,kFAAiB;AACxD,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,yBAAyB,mBAAO,CAAC,6EAAc;AAC/C,8BAA8B,mBAAO,CAAC,iFAAmB;AACzD,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD,OAAO,eAAe;AACtB;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA,OAAO,sBAAsB;;AAE7B;AACA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,6BAA6B;AACxC,WAAW,yCAAyC;AACpD,WAAW,mCAAmC;AAC9C,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,oBAAoB;;AAEpB;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,8CAA8C;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,SAAS,kBAAkB;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,cAAc,SAAS;AACvB;AACA;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,WAAW,yCAAyC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA,4CAA4C,0BAA0B;AACtE,mDAAmD,0BAA0B;;AAE7E;AACA,iBAAiB,oBAAoB;AACrC,sBAAsB,oBAAoB;AAC1C;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB;AACA;;AAEA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzPA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,2BAA2B,mBAAO,CAAC,2EAAgB;AACnD,OAAO,yCAAyC,GAAG,mBAAO,CAAC,uDAAW;AACtE,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA,yCAAyC;AACzC;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,aAAa;AACb,eAAe;AACf;AACA,4BAA4B,sDAAsD;AAClF,6BAA6B,QAAQ;AACrC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,kBAAkB;AAClC;AACA;AACA,qCAAqC,SAAS,eAAe,MAAM;AACnE;AACA;;AAEA;AACA;AACA;AACA,sDAAsD,MAAM,KAAK;AACjE;AACA,YAAY;AACZ;AACA;AACA;;AAEA;AACA,+DAA+D,kBAAkB;AACjF,uCAAuC,qBAAqB;;AAE5D;AACA,qBAAqB,kBAAkB;AACvC,OAAO;AACP;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA,gCAAgC,qCAAqC;AACrE;;AAEA;AACA,kEAAkE,cAAc;AAChF;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,cAAc,IAAI,wBAAwB;AAC7F;AACA;AACA;;AAEA;AACA,wBAAwB,cAAc,IAAI,wBAAwB;AAClE;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,eAAe;AAC5B,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,uBAAuB,8CAA8C;AACrE,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpKA,gBAAgB,mBAAO,CAAC,sFAAW;AACnC,iCAAiC,mBAAO,CAAC,8FAAe;;AAExD;;;;;;;;;;;;;;ACHA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AClDA,oBAAoB,mBAAO,CAAC,8FAAe;;AAE3C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,WAAW,oCAAoC;AAC/C;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,6DAAiB;;AAE9D;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD,yBAAyB,mBAAO,CAAC,sBAAQ;AACzC;;AAEA;;AAEA;AACA;AACA;AACA,sBAAsB,KAAK,0DAA0D,UAAU;AAC/F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,0FAAW;AACnC,iCAAiC,mBAAO,CAAC,uGAAwB;;AAEjE;;;;;;;;;;;;;;ACHA;AACA,aAAa,mBAAO,CAAC,qEAAqB;;AAE1C;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACpDA,2BAA2B,mBAAO,CAAC,oFAAW;AAC9C,kCAAkC,mBAAO,CAAC,4FAAe;;AAEzD;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,gBAAgB,mBAAO,CAAC,qEAAkB;;AAE1C,mBAAmB,SAAS;AAC5B,kCAAkC,wBAAwB;AAC1D,kCAAkC,0BAA0B;AAC5D;;AAEA;AACA;;;;;;;;;;;;;;ACRA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,kCAAkC,mBAAO,CAAC,qGAA6B;AACvE,wBAAwB,mBAAO,CAAC,iFAAmB;AACnD,2BAA2B,mBAAO,CAAC,uFAAsB;;AAEzD,OAAO,OAAO;AACd;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,2CAA2C;AAC9D,+BAA+B,qCAAqC;;AAEpE,iBAAiB,4CAA4C;AAC7D;;AAEA,uCAAuC,QAAQ;AAC/C;;AAEA;AACA;;AAEA;;AAEA,kBAAkB,kBAAkB;AACpC;;AAEA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,SAAS,IAAI;;AAEb;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,mDAAmD,SAAS;AAC5D;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,sBAAsB;AAC9C,yBAAyB,kEAAkE;AAC3F;AACA;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA,sCAAsC,uBAAuB;AAC7D;;AAEA;AACA,WAAW;;AAEX;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC3IA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,SAAS,SAAS;AAClB;AACA;AACA,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;;;;;;;;;;;;;;AC7QA,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3NA,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxVA,OAAO,uBAAuB,GAAG,mBAAO,CAAC,uDAAW;AACpD,mBAAmB,mBAAO,CAAC,2EAAqB;;AAEh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iCAAiC,UAAU;AAC3C,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACllBA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,OAAO,YAAY,GAAG,mBAAO,CAAC,kBAAM;AACpC,aAAa,mBAAO,CAAC,kBAAM;;AAE3B;AACA;;AAEA;AACA;AACA,aAAa,QAAQ;AACrB,eAAe;AACf;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;ACtBA,OAAO,wBAAwB,GAAG,mBAAO,CAAC,6DAAiB;;AAE3D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB,mBAAO,CAAC,+EAAQ;AACtC;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,0DAAc;;AAE1B,kBAAkB,mBAAO,CAAC,+EAAc;AACxC,kBAAkB,mBAAO,CAAC,+EAAc;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+DAA+D,UAAU;AACzE;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,2DAA2D,eAAe,kBAAkB,KAAK;AACjG;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,+BAA+B;AACvD;;;;;;;;;;;;;;ACpCA;AACA,KAAK,mBAAO,CAAC,qEAAM;AACnB,KAAK,mBAAO,CAAC,qEAAM;AACnB;;AAEA,mBAAmB,cAAc;;;;;;;;;;;;;;ACLjC;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACJD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,wFAAgB;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,6CAA6C;AAChE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACLD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,wFAAgB;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qEAAqE;AACxF;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,uBAAuB,mBAAO,CAAC,kFAAoB;AACnD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAwB;AACpE,OAAO,6BAA6B,GAAG,mBAAO,CAAC,0DAAc;;AAE7D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1FA,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,wBAAwB,mBAAO,CAAC,wEAAY;AAC5C,OAAO,QAAQ,GAAG,mBAAO,CAAC,gGAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA,mCAAmC,0BAA0B;AAC7D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;ACxCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpFA,eAAe,mBAAO,CAAC,kFAAU;AACjC;;AAEA;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,aAAa;AAChC;AACA;;;;;;;;;;;;;;ACXA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,sBAAsB,mBAAO,CAAC,qGAAyB;AACvD,uBAAuB,mBAAO,CAAC,sFAAyB;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB,aAAa,OAAO,uBAAuB,KAAK;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,eAAe,mBAAO,CAAC,2FAAiB;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,2BAA2B;AAC3B,8BAA8B;AAC9B,eAAe;AACf,iBAAiB;AACjB,qBAAqB,GAAG;AACxB;AACA,mBAAmB,8DAA8D,EAAE;AACnF;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,OAAO,6BAA6B,GAAG,mBAAO,CAAC,6DAAiB;AAChE,OAAO,qCAAqC,GAAG,mBAAO,CAAC,mGAA2B;AAClF,sBAAsB,mBAAO,CAAC,kGAAsB;AACpD,uBAAuB,mBAAO,CAAC,mFAAsB;;AAErD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,gEAAgE,eAAe,wBAAwB,OAAO;AAC9G;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uDAAuD,8BAA8B;;AAErF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,YAAY;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrHA,aAAa,mBAAO,CAAC,qEAAqB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,eAAe,mBAAO,CAAC,kFAAW;AAClC,OAAO,kCAAkC,GAAG,mBAAO,CAAC,mGAA2B;;AAE/E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzFA,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC,yBAAyB,oCAAoC,6BAA6B,EAAE;AAC5F;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACZA;AACA,OAAO,sDAAsD;AAC7D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,sDAAsD;AACrF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACXA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sDAAsD;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA;AACA,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,mGAAc;AAC1C,qBAAqB,mBAAO,CAAC,qGAAe;AAC5C,YAAY,mBAAmB,qDAAqD;AACpF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACXA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,6BAA6B,GAAG,mBAAO,CAAC,8EAAe;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,kBAAkB;AAC7B,qDAAqD,YAAY;AACjE,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA;AACA,OAAO,0BAA0B;AACjC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,0BAA0B;AACzD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACXA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB,WAAW,QAAQ;AACnB;AACA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,4BAA4B;AACrD;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA;;AAEA;AACA;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACZD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,OAAO,mCAAmC,GAAG,mBAAO,CAAC,+FAAgB;;AAErE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACfA;AACA,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wDAAwD;AAC3E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,iCAAiC;AACjE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACnCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yCAAyC;AAC5D,2BAA2B,yCAAyC,IAAI,gBAAgB;;;;;;;;;;;;;;ACdxF,0IAA0C;;;;;;;;;;;;;;ACA1C;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;AChDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gCAAgC;AACnD,2BAA2B,gCAAgC,IAAI,gBAAgB;;;;;;;;;;;;;;ACnB/E,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA;AACA;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;;AAEA,iEAAiE,gBAAgB;;;;;;;;;;;;;;ACNjF,mBAAmB,mBAAO,CAAC,gGAAgB;;AAE3C;;;;;;;;;;;;;;ACFA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,6BAA6B;AACpC,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,6BAA6B;AAC5D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;AACA,mBAAmB,qCAAqC;AACxD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,mGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5DA;AACA,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,WAAW;AAC9B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,YAAY;AAC3D;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,WAAW,8BAA8B,WAAW,IAAI,gBAAgB;;;;;;;;;;;;;;ACP3F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,kGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA;AACA,OAAO,gEAAgE;AACvE,oBAAoB,mBAAO,CAAC,uFAAc;AAC1C,qBAAqB,mBAAO,CAAC,yFAAe;AAC5C;AACA,wBAAwB,gEAAgE;AACxF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACdA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8EAAe;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gEAAgE;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qDAAqD;AAC7E;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1PA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;AAC7E,gBAAgB,mBAAO,CAAC,8EAA2B;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,OAAO,uCAAuC;AAC9C;AACA;;AAEA;AACA,mDAAmD,wBAAwB;AAC3E;AACA;AACA,wCAAwC,cAAc,mBAAmB;AACzE,GAAG;;AAEH;AACA;AACA,WAAW,8BAA8B;AACzC;AACA,yEAAyE,mBAAmB;AAC5F;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACtDA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,0BAA0B,mBAAO,CAAC,8FAA6B;AAC/D,2BAA2B,mBAAO,CAAC,sGAAiC;AACpE,OAAO,aAAa,GAAG,mBAAO,CAAC,4FAAyB;;AAExD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,iGAAkB;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChFA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,0BAA0B,mBAAO,CAAC,uFAAwB;;AAE1D;AACA,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,2CAA2C;AAC1E,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/CA;AACA,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACpBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D,2BAA2B,uCAAuC,IAAI,gBAAgB;;;;;;;;;;;;;;ACVtF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,0EAAW;AACnC,OAAO,oCAAoC,GAAG,mBAAO,CAAC,0DAAc;;AAEpE;AACA,WAAW,mBAAO,CAAC,gFAAW;AAC9B,SAAS,mBAAO,CAAC,4EAAS;AAC1B,eAAe,mBAAO,CAAC,wFAAe;AACtC,YAAY,mBAAO,CAAC,kFAAY;AAChC,kBAAkB;AAClB,iBAAiB;AACjB,oBAAoB;AACpB,wBAAwB;AACxB,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,eAAe,mBAAO,CAAC,wFAAe;AACtC,oBAAoB,mBAAO,CAAC,gGAAmB;AAC/C,aAAa,mBAAO,CAAC,oFAAa;AAClC,aAAa,mBAAO,CAAC,oFAAa;AAClC,cAAc,mBAAO,CAAC,sFAAc;AACpC,aAAa,mBAAO,CAAC,oFAAa;AAClC,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C,cAAc,mBAAO,CAAC,sFAAc;AACpC,iBAAiB,mBAAO,CAAC,4FAAiB;AAC1C,eAAe,mBAAO,CAAC,wFAAe;AACtC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,mBAAmB;AACnB,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C,0BAA0B;AAC1B,sBAAsB,mBAAO,CAAC,sGAAsB;AACpD,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,UAAU,mBAAO,CAAC,8EAAU;AAC5B,qBAAqB;AACrB,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,kBAAkB;AAClB,gBAAgB;AAChB,gBAAgB;AAChB,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,yBAAyB;AACzB,qBAAqB;AACrB,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD,2BAA2B;AAC3B,0BAA0B;AAC1B,2BAA2B;AAC3B,6BAA6B;AAC7B,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA,8BAA8B,gCAAgC;AAC9D;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxEA;AACA,OAAO,6CAA6C;AACpD,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,sCAAsC;AACrE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACXA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sCAAsC;AACzD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,mCAAmC;AAC5D;AACA;AACA;;AAEA;;AAEA;AACA,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,yCAAyC;AAC/E;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC3GA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kEAAkE;AACrF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;ACvCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;AACnD,OAAO,SAAS,GAAG,mBAAO,CAAC,6FAAgB;;AAE3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,OAAO,SAAS,GAAG,mBAAO,CAAC,6FAAgB;AAC3C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;;AAE7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,OAAO,sCAAsC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA;AACA,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC,2BAA2B,oBAAoB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTnE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,mBAAmB,mBAAO,CAAC,8FAAgB;;AAE3C,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,mBAAmB,mBAAO,CAAC,8FAAgB;;AAE3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACXA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;;AAEA;AACA,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,kFAAkF;AACzF,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oCAAoC;AACnE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,gDAAgD,8BAA8B;AAC9E;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,+CAA+C;AACzE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oCAAoC;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC/BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA;AACA,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACpCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,iBAAiB;AAC5B;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,qBAAqB;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,OAAO,mCAAmC,GAAG,mBAAO,CAAC,4FAAgB;;AAErE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D,2BAA2B,iCAAiC,IAAI,gBAAgB;;;;;;;;;;;;;;ACThF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/DA;AACA;;AAEA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,+CAA+C;AACtD,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,+CAA+C;AAC9E,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC/CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,6DAA6D;AACvF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;AClCA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA;AACA,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,wCAAwC;AAC/C,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,wCAAwC;AACvE,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrGA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;;AAEhD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,QAAQ;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA,mBAAmB,2BAA2B;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,2BAA2B,sBAAsB;AACjD,iCAAiC,uCAAuC;AACxE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;;AAEA,mBAAmB,2BAA2B;AAC9C,kCAAkC,2BAA2B,IAAI,gBAAgB;AACjF;;;;;;;;;;;;;;ACPA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;AAChD,OAAO,qBAAqB,GAAG,mBAAO,CAAC,sGAA8B;;AAErE;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;;AAEA,iBAAiB,oBAAoB;AACrC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iDAAiD,sBAAsB;AACvE,iCAAiC,oDAAoD;;AAErF;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,iDAAiD;AAChE,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,OAAO,QAAQ,GAAG,mBAAO,CAAC,sGAA8B;AACxD,eAAe,mBAAO,CAAC,0GAAgC;AACvD,OAAO,cAAc,GAAG,mBAAO,CAAC,4FAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kCAAkC,OAAO;AACzC,gBAAgB,QAAQ;AACxB,mBAAmB,QAAQ;AAC3B,+CAA+C;AAC/C,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,2BAA2B,sDAAsD;AACjF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA,8BAA8B,sDAAsD;AACpF;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtHA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,oEAAgB;;AAE5B,OAAO,uBAAuB,GAAG,mBAAO,CAAC,gEAAoB;AAC7D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1DA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY,8BAA8B,YAAY,IAAI,gBAAgB;;;;;;;;;;;;;;ACV7F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,oGAAgB;AACnD,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,8EAAe;;AAEzD;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,+FAAe;;AAEzC,mBAAmB,YAAY,OAAO,eAAe,YAAY,kBAAkB;;;;;;;;;;;;;;ACFnF,OAAO,mCAAmC,GAAG,mBAAO,CAAC,iGAAgB;;AAErE;AACA;AACA;AACA;;;;;;;;;;;;;;ACLA;AACA,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,6BAA6B;AAC7D;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE,2BAA2B,mDAAmD,IAAI,gBAAgB;;;;;;;;;;;;;;ACblG,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA,OAAO,8DAA8D;AACrE,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C;AACA,wBAAwB,8DAA8D;AACtF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACdA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,8BAA8B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,aAAa;AACxB,gDAAgD,YAAY;AAC5D,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnCA;AACA,WAAW,mBAAO,CAAC,6EAAW;AAC9B,YAAY,mBAAO,CAAC,+EAAY;AAChC;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,yEAAyE;AAC5F;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACVD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,kFAAW;AAC9B,YAAY,mBAAO,CAAC,oFAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,cAAc,OAAO,EAAE,EAAE,GAAG,cAAc;AAC1C;AACA;;AAEA;AACA;;AAEA,yBAAyB,+BAA+B;AACxD,6DAA6D,sBAAsB;AACnF;AACA;AACA,aAAa,UAAU,EAAE,IAAI;AAC7B;;AAEA,wBAAwB,QAAQ,GAAG,UAAU,cAAc,uBAAuB,EAAE,IAAI,EAAE,UAAU,EAAE,UAAU;;AAEhH;AACA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC7DA;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,4EAAW;AAC9B,YAAY,mBAAO,CAAC,8EAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC3BD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,eAAe;AAClC;AACA,CAAC;;;;;;;;;;;;;;ACJD,+IAAoD;;;;;;;;;;;;;;ACApD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,qBAAqB;AACxC;AACA,CAAC;;;;;;;;;;;;;;ACrBD,qCAAqC,2BAA2B;;AAEhE,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,gCAAgC,+BAA+B,KAAK;;AAEpE,YAAY;AACZ,GAAG;AACH;;;;;;;;;;;;;;ACtBA;AACA;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACdA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,iCAAiC,GAAG,mBAAO,CAAC,uDAAW;;AAE9D,mBAAmB,aAAoB;AACvC,mCAAmC,mBAAO,CAAC,0EAAiB,IAAI,mBAAO,CAAC,gEAAY;;AAEpF;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS,4CAA4C;;AAErD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,wDAAwD,wBAAwB;AAChF;AACA,OAAO;AACP;;AAEA;AACA;;AAEA,2BAA2B;AAC3B;AACA,oCAAoC;AACpC;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACbA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,oBAAoB;AAC/B,aAAa;AACb;AACA;AACA;AACA;AACA,gBAAgB,gBAAgB;AAChC;;AAEA;AACA,mEAAmE;AACnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,6BAA6B,iDAAiD;;AAE9E,0DAA0D,gBAAgB;AAC1E;AACA;AACA;AACA,eAAe,UAAU;AACzB;AACA,OAAO;AACP;AACA,eAAe,SAAS;AACxB;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;AC1DA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB,KAAK;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,gBAAgB;AAC3B,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACXA,OAAO,SAAS,GAAG,mBAAO,CAAC,kBAAM;AACjC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,uDAAW;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,eAAe,8BAA8B,KAAK;AAClD;AACA,kEAAkE,eAAe;AACjF;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,eAAe,KAAK,YAAY;AAC9D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,EAAE;AACf,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,0BAA0B;AACvC,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;;;;;;;;;;;;;;AC7UA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,+BAA+B,OAAO;AACtC;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA,GAAG;;;;;;;;;;;;;;ACHH,OAAO,OAAO;AACd;AACA,yCAAyC,gCAAgC,KAAK;;;;;;;;;;;;;;ACF9E,cAAc,mBAAO,CAAC,0DAAS;AAC/B,OAAO,iBAAiB,GAAG,mBAAO,CAAC,uDAAW;;AAE9C;AACA;AACA,GAAG,iFAAiF;AACpF;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;AC5CA;;AAEA,oCAAoC,SAAS,GAAG,KAAK,EAAE,uBAAuB;;;;;;;;;;;;;;;ACFjE;;AAEb,wFAAiC;;;;;;;;;;;;;;;ACFpB;;AAEb,WAAW,mBAAO,CAAC,4CAAU;;AAE7B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qDAAqD,cAAc;AACnE;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACpNa;;AAEb,cAAc,mBAAO,CAAC,qDAAW;;AAEjC;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;;ACZa;;AAEb;;AAEA,cAAc,mBAAO,CAAC,qDAAW;;AAEjC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kCAAkC,sCAAsC;AACxE;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;;;;;;;;;;;;;;;ACtHa;;AAEb,cAAc,mBAAO,CAAC,qDAAW;;AAEjC;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;;;ACfa;;AAEb,2FAAqC;AACrC,mBAAO,CAAC,qDAAW;AACnB,mBAAO,CAAC,2DAAc;AACtB,mBAAO,CAAC,yEAAqB;AAC7B,mBAAO,CAAC,2EAAsB;AAC9B,mBAAO,CAAC,mEAAkB;;;;;;;;;;;;;;;ACPb;;AAEb;AACA;;AAEA,cAAc,mBAAO,CAAC,qDAAW;AACjC,WAAW,mBAAO,CAAC,yCAAM;;AAEzB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA,wBAAwB;AACxB,aAAa,SAAS,EAAE,OAAO,SAAS,EAAE;AAC1C,IAAI;AACJ;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;AACA;AACA,+CAA+C;AAC/C,qBAAqB;AACrB,2CAA2C;AAC3C;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,QAAQ,SAAS;AACjB,MAAM,EAAE;AACR,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,cAAc;AAC/B;AACA;AACA;AACA,+CAA+C;AAC/C,qBAAqB;AACrB,cAAc;AACd,sCAAsC;AACtC,+CAA+C;AAC/C,4CAA4C;AAC5C,oBAAoB,sBAAsB,OAAO;AACjD,4BAA4B;AAC5B,MAAM;AACN,MAAM;AACN,2CAA2C;AAC3C,iCAAiC;AACjC,aAAa;AACb,yBAAyB;AACzB;AACA;AACA;AACA,6FAA6F;AAC7F,eAAe;AACf;AACA,KAAK;AACL;AACA,0BAA0B;AAC1B,gCAAgC;AAChC,MAAM;;AAEN;AACA;AACA;AACA,QAAQ,SAAS;AACjB,MAAM,EAAE;AACR,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;;ACjIa;;AAEb,cAAc,mBAAO,CAAC,qDAAW;;AAEjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA;AACA;AACA,SAAS,mBAAO,CAAC,6EAAa;AAC9B,iBAAiB,mBAAO,CAAC,6FAAqB;AAC9C,eAAe,mBAAO,CAAC,qGAAyB;AAChD,qBAAqB,mBAAO,CAAC,qGAAyB;AACtD,qBAAqB,mBAAO,CAAC,qGAAyB;AACtD,UAAU,mBAAO,CAAC,+EAAc;AAChC,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,mGAAwB;AAC1C,aAAa,mBAAO,CAAC,yGAA2B;AAChD,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,+GAA8B;AAChD,cAAc,mBAAO,CAAC,uHAAkC;AACxD,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,uHAAkC;AACpD,cAAc,mBAAO,CAAC,+HAAsC;AAC5D,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,yHAAmC;AACrD,aAAa,mBAAO,CAAC,+HAAsC;AAC3D,EAAE;AACF,E;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACjCA,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;;AAEA;AACA,gEAAgE,8GAAkC;AAClG;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,6B;;;;;;;;;;;;;ACjBA,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;ACnBA,uBAAuB,+GAAkC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,uB;;;;;;;;;;;;;AChDA,mBAAmB,mBAAO,CAAC,mFAAc;AACzC,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,oBAAoB,mBAAO,CAAC,qFAAe;AAC3C,0BAA0B,mBAAO,CAAC,iGAAqB;AACvD,0BAA0B,mBAAO,CAAC,iGAAqB;AACvD,2BAA2B,mBAAO,CAAC,mGAAsB;AACzD,yBAAyB,mBAAO,CAAC,+FAAoB;AACrD,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,4BAA4B,oHAAuC;;AAEnE;AACA,uBAAuB,mBAAO,CAAC,+FAAoB;AACnD,wBAAwB,mBAAO,CAAC,iGAAqB;AACrD,6BAA6B,mBAAO,CAAC,2GAA0B;AAC/D,gCAAgC,mBAAO,CAAC,mHAA8B;AACtE,wBAAwB,mBAAO,CAAC,iGAAqB;AACrD,kCAAkC,mBAAO,CAAC,qHAA+B;AACzE,2BAA2B,mBAAO,CAAC,yGAAyB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;AAGA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,+B;;;;;;;;;;;;;ACxMA,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,oC;;;;;;;;;;;;;ACrBA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;AC3FA,iBAAiB,mBAAO,CAAC,+EAAY;AACrC,cAAc,mBAAO,CAAC,gEAAiB;AACvC,mBAAmB,mBAAO,CAAC,wDAAa;AACxC,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,qBAAqB,sCAAsC;AAC3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,4B;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;ACfA,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;ACrBA;AACA;AACA;AACA;AACA;AACA,qCAAqC;AACrC;;AAEA;AACA;AACA;;AAEA,yB;;;;;;;;;;;;;ACZA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0B;;;;;;;;;;;;;ACRA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,mC;;;;;;;;;;;;;ACZA,gBAAgB,mBAAO,CAAC,gDAAS;AACjC,iCAAiC,yHAA4C;AAC7E,0BAA0B,mBAAO,CAAC,iGAAqB;;AAEvD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;AC1BA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,mC;;;;;;;;;;;;;ACZA,gBAAgB,mBAAO,CAAC,gDAAS;AACjC,eAAe,mBAAO,CAAC,2EAAU;;AAEjC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL,GAAG;AACH;AACA;;AAEA,kC;;;;;;;;;;;;;ACzDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;AChJA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,eAAe,mBAAO,CAAC,4EAAW;AAClC,kBAAkB,mBAAO,CAAC,sGAAa;AACvC,gBAAgB,mBAAO,CAAC,gDAAS;AACjC,uBAAuB,mBAAO,CAAC,sGAAwB;AACvD,6BAA6B,+IAAqD;;AAElF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC1CA,iCAAiC,0HAA6C;AAC9E;AACA;AACA;AACA;AACA;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACpFA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,mBAAmB,mBAAO,CAAC,mGAAc;AACzC,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C;AAC3C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACxDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4B;;;;;;;;;;;;;ACZA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,mBAAmB,mBAAO,CAAC,uGAAc;AACzC,gBAAgB,mBAAO,CAAC,gDAAS;;AAEjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,mEAAmE;AACnE,mEAAmE;AACnE,wEAAwE;AACxE,0DAA0D;AAC1D,wDAAwD;AACxD,wDAAwD;AACxD,6DAA6D;AAC7D;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC5DA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC3BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4B;;;;;;;;;;;;;ACdA,kBAAkB,mBAAO,CAAC,sGAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yB;;;;;;;;;;;;;AChBA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,gDAAS;AACjC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,eAAe,mBAAO,CAAC,sFAAU;;AAEjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC1CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wFAAW;;AAEnC;AACA;AACA;AACA;AACA,cAAc,gBAAgB;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACpBA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,uBAAuB,mBAAO,CAAC,sGAAwB;AACvD,6BAA6B,wIAA8C;AAC3E,OAAO,qBAAqB,GAAG,mBAAO,CAAC,+EAAc;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACtCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;AC7EA,kBAAkB,mBAAO,CAAC,2FAAa;AACvC,eAAe,mBAAO,CAAC,qFAAU;AACjC,cAAc,mBAAO,CAAC,0EAAU;AAChC,6BAA6B,sHAAyC;AACtE,kBAAkB,mBAAO,CAAC,4FAAmB;AAC7C,6BAA6B,oIAA0C;;AAEvE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACxCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACvBA,eAAe,mBAAO,CAAC,sFAAU;AACjC,eAAe,mBAAO,CAAC,sFAAU;AACjC,cAAc,mBAAO,CAAC,0EAAU;AAChC,6BAA6B,sHAAyC;AACtE,kBAAkB,mBAAO,CAAC,4FAAmB;AAC7C,6BAA6B,qIAA2C;;AAExE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACtCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACfA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA,wB;;;;;;;;;;;;;AC5CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;ACrCA,sBAAsB,mBAAO,CAAC,0FAAkB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,kFAAc;;AAExC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACVA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,gBAAgB,mBAAO,CAAC,gDAAS;AACjC,eAAe,mBAAO,CAAC,4EAAW;AAClC,uBAAuB,mBAAO,CAAC,sGAAwB;;AAEvD;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,IAAI;AACJ;AACA,EAAE;;AAEF;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;;;;;;;;;;;;;;ACvCA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA,EAAE;AACF;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC,E;;;;;;;;;;;;;ACnCD,mC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,yC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,+B;;;;;;;;;;;;;ACAA,iC;;;;;;;;;;;;;ACAA,kC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,iC;;;;;;;;;;;;;ACAA,iC;;;;;;UCAA;UACA;;UAEA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;;UAEA;UACA;;UAEA;UACA;UACA;;UAEA;UACA;;;;;WCxBA;WACA;WACA;WACA;WACA;WACA,gCAAgC,YAAY;WAC5C;WACA,E;;;;;WCPA;WACA;WACA;WACA;WACA,wCAAwC,yCAAyC;WACjF;WACA;WACA,E;;;;;WCPA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF,E;;;;;WCRA;WACA;WACA;WACA;WACA,E;;;;;WCJA,sF;;;;;WCAA;WACA;WACA;WACA,sDAAsD,kBAAkB;WACxE;WACA,+CAA+C,cAAc;WAC7D,E;;;;;WCNA,sD;;;;;WCAA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,sGAAsG;WACtG;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,GAAG,aAAa,kBAAkB;WAClC;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,E;;;;;WCvCA;WACA;WACA,WAAW,6BAA6B,iBAAiB,GAAG,qEAAqE;WACjI;WACA;WACA;WACA,qCAAqC,aAAa,EAAE,wDAAwD,2BAA2B,4BAA4B,2BAA2B,+CAA+C,mCAAmC;WAChR;WACA;WACA;WACA,+BAA+B,eAAe,oBAAoB,sDAAsD,gBAAgB,eAAe,KAAK,6DAA6D,SAAS,SAAS,QAAQ,eAAe,KAAK,eAAe,qGAAqG,WAAW,aAAa;WACnZ;WACA;WACA;WACA,gBAAgB,8BAA8B,qBAAqB,YAAY,sBAAsB,SAAS,iDAAiD,6FAA6F,WAAW,uBAAuB,2BAA2B,wBAAwB,KAAK,oCAAoC,oBAAoB,wBAAwB,oBAAoB,SAAS,KAAK,yBAAyB,KAAK,gCAAgC,yBAAyB,QAAQ,eAAe,KAAK,eAAe,4DAA4D;WACtoB;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA,CAAC;WACD;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,KAAK;WACL,IAAI,WAAW,YAAY;WAC3B,GAAG;WACH;WACA,C;;;;;;WC9JA,OAAO,UAAU;WACjB;WACA;WACA;;WAEA,6BAA6B,cAAc;;WAE3C;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,qBAAqB,MAAM,EAAE,KAAK,WAAW,QAAQ,MAAM,OAAO;WAClE;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA;WACA,uBAAuB,MAAM,EAAE,KAAK,YAAY,IAAI;WACpD;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA,OAAO;WACP,GAAG;WACH;;WAEA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA;WACA,SAAS;WACT;WACA;WACA;WACA,OAAO;WACP,KAAK;WACL;WACA;WACA,KAAK;WACL;WACA,GAAG;WACH;;WAEA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,eAAe,qBAAqB;WACpC;WACA;WACA;WACA;WACA,WAAW,sBAAsB;WACjC;WACA;;WAEA;WACA,gEAAgE;;WAEhE;WACA,+BAA+B;WAC/B;WACA;WACA;WACA,GAAG;WACH,aAAa;WACb;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,2FAA2F,kBAAkB;WAC7G;WACA,OAAO;WACP;WACA,OAAO;WACP,KAAK;WACL;WACA,IAAI;WACJ;WACA;WACA;;WAEA;;WAEA;;WAEA,kB;;;;UCzIA;UACA;UACA;UACA","file":"remoteEntry.js","sourcesContent":["\"use strict\";\n\nvar rawAsap = require(\"./raw\");\nvar freeTasks = [];\n\n/**\n * Calls a task as soon as possible after returning, in its own event, with\n * priority over IO events. An exception thrown in a task can be handled by\n * `process.on(\"uncaughtException\") or `domain.on(\"error\")`, but will otherwise\n * crash the process. If the error is handled, all subsequent tasks will\n * resume.\n *\n * @param {{call}} task A callable object, typically a function that takes no\n * arguments.\n */\nmodule.exports = asap;\nfunction asap(task) {\n    var rawTask;\n    if (freeTasks.length) {\n        rawTask = freeTasks.pop();\n    } else {\n        rawTask = new RawTask();\n    }\n    rawTask.task = task;\n    rawTask.domain = process.domain;\n    rawAsap(rawTask);\n}\n\nfunction RawTask() {\n    this.task = null;\n    this.domain = null;\n}\n\nRawTask.prototype.call = function () {\n    if (this.domain) {\n        this.domain.enter();\n    }\n    var threw = true;\n    try {\n        this.task.call();\n        threw = false;\n        // If the task throws an exception (presumably) Node.js restores the\n        // domain stack for the next event.\n        if (this.domain) {\n            this.domain.exit();\n        }\n    } finally {\n        // We use try/finally and a threw flag to avoid messing up stack traces\n        // when we catch and release errors.\n        if (threw) {\n            // In Node.js, uncaught exceptions are considered fatal errors.\n            // Re-throw them to interrupt flushing!\n            // Ensure that flushing continues if an uncaught exception is\n            // suppressed listening process.on(\"uncaughtException\") or\n            // domain.on(\"error\").\n            rawAsap.requestFlush();\n        }\n        // If the task threw an error, we do not want to exit the domain here.\n        // Exiting the domain would prevent the domain from catching the error.\n        this.task = null;\n        this.domain = null;\n        freeTasks.push(this);\n    }\n};\n\n","\"use strict\";\n\nvar domain; // The domain module is executed on demand\nvar hasSetImmediate = typeof setImmediate === \"function\";\n\n// Use the fastest means possible to execute a task in its own turn, with\n// priority over other events including network IO events in Node.js.\n//\n// An exception thrown by a task will permanently interrupt the processing of\n// subsequent tasks. The higher level `asap` function ensures that if an\n// exception is thrown by a task, that the task queue will continue flushing as\n// soon as possible, but if you use `rawAsap` directly, you are responsible to\n// either ensure that no exceptions are thrown from your task, or to manually\n// call `rawAsap.requestFlush` if an exception is thrown.\nmodule.exports = rawAsap;\nfunction rawAsap(task) {\n    if (!queue.length) {\n        requestFlush();\n        flushing = true;\n    }\n    // Avoids a function call\n    queue[queue.length] = task;\n}\n\nvar queue = [];\n// Once a flush has been requested, no further calls to `requestFlush` are\n// necessary until the next `flush` completes.\nvar flushing = false;\n// The position of the next task to execute in the task queue. This is\n// preserved between calls to `flush` so that it can be resumed if\n// a task throws an exception.\nvar index = 0;\n// If a task schedules additional tasks recursively, the task queue can grow\n// unbounded. To prevent memory excaustion, the task queue will periodically\n// truncate already-completed tasks.\nvar capacity = 1024;\n\n// The flush function processes all tasks that have been scheduled with\n// `rawAsap` unless and until one of those tasks throws an exception.\n// If a task throws an exception, `flush` ensures that its state will remain\n// consistent and will resume where it left off when called again.\n// However, `flush` does not make any arrangements to be called again if an\n// exception is thrown.\nfunction flush() {\n    while (index < queue.length) {\n        var currentIndex = index;\n        // Advance the index before calling the task. This ensures that we will\n        // begin flushing on the next task the task throws an error.\n        index = index + 1;\n        queue[currentIndex].call();\n        // Prevent leaking memory for long chains of recursive calls to `asap`.\n        // If we call `asap` within tasks scheduled by `asap`, the queue will\n        // grow, but to avoid an O(n) walk for every task we execute, we don't\n        // shift tasks off the queue after they have been executed.\n        // Instead, we periodically shift 1024 tasks off the queue.\n        if (index > capacity) {\n            // Manually shift all values starting at the index back to the\n            // beginning of the queue.\n            for (var scan = 0, newLength = queue.length - index; scan < newLength; scan++) {\n                queue[scan] = queue[scan + index];\n            }\n            queue.length -= index;\n            index = 0;\n        }\n    }\n    queue.length = 0;\n    index = 0;\n    flushing = false;\n}\n\nrawAsap.requestFlush = requestFlush;\nfunction requestFlush() {\n    // Ensure flushing is not bound to any domain.\n    // It is not sufficient to exit the domain, because domains exist on a stack.\n    // To execute code outside of any domain, the following dance is necessary.\n    var parentDomain = process.domain;\n    if (parentDomain) {\n        if (!domain) {\n            // Lazy execute the domain module.\n            // Only employed if the user elects to use domains.\n            domain = require(\"domain\");\n        }\n        domain.active = process.domain = null;\n    }\n\n    // `setImmediate` is slower that `process.nextTick`, but `process.nextTick`\n    // cannot handle recursion.\n    // `requestFlush` will only be called recursively from `asap.js`, to resume\n    // flushing after an error is thrown into a domain.\n    // Conveniently, `setImmediate` was introduced in the same version\n    // `process.nextTick` started throwing recursion errors.\n    if (flushing && hasSetImmediate) {\n        setImmediate(flush);\n    } else {\n        process.nextTick(flush);\n    }\n\n    if (parentDomain) {\n        domain.active = process.domain = parentDomain;\n    }\n}\n","module.exports = require('./lib/axios');","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar buildURL = require('./../helpers/buildURL');\nvar http = require('http');\nvar https = require('https');\nvar httpFollow = require('follow-redirects').http;\nvar httpsFollow = require('follow-redirects').https;\nvar url = require('url');\nvar zlib = require('zlib');\nvar pkg = require('./../../package.json');\nvar createError = require('../core/createError');\nvar enhanceError = require('../core/enhanceError');\n\nvar isHttps = /https:?/;\n\n/*eslint consistent-return:0*/\nmodule.exports = function httpAdapter(config) {\n  return new Promise(function dispatchHttpRequest(resolve, reject) {\n    var data = config.data;\n    var headers = config.headers;\n    var timer;\n    var aborted = false;\n\n    // Set User-Agent (required by some servers)\n    // Only set header if it hasn't been set in config\n    // See https://github.com/mzabriskie/axios/issues/69\n    if (!headers['User-Agent'] && !headers['user-agent']) {\n      headers['User-Agent'] = 'axios/' + pkg.version;\n    }\n\n    if (data && !utils.isStream(data)) {\n      if (Buffer.isBuffer(data)) {\n        // Nothing to do...\n      } else if (utils.isArrayBuffer(data)) {\n        data = new Buffer(new Uint8Array(data));\n      } else if (utils.isString(data)) {\n        data = new Buffer(data, 'utf-8');\n      } else {\n        return reject(createError(\n          'Data after transformation must be a string, an ArrayBuffer, a Buffer, or a Stream',\n          config\n        ));\n      }\n\n      // Add Content-Length header if data exists\n      headers['Content-Length'] = data.length;\n    }\n\n    // HTTP basic authentication\n    var auth = undefined;\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password || '';\n      auth = username + ':' + password;\n    }\n\n    // Parse url\n    var parsed = url.parse(config.url);\n    var protocol = parsed.protocol || 'http:';\n\n    if (!auth && parsed.auth) {\n      var urlAuth = parsed.auth.split(':');\n      var urlUsername = urlAuth[0] || '';\n      var urlPassword = urlAuth[1] || '';\n      auth = urlUsername + ':' + urlPassword;\n    }\n\n    if (auth) {\n      delete headers.Authorization;\n    }\n\n    var isHttpsRequest = isHttps.test(protocol);\n    var agent = isHttpsRequest ? config.httpsAgent : config.httpAgent;\n\n    var options = {\n      hostname: parsed.hostname,\n      port: parsed.port,\n      path: buildURL(parsed.path, config.params, config.paramsSerializer).replace(/^\\?/, ''),\n      method: config.method,\n      headers: headers,\n      agent: agent,\n      auth: auth\n    };\n\n    var proxy = config.proxy;\n    if (!proxy) {\n      var proxyEnv = protocol.slice(0, -1) + '_proxy';\n      var proxyUrl = process.env[proxyEnv] || process.env[proxyEnv.toUpperCase()];\n      if (proxyUrl) {\n        var parsedProxyUrl = url.parse(proxyUrl);\n        proxy = {\n          host: parsedProxyUrl.hostname,\n          port: parsedProxyUrl.port\n        };\n\n        if (parsedProxyUrl.auth) {\n          var proxyUrlAuth = parsedProxyUrl.auth.split(':');\n          proxy.auth = {\n            username: proxyUrlAuth[0],\n            password: proxyUrlAuth[1]\n          };\n        }\n      }\n    }\n\n    if (proxy) {\n      options.hostname = proxy.host;\n      options.host = proxy.host;\n      options.headers.host = parsed.hostname + (parsed.port ? ':' + parsed.port : '');\n      options.port = proxy.port;\n      options.path = protocol + '//' + parsed.hostname + (parsed.port ? ':' + parsed.port : '') + options.path;\n\n      // Basic proxy authorization\n      if (proxy.auth) {\n        var base64 = new Buffer(proxy.auth.username + ':' + proxy.auth.password, 'utf8').toString('base64');\n        options.headers['Proxy-Authorization'] = 'Basic ' + base64;\n      }\n    }\n\n    var transport;\n    var isHttpsProxy = isHttpsRequest && (proxy ? isHttps.test(proxy.protocol) : true);\n    if (config.maxRedirects === 0) {\n      transport = isHttpsProxy ? https : http;\n    } else {\n      if (config.maxRedirects) {\n        options.maxRedirects = config.maxRedirects;\n      }\n      transport = isHttpsProxy ? httpsFollow : httpFollow;\n    }\n\n    // Create the request\n    var req = transport.request(options, function handleResponse(res) {\n      if (aborted) return;\n\n      // Response has been received so kill timer that handles request timeout\n      clearTimeout(timer);\n      timer = null;\n\n      // uncompress the response body transparently if required\n      var stream = res;\n      switch (res.headers['content-encoding']) {\n      /*eslint default-case:0*/\n      case 'gzip':\n      case 'compress':\n      case 'deflate':\n        // add the unzipper to the body stream processing pipeline\n        stream = stream.pipe(zlib.createUnzip());\n\n        // remove the content-encoding in order to not confuse downstream operations\n        delete res.headers['content-encoding'];\n        break;\n      }\n\n      // return the last request in case of redirects\n      var lastRequest = res.req || req;\n\n      var response = {\n        status: res.statusCode,\n        statusText: res.statusMessage,\n        headers: res.headers,\n        config: config,\n        request: lastRequest\n      };\n\n      if (config.responseType === 'stream') {\n        response.data = stream;\n        settle(resolve, reject, response);\n      } else {\n        var responseBuffer = [];\n        stream.on('data', function handleStreamData(chunk) {\n          responseBuffer.push(chunk);\n\n          // make sure the content length is not over the maxContentLength if specified\n          if (config.maxContentLength > -1 && Buffer.concat(responseBuffer).length > config.maxContentLength) {\n            reject(createError('maxContentLength size of ' + config.maxContentLength + ' exceeded',\n              config, null, lastRequest));\n          }\n        });\n\n        stream.on('error', function handleStreamError(err) {\n          if (aborted) return;\n          reject(enhanceError(err, config, null, lastRequest));\n        });\n\n        stream.on('end', function handleStreamEnd() {\n          var responseData = Buffer.concat(responseBuffer);\n          if (config.responseType !== 'arraybuffer') {\n            responseData = responseData.toString('utf8');\n          }\n\n          response.data = responseData;\n          settle(resolve, reject, response);\n        });\n      }\n    });\n\n    // Handle errors\n    req.on('error', function handleRequestError(err) {\n      if (aborted) return;\n      reject(enhanceError(err, config, null, req));\n    });\n\n    // Handle request timeout\n    if (config.timeout && !timer) {\n      timer = setTimeout(function handleRequestTimeout() {\n        req.abort();\n        reject(createError('timeout of ' + config.timeout + 'ms exceeded', config, 'ECONNABORTED', req));\n        aborted = true;\n      }, config.timeout);\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (aborted) {\n          return;\n        }\n\n        req.abort();\n        reject(cancel);\n        aborted = true;\n      });\n    }\n\n    // Send the request\n    if (utils.isStream(data)) {\n      data.pipe(req);\n    } else {\n      req.end(data);\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar buildURL = require('./../helpers/buildURL');\nvar parseHeaders = require('./../helpers/parseHeaders');\nvar isURLSameOrigin = require('./../helpers/isURLSameOrigin');\nvar createError = require('../core/createError');\nvar btoa = (typeof window !== 'undefined' && window.btoa && window.btoa.bind(window)) || require('./../helpers/btoa');\n\nmodule.exports = function xhrAdapter(config) {\n  return new Promise(function dispatchXhrRequest(resolve, reject) {\n    var requestData = config.data;\n    var requestHeaders = config.headers;\n\n    if (utils.isFormData(requestData)) {\n      delete requestHeaders['Content-Type']; // Let the browser set it\n    }\n\n    var request = new XMLHttpRequest();\n    var loadEvent = 'onreadystatechange';\n    var xDomain = false;\n\n    // For IE 8/9 CORS support\n    // Only supports POST and GET calls and doesn't returns the response headers.\n    // DON'T do this for testing b/c XMLHttpRequest is mocked, not XDomainRequest.\n    if (process.env.NODE_ENV !== 'test' &&\n        typeof window !== 'undefined' &&\n        window.XDomainRequest && !('withCredentials' in request) &&\n        !isURLSameOrigin(config.url)) {\n      request = new window.XDomainRequest();\n      loadEvent = 'onload';\n      xDomain = true;\n      request.onprogress = function handleProgress() {};\n      request.ontimeout = function handleTimeout() {};\n    }\n\n    // HTTP basic authentication\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password || '';\n      requestHeaders.Authorization = 'Basic ' + btoa(username + ':' + password);\n    }\n\n    request.open(config.method.toUpperCase(), buildURL(config.url, config.params, config.paramsSerializer), true);\n\n    // Set the request timeout in MS\n    request.timeout = config.timeout;\n\n    // Listen for ready state\n    request[loadEvent] = function handleLoad() {\n      if (!request || (request.readyState !== 4 && !xDomain)) {\n        return;\n      }\n\n      // The request errored out and we didn't get a response, this will be\n      // handled by onerror instead\n      // With one exception: request that using file: protocol, most browsers\n      // will return status as 0 even though it's a successful request\n      if (request.status === 0 && !(request.responseURL && request.responseURL.indexOf('file:') === 0)) {\n        return;\n      }\n\n      // Prepare the response\n      var responseHeaders = 'getAllResponseHeaders' in request ? parseHeaders(request.getAllResponseHeaders()) : null;\n      var responseData = !config.responseType || config.responseType === 'text' ? request.responseText : request.response;\n      var response = {\n        data: responseData,\n        // IE sends 1223 instead of 204 (https://github.com/mzabriskie/axios/issues/201)\n        status: request.status === 1223 ? 204 : request.status,\n        statusText: request.status === 1223 ? 'No Content' : request.statusText,\n        headers: responseHeaders,\n        config: config,\n        request: request\n      };\n\n      settle(resolve, reject, response);\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle low level network errors\n    request.onerror = function handleError() {\n      // Real errors are hidden from us by the browser\n      // onerror should only fire if it's a network error\n      reject(createError('Network Error', config, null, request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle timeout\n    request.ontimeout = function handleTimeout() {\n      reject(createError('timeout of ' + config.timeout + 'ms exceeded', config, 'ECONNABORTED',\n        request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Add xsrf header\n    // This is only done if running in a standard browser environment.\n    // Specifically not if we're in a web worker, or react-native.\n    if (utils.isStandardBrowserEnv()) {\n      var cookies = require('./../helpers/cookies');\n\n      // Add xsrf header\n      var xsrfValue = (config.withCredentials || isURLSameOrigin(config.url)) && config.xsrfCookieName ?\n          cookies.read(config.xsrfCookieName) :\n          undefined;\n\n      if (xsrfValue) {\n        requestHeaders[config.xsrfHeaderName] = xsrfValue;\n      }\n    }\n\n    // Add headers to the request\n    if ('setRequestHeader' in request) {\n      utils.forEach(requestHeaders, function setRequestHeader(val, key) {\n        if (typeof requestData === 'undefined' && key.toLowerCase() === 'content-type') {\n          // Remove Content-Type if data is undefined\n          delete requestHeaders[key];\n        } else {\n          // Otherwise add header to the request\n          request.setRequestHeader(key, val);\n        }\n      });\n    }\n\n    // Add withCredentials to request if needed\n    if (config.withCredentials) {\n      request.withCredentials = true;\n    }\n\n    // Add responseType to request if needed\n    if (config.responseType) {\n      try {\n        request.responseType = config.responseType;\n      } catch (e) {\n        // Expected DOMException thrown by browsers not compatible XMLHttpRequest Level 2.\n        // But, this can be suppressed for 'json' type as it can be parsed by default 'transformResponse' function.\n        if (config.responseType !== 'json') {\n          throw e;\n        }\n      }\n    }\n\n    // Handle progress if needed\n    if (typeof config.onDownloadProgress === 'function') {\n      request.addEventListener('progress', config.onDownloadProgress);\n    }\n\n    // Not all browsers support upload events\n    if (typeof config.onUploadProgress === 'function' && request.upload) {\n      request.upload.addEventListener('progress', config.onUploadProgress);\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (!request) {\n          return;\n        }\n\n        request.abort();\n        reject(cancel);\n        // Clean up request\n        request = null;\n      });\n    }\n\n    if (requestData === undefined) {\n      requestData = null;\n    }\n\n    // Send the request\n    request.send(requestData);\n  });\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar bind = require('./helpers/bind');\nvar Axios = require('./core/Axios');\nvar defaults = require('./defaults');\n\n/**\n * Create an instance of Axios\n *\n * @param {Object} defaultConfig The default config for the instance\n * @return {Axios} A new instance of Axios\n */\nfunction createInstance(defaultConfig) {\n  var context = new Axios(defaultConfig);\n  var instance = bind(Axios.prototype.request, context);\n\n  // Copy axios.prototype to instance\n  utils.extend(instance, Axios.prototype, context);\n\n  // Copy context to instance\n  utils.extend(instance, context);\n\n  return instance;\n}\n\n// Create the default instance to be exported\nvar axios = createInstance(defaults);\n\n// Expose Axios class to allow class inheritance\naxios.Axios = Axios;\n\n// Factory for creating new instances\naxios.create = function create(instanceConfig) {\n  return createInstance(utils.merge(defaults, instanceConfig));\n};\n\n// Expose Cancel & CancelToken\naxios.Cancel = require('./cancel/Cancel');\naxios.CancelToken = require('./cancel/CancelToken');\naxios.isCancel = require('./cancel/isCancel');\n\n// Expose all/spread\naxios.all = function all(promises) {\n  return Promise.all(promises);\n};\naxios.spread = require('./helpers/spread');\n\nmodule.exports = axios;\n\n// Allow use of default import syntax in TypeScript\nmodule.exports.default = axios;\n","'use strict';\n\n/**\n * A `Cancel` is an object that is thrown when an operation is canceled.\n *\n * @class\n * @param {string=} message The message.\n */\nfunction Cancel(message) {\n  this.message = message;\n}\n\nCancel.prototype.toString = function toString() {\n  return 'Cancel' + (this.message ? ': ' + this.message : '');\n};\n\nCancel.prototype.__CANCEL__ = true;\n\nmodule.exports = Cancel;\n","'use strict';\n\nvar Cancel = require('./Cancel');\n\n/**\n * A `CancelToken` is an object that can be used to request cancellation of an operation.\n *\n * @class\n * @param {Function} executor The executor function.\n */\nfunction CancelToken(executor) {\n  if (typeof executor !== 'function') {\n    throw new TypeError('executor must be a function.');\n  }\n\n  var resolvePromise;\n  this.promise = new Promise(function promiseExecutor(resolve) {\n    resolvePromise = resolve;\n  });\n\n  var token = this;\n  executor(function cancel(message) {\n    if (token.reason) {\n      // Cancellation has already been requested\n      return;\n    }\n\n    token.reason = new Cancel(message);\n    resolvePromise(token.reason);\n  });\n}\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nCancelToken.prototype.throwIfRequested = function throwIfRequested() {\n  if (this.reason) {\n    throw this.reason;\n  }\n};\n\n/**\n * Returns an object that contains a new `CancelToken` and a function that, when called,\n * cancels the `CancelToken`.\n */\nCancelToken.source = function source() {\n  var cancel;\n  var token = new CancelToken(function executor(c) {\n    cancel = c;\n  });\n  return {\n    token: token,\n    cancel: cancel\n  };\n};\n\nmodule.exports = CancelToken;\n","'use strict';\n\nmodule.exports = function isCancel(value) {\n  return !!(value && value.__CANCEL__);\n};\n","'use strict';\n\nvar defaults = require('./../defaults');\nvar utils = require('./../utils');\nvar InterceptorManager = require('./InterceptorManager');\nvar dispatchRequest = require('./dispatchRequest');\n\n/**\n * Create a new instance of Axios\n *\n * @param {Object} instanceConfig The default config for the instance\n */\nfunction Axios(instanceConfig) {\n  this.defaults = instanceConfig;\n  this.interceptors = {\n    request: new InterceptorManager(),\n    response: new InterceptorManager()\n  };\n}\n\n/**\n * Dispatch a request\n *\n * @param {Object} config The config specific for this request (merged with this.defaults)\n */\nAxios.prototype.request = function request(config) {\n  /*eslint no-param-reassign:0*/\n  // Allow for axios('example/url'[, config]) a la fetch API\n  if (typeof config === 'string') {\n    config = utils.merge({\n      url: arguments[0]\n    }, arguments[1]);\n  }\n\n  config = utils.merge(defaults, this.defaults, { method: 'get' }, config);\n  config.method = config.method.toLowerCase();\n\n  // Hook up interceptors middleware\n  var chain = [dispatchRequest, undefined];\n  var promise = Promise.resolve(config);\n\n  this.interceptors.request.forEach(function unshiftRequestInterceptors(interceptor) {\n    chain.unshift(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  this.interceptors.response.forEach(function pushResponseInterceptors(interceptor) {\n    chain.push(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  while (chain.length) {\n    promise = promise.then(chain.shift(), chain.shift());\n  }\n\n  return promise;\n};\n\n// Provide aliases for supported request methods\nutils.forEach(['delete', 'get', 'head', 'options'], function forEachMethodNoData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, config) {\n    return this.request(utils.merge(config || {}, {\n      method: method,\n      url: url\n    }));\n  };\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, data, config) {\n    return this.request(utils.merge(config || {}, {\n      method: method,\n      url: url,\n      data: data\n    }));\n  };\n});\n\nmodule.exports = Axios;\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction InterceptorManager() {\n  this.handlers = [];\n}\n\n/**\n * Add a new interceptor to the stack\n *\n * @param {Function} fulfilled The function to handle `then` for a `Promise`\n * @param {Function} rejected The function to handle `reject` for a `Promise`\n *\n * @return {Number} An ID used to remove interceptor later\n */\nInterceptorManager.prototype.use = function use(fulfilled, rejected) {\n  this.handlers.push({\n    fulfilled: fulfilled,\n    rejected: rejected\n  });\n  return this.handlers.length - 1;\n};\n\n/**\n * Remove an interceptor from the stack\n *\n * @param {Number} id The ID that was returned by `use`\n */\nInterceptorManager.prototype.eject = function eject(id) {\n  if (this.handlers[id]) {\n    this.handlers[id] = null;\n  }\n};\n\n/**\n * Iterate over all the registered interceptors\n *\n * This method is particularly useful for skipping over any\n * interceptors that may have become `null` calling `eject`.\n *\n * @param {Function} fn The function to call for each interceptor\n */\nInterceptorManager.prototype.forEach = function forEach(fn) {\n  utils.forEach(this.handlers, function forEachHandler(h) {\n    if (h !== null) {\n      fn(h);\n    }\n  });\n};\n\nmodule.exports = InterceptorManager;\n","'use strict';\n\nvar enhanceError = require('./enhanceError');\n\n/**\n * Create an Error with the specified message, config, error code, request and response.\n *\n * @param {string} message The error message.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The created error.\n */\nmodule.exports = function createError(message, config, code, request, response) {\n  var error = new Error(message);\n  return enhanceError(error, config, code, request, response);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar transformData = require('./transformData');\nvar isCancel = require('../cancel/isCancel');\nvar defaults = require('../defaults');\nvar isAbsoluteURL = require('./../helpers/isAbsoluteURL');\nvar combineURLs = require('./../helpers/combineURLs');\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nfunction throwIfCancellationRequested(config) {\n  if (config.cancelToken) {\n    config.cancelToken.throwIfRequested();\n  }\n}\n\n/**\n * Dispatch a request to the server using the configured adapter.\n *\n * @param {object} config The config that is to be used for the request\n * @returns {Promise} The Promise to be fulfilled\n */\nmodule.exports = function dispatchRequest(config) {\n  throwIfCancellationRequested(config);\n\n  // Support baseURL config\n  if (config.baseURL && !isAbsoluteURL(config.url)) {\n    config.url = combineURLs(config.baseURL, config.url);\n  }\n\n  // Ensure headers exist\n  config.headers = config.headers || {};\n\n  // Transform request data\n  config.data = transformData(\n    config.data,\n    config.headers,\n    config.transformRequest\n  );\n\n  // Flatten headers\n  config.headers = utils.merge(\n    config.headers.common || {},\n    config.headers[config.method] || {},\n    config.headers || {}\n  );\n\n  utils.forEach(\n    ['delete', 'get', 'head', 'post', 'put', 'patch', 'common'],\n    function cleanHeaderConfig(method) {\n      delete config.headers[method];\n    }\n  );\n\n  var adapter = config.adapter || defaults.adapter;\n\n  return adapter(config).then(function onAdapterResolution(response) {\n    throwIfCancellationRequested(config);\n\n    // Transform response data\n    response.data = transformData(\n      response.data,\n      response.headers,\n      config.transformResponse\n    );\n\n    return response;\n  }, function onAdapterRejection(reason) {\n    if (!isCancel(reason)) {\n      throwIfCancellationRequested(config);\n\n      // Transform response data\n      if (reason && reason.response) {\n        reason.response.data = transformData(\n          reason.response.data,\n          reason.response.headers,\n          config.transformResponse\n        );\n      }\n    }\n\n    return Promise.reject(reason);\n  });\n};\n","'use strict';\n\n/**\n * Update an Error with the specified config, error code, and response.\n *\n * @param {Error} error The error to update.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The error.\n */\nmodule.exports = function enhanceError(error, config, code, request, response) {\n  error.config = config;\n  if (code) {\n    error.code = code;\n  }\n  error.request = request;\n  error.response = response;\n  return error;\n};\n","'use strict';\n\nvar createError = require('./createError');\n\n/**\n * Resolve or reject a Promise based on response status.\n *\n * @param {Function} resolve A function that resolves the promise.\n * @param {Function} reject A function that rejects the promise.\n * @param {object} response The response.\n */\nmodule.exports = function settle(resolve, reject, response) {\n  var validateStatus = response.config.validateStatus;\n  // Note: status is not exposed by XDomainRequest\n  if (!response.status || !validateStatus || validateStatus(response.status)) {\n    resolve(response);\n  } else {\n    reject(createError(\n      'Request failed with status code ' + response.status,\n      response.config,\n      null,\n      response.request,\n      response\n    ));\n  }\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n/**\n * Transform the data for a request or a response\n *\n * @param {Object|String} data The data to be transformed\n * @param {Array} headers The headers for the request or response\n * @param {Array|Function} fns A single function or Array of functions\n * @returns {*} The resulting transformed data\n */\nmodule.exports = function transformData(data, headers, fns) {\n  /*eslint no-param-reassign:0*/\n  utils.forEach(fns, function transform(fn) {\n    data = fn(data, headers);\n  });\n\n  return data;\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar normalizeHeaderName = require('./helpers/normalizeHeaderName');\n\nvar DEFAULT_CONTENT_TYPE = {\n  'Content-Type': 'application/x-www-form-urlencoded'\n};\n\nfunction setContentTypeIfUnset(headers, value) {\n  if (!utils.isUndefined(headers) && utils.isUndefined(headers['Content-Type'])) {\n    headers['Content-Type'] = value;\n  }\n}\n\nfunction getDefaultAdapter() {\n  var adapter;\n  if (typeof XMLHttpRequest !== 'undefined') {\n    // For browsers use XHR adapter\n    adapter = require('./adapters/xhr');\n  } else if (typeof process !== 'undefined') {\n    // For node use HTTP adapter\n    adapter = require('./adapters/http');\n  }\n  return adapter;\n}\n\nvar defaults = {\n  adapter: getDefaultAdapter(),\n\n  transformRequest: [function transformRequest(data, headers) {\n    normalizeHeaderName(headers, 'Content-Type');\n    if (utils.isFormData(data) ||\n      utils.isArrayBuffer(data) ||\n      utils.isBuffer(data) ||\n      utils.isStream(data) ||\n      utils.isFile(data) ||\n      utils.isBlob(data)\n    ) {\n      return data;\n    }\n    if (utils.isArrayBufferView(data)) {\n      return data.buffer;\n    }\n    if (utils.isURLSearchParams(data)) {\n      setContentTypeIfUnset(headers, 'application/x-www-form-urlencoded;charset=utf-8');\n      return data.toString();\n    }\n    if (utils.isObject(data)) {\n      setContentTypeIfUnset(headers, 'application/json;charset=utf-8');\n      return JSON.stringify(data);\n    }\n    return data;\n  }],\n\n  transformResponse: [function transformResponse(data) {\n    /*eslint no-param-reassign:0*/\n    if (typeof data === 'string') {\n      try {\n        data = JSON.parse(data);\n      } catch (e) { /* Ignore */ }\n    }\n    return data;\n  }],\n\n  timeout: 0,\n\n  xsrfCookieName: 'XSRF-TOKEN',\n  xsrfHeaderName: 'X-XSRF-TOKEN',\n\n  maxContentLength: -1,\n\n  validateStatus: function validateStatus(status) {\n    return status >= 200 && status < 300;\n  }\n};\n\ndefaults.headers = {\n  common: {\n    'Accept': 'application/json, text/plain, */*'\n  }\n};\n\nutils.forEach(['delete', 'get', 'head'], function forEachMethodNoData(method) {\n  defaults.headers[method] = {};\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  defaults.headers[method] = utils.merge(DEFAULT_CONTENT_TYPE);\n});\n\nmodule.exports = defaults;\n","'use strict';\n\nmodule.exports = function bind(fn, thisArg) {\n  return function wrap() {\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n    return fn.apply(thisArg, args);\n  };\n};\n","'use strict';\n\n// btoa polyfill for IE<10 courtesy https://github.com/davidchambers/Base64.js\n\nvar chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=';\n\nfunction E() {\n  this.message = 'String contains an invalid character';\n}\nE.prototype = new Error;\nE.prototype.code = 5;\nE.prototype.name = 'InvalidCharacterError';\n\nfunction btoa(input) {\n  var str = String(input);\n  var output = '';\n  for (\n    // initialize result and counter\n    var block, charCode, idx = 0, map = chars;\n    // if the next str index does not exist:\n    //   change the mapping table to \"=\"\n    //   check if d has no fractional digits\n    str.charAt(idx | 0) || (map = '=', idx % 1);\n    // \"8 - idx % 1 * 8\" generates the sequence 2, 4, 6, 8\n    output += map.charAt(63 & block >> 8 - idx % 1 * 8)\n  ) {\n    charCode = str.charCodeAt(idx += 3 / 4);\n    if (charCode > 0xFF) {\n      throw new E();\n    }\n    block = block << 8 | charCode;\n  }\n  return output;\n}\n\nmodule.exports = btoa;\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction encode(val) {\n  return encodeURIComponent(val).\n    replace(/%40/gi, '@').\n    replace(/%3A/gi, ':').\n    replace(/%24/g, '$').\n    replace(/%2C/gi, ',').\n    replace(/%20/g, '+').\n    replace(/%5B/gi, '[').\n    replace(/%5D/gi, ']');\n}\n\n/**\n * Build a URL by appending params to the end\n *\n * @param {string} url The base of the url (e.g., http://www.google.com)\n * @param {object} [params] The params to be appended\n * @returns {string} The formatted url\n */\nmodule.exports = function buildURL(url, params, paramsSerializer) {\n  /*eslint no-param-reassign:0*/\n  if (!params) {\n    return url;\n  }\n\n  var serializedParams;\n  if (paramsSerializer) {\n    serializedParams = paramsSerializer(params);\n  } else if (utils.isURLSearchParams(params)) {\n    serializedParams = params.toString();\n  } else {\n    var parts = [];\n\n    utils.forEach(params, function serialize(val, key) {\n      if (val === null || typeof val === 'undefined') {\n        return;\n      }\n\n      if (utils.isArray(val)) {\n        key = key + '[]';\n      }\n\n      if (!utils.isArray(val)) {\n        val = [val];\n      }\n\n      utils.forEach(val, function parseValue(v) {\n        if (utils.isDate(v)) {\n          v = v.toISOString();\n        } else if (utils.isObject(v)) {\n          v = JSON.stringify(v);\n        }\n        parts.push(encode(key) + '=' + encode(v));\n      });\n    });\n\n    serializedParams = parts.join('&');\n  }\n\n  if (serializedParams) {\n    url += (url.indexOf('?') === -1 ? '?' : '&') + serializedParams;\n  }\n\n  return url;\n};\n","'use strict';\n\n/**\n * Creates a new URL by combining the specified URLs\n *\n * @param {string} baseURL The base URL\n * @param {string} relativeURL The relative URL\n * @returns {string} The combined URL\n */\nmodule.exports = function combineURLs(baseURL, relativeURL) {\n  return relativeURL\n    ? baseURL.replace(/\\/+$/, '') + '/' + relativeURL.replace(/^\\/+/, '')\n    : baseURL;\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs support document.cookie\n  (function standardBrowserEnv() {\n    return {\n      write: function write(name, value, expires, path, domain, secure) {\n        var cookie = [];\n        cookie.push(name + '=' + encodeURIComponent(value));\n\n        if (utils.isNumber(expires)) {\n          cookie.push('expires=' + new Date(expires).toGMTString());\n        }\n\n        if (utils.isString(path)) {\n          cookie.push('path=' + path);\n        }\n\n        if (utils.isString(domain)) {\n          cookie.push('domain=' + domain);\n        }\n\n        if (secure === true) {\n          cookie.push('secure');\n        }\n\n        document.cookie = cookie.join('; ');\n      },\n\n      read: function read(name) {\n        var match = document.cookie.match(new RegExp('(^|;\\\\s*)(' + name + ')=([^;]*)'));\n        return (match ? decodeURIComponent(match[3]) : null);\n      },\n\n      remove: function remove(name) {\n        this.write(name, '', Date.now() - 86400000);\n      }\n    };\n  })() :\n\n  // Non standard browser env (web workers, react-native) lack needed support.\n  (function nonStandardBrowserEnv() {\n    return {\n      write: function write() {},\n      read: function read() { return null; },\n      remove: function remove() {}\n    };\n  })()\n);\n","'use strict';\n\n/**\n * Determines whether the specified URL is absolute\n *\n * @param {string} url The URL to test\n * @returns {boolean} True if the specified URL is absolute, otherwise false\n */\nmodule.exports = function isAbsoluteURL(url) {\n  // A URL is considered absolute if it begins with \"<scheme>://\" or \"//\" (protocol-relative URL).\n  // RFC 3986 defines scheme name as a sequence of characters beginning with a letter and followed\n  // by any combination of letters, digits, plus, period, or hyphen.\n  return /^([a-z][a-z\\d\\+\\-\\.]*:)?\\/\\//i.test(url);\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs have full support of the APIs needed to test\n  // whether the request URL is of the same origin as current location.\n  (function standardBrowserEnv() {\n    var msie = /(msie|trident)/i.test(navigator.userAgent);\n    var urlParsingNode = document.createElement('a');\n    var originURL;\n\n    /**\n    * Parse a URL to discover it's components\n    *\n    * @param {String} url The URL to be parsed\n    * @returns {Object}\n    */\n    function resolveURL(url) {\n      var href = url;\n\n      if (msie) {\n        // IE needs attribute set twice to normalize properties\n        urlParsingNode.setAttribute('href', href);\n        href = urlParsingNode.href;\n      }\n\n      urlParsingNode.setAttribute('href', href);\n\n      // urlParsingNode provides the UrlUtils interface - http://url.spec.whatwg.org/#urlutils\n      return {\n        href: urlParsingNode.href,\n        protocol: urlParsingNode.protocol ? urlParsingNode.protocol.replace(/:$/, '') : '',\n        host: urlParsingNode.host,\n        search: urlParsingNode.search ? urlParsingNode.search.replace(/^\\?/, '') : '',\n        hash: urlParsingNode.hash ? urlParsingNode.hash.replace(/^#/, '') : '',\n        hostname: urlParsingNode.hostname,\n        port: urlParsingNode.port,\n        pathname: (urlParsingNode.pathname.charAt(0) === '/') ?\n                  urlParsingNode.pathname :\n                  '/' + urlParsingNode.pathname\n      };\n    }\n\n    originURL = resolveURL(window.location.href);\n\n    /**\n    * Determine if a URL shares the same origin as the current location\n    *\n    * @param {String} requestURL The URL to test\n    * @returns {boolean} True if URL shares the same origin, otherwise false\n    */\n    return function isURLSameOrigin(requestURL) {\n      var parsed = (utils.isString(requestURL)) ? resolveURL(requestURL) : requestURL;\n      return (parsed.protocol === originURL.protocol &&\n            parsed.host === originURL.host);\n    };\n  })() :\n\n  // Non standard browser envs (web workers, react-native) lack needed support.\n  (function nonStandardBrowserEnv() {\n    return function isURLSameOrigin() {\n      return true;\n    };\n  })()\n);\n","'use strict';\n\nvar utils = require('../utils');\n\nmodule.exports = function normalizeHeaderName(headers, normalizedName) {\n  utils.forEach(headers, function processHeader(value, name) {\n    if (name !== normalizedName && name.toUpperCase() === normalizedName.toUpperCase()) {\n      headers[normalizedName] = value;\n      delete headers[name];\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n// Headers whose duplicates are ignored by node\n// c.f. https://nodejs.org/api/http.html#http_message_headers\nvar ignoreDuplicateOf = [\n  'age', 'authorization', 'content-length', 'content-type', 'etag',\n  'expires', 'from', 'host', 'if-modified-since', 'if-unmodified-since',\n  'last-modified', 'location', 'max-forwards', 'proxy-authorization',\n  'referer', 'retry-after', 'user-agent'\n];\n\n/**\n * Parse headers into an object\n *\n * ```\n * Date: Wed, 27 Aug 2014 08:58:49 GMT\n * Content-Type: application/json\n * Connection: keep-alive\n * Transfer-Encoding: chunked\n * ```\n *\n * @param {String} headers Headers needing to be parsed\n * @returns {Object} Headers parsed into an object\n */\nmodule.exports = function parseHeaders(headers) {\n  var parsed = {};\n  var key;\n  var val;\n  var i;\n\n  if (!headers) { return parsed; }\n\n  utils.forEach(headers.split('\\n'), function parser(line) {\n    i = line.indexOf(':');\n    key = utils.trim(line.substr(0, i)).toLowerCase();\n    val = utils.trim(line.substr(i + 1));\n\n    if (key) {\n      if (parsed[key] && ignoreDuplicateOf.indexOf(key) >= 0) {\n        return;\n      }\n      if (key === 'set-cookie') {\n        parsed[key] = (parsed[key] ? parsed[key] : []).concat([val]);\n      } else {\n        parsed[key] = parsed[key] ? parsed[key] + ', ' + val : val;\n      }\n    }\n  });\n\n  return parsed;\n};\n","'use strict';\n\n/**\n * Syntactic sugar for invoking a function and expanding an array for arguments.\n *\n * Common use case would be to use `Function.prototype.apply`.\n *\n *  ```js\n *  function f(x, y, z) {}\n *  var args = [1, 2, 3];\n *  f.apply(null, args);\n *  ```\n *\n * With `spread` this example can be re-written.\n *\n *  ```js\n *  spread(function(x, y, z) {})([1, 2, 3]);\n *  ```\n *\n * @param {Function} callback\n * @returns {Function}\n */\nmodule.exports = function spread(callback) {\n  return function wrap(arr) {\n    return callback.apply(null, arr);\n  };\n};\n","'use strict';\n\nvar bind = require('./helpers/bind');\nvar isBuffer = require('is-buffer');\n\n/*global toString:true*/\n\n// utils is a library of generic helper functions non-specific to axios\n\nvar toString = Object.prototype.toString;\n\n/**\n * Determine if a value is an Array\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Array, otherwise false\n */\nfunction isArray(val) {\n  return toString.call(val) === '[object Array]';\n}\n\n/**\n * Determine if a value is an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an ArrayBuffer, otherwise false\n */\nfunction isArrayBuffer(val) {\n  return toString.call(val) === '[object ArrayBuffer]';\n}\n\n/**\n * Determine if a value is a FormData\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an FormData, otherwise false\n */\nfunction isFormData(val) {\n  return (typeof FormData !== 'undefined') && (val instanceof FormData);\n}\n\n/**\n * Determine if a value is a view on an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a view on an ArrayBuffer, otherwise false\n */\nfunction isArrayBufferView(val) {\n  var result;\n  if ((typeof ArrayBuffer !== 'undefined') && (ArrayBuffer.isView)) {\n    result = ArrayBuffer.isView(val);\n  } else {\n    result = (val) && (val.buffer) && (val.buffer instanceof ArrayBuffer);\n  }\n  return result;\n}\n\n/**\n * Determine if a value is a String\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a String, otherwise false\n */\nfunction isString(val) {\n  return typeof val === 'string';\n}\n\n/**\n * Determine if a value is a Number\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Number, otherwise false\n */\nfunction isNumber(val) {\n  return typeof val === 'number';\n}\n\n/**\n * Determine if a value is undefined\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if the value is undefined, otherwise false\n */\nfunction isUndefined(val) {\n  return typeof val === 'undefined';\n}\n\n/**\n * Determine if a value is an Object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Object, otherwise false\n */\nfunction isObject(val) {\n  return val !== null && typeof val === 'object';\n}\n\n/**\n * Determine if a value is a Date\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Date, otherwise false\n */\nfunction isDate(val) {\n  return toString.call(val) === '[object Date]';\n}\n\n/**\n * Determine if a value is a File\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a File, otherwise false\n */\nfunction isFile(val) {\n  return toString.call(val) === '[object File]';\n}\n\n/**\n * Determine if a value is a Blob\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Blob, otherwise false\n */\nfunction isBlob(val) {\n  return toString.call(val) === '[object Blob]';\n}\n\n/**\n * Determine if a value is a Function\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Function, otherwise false\n */\nfunction isFunction(val) {\n  return toString.call(val) === '[object Function]';\n}\n\n/**\n * Determine if a value is a Stream\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Stream, otherwise false\n */\nfunction isStream(val) {\n  return isObject(val) && isFunction(val.pipe);\n}\n\n/**\n * Determine if a value is a URLSearchParams object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a URLSearchParams object, otherwise false\n */\nfunction isURLSearchParams(val) {\n  return typeof URLSearchParams !== 'undefined' && val instanceof URLSearchParams;\n}\n\n/**\n * Trim excess whitespace off the beginning and end of a string\n *\n * @param {String} str The String to trim\n * @returns {String} The String freed of excess whitespace\n */\nfunction trim(str) {\n  return str.replace(/^\\s*/, '').replace(/\\s*$/, '');\n}\n\n/**\n * Determine if we're running in a standard browser environment\n *\n * This allows axios to run in a web worker, and react-native.\n * Both environments support XMLHttpRequest, but not fully standard globals.\n *\n * web workers:\n *  typeof window -> undefined\n *  typeof document -> undefined\n *\n * react-native:\n *  navigator.product -> 'ReactNative'\n */\nfunction isStandardBrowserEnv() {\n  if (typeof navigator !== 'undefined' && navigator.product === 'ReactNative') {\n    return false;\n  }\n  return (\n    typeof window !== 'undefined' &&\n    typeof document !== 'undefined'\n  );\n}\n\n/**\n * Iterate over an Array or an Object invoking a function for each item.\n *\n * If `obj` is an Array callback will be called passing\n * the value, index, and complete array for each item.\n *\n * If 'obj' is an Object callback will be called passing\n * the value, key, and complete object for each property.\n *\n * @param {Object|Array} obj The object to iterate\n * @param {Function} fn The callback to invoke for each item\n */\nfunction forEach(obj, fn) {\n  // Don't bother if no value provided\n  if (obj === null || typeof obj === 'undefined') {\n    return;\n  }\n\n  // Force an array if not already something iterable\n  if (typeof obj !== 'object' && !isArray(obj)) {\n    /*eslint no-param-reassign:0*/\n    obj = [obj];\n  }\n\n  if (isArray(obj)) {\n    // Iterate over array values\n    for (var i = 0, l = obj.length; i < l; i++) {\n      fn.call(null, obj[i], i, obj);\n    }\n  } else {\n    // Iterate over object keys\n    for (var key in obj) {\n      if (Object.prototype.hasOwnProperty.call(obj, key)) {\n        fn.call(null, obj[key], key, obj);\n      }\n    }\n  }\n}\n\n/**\n * Accepts varargs expecting each argument to be an object, then\n * immutably merges the properties of each object and returns result.\n *\n * When multiple objects contain the same key the later object in\n * the arguments list will take precedence.\n *\n * Example:\n *\n * ```js\n * var result = merge({foo: 123}, {foo: 456});\n * console.log(result.foo); // outputs 456\n * ```\n *\n * @param {Object} obj1 Object to merge\n * @returns {Object} Result of all merge properties\n */\nfunction merge(/* obj1, obj2, obj3, ... */) {\n  var result = {};\n  function assignValue(val, key) {\n    if (typeof result[key] === 'object' && typeof val === 'object') {\n      result[key] = merge(result[key], val);\n    } else {\n      result[key] = val;\n    }\n  }\n\n  for (var i = 0, l = arguments.length; i < l; i++) {\n    forEach(arguments[i], assignValue);\n  }\n  return result;\n}\n\n/**\n * Extends object a by mutably adding to it the properties of object b.\n *\n * @param {Object} a The object to be extended\n * @param {Object} b The object to copy properties from\n * @param {Object} thisArg The object to bind function to\n * @return {Object} The resulting value of object a\n */\nfunction extend(a, b, thisArg) {\n  forEach(b, function assignValue(val, key) {\n    if (thisArg && typeof val === 'function') {\n      a[key] = bind(val, thisArg);\n    } else {\n      a[key] = val;\n    }\n  });\n  return a;\n}\n\nmodule.exports = {\n  isArray: isArray,\n  isArrayBuffer: isArrayBuffer,\n  isBuffer: isBuffer,\n  isFormData: isFormData,\n  isArrayBufferView: isArrayBufferView,\n  isString: isString,\n  isNumber: isNumber,\n  isObject: isObject,\n  isUndefined: isUndefined,\n  isDate: isDate,\n  isFile: isFile,\n  isBlob: isBlob,\n  isFunction: isFunction,\n  isStream: isStream,\n  isURLSearchParams: isURLSearchParams,\n  isStandardBrowserEnv: isStandardBrowserEnv,\n  forEach: forEach,\n  merge: merge,\n  extend: extend,\n  trim: trim\n};\n","module.exports = require('./lib/index').default;","'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.isNetworkError = isNetworkError;\nexports.isRetryableError = isRetryableError;\nexports.isSafeRequestError = isSafeRequestError;\nexports.isIdempotentRequestError = isIdempotentRequestError;\nexports.isNetworkOrIdempotentRequestError = isNetworkOrIdempotentRequestError;\nexports.exponentialDelay = exponentialDelay;\nexports.default = axiosRetry;\n\nvar _isRetryAllowed = require('is-retry-allowed');\n\nvar _isRetryAllowed2 = _interopRequireDefault(_isRetryAllowed);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nvar namespace = 'axios-retry';\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isNetworkError(error) {\n  return !error.response && Boolean(error.code) && // Prevents retrying cancelled requests\n  error.code !== 'ECONNABORTED' && // Prevents retrying timed out requests\n  (0, _isRetryAllowed2.default)(error); // Prevents retrying unsafe errors\n}\n\nvar SAFE_HTTP_METHODS = ['get', 'head', 'options'];\nvar IDEMPOTENT_HTTP_METHODS = SAFE_HTTP_METHODS.concat(['put', 'delete']);\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isRetryableError(error) {\n  return error.code !== 'ECONNABORTED' && (!error.response || error.response.status >= 500 && error.response.status <= 599);\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isSafeRequestError(error) {\n  if (!error.config) {\n    // Cannot determine if the request can be retried\n    return false;\n  }\n\n  return isRetryableError(error) && SAFE_HTTP_METHODS.indexOf(error.config.method) !== -1;\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isIdempotentRequestError(error) {\n  if (!error.config) {\n    // Cannot determine if the request can be retried\n    return false;\n  }\n\n  return isRetryableError(error) && IDEMPOTENT_HTTP_METHODS.indexOf(error.config.method) !== -1;\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isNetworkOrIdempotentRequestError(error) {\n  return isNetworkError(error) || isIdempotentRequestError(error);\n}\n\n/**\n * @return {number} - delay in milliseconds, always 0\n */\nfunction noDelay() {\n  return 0;\n}\n\n/**\n * @param  {number} [retryNumber=0]\n * @return {number} - delay in milliseconds\n */\nfunction exponentialDelay() {\n  var retryNumber = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n\n  var delay = Math.pow(2, retryNumber) * 100;\n  var randomSum = delay * 0.2 * Math.random(); // 0-20% of the delay\n  return delay + randomSum;\n}\n\n/**\n * Initializes and returns the retry state for the given request/config\n * @param  {AxiosRequestConfig} config\n * @return {Object}\n */\nfunction getCurrentState(config) {\n  var currentState = config[namespace] || {};\n  currentState.retryCount = currentState.retryCount || 0;\n  config[namespace] = currentState;\n  return currentState;\n}\n\n/**\n * Returns the axios-retry options for the current request\n * @param  {AxiosRequestConfig} config\n * @param  {AxiosRetryConfig} defaultOptions\n * @return {AxiosRetryConfig}\n */\nfunction getRequestOptions(config, defaultOptions) {\n  return Object.assign({}, defaultOptions, config[namespace]);\n}\n\n/**\n * @param  {Axios} axios\n * @param  {AxiosRequestConfig} config\n */\nfunction fixConfig(axios, config) {\n  if (axios.defaults.agent === config.agent) {\n    delete config.agent;\n  }\n  if (axios.defaults.httpAgent === config.httpAgent) {\n    delete config.httpAgent;\n  }\n  if (axios.defaults.httpsAgent === config.httpsAgent) {\n    delete config.httpsAgent;\n  }\n}\n\n/**\n * Adds response interceptors to an axios instance to retry requests failed due to network issues\n *\n * @example\n *\n * import axios from 'axios';\n *\n * axiosRetry(axios, { retries: 3 });\n *\n * axios.get('http://example.com/test') // The first request fails and the second returns 'ok'\n *   .then(result => {\n *     result.data; // 'ok'\n *   });\n *\n * // Exponential back-off retry delay between requests\n * axiosRetry(axios, { retryDelay : axiosRetry.exponentialDelay});\n *\n * // Custom retry delay\n * axiosRetry(axios, { retryDelay : (retryCount) => {\n *   return retryCount * 1000;\n * }});\n *\n * // Also works with custom axios instances\n * const client = axios.create({ baseURL: 'http://example.com' });\n * axiosRetry(client, { retries: 3 });\n *\n * client.get('/test') // The first request fails and the second returns 'ok'\n *   .then(result => {\n *     result.data; // 'ok'\n *   });\n *\n * // Allows request-specific configuration\n * client\n *   .get('/test', {\n *     'axios-retry': {\n *       retries: 0\n *     }\n *   })\n *   .catch(error => { // The first request fails\n *     error !== undefined\n *   });\n *\n * @param {Axios} axios An axios instance (the axios object or one created from axios.create)\n * @param {Object} [defaultOptions]\n * @param {number} [defaultOptions.retries=3] Number of retries\n * @param {boolean} [defaultOptions.shouldResetTimeout=false]\n *        Defines if the timeout should be reset between retries\n * @param {Function} [defaultOptions.retryCondition=isNetworkOrIdempotentRequestError]\n *        A function to determine if the error can be retried\n * @param {Function} [defaultOptions.retryDelay=noDelay]\n *        A function to determine the delay between retry requests\n */\nfunction axiosRetry(axios, defaultOptions) {\n  axios.interceptors.request.use(function (config) {\n    var currentState = getCurrentState(config);\n    currentState.lastRequestTime = Date.now();\n    return config;\n  });\n\n  axios.interceptors.response.use(null, function (error) {\n    var config = error.config;\n\n    // If we have no information to retry the request\n    if (!config) {\n      return Promise.reject(error);\n    }\n\n    var _getRequestOptions = getRequestOptions(config, defaultOptions),\n        _getRequestOptions$re = _getRequestOptions.retries,\n        retries = _getRequestOptions$re === undefined ? 3 : _getRequestOptions$re,\n        _getRequestOptions$re2 = _getRequestOptions.retryCondition,\n        retryCondition = _getRequestOptions$re2 === undefined ? isNetworkOrIdempotentRequestError : _getRequestOptions$re2,\n        _getRequestOptions$re3 = _getRequestOptions.retryDelay,\n        retryDelay = _getRequestOptions$re3 === undefined ? noDelay : _getRequestOptions$re3,\n        _getRequestOptions$sh = _getRequestOptions.shouldResetTimeout,\n        shouldResetTimeout = _getRequestOptions$sh === undefined ? false : _getRequestOptions$sh;\n\n    var currentState = getCurrentState(config);\n\n    var shouldRetry = retryCondition(error) && currentState.retryCount < retries;\n\n    if (shouldRetry) {\n      currentState.retryCount += 1;\n      var delay = retryDelay(currentState.retryCount, error);\n\n      // Axios fails merging this configuration to the default configuration because it has an issue\n      // with circular structures: https://github.com/mzabriskie/axios/issues/370\n      fixConfig(axios, config);\n\n      if (!shouldResetTimeout && config.timeout && currentState.lastRequestTime) {\n        var lastRequestDuration = Date.now() - currentState.lastRequestTime;\n        // Minimum 1ms timeout (passing 0 or less to XHR means no timeout)\n        config.timeout = Math.max(config.timeout - lastRequestDuration - delay, 1);\n      }\n\n      config.transformRequest = [function (data) {\n        return data;\n      }];\n\n      return new Promise(function (resolve) {\n        return setTimeout(function () {\n          return resolve(axios(config));\n        }, delay);\n      });\n    }\n\n    return Promise.reject(error);\n  });\n}\n\n// Compatibility with CommonJS\naxiosRetry.isNetworkError = isNetworkError;\naxiosRetry.isSafeRequestError = isSafeRequestError;\naxiosRetry.isIdempotentRequestError = isIdempotentRequestError;\naxiosRetry.isNetworkOrIdempotentRequestError = isNetworkOrIdempotentRequestError;\naxiosRetry.exponentialDelay = exponentialDelay;\naxiosRetry.isRetryableError = isRetryableError;\n//# sourceMappingURL=index.js.map","module.exports = require('./lib/axios');","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar buildFullPath = require('../core/buildFullPath');\nvar buildURL = require('./../helpers/buildURL');\nvar http = require('http');\nvar https = require('https');\nvar httpFollow = require('follow-redirects').http;\nvar httpsFollow = require('follow-redirects').https;\nvar url = require('url');\nvar zlib = require('zlib');\nvar pkg = require('./../../package.json');\nvar createError = require('../core/createError');\nvar enhanceError = require('../core/enhanceError');\n\nvar isHttps = /https:?/;\n\n/**\n *\n * @param {http.ClientRequestArgs} options\n * @param {AxiosProxyConfig} proxy\n * @param {string} location\n */\nfunction setProxy(options, proxy, location) {\n  options.hostname = proxy.host;\n  options.host = proxy.host;\n  options.port = proxy.port;\n  options.path = location;\n\n  // Basic proxy authorization\n  if (proxy.auth) {\n    var base64 = Buffer.from(proxy.auth.username + ':' + proxy.auth.password, 'utf8').toString('base64');\n    options.headers['Proxy-Authorization'] = 'Basic ' + base64;\n  }\n\n  // If a proxy is used, any redirects must also pass through the proxy\n  options.beforeRedirect = function beforeRedirect(redirection) {\n    redirection.headers.host = redirection.host;\n    setProxy(redirection, proxy, redirection.href);\n  };\n}\n\n/*eslint consistent-return:0*/\nmodule.exports = function httpAdapter(config) {\n  return new Promise(function dispatchHttpRequest(resolvePromise, rejectPromise) {\n    var resolve = function resolve(value) {\n      resolvePromise(value);\n    };\n    var reject = function reject(value) {\n      rejectPromise(value);\n    };\n    var data = config.data;\n    var headers = config.headers;\n\n    // Set User-Agent (required by some servers)\n    // Only set header if it hasn't been set in config\n    // See https://github.com/axios/axios/issues/69\n    if (!headers['User-Agent'] && !headers['user-agent']) {\n      headers['User-Agent'] = 'axios/' + pkg.version;\n    }\n\n    if (data && !utils.isStream(data)) {\n      if (Buffer.isBuffer(data)) {\n        // Nothing to do...\n      } else if (utils.isArrayBuffer(data)) {\n        data = Buffer.from(new Uint8Array(data));\n      } else if (utils.isString(data)) {\n        data = Buffer.from(data, 'utf-8');\n      } else {\n        return reject(createError(\n          'Data after transformation must be a string, an ArrayBuffer, a Buffer, or a Stream',\n          config\n        ));\n      }\n\n      // Add Content-Length header if data exists\n      headers['Content-Length'] = data.length;\n    }\n\n    // HTTP basic authentication\n    var auth = undefined;\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password || '';\n      auth = username + ':' + password;\n    }\n\n    // Parse url\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    var parsed = url.parse(fullPath);\n    var protocol = parsed.protocol || 'http:';\n\n    if (!auth && parsed.auth) {\n      var urlAuth = parsed.auth.split(':');\n      var urlUsername = urlAuth[0] || '';\n      var urlPassword = urlAuth[1] || '';\n      auth = urlUsername + ':' + urlPassword;\n    }\n\n    if (auth) {\n      delete headers.Authorization;\n    }\n\n    var isHttpsRequest = isHttps.test(protocol);\n    var agent = isHttpsRequest ? config.httpsAgent : config.httpAgent;\n\n    var options = {\n      path: buildURL(parsed.path, config.params, config.paramsSerializer).replace(/^\\?/, ''),\n      method: config.method.toUpperCase(),\n      headers: headers,\n      agent: agent,\n      agents: { http: config.httpAgent, https: config.httpsAgent },\n      auth: auth\n    };\n\n    if (config.socketPath) {\n      options.socketPath = config.socketPath;\n    } else {\n      options.hostname = parsed.hostname;\n      options.port = parsed.port;\n    }\n\n    var proxy = config.proxy;\n    if (!proxy && proxy !== false) {\n      var proxyEnv = protocol.slice(0, -1) + '_proxy';\n      var proxyUrl = process.env[proxyEnv] || process.env[proxyEnv.toUpperCase()];\n      if (proxyUrl) {\n        var parsedProxyUrl = url.parse(proxyUrl);\n        var noProxyEnv = process.env.no_proxy || process.env.NO_PROXY;\n        var shouldProxy = true;\n\n        if (noProxyEnv) {\n          var noProxy = noProxyEnv.split(',').map(function trim(s) {\n            return s.trim();\n          });\n\n          shouldProxy = !noProxy.some(function proxyMatch(proxyElement) {\n            if (!proxyElement) {\n              return false;\n            }\n            if (proxyElement === '*') {\n              return true;\n            }\n            if (proxyElement[0] === '.' &&\n                parsed.hostname.substr(parsed.hostname.length - proxyElement.length) === proxyElement) {\n              return true;\n            }\n\n            return parsed.hostname === proxyElement;\n          });\n        }\n\n        if (shouldProxy) {\n          proxy = {\n            host: parsedProxyUrl.hostname,\n            port: parsedProxyUrl.port,\n            protocol: parsedProxyUrl.protocol\n          };\n\n          if (parsedProxyUrl.auth) {\n            var proxyUrlAuth = parsedProxyUrl.auth.split(':');\n            proxy.auth = {\n              username: proxyUrlAuth[0],\n              password: proxyUrlAuth[1]\n            };\n          }\n        }\n      }\n    }\n\n    if (proxy) {\n      options.headers.host = parsed.hostname + (parsed.port ? ':' + parsed.port : '');\n      setProxy(options, proxy, protocol + '//' + parsed.hostname + (parsed.port ? ':' + parsed.port : '') + options.path);\n    }\n\n    var transport;\n    var isHttpsProxy = isHttpsRequest && (proxy ? isHttps.test(proxy.protocol) : true);\n    if (config.transport) {\n      transport = config.transport;\n    } else if (config.maxRedirects === 0) {\n      transport = isHttpsProxy ? https : http;\n    } else {\n      if (config.maxRedirects) {\n        options.maxRedirects = config.maxRedirects;\n      }\n      transport = isHttpsProxy ? httpsFollow : httpFollow;\n    }\n\n    if (config.maxBodyLength > -1) {\n      options.maxBodyLength = config.maxBodyLength;\n    }\n\n    // Create the request\n    var req = transport.request(options, function handleResponse(res) {\n      if (req.aborted) return;\n\n      // uncompress the response body transparently if required\n      var stream = res;\n\n      // return the last request in case of redirects\n      var lastRequest = res.req || req;\n\n\n      // if no content, is HEAD request or decompress disabled we should not decompress\n      if (res.statusCode !== 204 && lastRequest.method !== 'HEAD' && config.decompress !== false) {\n        switch (res.headers['content-encoding']) {\n        /*eslint default-case:0*/\n        case 'gzip':\n        case 'compress':\n        case 'deflate':\n        // add the unzipper to the body stream processing pipeline\n          stream = stream.pipe(zlib.createUnzip());\n\n          // remove the content-encoding in order to not confuse downstream operations\n          delete res.headers['content-encoding'];\n          break;\n        }\n      }\n\n      var response = {\n        status: res.statusCode,\n        statusText: res.statusMessage,\n        headers: res.headers,\n        config: config,\n        request: lastRequest\n      };\n\n      if (config.responseType === 'stream') {\n        response.data = stream;\n        settle(resolve, reject, response);\n      } else {\n        var responseBuffer = [];\n        stream.on('data', function handleStreamData(chunk) {\n          responseBuffer.push(chunk);\n\n          // make sure the content length is not over the maxContentLength if specified\n          if (config.maxContentLength > -1 && Buffer.concat(responseBuffer).length > config.maxContentLength) {\n            stream.destroy();\n            reject(createError('maxContentLength size of ' + config.maxContentLength + ' exceeded',\n              config, null, lastRequest));\n          }\n        });\n\n        stream.on('error', function handleStreamError(err) {\n          if (req.aborted) return;\n          reject(enhanceError(err, config, null, lastRequest));\n        });\n\n        stream.on('end', function handleStreamEnd() {\n          var responseData = Buffer.concat(responseBuffer);\n          if (config.responseType !== 'arraybuffer') {\n            responseData = responseData.toString(config.responseEncoding);\n            if (!config.responseEncoding || config.responseEncoding === 'utf8') {\n              responseData = utils.stripBOM(responseData);\n            }\n          }\n\n          response.data = responseData;\n          settle(resolve, reject, response);\n        });\n      }\n    });\n\n    // Handle errors\n    req.on('error', function handleRequestError(err) {\n      if (req.aborted && err.code !== 'ERR_FR_TOO_MANY_REDIRECTS') return;\n      reject(enhanceError(err, config, null, req));\n    });\n\n    // Handle request timeout\n    if (config.timeout) {\n      // Sometime, the response will be very slow, and does not respond, the connect event will be block by event loop system.\n      // And timer callback will be fired, and abort() will be invoked before connection, then get \"socket hang up\" and code ECONNRESET.\n      // At this time, if we have a large number of request, nodejs will hang up some socket on background. and the number will up and up.\n      // And then these socket which be hang up will devoring CPU little by little.\n      // ClientRequest.setTimeout will be fired on the specify milliseconds, and can make sure that abort() will be fired after connect.\n      req.setTimeout(config.timeout, function handleRequestTimeout() {\n        req.abort();\n        reject(createError('timeout of ' + config.timeout + 'ms exceeded', config, 'ECONNABORTED', req));\n      });\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (req.aborted) return;\n\n        req.abort();\n        reject(cancel);\n      });\n    }\n\n    // Send the request\n    if (utils.isStream(data)) {\n      data.on('error', function handleStreamError(err) {\n        reject(enhanceError(err, config, null, req));\n      }).pipe(req);\n    } else {\n      req.end(data);\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar cookies = require('./../helpers/cookies');\nvar buildURL = require('./../helpers/buildURL');\nvar buildFullPath = require('../core/buildFullPath');\nvar parseHeaders = require('./../helpers/parseHeaders');\nvar isURLSameOrigin = require('./../helpers/isURLSameOrigin');\nvar createError = require('../core/createError');\n\nmodule.exports = function xhrAdapter(config) {\n  return new Promise(function dispatchXhrRequest(resolve, reject) {\n    var requestData = config.data;\n    var requestHeaders = config.headers;\n\n    if (utils.isFormData(requestData)) {\n      delete requestHeaders['Content-Type']; // Let the browser set it\n    }\n\n    var request = new XMLHttpRequest();\n\n    // HTTP basic authentication\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password ? unescape(encodeURIComponent(config.auth.password)) : '';\n      requestHeaders.Authorization = 'Basic ' + btoa(username + ':' + password);\n    }\n\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    request.open(config.method.toUpperCase(), buildURL(fullPath, config.params, config.paramsSerializer), true);\n\n    // Set the request timeout in MS\n    request.timeout = config.timeout;\n\n    // Listen for ready state\n    request.onreadystatechange = function handleLoad() {\n      if (!request || request.readyState !== 4) {\n        return;\n      }\n\n      // The request errored out and we didn't get a response, this will be\n      // handled by onerror instead\n      // With one exception: request that using file: protocol, most browsers\n      // will return status as 0 even though it's a successful request\n      if (request.status === 0 && !(request.responseURL && request.responseURL.indexOf('file:') === 0)) {\n        return;\n      }\n\n      // Prepare the response\n      var responseHeaders = 'getAllResponseHeaders' in request ? parseHeaders(request.getAllResponseHeaders()) : null;\n      var responseData = !config.responseType || config.responseType === 'text' ? request.responseText : request.response;\n      var response = {\n        data: responseData,\n        status: request.status,\n        statusText: request.statusText,\n        headers: responseHeaders,\n        config: config,\n        request: request\n      };\n\n      settle(resolve, reject, response);\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle browser request cancellation (as opposed to a manual cancellation)\n    request.onabort = function handleAbort() {\n      if (!request) {\n        return;\n      }\n\n      reject(createError('Request aborted', config, 'ECONNABORTED', request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle low level network errors\n    request.onerror = function handleError() {\n      // Real errors are hidden from us by the browser\n      // onerror should only fire if it's a network error\n      reject(createError('Network Error', config, null, request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle timeout\n    request.ontimeout = function handleTimeout() {\n      var timeoutErrorMessage = 'timeout of ' + config.timeout + 'ms exceeded';\n      if (config.timeoutErrorMessage) {\n        timeoutErrorMessage = config.timeoutErrorMessage;\n      }\n      reject(createError(timeoutErrorMessage, config, 'ECONNABORTED',\n        request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Add xsrf header\n    // This is only done if running in a standard browser environment.\n    // Specifically not if we're in a web worker, or react-native.\n    if (utils.isStandardBrowserEnv()) {\n      // Add xsrf header\n      var xsrfValue = (config.withCredentials || isURLSameOrigin(fullPath)) && config.xsrfCookieName ?\n        cookies.read(config.xsrfCookieName) :\n        undefined;\n\n      if (xsrfValue) {\n        requestHeaders[config.xsrfHeaderName] = xsrfValue;\n      }\n    }\n\n    // Add headers to the request\n    if ('setRequestHeader' in request) {\n      utils.forEach(requestHeaders, function setRequestHeader(val, key) {\n        if (typeof requestData === 'undefined' && key.toLowerCase() === 'content-type') {\n          // Remove Content-Type if data is undefined\n          delete requestHeaders[key];\n        } else {\n          // Otherwise add header to the request\n          request.setRequestHeader(key, val);\n        }\n      });\n    }\n\n    // Add withCredentials to request if needed\n    if (!utils.isUndefined(config.withCredentials)) {\n      request.withCredentials = !!config.withCredentials;\n    }\n\n    // Add responseType to request if needed\n    if (config.responseType) {\n      try {\n        request.responseType = config.responseType;\n      } catch (e) {\n        // Expected DOMException thrown by browsers not compatible XMLHttpRequest Level 2.\n        // But, this can be suppressed for 'json' type as it can be parsed by default 'transformResponse' function.\n        if (config.responseType !== 'json') {\n          throw e;\n        }\n      }\n    }\n\n    // Handle progress if needed\n    if (typeof config.onDownloadProgress === 'function') {\n      request.addEventListener('progress', config.onDownloadProgress);\n    }\n\n    // Not all browsers support upload events\n    if (typeof config.onUploadProgress === 'function' && request.upload) {\n      request.upload.addEventListener('progress', config.onUploadProgress);\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (!request) {\n          return;\n        }\n\n        request.abort();\n        reject(cancel);\n        // Clean up request\n        request = null;\n      });\n    }\n\n    if (!requestData) {\n      requestData = null;\n    }\n\n    // Send the request\n    request.send(requestData);\n  });\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar bind = require('./helpers/bind');\nvar Axios = require('./core/Axios');\nvar mergeConfig = require('./core/mergeConfig');\nvar defaults = require('./defaults');\n\n/**\n * Create an instance of Axios\n *\n * @param {Object} defaultConfig The default config for the instance\n * @return {Axios} A new instance of Axios\n */\nfunction createInstance(defaultConfig) {\n  var context = new Axios(defaultConfig);\n  var instance = bind(Axios.prototype.request, context);\n\n  // Copy axios.prototype to instance\n  utils.extend(instance, Axios.prototype, context);\n\n  // Copy context to instance\n  utils.extend(instance, context);\n\n  return instance;\n}\n\n// Create the default instance to be exported\nvar axios = createInstance(defaults);\n\n// Expose Axios class to allow class inheritance\naxios.Axios = Axios;\n\n// Factory for creating new instances\naxios.create = function create(instanceConfig) {\n  return createInstance(mergeConfig(axios.defaults, instanceConfig));\n};\n\n// Expose Cancel & CancelToken\naxios.Cancel = require('./cancel/Cancel');\naxios.CancelToken = require('./cancel/CancelToken');\naxios.isCancel = require('./cancel/isCancel');\n\n// Expose all/spread\naxios.all = function all(promises) {\n  return Promise.all(promises);\n};\naxios.spread = require('./helpers/spread');\n\n// Expose isAxiosError\naxios.isAxiosError = require('./helpers/isAxiosError');\n\nmodule.exports = axios;\n\n// Allow use of default import syntax in TypeScript\nmodule.exports.default = axios;\n","'use strict';\n\n/**\n * A `Cancel` is an object that is thrown when an operation is canceled.\n *\n * @class\n * @param {string=} message The message.\n */\nfunction Cancel(message) {\n  this.message = message;\n}\n\nCancel.prototype.toString = function toString() {\n  return 'Cancel' + (this.message ? ': ' + this.message : '');\n};\n\nCancel.prototype.__CANCEL__ = true;\n\nmodule.exports = Cancel;\n","'use strict';\n\nvar Cancel = require('./Cancel');\n\n/**\n * A `CancelToken` is an object that can be used to request cancellation of an operation.\n *\n * @class\n * @param {Function} executor The executor function.\n */\nfunction CancelToken(executor) {\n  if (typeof executor !== 'function') {\n    throw new TypeError('executor must be a function.');\n  }\n\n  var resolvePromise;\n  this.promise = new Promise(function promiseExecutor(resolve) {\n    resolvePromise = resolve;\n  });\n\n  var token = this;\n  executor(function cancel(message) {\n    if (token.reason) {\n      // Cancellation has already been requested\n      return;\n    }\n\n    token.reason = new Cancel(message);\n    resolvePromise(token.reason);\n  });\n}\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nCancelToken.prototype.throwIfRequested = function throwIfRequested() {\n  if (this.reason) {\n    throw this.reason;\n  }\n};\n\n/**\n * Returns an object that contains a new `CancelToken` and a function that, when called,\n * cancels the `CancelToken`.\n */\nCancelToken.source = function source() {\n  var cancel;\n  var token = new CancelToken(function executor(c) {\n    cancel = c;\n  });\n  return {\n    token: token,\n    cancel: cancel\n  };\n};\n\nmodule.exports = CancelToken;\n","'use strict';\n\nmodule.exports = function isCancel(value) {\n  return !!(value && value.__CANCEL__);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar buildURL = require('../helpers/buildURL');\nvar InterceptorManager = require('./InterceptorManager');\nvar dispatchRequest = require('./dispatchRequest');\nvar mergeConfig = require('./mergeConfig');\n\n/**\n * Create a new instance of Axios\n *\n * @param {Object} instanceConfig The default config for the instance\n */\nfunction Axios(instanceConfig) {\n  this.defaults = instanceConfig;\n  this.interceptors = {\n    request: new InterceptorManager(),\n    response: new InterceptorManager()\n  };\n}\n\n/**\n * Dispatch a request\n *\n * @param {Object} config The config specific for this request (merged with this.defaults)\n */\nAxios.prototype.request = function request(config) {\n  /*eslint no-param-reassign:0*/\n  // Allow for axios('example/url'[, config]) a la fetch API\n  if (typeof config === 'string') {\n    config = arguments[1] || {};\n    config.url = arguments[0];\n  } else {\n    config = config || {};\n  }\n\n  config = mergeConfig(this.defaults, config);\n\n  // Set config.method\n  if (config.method) {\n    config.method = config.method.toLowerCase();\n  } else if (this.defaults.method) {\n    config.method = this.defaults.method.toLowerCase();\n  } else {\n    config.method = 'get';\n  }\n\n  // Hook up interceptors middleware\n  var chain = [dispatchRequest, undefined];\n  var promise = Promise.resolve(config);\n\n  this.interceptors.request.forEach(function unshiftRequestInterceptors(interceptor) {\n    chain.unshift(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  this.interceptors.response.forEach(function pushResponseInterceptors(interceptor) {\n    chain.push(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  while (chain.length) {\n    promise = promise.then(chain.shift(), chain.shift());\n  }\n\n  return promise;\n};\n\nAxios.prototype.getUri = function getUri(config) {\n  config = mergeConfig(this.defaults, config);\n  return buildURL(config.url, config.params, config.paramsSerializer).replace(/^\\?/, '');\n};\n\n// Provide aliases for supported request methods\nutils.forEach(['delete', 'get', 'head', 'options'], function forEachMethodNoData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: (config || {}).data\n    }));\n  };\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, data, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: data\n    }));\n  };\n});\n\nmodule.exports = Axios;\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction InterceptorManager() {\n  this.handlers = [];\n}\n\n/**\n * Add a new interceptor to the stack\n *\n * @param {Function} fulfilled The function to handle `then` for a `Promise`\n * @param {Function} rejected The function to handle `reject` for a `Promise`\n *\n * @return {Number} An ID used to remove interceptor later\n */\nInterceptorManager.prototype.use = function use(fulfilled, rejected) {\n  this.handlers.push({\n    fulfilled: fulfilled,\n    rejected: rejected\n  });\n  return this.handlers.length - 1;\n};\n\n/**\n * Remove an interceptor from the stack\n *\n * @param {Number} id The ID that was returned by `use`\n */\nInterceptorManager.prototype.eject = function eject(id) {\n  if (this.handlers[id]) {\n    this.handlers[id] = null;\n  }\n};\n\n/**\n * Iterate over all the registered interceptors\n *\n * This method is particularly useful for skipping over any\n * interceptors that may have become `null` calling `eject`.\n *\n * @param {Function} fn The function to call for each interceptor\n */\nInterceptorManager.prototype.forEach = function forEach(fn) {\n  utils.forEach(this.handlers, function forEachHandler(h) {\n    if (h !== null) {\n      fn(h);\n    }\n  });\n};\n\nmodule.exports = InterceptorManager;\n","'use strict';\n\nvar isAbsoluteURL = require('../helpers/isAbsoluteURL');\nvar combineURLs = require('../helpers/combineURLs');\n\n/**\n * Creates a new URL by combining the baseURL with the requestedURL,\n * only when the requestedURL is not already an absolute URL.\n * If the requestURL is absolute, this function returns the requestedURL untouched.\n *\n * @param {string} baseURL The base URL\n * @param {string} requestedURL Absolute or relative URL to combine\n * @returns {string} The combined full path\n */\nmodule.exports = function buildFullPath(baseURL, requestedURL) {\n  if (baseURL && !isAbsoluteURL(requestedURL)) {\n    return combineURLs(baseURL, requestedURL);\n  }\n  return requestedURL;\n};\n","'use strict';\n\nvar enhanceError = require('./enhanceError');\n\n/**\n * Create an Error with the specified message, config, error code, request and response.\n *\n * @param {string} message The error message.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The created error.\n */\nmodule.exports = function createError(message, config, code, request, response) {\n  var error = new Error(message);\n  return enhanceError(error, config, code, request, response);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar transformData = require('./transformData');\nvar isCancel = require('../cancel/isCancel');\nvar defaults = require('../defaults');\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nfunction throwIfCancellationRequested(config) {\n  if (config.cancelToken) {\n    config.cancelToken.throwIfRequested();\n  }\n}\n\n/**\n * Dispatch a request to the server using the configured adapter.\n *\n * @param {object} config The config that is to be used for the request\n * @returns {Promise} The Promise to be fulfilled\n */\nmodule.exports = function dispatchRequest(config) {\n  throwIfCancellationRequested(config);\n\n  // Ensure headers exist\n  config.headers = config.headers || {};\n\n  // Transform request data\n  config.data = transformData(\n    config.data,\n    config.headers,\n    config.transformRequest\n  );\n\n  // Flatten headers\n  config.headers = utils.merge(\n    config.headers.common || {},\n    config.headers[config.method] || {},\n    config.headers\n  );\n\n  utils.forEach(\n    ['delete', 'get', 'head', 'post', 'put', 'patch', 'common'],\n    function cleanHeaderConfig(method) {\n      delete config.headers[method];\n    }\n  );\n\n  var adapter = config.adapter || defaults.adapter;\n\n  return adapter(config).then(function onAdapterResolution(response) {\n    throwIfCancellationRequested(config);\n\n    // Transform response data\n    response.data = transformData(\n      response.data,\n      response.headers,\n      config.transformResponse\n    );\n\n    return response;\n  }, function onAdapterRejection(reason) {\n    if (!isCancel(reason)) {\n      throwIfCancellationRequested(config);\n\n      // Transform response data\n      if (reason && reason.response) {\n        reason.response.data = transformData(\n          reason.response.data,\n          reason.response.headers,\n          config.transformResponse\n        );\n      }\n    }\n\n    return Promise.reject(reason);\n  });\n};\n","'use strict';\n\n/**\n * Update an Error with the specified config, error code, and response.\n *\n * @param {Error} error The error to update.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The error.\n */\nmodule.exports = function enhanceError(error, config, code, request, response) {\n  error.config = config;\n  if (code) {\n    error.code = code;\n  }\n\n  error.request = request;\n  error.response = response;\n  error.isAxiosError = true;\n\n  error.toJSON = function toJSON() {\n    return {\n      // Standard\n      message: this.message,\n      name: this.name,\n      // Microsoft\n      description: this.description,\n      number: this.number,\n      // Mozilla\n      fileName: this.fileName,\n      lineNumber: this.lineNumber,\n      columnNumber: this.columnNumber,\n      stack: this.stack,\n      // Axios\n      config: this.config,\n      code: this.code\n    };\n  };\n  return error;\n};\n","'use strict';\n\nvar utils = require('../utils');\n\n/**\n * Config-specific merge-function which creates a new config-object\n * by merging two configuration objects together.\n *\n * @param {Object} config1\n * @param {Object} config2\n * @returns {Object} New object resulting from merging config2 to config1\n */\nmodule.exports = function mergeConfig(config1, config2) {\n  // eslint-disable-next-line no-param-reassign\n  config2 = config2 || {};\n  var config = {};\n\n  var valueFromConfig2Keys = ['url', 'method', 'data'];\n  var mergeDeepPropertiesKeys = ['headers', 'auth', 'proxy', 'params'];\n  var defaultToConfig2Keys = [\n    'baseURL', 'transformRequest', 'transformResponse', 'paramsSerializer',\n    'timeout', 'timeoutMessage', 'withCredentials', 'adapter', 'responseType', 'xsrfCookieName',\n    'xsrfHeaderName', 'onUploadProgress', 'onDownloadProgress', 'decompress',\n    'maxContentLength', 'maxBodyLength', 'maxRedirects', 'transport', 'httpAgent',\n    'httpsAgent', 'cancelToken', 'socketPath', 'responseEncoding'\n  ];\n  var directMergeKeys = ['validateStatus'];\n\n  function getMergedValue(target, source) {\n    if (utils.isPlainObject(target) && utils.isPlainObject(source)) {\n      return utils.merge(target, source);\n    } else if (utils.isPlainObject(source)) {\n      return utils.merge({}, source);\n    } else if (utils.isArray(source)) {\n      return source.slice();\n    }\n    return source;\n  }\n\n  function mergeDeepProperties(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(config1[prop], config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  }\n\n  utils.forEach(valueFromConfig2Keys, function valueFromConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(undefined, config2[prop]);\n    }\n  });\n\n  utils.forEach(mergeDeepPropertiesKeys, mergeDeepProperties);\n\n  utils.forEach(defaultToConfig2Keys, function defaultToConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(undefined, config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  });\n\n  utils.forEach(directMergeKeys, function merge(prop) {\n    if (prop in config2) {\n      config[prop] = getMergedValue(config1[prop], config2[prop]);\n    } else if (prop in config1) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  });\n\n  var axiosKeys = valueFromConfig2Keys\n    .concat(mergeDeepPropertiesKeys)\n    .concat(defaultToConfig2Keys)\n    .concat(directMergeKeys);\n\n  var otherKeys = Object\n    .keys(config1)\n    .concat(Object.keys(config2))\n    .filter(function filterAxiosKeys(key) {\n      return axiosKeys.indexOf(key) === -1;\n    });\n\n  utils.forEach(otherKeys, mergeDeepProperties);\n\n  return config;\n};\n","'use strict';\n\nvar createError = require('./createError');\n\n/**\n * Resolve or reject a Promise based on response status.\n *\n * @param {Function} resolve A function that resolves the promise.\n * @param {Function} reject A function that rejects the promise.\n * @param {object} response The response.\n */\nmodule.exports = function settle(resolve, reject, response) {\n  var validateStatus = response.config.validateStatus;\n  if (!response.status || !validateStatus || validateStatus(response.status)) {\n    resolve(response);\n  } else {\n    reject(createError(\n      'Request failed with status code ' + response.status,\n      response.config,\n      null,\n      response.request,\n      response\n    ));\n  }\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n/**\n * Transform the data for a request or a response\n *\n * @param {Object|String} data The data to be transformed\n * @param {Array} headers The headers for the request or response\n * @param {Array|Function} fns A single function or Array of functions\n * @returns {*} The resulting transformed data\n */\nmodule.exports = function transformData(data, headers, fns) {\n  /*eslint no-param-reassign:0*/\n  utils.forEach(fns, function transform(fn) {\n    data = fn(data, headers);\n  });\n\n  return data;\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar normalizeHeaderName = require('./helpers/normalizeHeaderName');\n\nvar DEFAULT_CONTENT_TYPE = {\n  'Content-Type': 'application/x-www-form-urlencoded'\n};\n\nfunction setContentTypeIfUnset(headers, value) {\n  if (!utils.isUndefined(headers) && utils.isUndefined(headers['Content-Type'])) {\n    headers['Content-Type'] = value;\n  }\n}\n\nfunction getDefaultAdapter() {\n  var adapter;\n  if (typeof XMLHttpRequest !== 'undefined') {\n    // For browsers use XHR adapter\n    adapter = require('./adapters/xhr');\n  } else if (typeof process !== 'undefined' && Object.prototype.toString.call(process) === '[object process]') {\n    // For node use HTTP adapter\n    adapter = require('./adapters/http');\n  }\n  return adapter;\n}\n\nvar defaults = {\n  adapter: getDefaultAdapter(),\n\n  transformRequest: [function transformRequest(data, headers) {\n    normalizeHeaderName(headers, 'Accept');\n    normalizeHeaderName(headers, 'Content-Type');\n    if (utils.isFormData(data) ||\n      utils.isArrayBuffer(data) ||\n      utils.isBuffer(data) ||\n      utils.isStream(data) ||\n      utils.isFile(data) ||\n      utils.isBlob(data)\n    ) {\n      return data;\n    }\n    if (utils.isArrayBufferView(data)) {\n      return data.buffer;\n    }\n    if (utils.isURLSearchParams(data)) {\n      setContentTypeIfUnset(headers, 'application/x-www-form-urlencoded;charset=utf-8');\n      return data.toString();\n    }\n    if (utils.isObject(data)) {\n      setContentTypeIfUnset(headers, 'application/json;charset=utf-8');\n      return JSON.stringify(data);\n    }\n    return data;\n  }],\n\n  transformResponse: [function transformResponse(data) {\n    /*eslint no-param-reassign:0*/\n    if (typeof data === 'string') {\n      try {\n        data = JSON.parse(data);\n      } catch (e) { /* Ignore */ }\n    }\n    return data;\n  }],\n\n  /**\n   * A timeout in milliseconds to abort a request. If set to 0 (default) a\n   * timeout is not created.\n   */\n  timeout: 0,\n\n  xsrfCookieName: 'XSRF-TOKEN',\n  xsrfHeaderName: 'X-XSRF-TOKEN',\n\n  maxContentLength: -1,\n  maxBodyLength: -1,\n\n  validateStatus: function validateStatus(status) {\n    return status >= 200 && status < 300;\n  }\n};\n\ndefaults.headers = {\n  common: {\n    'Accept': 'application/json, text/plain, */*'\n  }\n};\n\nutils.forEach(['delete', 'get', 'head'], function forEachMethodNoData(method) {\n  defaults.headers[method] = {};\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  defaults.headers[method] = utils.merge(DEFAULT_CONTENT_TYPE);\n});\n\nmodule.exports = defaults;\n","'use strict';\n\nmodule.exports = function bind(fn, thisArg) {\n  return function wrap() {\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n    return fn.apply(thisArg, args);\n  };\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction encode(val) {\n  return encodeURIComponent(val).\n    replace(/%3A/gi, ':').\n    replace(/%24/g, '$').\n    replace(/%2C/gi, ',').\n    replace(/%20/g, '+').\n    replace(/%5B/gi, '[').\n    replace(/%5D/gi, ']');\n}\n\n/**\n * Build a URL by appending params to the end\n *\n * @param {string} url The base of the url (e.g., http://www.google.com)\n * @param {object} [params] The params to be appended\n * @returns {string} The formatted url\n */\nmodule.exports = function buildURL(url, params, paramsSerializer) {\n  /*eslint no-param-reassign:0*/\n  if (!params) {\n    return url;\n  }\n\n  var serializedParams;\n  if (paramsSerializer) {\n    serializedParams = paramsSerializer(params);\n  } else if (utils.isURLSearchParams(params)) {\n    serializedParams = params.toString();\n  } else {\n    var parts = [];\n\n    utils.forEach(params, function serialize(val, key) {\n      if (val === null || typeof val === 'undefined') {\n        return;\n      }\n\n      if (utils.isArray(val)) {\n        key = key + '[]';\n      } else {\n        val = [val];\n      }\n\n      utils.forEach(val, function parseValue(v) {\n        if (utils.isDate(v)) {\n          v = v.toISOString();\n        } else if (utils.isObject(v)) {\n          v = JSON.stringify(v);\n        }\n        parts.push(encode(key) + '=' + encode(v));\n      });\n    });\n\n    serializedParams = parts.join('&');\n  }\n\n  if (serializedParams) {\n    var hashmarkIndex = url.indexOf('#');\n    if (hashmarkIndex !== -1) {\n      url = url.slice(0, hashmarkIndex);\n    }\n\n    url += (url.indexOf('?') === -1 ? '?' : '&') + serializedParams;\n  }\n\n  return url;\n};\n","'use strict';\n\n/**\n * Creates a new URL by combining the specified URLs\n *\n * @param {string} baseURL The base URL\n * @param {string} relativeURL The relative URL\n * @returns {string} The combined URL\n */\nmodule.exports = function combineURLs(baseURL, relativeURL) {\n  return relativeURL\n    ? baseURL.replace(/\\/+$/, '') + '/' + relativeURL.replace(/^\\/+/, '')\n    : baseURL;\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs support document.cookie\n    (function standardBrowserEnv() {\n      return {\n        write: function write(name, value, expires, path, domain, secure) {\n          var cookie = [];\n          cookie.push(name + '=' + encodeURIComponent(value));\n\n          if (utils.isNumber(expires)) {\n            cookie.push('expires=' + new Date(expires).toGMTString());\n          }\n\n          if (utils.isString(path)) {\n            cookie.push('path=' + path);\n          }\n\n          if (utils.isString(domain)) {\n            cookie.push('domain=' + domain);\n          }\n\n          if (secure === true) {\n            cookie.push('secure');\n          }\n\n          document.cookie = cookie.join('; ');\n        },\n\n        read: function read(name) {\n          var match = document.cookie.match(new RegExp('(^|;\\\\s*)(' + name + ')=([^;]*)'));\n          return (match ? decodeURIComponent(match[3]) : null);\n        },\n\n        remove: function remove(name) {\n          this.write(name, '', Date.now() - 86400000);\n        }\n      };\n    })() :\n\n  // Non standard browser env (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return {\n        write: function write() {},\n        read: function read() { return null; },\n        remove: function remove() {}\n      };\n    })()\n);\n","'use strict';\n\n/**\n * Determines whether the specified URL is absolute\n *\n * @param {string} url The URL to test\n * @returns {boolean} True if the specified URL is absolute, otherwise false\n */\nmodule.exports = function isAbsoluteURL(url) {\n  // A URL is considered absolute if it begins with \"<scheme>://\" or \"//\" (protocol-relative URL).\n  // RFC 3986 defines scheme name as a sequence of characters beginning with a letter and followed\n  // by any combination of letters, digits, plus, period, or hyphen.\n  return /^([a-z][a-z\\d\\+\\-\\.]*:)?\\/\\//i.test(url);\n};\n","'use strict';\n\n/**\n * Determines whether the payload is an error thrown by Axios\n *\n * @param {*} payload The value to test\n * @returns {boolean} True if the payload is an error thrown by Axios, otherwise false\n */\nmodule.exports = function isAxiosError(payload) {\n  return (typeof payload === 'object') && (payload.isAxiosError === true);\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs have full support of the APIs needed to test\n  // whether the request URL is of the same origin as current location.\n    (function standardBrowserEnv() {\n      var msie = /(msie|trident)/i.test(navigator.userAgent);\n      var urlParsingNode = document.createElement('a');\n      var originURL;\n\n      /**\n    * Parse a URL to discover it's components\n    *\n    * @param {String} url The URL to be parsed\n    * @returns {Object}\n    */\n      function resolveURL(url) {\n        var href = url;\n\n        if (msie) {\n        // IE needs attribute set twice to normalize properties\n          urlParsingNode.setAttribute('href', href);\n          href = urlParsingNode.href;\n        }\n\n        urlParsingNode.setAttribute('href', href);\n\n        // urlParsingNode provides the UrlUtils interface - http://url.spec.whatwg.org/#urlutils\n        return {\n          href: urlParsingNode.href,\n          protocol: urlParsingNode.protocol ? urlParsingNode.protocol.replace(/:$/, '') : '',\n          host: urlParsingNode.host,\n          search: urlParsingNode.search ? urlParsingNode.search.replace(/^\\?/, '') : '',\n          hash: urlParsingNode.hash ? urlParsingNode.hash.replace(/^#/, '') : '',\n          hostname: urlParsingNode.hostname,\n          port: urlParsingNode.port,\n          pathname: (urlParsingNode.pathname.charAt(0) === '/') ?\n            urlParsingNode.pathname :\n            '/' + urlParsingNode.pathname\n        };\n      }\n\n      originURL = resolveURL(window.location.href);\n\n      /**\n    * Determine if a URL shares the same origin as the current location\n    *\n    * @param {String} requestURL The URL to test\n    * @returns {boolean} True if URL shares the same origin, otherwise false\n    */\n      return function isURLSameOrigin(requestURL) {\n        var parsed = (utils.isString(requestURL)) ? resolveURL(requestURL) : requestURL;\n        return (parsed.protocol === originURL.protocol &&\n            parsed.host === originURL.host);\n      };\n    })() :\n\n  // Non standard browser envs (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return function isURLSameOrigin() {\n        return true;\n      };\n    })()\n);\n","'use strict';\n\nvar utils = require('../utils');\n\nmodule.exports = function normalizeHeaderName(headers, normalizedName) {\n  utils.forEach(headers, function processHeader(value, name) {\n    if (name !== normalizedName && name.toUpperCase() === normalizedName.toUpperCase()) {\n      headers[normalizedName] = value;\n      delete headers[name];\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n// Headers whose duplicates are ignored by node\n// c.f. https://nodejs.org/api/http.html#http_message_headers\nvar ignoreDuplicateOf = [\n  'age', 'authorization', 'content-length', 'content-type', 'etag',\n  'expires', 'from', 'host', 'if-modified-since', 'if-unmodified-since',\n  'last-modified', 'location', 'max-forwards', 'proxy-authorization',\n  'referer', 'retry-after', 'user-agent'\n];\n\n/**\n * Parse headers into an object\n *\n * ```\n * Date: Wed, 27 Aug 2014 08:58:49 GMT\n * Content-Type: application/json\n * Connection: keep-alive\n * Transfer-Encoding: chunked\n * ```\n *\n * @param {String} headers Headers needing to be parsed\n * @returns {Object} Headers parsed into an object\n */\nmodule.exports = function parseHeaders(headers) {\n  var parsed = {};\n  var key;\n  var val;\n  var i;\n\n  if (!headers) { return parsed; }\n\n  utils.forEach(headers.split('\\n'), function parser(line) {\n    i = line.indexOf(':');\n    key = utils.trim(line.substr(0, i)).toLowerCase();\n    val = utils.trim(line.substr(i + 1));\n\n    if (key) {\n      if (parsed[key] && ignoreDuplicateOf.indexOf(key) >= 0) {\n        return;\n      }\n      if (key === 'set-cookie') {\n        parsed[key] = (parsed[key] ? parsed[key] : []).concat([val]);\n      } else {\n        parsed[key] = parsed[key] ? parsed[key] + ', ' + val : val;\n      }\n    }\n  });\n\n  return parsed;\n};\n","'use strict';\n\n/**\n * Syntactic sugar for invoking a function and expanding an array for arguments.\n *\n * Common use case would be to use `Function.prototype.apply`.\n *\n *  ```js\n *  function f(x, y, z) {}\n *  var args = [1, 2, 3];\n *  f.apply(null, args);\n *  ```\n *\n * With `spread` this example can be re-written.\n *\n *  ```js\n *  spread(function(x, y, z) {})([1, 2, 3]);\n *  ```\n *\n * @param {Function} callback\n * @returns {Function}\n */\nmodule.exports = function spread(callback) {\n  return function wrap(arr) {\n    return callback.apply(null, arr);\n  };\n};\n","'use strict';\n\nvar bind = require('./helpers/bind');\n\n/*global toString:true*/\n\n// utils is a library of generic helper functions non-specific to axios\n\nvar toString = Object.prototype.toString;\n\n/**\n * Determine if a value is an Array\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Array, otherwise false\n */\nfunction isArray(val) {\n  return toString.call(val) === '[object Array]';\n}\n\n/**\n * Determine if a value is undefined\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if the value is undefined, otherwise false\n */\nfunction isUndefined(val) {\n  return typeof val === 'undefined';\n}\n\n/**\n * Determine if a value is a Buffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Buffer, otherwise false\n */\nfunction isBuffer(val) {\n  return val !== null && !isUndefined(val) && val.constructor !== null && !isUndefined(val.constructor)\n    && typeof val.constructor.isBuffer === 'function' && val.constructor.isBuffer(val);\n}\n\n/**\n * Determine if a value is an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an ArrayBuffer, otherwise false\n */\nfunction isArrayBuffer(val) {\n  return toString.call(val) === '[object ArrayBuffer]';\n}\n\n/**\n * Determine if a value is a FormData\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an FormData, otherwise false\n */\nfunction isFormData(val) {\n  return (typeof FormData !== 'undefined') && (val instanceof FormData);\n}\n\n/**\n * Determine if a value is a view on an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a view on an ArrayBuffer, otherwise false\n */\nfunction isArrayBufferView(val) {\n  var result;\n  if ((typeof ArrayBuffer !== 'undefined') && (ArrayBuffer.isView)) {\n    result = ArrayBuffer.isView(val);\n  } else {\n    result = (val) && (val.buffer) && (val.buffer instanceof ArrayBuffer);\n  }\n  return result;\n}\n\n/**\n * Determine if a value is a String\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a String, otherwise false\n */\nfunction isString(val) {\n  return typeof val === 'string';\n}\n\n/**\n * Determine if a value is a Number\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Number, otherwise false\n */\nfunction isNumber(val) {\n  return typeof val === 'number';\n}\n\n/**\n * Determine if a value is an Object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Object, otherwise false\n */\nfunction isObject(val) {\n  return val !== null && typeof val === 'object';\n}\n\n/**\n * Determine if a value is a plain Object\n *\n * @param {Object} val The value to test\n * @return {boolean} True if value is a plain Object, otherwise false\n */\nfunction isPlainObject(val) {\n  if (toString.call(val) !== '[object Object]') {\n    return false;\n  }\n\n  var prototype = Object.getPrototypeOf(val);\n  return prototype === null || prototype === Object.prototype;\n}\n\n/**\n * Determine if a value is a Date\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Date, otherwise false\n */\nfunction isDate(val) {\n  return toString.call(val) === '[object Date]';\n}\n\n/**\n * Determine if a value is a File\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a File, otherwise false\n */\nfunction isFile(val) {\n  return toString.call(val) === '[object File]';\n}\n\n/**\n * Determine if a value is a Blob\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Blob, otherwise false\n */\nfunction isBlob(val) {\n  return toString.call(val) === '[object Blob]';\n}\n\n/**\n * Determine if a value is a Function\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Function, otherwise false\n */\nfunction isFunction(val) {\n  return toString.call(val) === '[object Function]';\n}\n\n/**\n * Determine if a value is a Stream\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Stream, otherwise false\n */\nfunction isStream(val) {\n  return isObject(val) && isFunction(val.pipe);\n}\n\n/**\n * Determine if a value is a URLSearchParams object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a URLSearchParams object, otherwise false\n */\nfunction isURLSearchParams(val) {\n  return typeof URLSearchParams !== 'undefined' && val instanceof URLSearchParams;\n}\n\n/**\n * Trim excess whitespace off the beginning and end of a string\n *\n * @param {String} str The String to trim\n * @returns {String} The String freed of excess whitespace\n */\nfunction trim(str) {\n  return str.replace(/^\\s*/, '').replace(/\\s*$/, '');\n}\n\n/**\n * Determine if we're running in a standard browser environment\n *\n * This allows axios to run in a web worker, and react-native.\n * Both environments support XMLHttpRequest, but not fully standard globals.\n *\n * web workers:\n *  typeof window -> undefined\n *  typeof document -> undefined\n *\n * react-native:\n *  navigator.product -> 'ReactNative'\n * nativescript\n *  navigator.product -> 'NativeScript' or 'NS'\n */\nfunction isStandardBrowserEnv() {\n  if (typeof navigator !== 'undefined' && (navigator.product === 'ReactNative' ||\n                                           navigator.product === 'NativeScript' ||\n                                           navigator.product === 'NS')) {\n    return false;\n  }\n  return (\n    typeof window !== 'undefined' &&\n    typeof document !== 'undefined'\n  );\n}\n\n/**\n * Iterate over an Array or an Object invoking a function for each item.\n *\n * If `obj` is an Array callback will be called passing\n * the value, index, and complete array for each item.\n *\n * If 'obj' is an Object callback will be called passing\n * the value, key, and complete object for each property.\n *\n * @param {Object|Array} obj The object to iterate\n * @param {Function} fn The callback to invoke for each item\n */\nfunction forEach(obj, fn) {\n  // Don't bother if no value provided\n  if (obj === null || typeof obj === 'undefined') {\n    return;\n  }\n\n  // Force an array if not already something iterable\n  if (typeof obj !== 'object') {\n    /*eslint no-param-reassign:0*/\n    obj = [obj];\n  }\n\n  if (isArray(obj)) {\n    // Iterate over array values\n    for (var i = 0, l = obj.length; i < l; i++) {\n      fn.call(null, obj[i], i, obj);\n    }\n  } else {\n    // Iterate over object keys\n    for (var key in obj) {\n      if (Object.prototype.hasOwnProperty.call(obj, key)) {\n        fn.call(null, obj[key], key, obj);\n      }\n    }\n  }\n}\n\n/**\n * Accepts varargs expecting each argument to be an object, then\n * immutably merges the properties of each object and returns result.\n *\n * When multiple objects contain the same key the later object in\n * the arguments list will take precedence.\n *\n * Example:\n *\n * ```js\n * var result = merge({foo: 123}, {foo: 456});\n * console.log(result.foo); // outputs 456\n * ```\n *\n * @param {Object} obj1 Object to merge\n * @returns {Object} Result of all merge properties\n */\nfunction merge(/* obj1, obj2, obj3, ... */) {\n  var result = {};\n  function assignValue(val, key) {\n    if (isPlainObject(result[key]) && isPlainObject(val)) {\n      result[key] = merge(result[key], val);\n    } else if (isPlainObject(val)) {\n      result[key] = merge({}, val);\n    } else if (isArray(val)) {\n      result[key] = val.slice();\n    } else {\n      result[key] = val;\n    }\n  }\n\n  for (var i = 0, l = arguments.length; i < l; i++) {\n    forEach(arguments[i], assignValue);\n  }\n  return result;\n}\n\n/**\n * Extends object a by mutably adding to it the properties of object b.\n *\n * @param {Object} a The object to be extended\n * @param {Object} b The object to copy properties from\n * @param {Object} thisArg The object to bind function to\n * @return {Object} The resulting value of object a\n */\nfunction extend(a, b, thisArg) {\n  forEach(b, function assignValue(val, key) {\n    if (thisArg && typeof val === 'function') {\n      a[key] = bind(val, thisArg);\n    } else {\n      a[key] = val;\n    }\n  });\n  return a;\n}\n\n/**\n * Remove byte order marker. This catches EF BB BF (the UTF-8 BOM)\n *\n * @param {string} content with BOM\n * @return {string} content value without BOM\n */\nfunction stripBOM(content) {\n  if (content.charCodeAt(0) === 0xFEFF) {\n    content = content.slice(1);\n  }\n  return content;\n}\n\nmodule.exports = {\n  isArray: isArray,\n  isArrayBuffer: isArrayBuffer,\n  isBuffer: isBuffer,\n  isFormData: isFormData,\n  isArrayBufferView: isArrayBufferView,\n  isString: isString,\n  isNumber: isNumber,\n  isObject: isObject,\n  isPlainObject: isPlainObject,\n  isUndefined: isUndefined,\n  isDate: isDate,\n  isFile: isFile,\n  isBlob: isBlob,\n  isFunction: isFunction,\n  isStream: isStream,\n  isURLSearchParams: isURLSearchParams,\n  isStandardBrowserEnv: isStandardBrowserEnv,\n  forEach: forEach,\n  merge: merge,\n  extend: extend,\n  trim: trim,\n  stripBOM: stripBOM\n};\n","/**\n * Helpers.\n */\n\nvar s = 1000;\nvar m = s * 60;\nvar h = m * 60;\nvar d = h * 24;\nvar y = d * 365.25;\n\n/**\n * Parse or format the given `val`.\n *\n * Options:\n *\n *  - `long` verbose formatting [false]\n *\n * @param {String|Number} val\n * @param {Object} [options]\n * @throws {Error} throw an error if val is not a non-empty string or a number\n * @return {String|Number}\n * @api public\n */\n\nmodule.exports = function(val, options) {\n  options = options || {};\n  var type = typeof val;\n  if (type === 'string' && val.length > 0) {\n    return parse(val);\n  } else if (type === 'number' && isNaN(val) === false) {\n    return options.long ? fmtLong(val) : fmtShort(val);\n  }\n  throw new Error(\n    'val is not a non-empty string or a valid number. val=' +\n      JSON.stringify(val)\n  );\n};\n\n/**\n * Parse the given `str` and return milliseconds.\n *\n * @param {String} str\n * @return {Number}\n * @api private\n */\n\nfunction parse(str) {\n  str = String(str);\n  if (str.length > 100) {\n    return;\n  }\n  var match = /^((?:\\d+)?\\.?\\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|years?|yrs?|y)?$/i.exec(\n    str\n  );\n  if (!match) {\n    return;\n  }\n  var n = parseFloat(match[1]);\n  var type = (match[2] || 'ms').toLowerCase();\n  switch (type) {\n    case 'years':\n    case 'year':\n    case 'yrs':\n    case 'yr':\n    case 'y':\n      return n * y;\n    case 'days':\n    case 'day':\n    case 'd':\n      return n * d;\n    case 'hours':\n    case 'hour':\n    case 'hrs':\n    case 'hr':\n    case 'h':\n      return n * h;\n    case 'minutes':\n    case 'minute':\n    case 'mins':\n    case 'min':\n    case 'm':\n      return n * m;\n    case 'seconds':\n    case 'second':\n    case 'secs':\n    case 'sec':\n    case 's':\n      return n * s;\n    case 'milliseconds':\n    case 'millisecond':\n    case 'msecs':\n    case 'msec':\n    case 'ms':\n      return n;\n    default:\n      return undefined;\n  }\n}\n\n/**\n * Short format for `ms`.\n *\n * @param {Number} ms\n * @return {String}\n * @api private\n */\n\nfunction fmtShort(ms) {\n  if (ms >= d) {\n    return Math.round(ms / d) + 'd';\n  }\n  if (ms >= h) {\n    return Math.round(ms / h) + 'h';\n  }\n  if (ms >= m) {\n    return Math.round(ms / m) + 'm';\n  }\n  if (ms >= s) {\n    return Math.round(ms / s) + 's';\n  }\n  return ms + 'ms';\n}\n\n/**\n * Long format for `ms`.\n *\n * @param {Number} ms\n * @return {String}\n * @api private\n */\n\nfunction fmtLong(ms) {\n  return plural(ms, d, 'day') ||\n    plural(ms, h, 'hour') ||\n    plural(ms, m, 'minute') ||\n    plural(ms, s, 'second') ||\n    ms + ' ms';\n}\n\n/**\n * Pluralization helper.\n */\n\nfunction plural(ms, n, name) {\n  if (ms < n) {\n    return;\n  }\n  if (ms < n * 1.5) {\n    return Math.floor(ms / n) + ' ' + name;\n  }\n  return Math.ceil(ms / n) + ' ' + name + 's';\n}\n","/**\n * This is the web browser implementation of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = require('./debug');\nexports.log = log;\nexports.formatArgs = formatArgs;\nexports.save = save;\nexports.load = load;\nexports.useColors = useColors;\nexports.storage = 'undefined' != typeof chrome\n               && 'undefined' != typeof chrome.storage\n                  ? chrome.storage.local\n                  : localstorage();\n\n/**\n * Colors.\n */\n\nexports.colors = [\n  'lightseagreen',\n  'forestgreen',\n  'goldenrod',\n  'dodgerblue',\n  'darkorchid',\n  'crimson'\n];\n\n/**\n * Currently only WebKit-based Web Inspectors, Firefox >= v31,\n * and the Firebug extension (any Firefox version) are known\n * to support \"%c\" CSS customizations.\n *\n * TODO: add a `localStorage` variable to explicitly enable/disable colors\n */\n\nfunction useColors() {\n  // NB: In an Electron preload script, document will be defined but not fully\n  // initialized. Since we know we're in Chrome, we'll just detect this case\n  // explicitly\n  if (typeof window !== 'undefined' && window.process && window.process.type === 'renderer') {\n    return true;\n  }\n\n  // is webkit? http://stackoverflow.com/a/16459606/376773\n  // document is undefined in react-native: https://github.com/facebook/react-native/pull/1632\n  return (typeof document !== 'undefined' && document.documentElement && document.documentElement.style && document.documentElement.style.WebkitAppearance) ||\n    // is firebug? http://stackoverflow.com/a/398120/376773\n    (typeof window !== 'undefined' && window.console && (window.console.firebug || (window.console.exception && window.console.table))) ||\n    // is firefox >= v31?\n    // https://developer.mozilla.org/en-US/docs/Tools/Web_Console#Styling_messages\n    (typeof navigator !== 'undefined' && navigator.userAgent && navigator.userAgent.toLowerCase().match(/firefox\\/(\\d+)/) && parseInt(RegExp.$1, 10) >= 31) ||\n    // double check webkit in userAgent just in case we are in a worker\n    (typeof navigator !== 'undefined' && navigator.userAgent && navigator.userAgent.toLowerCase().match(/applewebkit\\/(\\d+)/));\n}\n\n/**\n * Map %j to `JSON.stringify()`, since no Web Inspectors do that by default.\n */\n\nexports.formatters.j = function(v) {\n  try {\n    return JSON.stringify(v);\n  } catch (err) {\n    return '[UnexpectedJSONParseError]: ' + err.message;\n  }\n};\n\n\n/**\n * Colorize log arguments if enabled.\n *\n * @api public\n */\n\nfunction formatArgs(args) {\n  var useColors = this.useColors;\n\n  args[0] = (useColors ? '%c' : '')\n    + this.namespace\n    + (useColors ? ' %c' : ' ')\n    + args[0]\n    + (useColors ? '%c ' : ' ')\n    + '+' + exports.humanize(this.diff);\n\n  if (!useColors) return;\n\n  var c = 'color: ' + this.color;\n  args.splice(1, 0, c, 'color: inherit')\n\n  // the final \"%c\" is somewhat tricky, because there could be other\n  // arguments passed either before or after the %c, so we need to\n  // figure out the correct index to insert the CSS into\n  var index = 0;\n  var lastC = 0;\n  args[0].replace(/%[a-zA-Z%]/g, function(match) {\n    if ('%%' === match) return;\n    index++;\n    if ('%c' === match) {\n      // we only are interested in the *last* %c\n      // (the user may have provided their own)\n      lastC = index;\n    }\n  });\n\n  args.splice(lastC, 0, c);\n}\n\n/**\n * Invokes `console.log()` when available.\n * No-op when `console.log` is not a \"function\".\n *\n * @api public\n */\n\nfunction log() {\n  // this hackery is required for IE8/9, where\n  // the `console.log` function doesn't have 'apply'\n  return 'object' === typeof console\n    && console.log\n    && Function.prototype.apply.call(console.log, console, arguments);\n}\n\n/**\n * Save `namespaces`.\n *\n * @param {String} namespaces\n * @api private\n */\n\nfunction save(namespaces) {\n  try {\n    if (null == namespaces) {\n      exports.storage.removeItem('debug');\n    } else {\n      exports.storage.debug = namespaces;\n    }\n  } catch(e) {}\n}\n\n/**\n * Load `namespaces`.\n *\n * @return {String} returns the previously persisted debug modes\n * @api private\n */\n\nfunction load() {\n  var r;\n  try {\n    r = exports.storage.debug;\n  } catch(e) {}\n\n  // If debug isn't set in LS, and we're in Electron, try to load $DEBUG\n  if (!r && typeof process !== 'undefined' && 'env' in process) {\n    r = process.env.DEBUG;\n  }\n\n  return r;\n}\n\n/**\n * Enable namespaces listed in `localStorage.debug` initially.\n */\n\nexports.enable(load());\n\n/**\n * Localstorage attempts to return the localstorage.\n *\n * This is necessary because safari throws\n * when a user disables cookies/localstorage\n * and you attempt to access it.\n *\n * @return {LocalStorage}\n * @api private\n */\n\nfunction localstorage() {\n  try {\n    return window.localStorage;\n  } catch (e) {}\n}\n","\n/**\n * This is the common logic for both the Node.js and web browser\n * implementations of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = createDebug.debug = createDebug['default'] = createDebug;\nexports.coerce = coerce;\nexports.disable = disable;\nexports.enable = enable;\nexports.enabled = enabled;\nexports.humanize = require('ms');\n\n/**\n * The currently active debug mode names, and names to skip.\n */\n\nexports.names = [];\nexports.skips = [];\n\n/**\n * Map of special \"%n\" handling functions, for the debug \"format\" argument.\n *\n * Valid key names are a single, lower or upper-case letter, i.e. \"n\" and \"N\".\n */\n\nexports.formatters = {};\n\n/**\n * Previous log timestamp.\n */\n\nvar prevTime;\n\n/**\n * Select a color.\n * @param {String} namespace\n * @return {Number}\n * @api private\n */\n\nfunction selectColor(namespace) {\n  var hash = 0, i;\n\n  for (i in namespace) {\n    hash  = ((hash << 5) - hash) + namespace.charCodeAt(i);\n    hash |= 0; // Convert to 32bit integer\n  }\n\n  return exports.colors[Math.abs(hash) % exports.colors.length];\n}\n\n/**\n * Create a debugger with the given `namespace`.\n *\n * @param {String} namespace\n * @return {Function}\n * @api public\n */\n\nfunction createDebug(namespace) {\n\n  function debug() {\n    // disabled?\n    if (!debug.enabled) return;\n\n    var self = debug;\n\n    // set `diff` timestamp\n    var curr = +new Date();\n    var ms = curr - (prevTime || curr);\n    self.diff = ms;\n    self.prev = prevTime;\n    self.curr = curr;\n    prevTime = curr;\n\n    // turn the `arguments` into a proper Array\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n\n    args[0] = exports.coerce(args[0]);\n\n    if ('string' !== typeof args[0]) {\n      // anything else let's inspect with %O\n      args.unshift('%O');\n    }\n\n    // apply any `formatters` transformations\n    var index = 0;\n    args[0] = args[0].replace(/%([a-zA-Z%])/g, function(match, format) {\n      // if we encounter an escaped % then don't increase the array index\n      if (match === '%%') return match;\n      index++;\n      var formatter = exports.formatters[format];\n      if ('function' === typeof formatter) {\n        var val = args[index];\n        match = formatter.call(self, val);\n\n        // now we need to remove `args[index]` since it's inlined in the `format`\n        args.splice(index, 1);\n        index--;\n      }\n      return match;\n    });\n\n    // apply env-specific formatting (colors, etc.)\n    exports.formatArgs.call(self, args);\n\n    var logFn = debug.log || exports.log || console.log.bind(console);\n    logFn.apply(self, args);\n  }\n\n  debug.namespace = namespace;\n  debug.enabled = exports.enabled(namespace);\n  debug.useColors = exports.useColors();\n  debug.color = selectColor(namespace);\n\n  // env-specific initialization logic for debug instances\n  if ('function' === typeof exports.init) {\n    exports.init(debug);\n  }\n\n  return debug;\n}\n\n/**\n * Enables a debug mode by namespaces. This can include modes\n * separated by a colon and wildcards.\n *\n * @param {String} namespaces\n * @api public\n */\n\nfunction enable(namespaces) {\n  exports.save(namespaces);\n\n  exports.names = [];\n  exports.skips = [];\n\n  var split = (typeof namespaces === 'string' ? namespaces : '').split(/[\\s,]+/);\n  var len = split.length;\n\n  for (var i = 0; i < len; i++) {\n    if (!split[i]) continue; // ignore empty strings\n    namespaces = split[i].replace(/\\*/g, '.*?');\n    if (namespaces[0] === '-') {\n      exports.skips.push(new RegExp('^' + namespaces.substr(1) + '$'));\n    } else {\n      exports.names.push(new RegExp('^' + namespaces + '$'));\n    }\n  }\n}\n\n/**\n * Disable debug output.\n *\n * @api public\n */\n\nfunction disable() {\n  exports.enable('');\n}\n\n/**\n * Returns true if the given mode name is enabled, false otherwise.\n *\n * @param {String} name\n * @return {Boolean}\n * @api public\n */\n\nfunction enabled(name) {\n  var i, len;\n  for (i = 0, len = exports.skips.length; i < len; i++) {\n    if (exports.skips[i].test(name)) {\n      return false;\n    }\n  }\n  for (i = 0, len = exports.names.length; i < len; i++) {\n    if (exports.names[i].test(name)) {\n      return true;\n    }\n  }\n  return false;\n}\n\n/**\n * Coerce `val`.\n *\n * @param {Mixed} val\n * @return {Mixed}\n * @api private\n */\n\nfunction coerce(val) {\n  if (val instanceof Error) return val.stack || val.message;\n  return val;\n}\n","/**\n * Detect Electron renderer process, which is node, but we should\n * treat as a browser.\n */\n\nif (typeof process !== 'undefined' && process.type === 'renderer') {\n  module.exports = require('./browser.js');\n} else {\n  module.exports = require('./node.js');\n}\n","/**\n * Module dependencies.\n */\n\nvar tty = require('tty');\nvar util = require('util');\n\n/**\n * This is the Node.js implementation of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = require('./debug');\nexports.init = init;\nexports.log = log;\nexports.formatArgs = formatArgs;\nexports.save = save;\nexports.load = load;\nexports.useColors = useColors;\n\n/**\n * Colors.\n */\n\nexports.colors = [6, 2, 3, 4, 5, 1];\n\n/**\n * Build up the default `inspectOpts` object from the environment variables.\n *\n *   $ DEBUG_COLORS=no DEBUG_DEPTH=10 DEBUG_SHOW_HIDDEN=enabled node script.js\n */\n\nexports.inspectOpts = Object.keys(process.env).filter(function (key) {\n  return /^debug_/i.test(key);\n}).reduce(function (obj, key) {\n  // camel-case\n  var prop = key\n    .substring(6)\n    .toLowerCase()\n    .replace(/_([a-z])/g, function (_, k) { return k.toUpperCase() });\n\n  // coerce string value into JS value\n  var val = process.env[key];\n  if (/^(yes|on|true|enabled)$/i.test(val)) val = true;\n  else if (/^(no|off|false|disabled)$/i.test(val)) val = false;\n  else if (val === 'null') val = null;\n  else val = Number(val);\n\n  obj[prop] = val;\n  return obj;\n}, {});\n\n/**\n * The file descriptor to write the `debug()` calls to.\n * Set the `DEBUG_FD` env variable to override with another value. i.e.:\n *\n *   $ DEBUG_FD=3 node script.js 3>debug.log\n */\n\nvar fd = parseInt(process.env.DEBUG_FD, 10) || 2;\n\nif (1 !== fd && 2 !== fd) {\n  util.deprecate(function(){}, 'except for stderr(2) and stdout(1), any other usage of DEBUG_FD is deprecated. Override debug.log if you want to use a different log function (https://git.io/debug_fd)')()\n}\n\nvar stream = 1 === fd ? process.stdout :\n             2 === fd ? process.stderr :\n             createWritableStdioStream(fd);\n\n/**\n * Is stdout a TTY? Colored output is enabled when `true`.\n */\n\nfunction useColors() {\n  return 'colors' in exports.inspectOpts\n    ? Boolean(exports.inspectOpts.colors)\n    : tty.isatty(fd);\n}\n\n/**\n * Map %o to `util.inspect()`, all on a single line.\n */\n\nexports.formatters.o = function(v) {\n  this.inspectOpts.colors = this.useColors;\n  return util.inspect(v, this.inspectOpts)\n    .split('\\n').map(function(str) {\n      return str.trim()\n    }).join(' ');\n};\n\n/**\n * Map %o to `util.inspect()`, allowing multiple lines if needed.\n */\n\nexports.formatters.O = function(v) {\n  this.inspectOpts.colors = this.useColors;\n  return util.inspect(v, this.inspectOpts);\n};\n\n/**\n * Adds ANSI color escape codes if enabled.\n *\n * @api public\n */\n\nfunction formatArgs(args) {\n  var name = this.namespace;\n  var useColors = this.useColors;\n\n  if (useColors) {\n    var c = this.color;\n    var prefix = '  \\u001b[3' + c + ';1m' + name + ' ' + '\\u001b[0m';\n\n    args[0] = prefix + args[0].split('\\n').join('\\n' + prefix);\n    args.push('\\u001b[3' + c + 'm+' + exports.humanize(this.diff) + '\\u001b[0m');\n  } else {\n    args[0] = new Date().toUTCString()\n      + ' ' + name + ' ' + args[0];\n  }\n}\n\n/**\n * Invokes `util.format()` with the specified arguments and writes to `stream`.\n */\n\nfunction log() {\n  return stream.write(util.format.apply(util, arguments) + '\\n');\n}\n\n/**\n * Save `namespaces`.\n *\n * @param {String} namespaces\n * @api private\n */\n\nfunction save(namespaces) {\n  if (null == namespaces) {\n    // If you set a process.env field to null or undefined, it gets cast to the\n    // string 'null' or 'undefined'. Just delete instead.\n    delete process.env.DEBUG;\n  } else {\n    process.env.DEBUG = namespaces;\n  }\n}\n\n/**\n * Load `namespaces`.\n *\n * @return {String} returns the previously persisted debug modes\n * @api private\n */\n\nfunction load() {\n  return process.env.DEBUG;\n}\n\n/**\n * Copied from `node/src/node.js`.\n *\n * XXX: It's lame that node doesn't expose this API out-of-the-box. It also\n * relies on the undocumented `tty_wrap.guessHandleType()` which is also lame.\n */\n\nfunction createWritableStdioStream (fd) {\n  var stream;\n  var tty_wrap = process.binding('tty_wrap');\n\n  // Note stream._type is used for test-module-load-list.js\n\n  switch (tty_wrap.guessHandleType(fd)) {\n    case 'TTY':\n      stream = new tty.WriteStream(fd);\n      stream._type = 'tty';\n\n      // Hack to have stream not keep the event loop alive.\n      // See https://github.com/joyent/node/issues/1726\n      if (stream._handle && stream._handle.unref) {\n        stream._handle.unref();\n      }\n      break;\n\n    case 'FILE':\n      var fs = require('fs');\n      stream = new fs.SyncWriteStream(fd, { autoClose: false });\n      stream._type = 'fs';\n      break;\n\n    case 'PIPE':\n    case 'TCP':\n      var net = require('net');\n      stream = new net.Socket({\n        fd: fd,\n        readable: false,\n        writable: true\n      });\n\n      // FIXME Should probably have an option in net.Socket to create a\n      // stream from an existing fd which is writable only. But for now\n      // we'll just add this hack and set the `readable` member to false.\n      // Test: ./node test/fixtures/echo.js < /etc/passwd\n      stream.readable = false;\n      stream.read = null;\n      stream._type = 'pipe';\n\n      // FIXME Hack to have stream not keep the event loop alive.\n      // See https://github.com/joyent/node/issues/1726\n      if (stream._handle && stream._handle.unref) {\n        stream._handle.unref();\n      }\n      break;\n\n    default:\n      // Probably an error on in uv_guess_handle()\n      throw new Error('Implement me. Unknown stream file type!');\n  }\n\n  // For supporting legacy API we put the FD here.\n  stream.fd = fd;\n\n  stream._isStdio = true;\n\n  return stream;\n}\n\n/**\n * Init logic for `debug` instances.\n *\n * Create a new `inspectOpts` object in case `useColors` is set\n * differently for a particular `debug` instance.\n */\n\nfunction init (debug) {\n  debug.inspectOpts = {};\n\n  var keys = Object.keys(exports.inspectOpts);\n  for (var i = 0; i < keys.length; i++) {\n    debug.inspectOpts[keys[i]] = exports.inspectOpts[keys[i]];\n  }\n}\n\n/**\n * Enable namespaces listed in `process.env.DEBUG` initially.\n */\n\nexports.enable(load());\n","var debug;\ntry {\n  /* eslint global-require: off */\n  debug = require(\"debug\")(\"follow-redirects\");\n}\ncatch (error) {\n  debug = function () { /* */ };\n}\nmodule.exports = debug;\n","var url = require(\"url\");\nvar URL = url.URL;\nvar http = require(\"http\");\nvar https = require(\"https\");\nvar Writable = require(\"stream\").Writable;\nvar assert = require(\"assert\");\nvar debug = require(\"./debug\");\n\n// Create handlers that pass events from native requests\nvar eventHandlers = Object.create(null);\n[\"abort\", \"aborted\", \"connect\", \"error\", \"socket\", \"timeout\"].forEach(function (event) {\n  eventHandlers[event] = function (arg1, arg2, arg3) {\n    this._redirectable.emit(event, arg1, arg2, arg3);\n  };\n});\n\n// Error types with codes\nvar RedirectionError = createErrorType(\n  \"ERR_FR_REDIRECTION_FAILURE\",\n  \"\"\n);\nvar TooManyRedirectsError = createErrorType(\n  \"ERR_FR_TOO_MANY_REDIRECTS\",\n  \"Maximum number of redirects exceeded\"\n);\nvar MaxBodyLengthExceededError = createErrorType(\n  \"ERR_FR_MAX_BODY_LENGTH_EXCEEDED\",\n  \"Request body larger than maxBodyLength limit\"\n);\nvar WriteAfterEndError = createErrorType(\n  \"ERR_STREAM_WRITE_AFTER_END\",\n  \"write after end\"\n);\n\n// An HTTP(S) request that can be redirected\nfunction RedirectableRequest(options, responseCallback) {\n  // Initialize the request\n  Writable.call(this);\n  this._sanitizeOptions(options);\n  this._options = options;\n  this._ended = false;\n  this._ending = false;\n  this._redirectCount = 0;\n  this._redirects = [];\n  this._requestBodyLength = 0;\n  this._requestBodyBuffers = [];\n\n  // Attach a callback if passed\n  if (responseCallback) {\n    this.on(\"response\", responseCallback);\n  }\n\n  // React to responses of native requests\n  var self = this;\n  this._onNativeResponse = function (response) {\n    self._processResponse(response);\n  };\n\n  // Perform the first request\n  this._performRequest();\n}\nRedirectableRequest.prototype = Object.create(Writable.prototype);\n\n// Writes buffered data to the current native request\nRedirectableRequest.prototype.write = function (data, encoding, callback) {\n  // Writing is not allowed if end has been called\n  if (this._ending) {\n    throw new WriteAfterEndError();\n  }\n\n  // Validate input and shift parameters if necessary\n  if (!(typeof data === \"string\" || typeof data === \"object\" && (\"length\" in data))) {\n    throw new TypeError(\"data should be a string, Buffer or Uint8Array\");\n  }\n  if (typeof encoding === \"function\") {\n    callback = encoding;\n    encoding = null;\n  }\n\n  // Ignore empty buffers, since writing them doesn't invoke the callback\n  // https://github.com/nodejs/node/issues/22066\n  if (data.length === 0) {\n    if (callback) {\n      callback();\n    }\n    return;\n  }\n  // Only write when we don't exceed the maximum body length\n  if (this._requestBodyLength + data.length <= this._options.maxBodyLength) {\n    this._requestBodyLength += data.length;\n    this._requestBodyBuffers.push({ data: data, encoding: encoding });\n    this._currentRequest.write(data, encoding, callback);\n  }\n  // Error when we exceed the maximum body length\n  else {\n    this.emit(\"error\", new MaxBodyLengthExceededError());\n    this.abort();\n  }\n};\n\n// Ends the current native request\nRedirectableRequest.prototype.end = function (data, encoding, callback) {\n  // Shift parameters if necessary\n  if (typeof data === \"function\") {\n    callback = data;\n    data = encoding = null;\n  }\n  else if (typeof encoding === \"function\") {\n    callback = encoding;\n    encoding = null;\n  }\n\n  // Write data if needed and end\n  if (!data) {\n    this._ended = this._ending = true;\n    this._currentRequest.end(null, null, callback);\n  }\n  else {\n    var self = this;\n    var currentRequest = this._currentRequest;\n    this.write(data, encoding, function () {\n      self._ended = true;\n      currentRequest.end(null, null, callback);\n    });\n    this._ending = true;\n  }\n};\n\n// Sets a header value on the current native request\nRedirectableRequest.prototype.setHeader = function (name, value) {\n  this._options.headers[name] = value;\n  this._currentRequest.setHeader(name, value);\n};\n\n// Clears a header value on the current native request\nRedirectableRequest.prototype.removeHeader = function (name) {\n  delete this._options.headers[name];\n  this._currentRequest.removeHeader(name);\n};\n\n// Global timeout for all underlying requests\nRedirectableRequest.prototype.setTimeout = function (msecs, callback) {\n  if (callback) {\n    this.once(\"timeout\", callback);\n  }\n\n  if (this.socket) {\n    startTimer(this, msecs);\n  }\n  else {\n    var self = this;\n    this._currentRequest.once(\"socket\", function () {\n      startTimer(self, msecs);\n    });\n  }\n\n  this.once(\"response\", clearTimer);\n  this.once(\"error\", clearTimer);\n\n  return this;\n};\n\nfunction startTimer(request, msecs) {\n  clearTimeout(request._timeout);\n  request._timeout = setTimeout(function () {\n    request.emit(\"timeout\");\n  }, msecs);\n}\n\nfunction clearTimer() {\n  clearTimeout(this._timeout);\n}\n\n// Proxy all other public ClientRequest methods\n[\n  \"abort\", \"flushHeaders\", \"getHeader\",\n  \"setNoDelay\", \"setSocketKeepAlive\",\n].forEach(function (method) {\n  RedirectableRequest.prototype[method] = function (a, b) {\n    return this._currentRequest[method](a, b);\n  };\n});\n\n// Proxy all public ClientRequest properties\n[\"aborted\", \"connection\", \"socket\"].forEach(function (property) {\n  Object.defineProperty(RedirectableRequest.prototype, property, {\n    get: function () { return this._currentRequest[property]; },\n  });\n});\n\nRedirectableRequest.prototype._sanitizeOptions = function (options) {\n  // Ensure headers are always present\n  if (!options.headers) {\n    options.headers = {};\n  }\n\n  // Since http.request treats host as an alias of hostname,\n  // but the url module interprets host as hostname plus port,\n  // eliminate the host property to avoid confusion.\n  if (options.host) {\n    // Use hostname if set, because it has precedence\n    if (!options.hostname) {\n      options.hostname = options.host;\n    }\n    delete options.host;\n  }\n\n  // Complete the URL object when necessary\n  if (!options.pathname && options.path) {\n    var searchPos = options.path.indexOf(\"?\");\n    if (searchPos < 0) {\n      options.pathname = options.path;\n    }\n    else {\n      options.pathname = options.path.substring(0, searchPos);\n      options.search = options.path.substring(searchPos);\n    }\n  }\n};\n\n\n// Executes the next native request (initial or redirect)\nRedirectableRequest.prototype._performRequest = function () {\n  // Load the native protocol\n  var protocol = this._options.protocol;\n  var nativeProtocol = this._options.nativeProtocols[protocol];\n  if (!nativeProtocol) {\n    this.emit(\"error\", new TypeError(\"Unsupported protocol \" + protocol));\n    return;\n  }\n\n  // If specified, use the agent corresponding to the protocol\n  // (HTTP and HTTPS use different types of agents)\n  if (this._options.agents) {\n    var scheme = protocol.substr(0, protocol.length - 1);\n    this._options.agent = this._options.agents[scheme];\n  }\n\n  // Create the native request\n  var request = this._currentRequest =\n        nativeProtocol.request(this._options, this._onNativeResponse);\n  this._currentUrl = url.format(this._options);\n\n  // Set up event handlers\n  request._redirectable = this;\n  for (var event in eventHandlers) {\n    /* istanbul ignore else */\n    if (event) {\n      request.on(event, eventHandlers[event]);\n    }\n  }\n\n  // End a redirected request\n  // (The first request must be ended explicitly with RedirectableRequest#end)\n  if (this._isRedirect) {\n    // Write the request entity and end.\n    var i = 0;\n    var self = this;\n    var buffers = this._requestBodyBuffers;\n    (function writeNext(error) {\n      // Only write if this request has not been redirected yet\n      /* istanbul ignore else */\n      if (request === self._currentRequest) {\n        // Report any write errors\n        /* istanbul ignore if */\n        if (error) {\n          self.emit(\"error\", error);\n        }\n        // Write the next buffer if there are still left\n        else if (i < buffers.length) {\n          var buffer = buffers[i++];\n          /* istanbul ignore else */\n          if (!request.finished) {\n            request.write(buffer.data, buffer.encoding, writeNext);\n          }\n        }\n        // End the request if `end` has been called on us\n        else if (self._ended) {\n          request.end();\n        }\n      }\n    }());\n  }\n};\n\n// Processes a response from the current native request\nRedirectableRequest.prototype._processResponse = function (response) {\n  // Store the redirected response\n  var statusCode = response.statusCode;\n  if (this._options.trackRedirects) {\n    this._redirects.push({\n      url: this._currentUrl,\n      headers: response.headers,\n      statusCode: statusCode,\n    });\n  }\n\n  // RFC7231§6.4: The 3xx (Redirection) class of status code indicates\n  // that further action needs to be taken by the user agent in order to\n  // fulfill the request. If a Location header field is provided,\n  // the user agent MAY automatically redirect its request to the URI\n  // referenced by the Location field value,\n  // even if the specific status code is not understood.\n  var location = response.headers.location;\n  if (location && this._options.followRedirects !== false &&\n      statusCode >= 300 && statusCode < 400) {\n    // Abort the current request\n    this._currentRequest.removeAllListeners();\n    this._currentRequest.on(\"error\", noop);\n    this._currentRequest.abort();\n    // Discard the remainder of the response to avoid waiting for data\n    response.destroy();\n\n    // RFC7231§6.4: A client SHOULD detect and intervene\n    // in cyclical redirections (i.e., \"infinite\" redirection loops).\n    if (++this._redirectCount > this._options.maxRedirects) {\n      this.emit(\"error\", new TooManyRedirectsError());\n      return;\n    }\n\n    // RFC7231§6.4: Automatic redirection needs to done with\n    // care for methods not known to be safe, […]\n    // RFC7231§6.4.2–3: For historical reasons, a user agent MAY change\n    // the request method from POST to GET for the subsequent request.\n    if ((statusCode === 301 || statusCode === 302) && this._options.method === \"POST\" ||\n        // RFC7231§6.4.4: The 303 (See Other) status code indicates that\n        // the server is redirecting the user agent to a different resource […]\n        // A user agent can perform a retrieval request targeting that URI\n        // (a GET or HEAD request if using HTTP) […]\n        (statusCode === 303) && !/^(?:GET|HEAD)$/.test(this._options.method)) {\n      this._options.method = \"GET\";\n      // Drop a possible entity and headers related to it\n      this._requestBodyBuffers = [];\n      removeMatchingHeaders(/^content-/i, this._options.headers);\n    }\n\n    // Drop the Host header, as the redirect might lead to a different host\n    var previousHostName = removeMatchingHeaders(/^host$/i, this._options.headers) ||\n      url.parse(this._currentUrl).hostname;\n\n    // Create the redirected request\n    var redirectUrl = url.resolve(this._currentUrl, location);\n    debug(\"redirecting to\", redirectUrl);\n    this._isRedirect = true;\n    var redirectUrlParts = url.parse(redirectUrl);\n    Object.assign(this._options, redirectUrlParts);\n\n    // Drop the Authorization header if redirecting to another host\n    if (redirectUrlParts.hostname !== previousHostName) {\n      removeMatchingHeaders(/^authorization$/i, this._options.headers);\n    }\n\n    // Evaluate the beforeRedirect callback\n    if (typeof this._options.beforeRedirect === \"function\") {\n      var responseDetails = { headers: response.headers };\n      try {\n        this._options.beforeRedirect.call(null, this._options, responseDetails);\n      }\n      catch (err) {\n        this.emit(\"error\", err);\n        return;\n      }\n      this._sanitizeOptions(this._options);\n    }\n\n    // Perform the redirected request\n    try {\n      this._performRequest();\n    }\n    catch (cause) {\n      var error = new RedirectionError(\"Redirected request failed: \" + cause.message);\n      error.cause = cause;\n      this.emit(\"error\", error);\n    }\n  }\n  else {\n    // The response is not a redirect; return it as-is\n    response.responseUrl = this._currentUrl;\n    response.redirects = this._redirects;\n    this.emit(\"response\", response);\n\n    // Clean up\n    this._requestBodyBuffers = [];\n  }\n};\n\n// Wraps the key/value object of protocols with redirect functionality\nfunction wrap(protocols) {\n  // Default settings\n  var exports = {\n    maxRedirects: 21,\n    maxBodyLength: 10 * 1024 * 1024,\n  };\n\n  // Wrap each protocol\n  var nativeProtocols = {};\n  Object.keys(protocols).forEach(function (scheme) {\n    var protocol = scheme + \":\";\n    var nativeProtocol = nativeProtocols[protocol] = protocols[scheme];\n    var wrappedProtocol = exports[scheme] = Object.create(nativeProtocol);\n\n    // Executes a request, following redirects\n    wrappedProtocol.request = function (input, options, callback) {\n      // Parse parameters\n      if (typeof input === \"string\") {\n        var urlStr = input;\n        try {\n          input = urlToOptions(new URL(urlStr));\n        }\n        catch (err) {\n          /* istanbul ignore next */\n          input = url.parse(urlStr);\n        }\n      }\n      else if (URL && (input instanceof URL)) {\n        input = urlToOptions(input);\n      }\n      else {\n        callback = options;\n        options = input;\n        input = { protocol: protocol };\n      }\n      if (typeof options === \"function\") {\n        callback = options;\n        options = null;\n      }\n\n      // Set defaults\n      options = Object.assign({\n        maxRedirects: exports.maxRedirects,\n        maxBodyLength: exports.maxBodyLength,\n      }, input, options);\n      options.nativeProtocols = nativeProtocols;\n\n      assert.equal(options.protocol, protocol, \"protocol mismatch\");\n      debug(\"options\", options);\n      return new RedirectableRequest(options, callback);\n    };\n\n    // Executes a GET request, following redirects\n    wrappedProtocol.get = function (input, options, callback) {\n      var request = wrappedProtocol.request(input, options, callback);\n      request.end();\n      return request;\n    };\n  });\n  return exports;\n}\n\n/* istanbul ignore next */\nfunction noop() { /* empty */ }\n\n// from https://github.com/nodejs/node/blob/master/lib/internal/url.js\nfunction urlToOptions(urlObject) {\n  var options = {\n    protocol: urlObject.protocol,\n    hostname: urlObject.hostname.startsWith(\"[\") ?\n      /* istanbul ignore next */\n      urlObject.hostname.slice(1, -1) :\n      urlObject.hostname,\n    hash: urlObject.hash,\n    search: urlObject.search,\n    pathname: urlObject.pathname,\n    path: urlObject.pathname + urlObject.search,\n    href: urlObject.href,\n  };\n  if (urlObject.port !== \"\") {\n    options.port = Number(urlObject.port);\n  }\n  return options;\n}\n\nfunction removeMatchingHeaders(regex, headers) {\n  var lastValue;\n  for (var header in headers) {\n    if (regex.test(header)) {\n      lastValue = headers[header];\n      delete headers[header];\n    }\n  }\n  return lastValue;\n}\n\nfunction createErrorType(code, defaultMessage) {\n  function CustomError(message) {\n    Error.captureStackTrace(this, this.constructor);\n    this.message = message || defaultMessage;\n  }\n  CustomError.prototype = new Error();\n  CustomError.prototype.constructor = CustomError;\n  CustomError.prototype.name = \"Error [\" + code + \"]\";\n  CustomError.prototype.code = code;\n  return CustomError;\n}\n\n// Exports\nmodule.exports = wrap({ http: http, https: https });\nmodule.exports.wrap = wrap;\n","/*!\n * Determine if an object is a Buffer\n *\n * @author   Feross Aboukhadijeh <https://feross.org>\n * @license  MIT\n */\n\n// The _isBuffer check is for Safari 5-7 support, because it's missing\n// Object.prototype.constructor. Remove this eventually\nmodule.exports = function (obj) {\n  return obj != null && (isBuffer(obj) || isSlowBuffer(obj) || !!obj._isBuffer)\n}\n\nfunction isBuffer (obj) {\n  return !!obj.constructor && typeof obj.constructor.isBuffer === 'function' && obj.constructor.isBuffer(obj)\n}\n\n// For Node v0.10 support. Remove this eventually.\nfunction isSlowBuffer (obj) {\n  return typeof obj.readFloatLE === 'function' && typeof obj.slice === 'function' && isBuffer(obj.slice(0, 0))\n}\n","'use strict';\n\nvar WHITELIST = [\n\t'ETIMEDOUT',\n\t'ECONNRESET',\n\t'EADDRINUSE',\n\t'ESOCKETTIMEDOUT',\n\t'ECONNREFUSED',\n\t'EPIPE',\n\t'EHOSTUNREACH',\n\t'EAI_AGAIN'\n];\n\nvar BLACKLIST = [\n\t'ENOTFOUND',\n\t'ENETUNREACH',\n\n\t// SSL errors from https://github.com/nodejs/node/blob/ed3d8b13ee9a705d89f9e0397d9e96519e7e47ac/src/node_crypto.cc#L1950\n\t'UNABLE_TO_GET_ISSUER_CERT',\n\t'UNABLE_TO_GET_CRL',\n\t'UNABLE_TO_DECRYPT_CERT_SIGNATURE',\n\t'UNABLE_TO_DECRYPT_CRL_SIGNATURE',\n\t'UNABLE_TO_DECODE_ISSUER_PUBLIC_KEY',\n\t'CERT_SIGNATURE_FAILURE',\n\t'CRL_SIGNATURE_FAILURE',\n\t'CERT_NOT_YET_VALID',\n\t'CERT_HAS_EXPIRED',\n\t'CRL_NOT_YET_VALID',\n\t'CRL_HAS_EXPIRED',\n\t'ERROR_IN_CERT_NOT_BEFORE_FIELD',\n\t'ERROR_IN_CERT_NOT_AFTER_FIELD',\n\t'ERROR_IN_CRL_LAST_UPDATE_FIELD',\n\t'ERROR_IN_CRL_NEXT_UPDATE_FIELD',\n\t'OUT_OF_MEM',\n\t'DEPTH_ZERO_SELF_SIGNED_CERT',\n\t'SELF_SIGNED_CERT_IN_CHAIN',\n\t'UNABLE_TO_GET_ISSUER_CERT_LOCALLY',\n\t'UNABLE_TO_VERIFY_LEAF_SIGNATURE',\n\t'CERT_CHAIN_TOO_LONG',\n\t'CERT_REVOKED',\n\t'INVALID_CA',\n\t'PATH_LENGTH_EXCEEDED',\n\t'INVALID_PURPOSE',\n\t'CERT_UNTRUSTED',\n\t'CERT_REJECTED'\n];\n\nmodule.exports = function (err) {\n\tif (!err || !err.code) {\n\t\treturn true;\n\t}\n\n\tif (WHITELIST.indexOf(err.code) !== -1) {\n\t\treturn true;\n\t}\n\n\tif (BLACKLIST.indexOf(err.code) !== -1) {\n\t\treturn false;\n\t}\n\n\treturn true;\n};\n","const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ResourceTypes = require('./src/protocol/resourceTypes')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  ResourceTypes,\n}\n","const createRetry = require('../retry')\nconst flatten = require('../utils/flatten')\nconst waitFor = require('../utils/waitFor')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst { KafkaJSNonRetriableError, KafkaJSDeleteGroupsError } = require('../errors')\nconst RESOURCE_TYPES = require('../protocol/resourceTypes')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {import('../instrumentation/emitter')} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {array} topics\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @param {boolean} [waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'TOPIC_ALREADY_EXISTS') {\n          return false\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topic }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const partitionsToFetch = partitions.map(partition => ({ partition }))\n\n    const { responses } = await coordinator.offsetFetch({\n      groupId,\n      topics: [{ topic, partitions: partitionsToFetch }],\n    })\n\n    return responses\n      .filter(response => response.topic === topic)\n      .map(({ partitions }) =>\n        partitions.map(({ partition, offset, metadata }) => ({\n          partition,\n          offset,\n          metadata: metadata || null,\n        }))\n      )\n      .pop()\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.describeConfigs({ resources, includeSynonyms })\n        return response\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const response = await broker.alterConfigs({ resources, validateOnly: !!validateOnly })\n        return response\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @deprecated - This method was replaced by `fetchTopicMetadata`. This implementation\n   * is limited by the topics in the target group, so it can't fetch all topics when\n   * necessary.\n   *\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics of which we are aware.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const getTopicMetadata = async options => {\n    const { topics } = options || {}\n\n    if (topics) {\n      await Promise.all(\n        topics.map(async topic => {\n          if (!topic) {\n            throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n          }\n\n          try {\n            await cluster.addTargetTopic(topic)\n          } catch (e) {\n            e.message = `Failed to add target topic ${topic}: ${e.message}`\n            throw e\n          }\n        })\n      )\n    }\n\n    await cluster.refreshMetadataIfNecessary()\n    const targetTopics = topics || [...cluster.targetTopics]\n\n    return {\n      topics: await Promise.all(\n        targetTopics.map(async topic => ({\n          name: topic,\n          partitions: cluster.findTopicPartitionMetadata(topic),\n        }))\n      ),\n    }\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = flatten(\n          res.map(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n        ).filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = flatten(res.map(({ results }) => results))\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {Function} listener\n   * @return {Function}\n   */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    getTopicMetadata,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../utils/long')\nconst Lock = require('../utils/lock')\nconst { Types: Compression } = require('../protocol/message/compression')\nconst { requests, lookup } = require('../protocol/requests')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst apiKeys = require('../protocol/requests/apiKeys')\nconst SASLAuthenticator = require('./saslAuthenticator')\nconst shuffle = require('../utils/shuffle')\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Broker:shouldReauthenticate'),\n  SEND_REQUEST: Symbol('private:Broker:sendRequest'),\n}\n\n/**\n * Each node in a Kafka cluster is called broker. This class contains\n * the high-level operations a node can perform.\n *\n * @type {import(\"../../types\").Broker}\n * @param {Connection} connection\n * @param {Object} logger\n * @param {Object} [versions=null] The object with all available versions and APIs\n *                                 supported by this cluster. The output of broker#apiVersions\n * @param {number} [authenticationTimeout=1000]\n * @param {boolean} [allowAutoTopicCreation=true] If this and the broker config 'auto.create.topics.enable'\n *                                                are true, topics that don't exist will be created when\n *                                                fetching metadata.\n * @param {boolean} [supportAuthenticationProtocol=null] If the server supports the SASLAuthenticate protocol\n */\nmodule.exports = class Broker {\n  constructor({\n    connection,\n    logger,\n    nodeId = null,\n    versions = null,\n    authenticationTimeout = 1000,\n    reauthenticationThreshold = 10000,\n    allowAutoTopicCreation = true,\n    supportAuthenticationProtocol = null,\n  }) {\n    this.connection = connection\n    this.nodeId = nodeId\n    this.rootLogger = logger\n    this.logger = logger.namespace('Broker')\n    this.versions = versions\n    this.authenticationTimeout = authenticationTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n    this.allowAutoTopicCreation = allowAutoTopicCreation\n    this.supportAuthenticationProtocol = supportAuthenticationProtocol\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n\n    // The lock timeout has twice the connectionTimeout because the same timeout is used\n    // for the first apiVersions call\n    const lockTimeout = 2 * this.connection.connectionTimeout + this.authenticationTimeout\n    this.brokerAddress = `${this.connection.host}:${this.connection.port}`\n\n    this.lock = new Lock({\n      timeout: lockTimeout,\n      description: `connect to broker ${this.brokerAddress}`,\n    })\n\n    this.lookupRequest = () => {\n      throw new Error('Broker not connected')\n    }\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isConnected() {\n    const { connected, sasl } = this.connection\n    const isAuthenticated = this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n    return sasl ? connected && isAuthenticated : connected\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async connect() {\n    try {\n      await this.lock.acquire()\n      if (this.isConnected()) {\n        return\n      }\n\n      this.authenticatedAt = null\n      await this.connection.connect()\n\n      if (!this.versions) {\n        this.versions = await this.apiVersions()\n      }\n\n      this.lookupRequest = lookup(this.versions)\n\n      if (this.supportAuthenticationProtocol === null) {\n        try {\n          this.lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n          this.supportAuthenticationProtocol = true\n        } catch (_) {\n          this.supportAuthenticationProtocol = false\n        }\n\n        this.logger.debug(`Verified support for SaslAuthenticate`, {\n          broker: this.brokerAddress,\n          supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n        })\n      }\n\n      if (this.authenticatedAt == null && this.connection.sasl) {\n        const authenticator = new SASLAuthenticator(\n          this.connection,\n          this.rootLogger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    } finally {\n      await this.lock.release()\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    await this.connection.disconnect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async apiVersions() {\n    let response\n    const availableVersions = requests.ApiVersions.versions\n      .map(Number)\n      .sort()\n      .reverse()\n\n    // Find the best version implemented by the server\n    for (const candidateVersion of availableVersions) {\n      try {\n        const apiVersions = requests.ApiVersions.protocol({ version: candidateVersion })\n        response = await this[PRIVATE.SEND_REQUEST]({\n          ...apiVersions(),\n          requestTimeout: this.connection.connectionTimeout,\n        })\n        break\n      } catch (e) {\n        if (e.type !== 'UNSUPPORTED_VERSION') {\n          throw e\n        }\n      }\n    }\n\n    if (!response) {\n      throw new KafkaJSNonRetriableError('API Versions not supported')\n    }\n\n    return response.apiVersions.reduce(\n      (obj, version) =>\n        Object.assign(obj, {\n          [version.apiKey]: {\n            minVersion: version.minVersion,\n            maxVersion: version.maxVersion,\n          },\n        }),\n      {}\n    )\n  }\n\n  /**\n   * @public\n   * @type {import(\"../../types\").Broker['metadata']}\n   * @param {Array} [topics=[]] An array of topics to fetch metadata for.\n   *                            If no topics are specified fetch metadata for all topics\n   */\n  async metadata(topics = []) {\n    const metadata = this.lookupRequest(apiKeys.Metadata, requests.Metadata)\n    const shuffledTopics = shuffle(topics)\n    return await this[PRIVATE.SEND_REQUEST](\n      metadata({ topics: shuffledTopics, allowAutoTopicCreation: this.allowAutoTopicCreation })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Array} topicData An array of messages per topic and per partition, example:\n   *                          [\n   *                            {\n   *                              topic: 'test-topic-1',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 0,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '1', value: 'A' },\n   *                                    { key: '2', value: 'B' },\n   *                                  ]\n   *                                },\n   *                                {\n   *                                  partition: 1,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '3', value: 'C' },\n   *                                  ]\n   *                                }\n   *                              ]\n   *                            },\n   *                            {\n   *                              topic: 'test-topic-2',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 4,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '32', value: 'E' },\n   *                                  ]\n   *                                },\n   *                              ]\n   *                            },\n   *                          ]\n   * @param {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @param {number} [timeout=30000] The time to await a response in ms\n   * @param {string} [transactionalId=null]\n   * @param {number} [producerId=-1] Broker assigned producerId\n   * @param {number} [producerEpoch=0] Broker assigned producerEpoch\n   * @param {Compression.Types} [compression=Compression.Types.None] Compression codec\n   * @returns {Promise}\n   */\n  async produce({\n    topicData,\n    transactionalId,\n    producerId,\n    producerEpoch,\n    acks = -1,\n    timeout = 30000,\n    compression = Compression.None,\n  }) {\n    const produce = this.lookupRequest(apiKeys.Produce, requests.Produce)\n    return await this[PRIVATE.SEND_REQUEST](\n      produce({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {number} replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} isolationLevel=1 This setting controls the visibility of transactional records. Default READ_COMMITTED.\n   * @param {number} maxWaitTime=5000 Maximum time in ms to wait for the response\n   * @param {number} minBytes=1 Minimum bytes to accumulate in the response\n   * @param {number} maxBytes=10485760 Maximum bytes to accumulate in the response. Note that this is\n   *                                   not an absolute maximum, if the first message in the first non-empty\n   *                                   partition of the fetch is larger than this value, the message will still\n   *                                   be returned to ensure that progress can be made. Default 10MB.\n   * @param {Array} topics Topics to fetch\n   *                        [\n   *                          {\n   *                            topic: 'topic-name',\n   *                            partitions: [\n   *                              {\n   *                                partition: 0,\n   *                                fetchOffset: '4124',\n   *                                maxBytes: 2048\n   *                              }\n   *                            ]\n   *                          }\n   *                        ]\n   * @param {string} rackId='' A rack identifier for this client. This can be any string value which indicates where this\n   *                           client is physically located. It corresponds with the broker config `broker.rack`.\n   * @returns {Promise}\n   */\n  async fetch({\n    replicaId,\n    isolationLevel,\n    maxWaitTime = 5000,\n    minBytes = 1,\n    maxBytes = 10485760,\n    topics,\n    rackId = '',\n  }) {\n    // TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {string} groupId The group id\n   * @param {number} groupGenerationId The generation of the group\n   * @param {string} memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {string} groupId The unique group id\n   * @param {CoordinatorType} coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {string} groupId The unique group id\n   * @param {number} sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {string} memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {number} generationId\n   * @param {string} memberId\n   * @param {object} groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {number} replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {number} groupGenerationId\n   * @param {string} memberId\n   * @param {number} [retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {object} topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {Array} groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {Array} topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Array} topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Array<string>} topics An array of topics to be deleted\n   * @param {number} [timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node. Values <= 0 will trigger topic deletion and return\n   *                                immediately\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {Array<ResourceQuery>} resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {Array<ResourceConfig>} resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {number} transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {string} transactionalId The transactional id corresponding to the transaction.\n   * @param {number} producerId Current producer id in use by the transactional id.\n   * @param {number} producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {string} transactionalId The transactional id corresponding to the transaction.\n   * @param {number} producerId Current producer id in use by the transactional id.\n   * @param {number} producerEpoch Current epoch associated with the producer id.\n   * @param {string} groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {OffsetCommitTopic[]} topics\n   * @param {string} transactionalId The transactional id corresponding to the transaction.\n   * @param {string} groupId The unique group identifier (for the consumer group)\n   * @param {number} producerId Current producer id in use by the transactional id.\n   * @param {number} producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {string} transactionalId The transactional id corresponding to the transaction.\n   * @param {number} producerId Current producer id in use by the transactional id.\n   * @param {number} producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {Array<string>} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    try {\n      return await this.connection.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n","const awsIam = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class AWSIAMAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLAWSIAMAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (!sasl.authorizationIdentity) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n    }\n    if (!sasl.accessKeyId) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n    }\n    if (!sasl.secretAccessKey) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n    }\n    if (!sasl.sessionToken) {\n      sasl.sessionToken = ''\n    }\n\n    const request = awsIam.request(sasl)\n    const response = awsIam.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL AWS-IAM', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL AWS-IAM authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL AWS-IAM authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst PlainAuthenticator = require('./plain')\nconst SCRAM256Authenticator = require('./scram256')\nconst SCRAM512Authenticator = require('./scram512')\nconst AWSIAMAuthenticator = require('./awsIam')\nconst OAuthBearerAuthenticator = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst AUTHENTICATORS = {\n  PLAIN: PlainAuthenticator,\n  'SCRAM-SHA-256': SCRAM256Authenticator,\n  'SCRAM-SHA-512': SCRAM512Authenticator,\n  AWS: AWSIAMAuthenticator,\n  OAUTHBEARER: OAuthBearerAuthenticator,\n}\n\nconst SUPPORTED_MECHANISMS = Object.keys(AUTHENTICATORS)\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    if (!SUPPORTED_MECHANISMS.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the client`\n      )\n    }\n\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response, authExpectResponse }) => {\n      if (this.protocolAuthentication) {\n        const { buffer: requestAuthBytes } = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!authExpectResponse) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.authenticate({ request, response, authExpectResponse })\n    }\n\n    const Authenticator = AUTHENTICATORS[mechanism]\n    await new Authenticator(this.connection, this.logger, saslAuthenticate).authenticate()\n  }\n}\n","/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst oauthBearer = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class OAuthBearerAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLOAuthBearerAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.oauthBearerProvider == null) {\n      throw new KafkaJSSASLAuthenticationError(\n        'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n      )\n    }\n\n    const { oauthBearerProvider } = sasl\n\n    const oauthBearerToken = await oauthBearerProvider()\n\n    if (oauthBearerToken.value == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n    }\n\n    const request = await oauthBearer.request(sasl, oauthBearerToken)\n    const response = oauthBearer.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL OAUTHBEARER authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const plain = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class PlainAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLPlainAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n    }\n\n    const request = plain.request(sasl)\n    const response = plain.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL PLAIN', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL PLAIN authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL PLAIN authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {Connection} connection\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(connection, logger, saslAuthenticate, digestDefinition) {\n    this.connection = connection\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const { host, port, sasl } = this.connection\n    const broker = `${host}:${port}`\n\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.connection.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.connection.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM256Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM256Authenticator'), saslAuthenticate, DIGESTS.SHA256)\n  }\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM512Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM512Authenticator'), saslAuthenticate, DIGESTS.SHA512)\n  }\n}\n","const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connection.host !== host ||\n  broker.connection.port !== port ||\n  broker.connection.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {ConnectionBuilder} connectionBuilder\n   * @param {Logger} logger\n   * @param {Object} retry\n   * @param {number} authenticationTimeout\n   * @param {number} reauthenticationThreshold\n   * @param {number} metadataMaxAge\n   */\n  constructor({\n    connectionBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionBuilder = connectionBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        ...options,\n      })\n\n    this.brokers = {}\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    this.seedBroker = this.createBroker({\n      connection: await this.connectionBuilder.build(),\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<null>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @param {String} host\n   * @param {Number} port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connection.host === host && broker.connection.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connection\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connection.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n                connection: await this.connectionBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /**\n   * @public\n   * @param {string} nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {Promise<{ nodeId<String>, broker<Broker> }>} callback\n   * @returns {Promise<null>}\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n\n          // Connection refused means this node is down, or the cluster is restarting,\n          // which requires metadata refresh to discover the new nodes\n          if (e.code === 'ECONNREFUSED') {\n            return bail(e)\n          }\n\n          // Rebuild the connection since it can't recover from illegal SASL state\n          broker.connection = await this.connectionBuilder.build({\n            host: broker.connection.host,\n            port: broker.connection.port,\n            rack: broker.connection.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n","const Connection = require('../network/connection')\nconst { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\n\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  retry,\n  logger,\n  instrumentationEmitter = null,\n}) => {\n  let index = 0\n\n  const getBrokers = async () => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers parameter should not be null`)\n    }\n\n    // static list\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n      return brokers\n    }\n\n    // dynamic brokers\n    let list\n    try {\n      list = await brokers()\n    } catch (e) {\n      const wrappedError = new KafkaJSConnectionError(\n        `Failed to connect: \"config.brokers\" threw: ${e.message}`\n      )\n      wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n      throw wrappedError\n    }\n\n    if (!list || list.length === 0) {\n      throw new KafkaJSConnectionError(\n        `Failed to connect: \"config.brokers\" returned void or empty array`\n      )\n    }\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new Connection({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        retry,\n        logger,\n      })\n    },\n  }\n}\n","const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst createRetry = require('../retry')\nconst connectionBuilder = require('./connectionBuilder')\nconst flatten = require('../utils/flatten')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\n/**\n * @param {Array<string>} brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n * @param {Object} ssl\n * @param {Object} sasl\n * @param {string} clientId\n * @param {number} connectionTimeout - in milliseconds\n * @param {number} authenticationTimeout - in milliseconds\n * @param {number} reauthenticationThreshold - in milliseconds\n * @param {number} [requestTimeout=30000] - in milliseconds\n * @param {number} metadataMaxAge - in milliseconds\n * @param {boolean} allowAutoTopicCreation\n * @param {number} maxInFlightRequests\n * @param {number} isolationLevel\n * @param {Object} retry\n * @param {Logger} logger\n * @param {Map} offsets\n * @param {InstrumentationEventEmitter} [instrumentationEmitter=null]\n */\nmodule.exports = class Cluster {\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retry = { ...retry }\n    this.retrier = createRetry(this.retry)\n    this.connectionBuilder = connectionBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n      retry,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionBuilder: this.connectionBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      reauthenticationThreshold,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<null>}\n   */\n  async connect() {\n    await this.brokerPool.connect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<null>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {String} host\n   * @param {Number} port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata() {\n    await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Metadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (e.type === 'INVALID_TOPIC_EXCEPTION') {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /**\n   * @public\n   * @param {string} nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.code === 'ECONNREFUSED'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findControllerBroker() {\n    const { metadata } = this.brokerPool\n\n    if (!metadata || metadata.controllerId == null) {\n      throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n    }\n\n    const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(\n        `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n      )\n    }\n\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {Array<number>} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {number} [coordinatorType=0]\n   * @returns {Promise<Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} groupId\n   * @param {number} [coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).map(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = flatten(responses).reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {string} groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {string|number} partition\n   * @param {string} offset\n   * @returns {undefined}\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n","const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n","const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {number} version\n   * @param {Object<String,Array>} assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n","const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n","const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\nconst flatten = require('../../../utils/flatten')\n\n/**\n * RoundRobinAssigner\n * @param {Cluster} cluster\n * @returns {function}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 1,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {array} members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {array} topics\n   * @returns {array} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartionArrays = topics.map(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n    const topicsPartitions = flatten(topicsPartionArrays)\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = []\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n","/**\n * @template T\n * @return {{lock: Promise<T>, unlock: (v?: T) => void, unlockWithError: (e: Error) => void}}\n */\nmodule.exports = () => {\n  let unlock\n  let unlockWithError\n  const lock = new Promise(resolve => {\n    unlock = resolve\n    unlockWithError = resolve\n  })\n\n  return { lock, unlock, unlockWithError }\n}\n","const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n","const flatten = require('../utils/flatten')\nconst sleep = require('../utils/sleep')\nconst BufferedAsyncIterator = require('../utils/bufferedAsyncIterator')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { HEARTBEAT, CONNECT },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nmodule.exports = class ConsumerGroup {\n  constructor({\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async join() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async sync() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {string} topic\n   * @param {number} partition\n   * @param {string} offset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    const { groupId, generationId, memberId } = this\n    const now = Date.now()\n\n    if (memberId && now >= this.lastRequest + interval) {\n      const payload = {\n        groupId,\n        memberId,\n        groupGenerationId: generationId,\n      }\n\n      await this.coordinator.heartbeat(payload)\n      this.instrumentationEmitter.emit(HEARTBEAT, payload)\n      this.lastRequest = Date.now()\n    }\n  }\n\n  async fetch() {\n    try {\n      const { topics, maxBytesPerPartition, maxWaitTime, minBytes, maxBytes } = this\n      /** @type {{[nodeId: string]: {topic: string, partitions: { partition: number; fetchOffset: string; maxBytes: number }[]}[]}} */\n      const requestsPerNode = {}\n\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      while (this.seekOffset.size > 0) {\n        const seekEntry = this.seekOffset.pop()\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n\n      const pausedTopicPartitions = this.subscriptionState.paused()\n      const activeTopicPartitions = this.subscriptionState.active()\n\n      const activePartitions = flatten(activeTopicPartitions.map(({ partitions }) => partitions))\n      const activeTopics = activeTopicPartitions\n        .filter(({ partitions }) => partitions.length > 0)\n        .map(({ topic }) => topic)\n\n      if (activePartitions.length === 0) {\n        this.logger.debug(`No active topic partitions, sleeping for ${this.maxWaitTime}ms`, {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        })\n\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      await this.offsetManager.resolveOffsets()\n\n      this.logger.debug(\n        `Fetching from ${activePartitions.length} partitions for ${activeTopics.length} out of ${topics.length} topics`,\n        {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        }\n      )\n\n      for (const topicPartition of activeTopicPartitions) {\n        const partitionsPerNode = this.findReadReplicaForPartitions(\n          topicPartition.topic,\n          topicPartition.partitions\n        )\n\n        const nodeIds = keys(partitionsPerNode)\n        const committedOffsets = this.offsetManager.committedOffsets()\n\n        for (const nodeId of nodeIds) {\n          const partitions = partitionsPerNode[nodeId]\n            .filter(partition => {\n              /**\n               * When recovering from OffsetOutOfRange, each partition can recover\n               * concurrently, which invalidates resolved and committed offsets as part\n               * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n               * scenarios this can initiate a new fetch with invalid offsets.\n               *\n               * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n               * which increased concurrency, making this more likely to happen.\n               *\n               * This is solved by only making requests for partitions with initialized offsets.\n               *\n               * See the following pull request which explains the context of the problem:\n               * @issue https://github.com/tulios/kafkajs/pull/578\n               */\n              return committedOffsets[topicPartition.topic][partition] != null\n            })\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager\n                .nextOffset(topicPartition.topic, partition)\n                .toString(),\n              maxBytes: maxBytesPerPartition,\n            }))\n\n          requestsPerNode[nodeId] = requestsPerNode[nodeId] || []\n          requestsPerNode[nodeId].push({ topic: topicPartition.topic, partitions })\n        }\n      }\n\n      const requests = keys(requestsPerNode).map(async nodeId => {\n        const broker = await this.cluster.findBroker({ nodeId })\n        const { responses } = await broker.fetch({\n          maxWaitTime,\n          minBytes,\n          maxBytes,\n          isolationLevel: this.isolationLevel,\n          topics: requestsPerNode[nodeId],\n          rackId: this.rackId,\n        })\n\n        const batchesPerPartition = responses.map(({ topicName, partitions }) => {\n          const topicRequestData = requestsPerNode[nodeId].find(({ topic }) => topic === topicName)\n          let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n          if (!preferredReadReplicas) {\n            this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n          }\n\n          return partitions\n            .filter(\n              partitionData =>\n                !this.seekOffset.has(topicName, partitionData.partition) &&\n                !this.subscriptionState.isPaused(topicName, partitionData.partition)\n            )\n            .map(partitionData => {\n              const { partition, preferredReadReplica } = partitionData\n              if (preferredReadReplica != null && preferredReadReplica !== -1) {\n                const { nodeId: currentPreferredReadReplica } =\n                  preferredReadReplicas[partition] || {}\n                if (currentPreferredReadReplica !== preferredReadReplica) {\n                  this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                    groupId: this.groupId,\n                    memberId: this.memberId,\n                    topic: topicName,\n                    partition,\n                  })\n                }\n                preferredReadReplicas[partition] = {\n                  nodeId: preferredReadReplica,\n                  expireAt: Date.now() + this.metadataMaxAge,\n                }\n              }\n\n              const partitionRequestData = topicRequestData.partitions.find(\n                ({ partition }) => partition === partitionData.partition\n              )\n\n              const fetchedOffset = partitionRequestData.fetchOffset\n              const batch = new Batch(topicName, fetchedOffset, partitionData)\n\n              /**\n               * Resolve the offset to skip the control batch since `eachBatch` or `eachMessage` callbacks\n               * won't process empty batches\n               *\n               * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n               */\n              if (batch.isEmptyControlRecord() || batch.isEmptyDueToLogCompactedMessages()) {\n                this.resolveOffset({\n                  topic: batch.topic,\n                  partition: batch.partition,\n                  offset: batch.lastOffset(),\n                })\n              }\n\n              return batch\n            })\n        })\n\n        return flatten(batchesPerPartition)\n      })\n\n      // fetch can generate empty requests when the consumer group receives an assignment\n      // with more topics than the subscribed, so to prevent a busy loop we wait the\n      // configured max wait time\n      if (requests.length === 0) {\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      return BufferedAsyncIterator(requests, e => this.recoverFromFetch(e))\n    } catch (e) {\n      await this.recoverFromFetch(e)\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.join()\n      await this.sync()\n      throw new KafkaJSError(e.message)\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.join()\n      await this.sync()\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n}\n","const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n","const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {import('../instrumentation/emitter')} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  const topics = {}\n  let runner = null\n  let consumerGroup = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  const createConsumerGroup = ({ autoCommitInterval, autoCommitThreshold }) => {\n    return new ConsumerGroup({\n      logger: rootLogger,\n      topics: keys(topics),\n      topicConfigurations: topics,\n      cluster,\n      groupId,\n      assigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      instrumentationEmitter,\n      autoCommitInterval,\n      autoCommitThreshold,\n      isolationLevel,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  const createRunner = ({\n    eachBatchAutoResolve,\n    eachBatch,\n    eachMessage,\n    onCrash,\n    autoCommit,\n    partitionsConsumedConcurrently,\n  }) => {\n    return new Runner({\n      autoCommit,\n      logger: rootLogger,\n      consumerGroup,\n      instrumentationEmitter,\n      eachBatchAutoResolve,\n      eachBatch,\n      eachMessage,\n      heartbeatInterval,\n      retry,\n      onCrash,\n      partitionsConsumedConcurrently,\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {}\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      logger.info('Stopped', { groupId })\n    } catch (e) {}\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const isRegExp = topic instanceof RegExp\n    if (typeof topic !== 'string' && !isRegExp) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid topic ${topic} (${typeof topic}), the topic name has to be a String or a RegExp`\n      )\n    }\n\n    const topicsToSubscribe = []\n    if (isRegExp) {\n      const topicRegExp = topic\n      const metadata = await cluster.metadata()\n      const matchedTopics = metadata.topicMetadata\n        .map(({ topic: topicName }) => topicName)\n        .filter(topicName => topicRegExp.test(topicName))\n\n      logger.debug('Subscription based on RegExp', {\n        groupId,\n        topicRegExp: topicRegExp.toString(),\n        matchedTopics,\n      })\n\n      topicsToSubscribe.push(...matchedTopics)\n    } else {\n      topicsToSubscribe.push(topic)\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    consumerGroup = createConsumerGroup({\n      autoCommitInterval,\n      autoCommitThreshold,\n    })\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n      runner = createRunner({\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        partitionsConsumedConcurrently,\n      })\n\n      await runner.start()\n    }\n\n    const restart = onCrash => {\n      consumerGroup = createConsumerGroup({\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      start(onCrash)\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n      })\n\n      if (e.name === 'KafkaJSNumberOfRetriesExceeded' || e.retriable === true) {\n        const shouldRestart =\n          !retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                originalError: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          }))\n\n        if (shouldRestart) {\n          const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n          logger.error(`Restarting the consumer in ${retryTime}ms`, {\n            retryCount: e.retryCount,\n            retryTime,\n            groupId,\n          })\n\n          setTimeout(() => restart(onCrash), retryTime)\n        }\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../../utils/long')\nconst flatten = require('../../utils/flatten')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Broker}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @param {string} offset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.map(subtractTopicOffsets)\n    return flatten(offsetsDiff).reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {string} topic\n   * @param {number} partition\n   * @param {string} offset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {OffsetsByTopicPartition}\n   *\n   * @typedef {Object} OffsetsByTopicPartition\n   * @property {TopicOffsets[]} topics\n   *\n   * @typedef {Object} TopicOffsets\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {string} partition\n   * @property {string} offset\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n        assign(this.committedOffsets()[topic], updatedOffsets)\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () =>\n      unresolvedPartitions.filter(t => t.partitions.length > 0).length > 0\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {string} topic\n   * @param {number} partition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n","const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n","const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n","const EventEmitter = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst limitConcurrency = require('../utils/concurrency')\nconst { KafkaJSError } = require('../errors')\nconst barrier = require('./barrier')\n\nconst {\n  events: { GROUP_JOIN, FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS },\n} = require('./instrumentationEvents')\n\nconst isRebalancing = e =>\n  e.type === 'REBALANCE_IN_PROGRESS' || e.type === 'NOT_COORDINATOR_FOR_GROUP'\n\nconst isKafkaJSError = e => e instanceof KafkaJSError\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} [options.partitionsConsumedConcurrently]\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} options.eachBatch\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} options.eachMessage\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.partitionsConsumedConcurrently = partitionsConsumedConcurrently\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async join() {\n    const startJoin = Date.now()\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.join()\n        await this.consumerGroup.sync()\n\n        this.running = true\n\n        const memberAssignment = this.consumerGroup\n          .assigned()\n          .reduce((result, { topic, partitions }) => ({ ...result, [topic]: partitions }), {})\n\n        const payload = {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          leaderId: this.consumerGroup.leaderId,\n          isLeader: this.consumerGroup.isLeader(),\n          memberAssignment,\n          groupProtocol: this.consumerGroup.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry. Throwing a retriable error to allow the retrier\n          // to keep going\n          throw new KafkaJSError('The group is rebalancing')\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  async scheduleJoin() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n      return\n    }\n\n    return this.join().catch(this.onCrash)\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.join()\n\n      this.running = true\n      this.scheduleFetch()\n    } catch (e) {\n      this.onCrash(e)\n    }\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({ topic, partition, message })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n      await this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: async () => {\n          await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n        },\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch() {\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, {})\n\n    const iterator = await this.consumerGroup.fetch()\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n    })\n\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n    }\n\n    const { lock, unlock, unlockWithError } = barrier()\n    const concurrently = limitConcurrency({ limit: this.partitionsConsumedConcurrently })\n\n    let requestsCompleted = false\n    let numberOfExecutions = 0\n    let expectedNumberOfExecutions = 0\n    const enqueuedTasks = []\n\n    while (true) {\n      const result = iterator.next()\n\n      if (result.done) {\n        break\n      }\n\n      if (!this.running) {\n        result.value.catch(error => {\n          this.logger.debug('Ignoring error in fetch request while stopping runner', {\n            error: error.message || error,\n            stack: error.stack,\n          })\n        })\n\n        continue\n      }\n\n      enqueuedTasks.push(async () => {\n        const batches = await result.value\n        expectedNumberOfExecutions += batches.length\n\n        batches.map(batch =>\n          concurrently(async () => {\n            try {\n              if (!this.running) {\n                return\n              }\n\n              if (batch.isEmpty()) {\n                return\n              }\n\n              await onBatch(batch)\n              await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n            } catch (e) {\n              unlockWithError(e)\n            } finally {\n              numberOfExecutions++\n              if (requestsCompleted && numberOfExecutions === expectedNumberOfExecutions) {\n                unlock()\n              }\n            }\n          }).catch(unlockWithError)\n        )\n      })\n    }\n\n    await Promise.all(enqueuedTasks.map(fn => fn()))\n    requestsCompleted = true\n\n    if (expectedNumberOfExecutions === numberOfExecutions) {\n      unlock()\n    }\n\n    const error = await lock\n    if (error) {\n      throw error\n    }\n\n    await this.autoCommitOffsets()\n    await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n  }\n\n  async scheduleFetch() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.consuming = true\n        await this.fetch()\n        this.consuming = false\n\n        if (this.running) {\n          setImmediate(() => this.scheduleFetch())\n        }\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.error('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSOffsetOutOfRange') {\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while fetching data, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      } finally {\n        this.consuming = false\n      }\n    }).catch(this.onCrash)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.error('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError('The group is rebalancing'))\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError('The group is rebalancing'))\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n","module.exports = class SeekOffsets extends Map {\n  set(topic, partition, offset) {\n    super.set([topic, partition], offset)\n  }\n\n  has(topic, partition) {\n    return Array.from(this.keys()).some(([t, p]) => t === topic && p === partition)\n  }\n\n  pop() {\n    if (this.size === 0) {\n      return\n    }\n\n    const [key, offset] = this.entries().next().value\n    this.delete(key)\n    const [topic, partition] = key\n    return { topic, partition, offset }\n  }\n}\n","const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n","module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n","class KafkaJSError extends Error {\n  constructor(e, { retriable = true } = {}) {\n    super(e)\n    Error.captureStackTrace(this, this.constructor)\n    this.message = e.message || e\n    this.name = 'KafkaJSError'\n    this.retriable = retriable\n    this.helpUrl = e.helpUrl\n  }\n}\n\nclass KafkaJSNonRetriableError extends KafkaJSError {\n  constructor(e) {\n    super(e, { retriable: false })\n    this.name = 'KafkaJSNonRetriableError'\n  }\n}\n\nclass KafkaJSProtocolError extends KafkaJSError {\n  constructor(e) {\n    super(e, { retriable: e.retriable })\n    this.type = e.type\n    this.code = e.code\n    this.name = 'KafkaJSProtocolError'\n  }\n}\n\nclass KafkaJSOffsetOutOfRange extends KafkaJSProtocolError {\n  constructor(e, { topic, partition }) {\n    super(e)\n    this.topic = topic\n    this.partition = partition\n    this.name = 'KafkaJSOffsetOutOfRange'\n  }\n}\n\nclass KafkaJSMemberIdRequired extends KafkaJSProtocolError {\n  constructor(e, { memberId }) {\n    super(e)\n    this.memberId = memberId\n    this.name = 'KafkaJSMemberIdRequired'\n  }\n}\n\nclass KafkaJSNumberOfRetriesExceeded extends KafkaJSNonRetriableError {\n  constructor(e, { retryCount, retryTime }) {\n    super(e)\n    this.stack = `${this.name}\\n  Caused by: ${e.stack}`\n    this.originalError = e\n    this.retryCount = retryCount\n    this.retryTime = retryTime\n    this.name = 'KafkaJSNumberOfRetriesExceeded'\n  }\n}\n\nclass KafkaJSConnectionError extends KafkaJSError {\n  constructor(e, { broker, code } = {}) {\n    super(e)\n    this.broker = broker\n    this.code = code\n    this.name = 'KafkaJSConnectionError'\n  }\n}\n\nclass KafkaJSConnectionClosedError extends KafkaJSConnectionError {\n  constructor(e, { host, port } = {}) {\n    super(e, { broker: `${host}:${port}` })\n    this.host = host\n    this.port = port\n    this.name = 'KafkaJSConnectionClosedError'\n  }\n}\n\nclass KafkaJSRequestTimeoutError extends KafkaJSError {\n  constructor(e, { broker, correlationId, createdAt, sentAt, pendingDuration } = {}) {\n    super(e)\n    this.broker = broker\n    this.correlationId = correlationId\n    this.createdAt = createdAt\n    this.sentAt = sentAt\n    this.pendingDuration = pendingDuration\n    this.name = 'KafkaJSRequestTimeoutError'\n  }\n}\n\nclass KafkaJSMetadataNotLoaded extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSMetadataNotLoaded'\n  }\n}\nclass KafkaJSTopicMetadataNotLoaded extends KafkaJSMetadataNotLoaded {\n  constructor(e, { topic } = {}) {\n    super(e)\n    this.topic = topic\n    this.name = 'KafkaJSTopicMetadataNotLoaded'\n  }\n}\nclass KafkaJSStaleTopicMetadataAssignment extends KafkaJSError {\n  constructor(e, { topic, unknownPartitions } = {}) {\n    super(e)\n    this.topic = topic\n    this.unknownPartitions = unknownPartitions\n    this.name = 'KafkaJSStaleTopicMetadataAssignment'\n  }\n}\n\nclass KafkaJSDeleteGroupsError extends KafkaJSError {\n  constructor(e, groups = []) {\n    super(e)\n    this.groups = groups\n    this.name = 'KafkaJSDeleteGroupsError'\n  }\n}\n\nclass KafkaJSServerDoesNotSupportApiKey extends KafkaJSNonRetriableError {\n  constructor(e, { apiKey, apiName } = {}) {\n    super(e)\n    this.apiKey = apiKey\n    this.apiName = apiName\n    this.name = 'KafkaJSServerDoesNotSupportApiKey'\n  }\n}\n\nclass KafkaJSBrokerNotFound extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSBrokerNotFound'\n  }\n}\n\nclass KafkaJSPartialMessageError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSPartialMessageError'\n  }\n}\n\nclass KafkaJSSASLAuthenticationError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSSASLAuthenticationError'\n  }\n}\n\nclass KafkaJSGroupCoordinatorNotFound extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSGroupCoordinatorNotFound'\n  }\n}\n\nclass KafkaJSNotImplemented extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNotImplemented'\n  }\n}\n\nclass KafkaJSTimeout extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSTimeout'\n  }\n}\n\nclass KafkaJSLockTimeout extends KafkaJSTimeout {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSLockTimeout'\n  }\n}\n\nclass KafkaJSUnsupportedMagicByteInMessageSet extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSUnsupportedMagicByteInMessageSet'\n  }\n}\n\nmodule.exports = {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSPartialMessageError,\n  KafkaJSBrokerNotFound,\n  KafkaJSProtocolError,\n  KafkaJSConnectionError,\n  KafkaJSConnectionClosedError,\n  KafkaJSRequestTimeoutError,\n  KafkaJSSASLAuthenticationError,\n  KafkaJSNumberOfRetriesExceeded,\n  KafkaJSOffsetOutOfRange,\n  KafkaJSMemberIdRequired,\n  KafkaJSGroupCoordinatorNotFound,\n  KafkaJSNotImplemented,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSStaleTopicMetadataAssignment,\n  KafkaJSDeleteGroupsError,\n  KafkaJSTimeout,\n  KafkaJSLockTimeout,\n  KafkaJSServerDoesNotSupportApiKey,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n}\n","const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\n\nmodule.exports = class Client {\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = false,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n","const EventEmitter = require('events')\nconst InstrumentationEvent = require('./event')\nconst { KafkaJSError } = require('../errors')\n\nmodule.exports = class InstrumentationEventEmitter {\n  constructor() {\n    this.emitter = new EventEmitter()\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {Object} payload\n   */\n  emit(eventName, payload) {\n    if (!eventName) {\n      throw new KafkaJSError('Invalid event name', { retriable: false })\n    }\n\n    if (this.emitter.listenerCount(eventName) > 0) {\n      const event = new InstrumentationEvent(eventName, payload)\n      this.emitter.emit(eventName, event)\n    }\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {Function} listener\n   * @returns {Function} removeListener\n   */\n  addListener(eventName, listener) {\n    this.emitter.addListener(eventName, listener)\n    return () => this.emitter.removeListener(eventName, listener)\n  }\n}\n","let id = 0\nconst nextId = () => {\n  if (id === Number.MAX_VALUE) {\n    id = 0\n  }\n\n  return id++\n}\n\nclass InstrumentationEvent {\n  /**\n   * @param {String} type\n   * @param {Object} payload\n   */\n  constructor(type, payload) {\n    this.id = nextId()\n    this.type = type\n    this.timestamp = Date.now()\n    this.payload = payload\n  }\n}\n\nmodule.exports = InstrumentationEvent\n","module.exports = namespace => type => `${namespace}.${type}`\n","const { LEVELS: logLevel } = require('./index')\n\nmodule.exports = () => ({ namespace, level, label, log }) => {\n  const prefix = namespace ? `[${namespace}] ` : ''\n  const message = JSON.stringify(\n    Object.assign({ level: label }, log, {\n      message: `${prefix}${log.message}`,\n    })\n  )\n\n  switch (level) {\n    case logLevel.INFO:\n      return console.info(message)\n    case logLevel.ERROR:\n      return console.error(message)\n    case logLevel.WARN:\n      return console.warn(message)\n    case logLevel.DEBUG:\n      return console.log(message)\n  }\n}\n","const { assign } = Object\n\nconst LEVELS = {\n  NOTHING: 0,\n  ERROR: 1,\n  WARN: 2,\n  INFO: 4,\n  DEBUG: 5,\n}\n\nconst createLevel = (label, level, currentLevel, namespace, logFunction) => (\n  message,\n  extra = {}\n) => {\n  if (level > currentLevel()) return\n  logFunction({\n    namespace,\n    level,\n    label,\n    log: assign(\n      {\n        timestamp: new Date().toISOString(),\n        logger: 'kafkajs',\n        message,\n      },\n      extra\n    ),\n  })\n}\n\nconst evaluateLogLevel = logLevel => {\n  const envLogLevel = (process.env.KAFKAJS_LOG_LEVEL || '').toUpperCase()\n  return LEVELS[envLogLevel] == null ? logLevel : LEVELS[envLogLevel]\n}\n\nconst createLogger = ({ level = LEVELS.INFO, logCreator } = {}) => {\n  let logLevel = evaluateLogLevel(level)\n  const logFunction = logCreator(logLevel)\n\n  const createNamespace = (namespace, logLevel = null) => {\n    const namespaceLogLevel = evaluateLogLevel(logLevel)\n    return createLogFunctions(namespace, namespaceLogLevel)\n  }\n\n  const createLogFunctions = (namespace, namespaceLogLevel = null) => {\n    const currentLogLevel = () => (namespaceLogLevel == null ? logLevel : namespaceLogLevel)\n    const logger = {\n      info: createLevel('INFO', LEVELS.INFO, currentLogLevel, namespace, logFunction),\n      error: createLevel('ERROR', LEVELS.ERROR, currentLogLevel, namespace, logFunction),\n      warn: createLevel('WARN', LEVELS.WARN, currentLogLevel, namespace, logFunction),\n      debug: createLevel('DEBUG', LEVELS.DEBUG, currentLogLevel, namespace, logFunction),\n    }\n\n    return assign(logger, {\n      namespace: createNamespace,\n      setLogLevel: newLevel => {\n        logLevel = newLevel\n      },\n    })\n  }\n\n  return createLogFunctions()\n}\n\nmodule.exports = {\n  LEVELS,\n  createLogger,\n}\n","const createRetry = require('../retry')\nconst createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\n/**\n * @param {string} host\n * @param {number} port\n * @param {Object} logger\n * @param {string} clientId='kafkajs'\n * @param {number} requestTimeout The maximum amount of time the client will wait for the response of a request,\n *                                in milliseconds\n * @param {string} [rack=null]\n * @param {Object} [ssl=null] Options for the TLS Secure Context. It accepts all options,\n *                            usually \"cert\", \"key\" and \"ca\". More information at\n *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n * @param {Object} [sasl=null] Attributes used for SASL authentication. Options based on the\n *                             key \"mechanism\". Connection is not actively using the SASL attributes\n *                             but acting as a data object for this information\n * @param {number} [connectionTimeout=1000] The connection timeout, in milliseconds\n * @param {Object} [retry=null] Configurations for the built-in retry mechanism. More information at the\n *                              retry module inside network\n * @param {number} [maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n *                                            enqueuing\n * @param {InstrumentationEventEmitter} [instrumentationEmitter=null]\n */\nmodule.exports = class Connection {\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout = 1000,\n    enforceRequestTimeout = false,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n    retry = {},\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.retry = retry\n    this.retrier = createRetry({ ...this.retry })\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.connected,\n    })\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n  }\n\n  get connected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.connected) {\n        return resolve(true)\n      }\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.connected\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        await this.disconnect()\n        this.rejectRequests(error)\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        await this.disconnect()\n        this.rejectRequests(error)\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  authenticate({ authExpectResponse = false, request, response }) {\n    this.authExpectResponse = authExpectResponse\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload.buffer, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.connected) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n","const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n","const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n","const EventEmitter = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {number} maxInFlightRequests\n   * @param {number} requestTimeout\n   * @param {string} clientId\n   * @param {string} broker\n   * @param {Logger} logger\n   * @param {InstrumentationEventEmitter} [instrumentationEmitter=null]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime) {\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n      },\n    })\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {number} correlationId\n   * @param {Buffer} payload\n   * @param {number} size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    const timeUntilUnthrottled = this.throttledUntil - Date.now()\n    if (timeUntilUnthrottled > 0 && !this.throttleCheckTimeoutId) {\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, timeUntilUnthrottled)\n    }\n  }\n}\n","const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {number} requestTimeout\n   * @param {string} broker - e.g: 127.0.0.1:9092\n   * @param {RequestEntry} entry\n   * @param {boolean} expectResponse\n   * @param {Function} send\n   * @param {InstrumentationEventEmitter} [instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n","module.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n","const KEEP_ALIVE_DELAY = 60000 // in ms\n\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(Object.assign({ host, port, servername: host }, ssl), onConnect)\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n","module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        firstSequence: sequencePerPartition[partition],\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n","const createRetry = require('../../retry')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n","const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n","module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n","module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n","const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.Logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {import('../instrumentation/emitter')} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /**\n   * @param {string} eventName\n   * @param {AsyncFunction} listener\n   * @return {Function} removeListener\n   */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECT\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        return await sendMessages({\n          acks,\n          timeout,\n          compression,\n          topicMessages: mergedTopicMessages,\n        })\n      } catch (error) {\n        if (error.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: error.host, port: error.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${error.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw error\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          error.name === 'KafkaJSConnectionError' ||\n          error.name === 'KafkaJSConnectionClosedError' ||\n          (error.name === 'KafkaJSProtocolError' && error.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${error.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw error\n        }\n\n        // Skip retries for errors not related to the Kafka protocol\n        logger.error(`${error.message}`, { retryCount, retryTime })\n        bail(error)\n      }\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('./partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n","const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  let counter = randomBytes(32).readUInt32BE(0)\n\n  return ({ topic, partitionMetadata, message }) => {\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counter) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counter) % numPartitions\n  }\n}\n","const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../default/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n","const DefaultPartitioner = require('./default')\nconst JavaCompatiblePartitioner = require('./defaultJava')\n\nmodule.exports = {\n  DefaultPartitioner,\n  JavaCompatiblePartitioner,\n}\n","const flatten = require('../utils/flatten')\n\nmodule.exports = ({ topics }) => {\n  const partitions = topics.map(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n\n  return flatten(partitions)\n}\n","const createRetry = require('../retry')\nconst flatten = require('../utils/flatten')\nconst { KafkaJSMetadataNotLoaded } = require('../errors')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\nconst TOTAL_INDIVIDUAL_ATTEMPTS = 5\nconst staleMetadata = e =>\n  ['UNKNOWN_TOPIC_OR_PARTITION', 'LEADER_NOT_AVAILABLE', 'NOT_LEADER_FOR_PARTITION'].includes(\n    e.type\n  )\n\nmodule.exports = ({ logger, cluster, partitioner, eosManager }) => {\n  const retrier = createRetry({ retries: TOTAL_INDIVIDUAL_ATTEMPTS })\n\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    const responsePerBroker = new Map()\n\n    const topics = topicMessages.map(({ topic }) => topic)\n    await cluster.addMultipleTargetTopics(topics)\n\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const sequencePerPartition = partitions.reduce((result, partition) => {\n          result[partition] = eosManager.getSequence(topic, partition)\n          return result\n        }, {})\n\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n          sequencePerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            sequencePerPartition,\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          const response = await broker.produce({\n            transactionalId: eosManager.isTransactional()\n              ? eosManager.getTransactionalId()\n              : undefined,\n            producerId: eosManager.getProducerId(),\n            producerEpoch: eosManager.getProducerEpoch(),\n            acks,\n            timeout,\n            compression,\n            topicData,\n          })\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          formattedResponse.forEach(({ topicName, partition }) => {\n            const increment = topicMetadata.get(topicName).messagesPerPartition[partition].length\n\n            eosManager.updateSequence(topicName, partition, increment)\n          })\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        }\n      })\n    }\n\n    const makeRequests = async (bail, retryCount, retryTime) => {\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        const responses = Array.from(responsePerBroker.values())\n        return flatten(responses)\n      } catch (e) {\n        if (staleMetadata(e) || e.name === 'KafkaJSMetadataNotLoaded') {\n          await cluster.refreshMetadata()\n        }\n\n        throw e\n      }\n    }\n\n    return retrier(makeRequests).catch(e => {\n      throw e.originalError || e\n    })\n  }\n}\n","// From: https://kafka.apache.org/protocol.html#The_Messages_FindCoordinator\n\n/**\n * @typedef {number} CoordinatorType\n *\n * Enum for the types of coordinator to find.\n * @enum {CoordinatorType}\n */\nmodule.exports = {\n  GROUP: 0,\n  TRANSACTION: 1,\n}\n","// Based on https://github.com/brianloveswords/buffer-crc32/blob/master/index.js\n\nvar CRC_TABLE = new Int32Array([\n  0x00000000,\n  0x77073096,\n  0xee0e612c,\n  0x990951ba,\n  0x076dc419,\n  0x706af48f,\n  0xe963a535,\n  0x9e6495a3,\n  0x0edb8832,\n  0x79dcb8a4,\n  0xe0d5e91e,\n  0x97d2d988,\n  0x09b64c2b,\n  0x7eb17cbd,\n  0xe7b82d07,\n  0x90bf1d91,\n  0x1db71064,\n  0x6ab020f2,\n  0xf3b97148,\n  0x84be41de,\n  0x1adad47d,\n  0x6ddde4eb,\n  0xf4d4b551,\n  0x83d385c7,\n  0x136c9856,\n  0x646ba8c0,\n  0xfd62f97a,\n  0x8a65c9ec,\n  0x14015c4f,\n  0x63066cd9,\n  0xfa0f3d63,\n  0x8d080df5,\n  0x3b6e20c8,\n  0x4c69105e,\n  0xd56041e4,\n  0xa2677172,\n  0x3c03e4d1,\n  0x4b04d447,\n  0xd20d85fd,\n  0xa50ab56b,\n  0x35b5a8fa,\n  0x42b2986c,\n  0xdbbbc9d6,\n  0xacbcf940,\n  0x32d86ce3,\n  0x45df5c75,\n  0xdcd60dcf,\n  0xabd13d59,\n  0x26d930ac,\n  0x51de003a,\n  0xc8d75180,\n  0xbfd06116,\n  0x21b4f4b5,\n  0x56b3c423,\n  0xcfba9599,\n  0xb8bda50f,\n  0x2802b89e,\n  0x5f058808,\n  0xc60cd9b2,\n  0xb10be924,\n  0x2f6f7c87,\n  0x58684c11,\n  0xc1611dab,\n  0xb6662d3d,\n  0x76dc4190,\n  0x01db7106,\n  0x98d220bc,\n  0xefd5102a,\n  0x71b18589,\n  0x06b6b51f,\n  0x9fbfe4a5,\n  0xe8b8d433,\n  0x7807c9a2,\n  0x0f00f934,\n  0x9609a88e,\n  0xe10e9818,\n  0x7f6a0dbb,\n  0x086d3d2d,\n  0x91646c97,\n  0xe6635c01,\n  0x6b6b51f4,\n  0x1c6c6162,\n  0x856530d8,\n  0xf262004e,\n  0x6c0695ed,\n  0x1b01a57b,\n  0x8208f4c1,\n  0xf50fc457,\n  0x65b0d9c6,\n  0x12b7e950,\n  0x8bbeb8ea,\n  0xfcb9887c,\n  0x62dd1ddf,\n  0x15da2d49,\n  0x8cd37cf3,\n  0xfbd44c65,\n  0x4db26158,\n  0x3ab551ce,\n  0xa3bc0074,\n  0xd4bb30e2,\n  0x4adfa541,\n  0x3dd895d7,\n  0xa4d1c46d,\n  0xd3d6f4fb,\n  0x4369e96a,\n  0x346ed9fc,\n  0xad678846,\n  0xda60b8d0,\n  0x44042d73,\n  0x33031de5,\n  0xaa0a4c5f,\n  0xdd0d7cc9,\n  0x5005713c,\n  0x270241aa,\n  0xbe0b1010,\n  0xc90c2086,\n  0x5768b525,\n  0x206f85b3,\n  0xb966d409,\n  0xce61e49f,\n  0x5edef90e,\n  0x29d9c998,\n  0xb0d09822,\n  0xc7d7a8b4,\n  0x59b33d17,\n  0x2eb40d81,\n  0xb7bd5c3b,\n  0xc0ba6cad,\n  0xedb88320,\n  0x9abfb3b6,\n  0x03b6e20c,\n  0x74b1d29a,\n  0xead54739,\n  0x9dd277af,\n  0x04db2615,\n  0x73dc1683,\n  0xe3630b12,\n  0x94643b84,\n  0x0d6d6a3e,\n  0x7a6a5aa8,\n  0xe40ecf0b,\n  0x9309ff9d,\n  0x0a00ae27,\n  0x7d079eb1,\n  0xf00f9344,\n  0x8708a3d2,\n  0x1e01f268,\n  0x6906c2fe,\n  0xf762575d,\n  0x806567cb,\n  0x196c3671,\n  0x6e6b06e7,\n  0xfed41b76,\n  0x89d32be0,\n  0x10da7a5a,\n  0x67dd4acc,\n  0xf9b9df6f,\n  0x8ebeeff9,\n  0x17b7be43,\n  0x60b08ed5,\n  0xd6d6a3e8,\n  0xa1d1937e,\n  0x38d8c2c4,\n  0x4fdff252,\n  0xd1bb67f1,\n  0xa6bc5767,\n  0x3fb506dd,\n  0x48b2364b,\n  0xd80d2bda,\n  0xaf0a1b4c,\n  0x36034af6,\n  0x41047a60,\n  0xdf60efc3,\n  0xa867df55,\n  0x316e8eef,\n  0x4669be79,\n  0xcb61b38c,\n  0xbc66831a,\n  0x256fd2a0,\n  0x5268e236,\n  0xcc0c7795,\n  0xbb0b4703,\n  0x220216b9,\n  0x5505262f,\n  0xc5ba3bbe,\n  0xb2bd0b28,\n  0x2bb45a92,\n  0x5cb36a04,\n  0xc2d7ffa7,\n  0xb5d0cf31,\n  0x2cd99e8b,\n  0x5bdeae1d,\n  0x9b64c2b0,\n  0xec63f226,\n  0x756aa39c,\n  0x026d930a,\n  0x9c0906a9,\n  0xeb0e363f,\n  0x72076785,\n  0x05005713,\n  0x95bf4a82,\n  0xe2b87a14,\n  0x7bb12bae,\n  0x0cb61b38,\n  0x92d28e9b,\n  0xe5d5be0d,\n  0x7cdcefb7,\n  0x0bdbdf21,\n  0x86d3d2d4,\n  0xf1d4e242,\n  0x68ddb3f8,\n  0x1fda836e,\n  0x81be16cd,\n  0xf6b9265b,\n  0x6fb077e1,\n  0x18b74777,\n  0x88085ae6,\n  0xff0f6a70,\n  0x66063bca,\n  0x11010b5c,\n  0x8f659eff,\n  0xf862ae69,\n  0x616bffd3,\n  0x166ccf45,\n  0xa00ae278,\n  0xd70dd2ee,\n  0x4e048354,\n  0x3903b3c2,\n  0xa7672661,\n  0xd06016f7,\n  0x4969474d,\n  0x3e6e77db,\n  0xaed16a4a,\n  0xd9d65adc,\n  0x40df0b66,\n  0x37d83bf0,\n  0xa9bcae53,\n  0xdebb9ec5,\n  0x47b2cf7f,\n  0x30b5ffe9,\n  0xbdbdf21c,\n  0xcabac28a,\n  0x53b39330,\n  0x24b4a3a6,\n  0xbad03605,\n  0xcdd70693,\n  0x54de5729,\n  0x23d967bf,\n  0xb3667a2e,\n  0xc4614ab8,\n  0x5d681b02,\n  0x2a6f2b94,\n  0xb40bbe37,\n  0xc30c8ea1,\n  0x5a05df1b,\n  0x2d02ef8d,\n])\n\nmodule.exports = encoder => {\n  const { buffer } = encoder\n  const l = buffer.length\n  let crc = -1\n  for (let n = 0; n < l; n++) {\n    crc = CRC_TABLE[(crc ^ buffer[n]) & 0xff] ^ (crc >>> 8)\n  }\n  return crc ^ -1\n}\n","const Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\n\nmodule.exports = class Decoder {\n  static int32Size() {\n    return INT32_SIZE\n  }\n\n  static decodeZigZag(value) {\n    return (value >>> 1) ^ -(value & 1)\n  }\n\n  static decodeZigZag64(longValue) {\n    return longValue.shiftRightUnsigned(1).xor(longValue.and(Long.fromInt(1)).negate())\n  }\n\n  constructor(buffer) {\n    this.buffer = buffer\n    this.offset = 0\n  }\n\n  readInt8() {\n    const value = this.buffer.readInt8(this.offset)\n    this.offset += INT8_SIZE\n    return value\n  }\n\n  canReadInt16() {\n    return this.canReadBytes(INT16_SIZE)\n  }\n\n  readInt16() {\n    const value = this.buffer.readInt16BE(this.offset)\n    this.offset += INT16_SIZE\n    return value\n  }\n\n  canReadInt32() {\n    return this.canReadBytes(INT32_SIZE)\n  }\n\n  readInt32() {\n    const value = this.buffer.readInt32BE(this.offset)\n    this.offset += INT32_SIZE\n    return value\n  }\n\n  canReadInt64() {\n    return this.canReadBytes(INT64_SIZE)\n  }\n\n  readInt64() {\n    const first = this.buffer[this.offset]\n    const last = this.buffer[this.offset + 7]\n\n    const low =\n      (first << 24) + // Overflow\n      this.buffer[this.offset + 1] * 2 ** 16 +\n      this.buffer[this.offset + 2] * 2 ** 8 +\n      this.buffer[this.offset + 3]\n    const high =\n      this.buffer[this.offset + 4] * 2 ** 24 +\n      this.buffer[this.offset + 5] * 2 ** 16 +\n      this.buffer[this.offset + 6] * 2 ** 8 +\n      last\n    this.offset += INT64_SIZE\n\n    return (BigInt(low) << 32n) + BigInt(high)\n  }\n\n  readString() {\n    const byteLength = this.readInt16()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  readVarIntString() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  canReadBytes(length) {\n    return Buffer.byteLength(this.buffer) - this.offset >= length\n  }\n\n  readBytes(byteLength = this.readInt32()) {\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readVarIntBytes() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readBoolean() {\n    return this.readInt8() === 1\n  }\n\n  readAll() {\n    const result = this.buffer.slice(this.offset)\n    this.offset += Buffer.byteLength(this.buffer)\n    return result\n  }\n\n  readArray(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  readVarIntArray(reader) {\n    const length = this.readVarInt()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  async readArrayAsync(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = await reader(this)\n    }\n\n    return array\n  }\n\n  readVarInt() {\n    let currentByte\n    let result = 0\n    let i = 0\n\n    do {\n      currentByte = this.buffer[this.offset++]\n      result += (currentByte & OTHER_BITS) << i\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag(result)\n  }\n\n  readVarLong() {\n    let currentByte\n    let result = Long.fromInt(0)\n    let i = 0\n\n    do {\n      currentByte = this.buffer[this.offset++]\n      result = result.add(Long.fromInt(currentByte & OTHER_BITS).shiftLeft(i))\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag64(result)\n  }\n\n  slice(size) {\n    return new Decoder(this.buffer.slice(this.offset, this.offset + size))\n  }\n\n  forward(size) {\n    this.offset += size\n  }\n}\n","const Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\nconst UNSIGNED_INT32_MAX_NUMBER = 0xffffff80\nconst UNSIGNED_INT64_MAX_NUMBER = 0xffffffffffffff80n\n\nmodule.exports = class Encoder {\n  static encodeZigZag(value) {\n    return (value << 1) ^ (value >> 31)\n  }\n\n  static encodeZigZag64(value) {\n    const longValue = Long.fromValue(value)\n    return longValue.shiftLeft(1).xor(longValue.shiftRight(63))\n  }\n\n  static sizeOfVarInt(value) {\n    let encodedValue = this.encodeZigZag(value)\n    let bytes = 1\n\n    while ((encodedValue & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      bytes += 1\n      encodedValue >>>= 7\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarLong(value) {\n    let longValue = Encoder.encodeZigZag64(value)\n    let bytes = 1\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      bytes += 1\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarIntBytes(value) {\n    const size = value == null ? -1 : Buffer.byteLength(value)\n\n    if (size < 0) {\n      return Encoder.sizeOfVarInt(-1)\n    }\n\n    return Encoder.sizeOfVarInt(size) + size\n  }\n\n  static nextPowerOfTwo(value) {\n    return 1 << (31 - Math.clz32(value) + 1)\n  }\n\n  /**\n   * Construct a new encoder with the given initial size\n   *\n   * @param {number} [initialSize] initial size\n   */\n  constructor(initialSize = 511) {\n    this.buf = Buffer.alloc(Encoder.nextPowerOfTwo(initialSize))\n    this.offset = 0\n  }\n\n  /**\n   * @param {Buffer} buffer\n   */\n  writeBufferInternal(buffer) {\n    const bufferLength = buffer.length\n    this.ensureAvailable(bufferLength)\n    buffer.copy(this.buf, this.offset, 0)\n    this.offset += bufferLength\n  }\n\n  ensureAvailable(length) {\n    if (this.offset + length > this.buf.length) {\n      const newLength = Encoder.nextPowerOfTwo(this.offset + length)\n      const newBuffer = Buffer.alloc(newLength)\n      this.buf.copy(newBuffer, 0, 0, this.offset)\n      this.buf = newBuffer\n    }\n  }\n\n  get buffer() {\n    return this.buf.slice(0, this.offset)\n  }\n\n  writeInt8(value) {\n    this.ensureAvailable(INT8_SIZE)\n    this.buf.writeInt8(value, this.offset)\n    this.offset += INT8_SIZE\n    return this\n  }\n\n  writeInt16(value) {\n    this.ensureAvailable(INT16_SIZE)\n    this.buf.writeInt16BE(value, this.offset)\n    this.offset += INT16_SIZE\n    return this\n  }\n\n  writeInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeUInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeUInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeInt64(value) {\n    this.ensureAvailable(INT64_SIZE)\n    const longValue = Long.fromValue(value)\n    this.buf.writeInt32BE(longValue.getHighBits(), this.offset)\n    this.buf.writeInt32BE(longValue.getLowBits(), this.offset + INT32_SIZE)\n    this.offset += INT64_SIZE\n    return this\n  }\n\n  writeBoolean(value) {\n    value ? this.writeInt8(1) : this.writeInt8(0)\n    return this\n  }\n\n  writeString(value) {\n    if (value == null) {\n      this.writeInt16(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.ensureAvailable(INT16_SIZE + byteLength)\n    this.writeInt16(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeVarIntString(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.writeVarInt(byteLength)\n    this.ensureAvailable(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeBytes(value) {\n    if (value == null) {\n      this.writeInt32(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.ensureAvailable(INT32_SIZE + value.length)\n      this.writeInt32(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.ensureAvailable(INT32_SIZE + byteLength)\n      this.writeInt32(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeVarIntBytes(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.writeVarInt(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.writeVarInt(byteLength)\n      this.ensureAvailable(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeEncoder(value) {\n    if (value == null || !Buffer.isBuffer(value.buf)) {\n      throw new Error('value should be an instance of Encoder')\n    }\n\n    this.writeBufferInternal(value.buffer)\n    return this\n  }\n\n  writeEncoderArray(value) {\n    if (!Array.isArray(value) || value.some(v => v == null || !Buffer.isBuffer(v.buf))) {\n      throw new Error('all values should be an instance of Encoder[]')\n    }\n\n    value.forEach(v => {\n      this.writeBufferInternal(v.buffer)\n    })\n    return this\n  }\n\n  writeBuffer(value) {\n    if (!Buffer.isBuffer(value)) {\n      throw new Error('value should be an instance of Buffer')\n    }\n\n    this.writeBufferInternal(value)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   */\n  writeNullableArray(array, type) {\n    // A null value is encoded with length of -1 and there are no following bytes\n    // On the context of this library, empty array and null are the same thing\n    const length = array.length !== 0 ? array.length : -1\n    this.writeArray(array, type, length)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   * @param {number} [length]\n   */\n  writeArray(array, type, length) {\n    const arrayLength = length == null ? array.length : length\n    this.writeInt32(arrayLength)\n    if (type !== undefined) {\n      switch (type) {\n        case 'int32':\n        case 'number':\n          array.forEach(value => this.writeInt32(value))\n          break\n        case 'string':\n          array.forEach(value => this.writeString(value))\n          break\n        case 'object':\n          this.writeEncoderArray(array)\n          break\n      }\n    } else {\n      array.forEach(value => {\n        switch (typeof value) {\n          case 'number':\n            this.writeInt32(value)\n            break\n          case 'string':\n            this.writeString(value)\n            break\n          case 'object':\n            this.writeEncoder(value)\n            break\n        }\n      })\n    }\n    return this\n  }\n\n  writeVarIntArray(array, type) {\n    if (type === 'object') {\n      this.writeVarInt(array.length)\n      this.writeEncoderArray(array)\n    } else {\n      const objectArray = array.filter(v => typeof v === 'object')\n      this.writeVarInt(objectArray.length)\n      this.writeEncoderArray(objectArray)\n    }\n    return this\n  }\n\n  // Based on:\n  // https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/util/Varint.java#L106\n  writeVarInt(value) {\n    const byteArray = []\n    let encodedValue = Encoder.encodeZigZag(value)\n\n    while ((encodedValue & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      byteArray.push((encodedValue & OTHER_BITS) | MOST_SIGNIFICANT_BIT)\n      encodedValue >>>= 7\n    }\n\n    byteArray.push(encodedValue & OTHER_BITS)\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  writeVarLong(value) {\n    const byteArray = []\n    let longValue = Encoder.encodeZigZag64(value)\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      byteArray.push(\n        longValue\n          .and(OTHER_BITS)\n          .or(MOST_SIGNIFICANT_BIT)\n          .toInt()\n      )\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    byteArray.push(longValue.toInt())\n\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  size() {\n    // We can use the offset here directly, because we anyways will not re-encode the buffer when writing\n    return this.offset\n  }\n\n  toJSON() {\n    return this.buffer.toJSON()\n  }\n}\n","const { KafkaJSProtocolError } = require('../errors')\nconst websiteUrl = require('../utils/websiteUrl')\n\nconst errorCodes = [\n  {\n    type: 'UNKNOWN',\n    code: -1,\n    retriable: false,\n    message: 'The server experienced an unexpected error when processing the request',\n  },\n  {\n    type: 'OFFSET_OUT_OF_RANGE',\n    code: 1,\n    retriable: false,\n    message: 'The requested offset is not within the range of offsets maintained by the server',\n  },\n  {\n    type: 'CORRUPT_MESSAGE',\n    code: 2,\n    retriable: true,\n    message:\n      'This message has failed its CRC checksum, exceeds the valid size, or is otherwise corrupt',\n  },\n  {\n    type: 'UNKNOWN_TOPIC_OR_PARTITION',\n    code: 3,\n    retriable: true,\n    message: 'This server does not host this topic-partition',\n  },\n  {\n    type: 'INVALID_FETCH_SIZE',\n    code: 4,\n    retriable: false,\n    message: 'The requested fetch size is invalid',\n  },\n  {\n    type: 'LEADER_NOT_AVAILABLE',\n    code: 5,\n    retriable: true,\n    message:\n      'There is no leader for this topic-partition as we are in the middle of a leadership election',\n  },\n  {\n    type: 'NOT_LEADER_FOR_PARTITION',\n    code: 6,\n    retriable: true,\n    message: 'This server is not the leader for that topic-partition',\n  },\n  {\n    type: 'REQUEST_TIMED_OUT',\n    code: 7,\n    retriable: true,\n    message: 'The request timed out',\n  },\n  {\n    type: 'BROKER_NOT_AVAILABLE',\n    code: 8,\n    retriable: false,\n    message: 'The broker is not available',\n  },\n  {\n    type: 'REPLICA_NOT_AVAILABLE',\n    code: 9,\n    retriable: false,\n    message: 'The replica is not available for the requested topic-partition',\n  },\n  {\n    type: 'MESSAGE_TOO_LARGE',\n    code: 10,\n    retriable: false,\n    message:\n      'The request included a message larger than the max message size the server will accept',\n  },\n  {\n    type: 'STALE_CONTROLLER_EPOCH',\n    code: 11,\n    retriable: false,\n    message: 'The controller moved to another broker',\n  },\n  {\n    type: 'OFFSET_METADATA_TOO_LARGE',\n    code: 12,\n    retriable: false,\n    message: 'The metadata field of the offset request was too large',\n  },\n  {\n    type: 'NETWORK_EXCEPTION',\n    code: 13,\n    retriable: true,\n    message: 'The server disconnected before a response was received',\n  },\n  {\n    type: 'GROUP_LOAD_IN_PROGRESS',\n    code: 14,\n    retriable: true,\n    message: \"The coordinator is loading and hence can't process requests for this group\",\n  },\n  {\n    type: 'GROUP_COORDINATOR_NOT_AVAILABLE',\n    code: 15,\n    retriable: true,\n    message: 'The group coordinator is not available',\n  },\n  {\n    type: 'NOT_COORDINATOR_FOR_GROUP',\n    code: 16,\n    retriable: true,\n    message: 'This is not the correct coordinator for this group',\n  },\n  {\n    type: 'INVALID_TOPIC_EXCEPTION',\n    code: 17,\n    retriable: false,\n    message: 'The request attempted to perform an operation on an invalid topic',\n  },\n  {\n    type: 'RECORD_LIST_TOO_LARGE',\n    code: 18,\n    retriable: false,\n    message:\n      'The request included message batch larger than the configured segment size on the server',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS',\n    code: 19,\n    retriable: true,\n    message: 'Messages are rejected since there are fewer in-sync replicas than required',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS_AFTER_APPEND',\n    code: 20,\n    retriable: true,\n    message: 'Messages are written to the log, but to fewer in-sync replicas than required',\n  },\n  {\n    type: 'INVALID_REQUIRED_ACKS',\n    code: 21,\n    retriable: false,\n    message: 'Produce request specified an invalid value for required acks',\n  },\n  {\n    type: 'ILLEGAL_GENERATION',\n    code: 22,\n    retriable: false,\n    message: 'Specified group generation id is not valid',\n  },\n  {\n    type: 'INCONSISTENT_GROUP_PROTOCOL',\n    code: 23,\n    retriable: false,\n    message:\n      \"The group member's supported protocols are incompatible with those of existing members\",\n  },\n  {\n    type: 'INVALID_GROUP_ID',\n    code: 24,\n    retriable: false,\n    message: 'The configured groupId is invalid',\n  },\n  {\n    type: 'UNKNOWN_MEMBER_ID',\n    code: 25,\n    retriable: false,\n    message: 'The coordinator is not aware of this member',\n  },\n  {\n    type: 'INVALID_SESSION_TIMEOUT',\n    code: 26,\n    retriable: false,\n    message:\n      'The session timeout is not within the range allowed by the broker (as configured by group.min.session.timeout.ms and group.max.session.timeout.ms)',\n  },\n  {\n    type: 'REBALANCE_IN_PROGRESS',\n    code: 27,\n    retriable: false,\n    message: 'The group is rebalancing, so a rejoin is needed',\n    helpUrl: websiteUrl('docs/faq', 'what-does-it-mean-to-get-rebalance-in-progress-errors'),\n  },\n  {\n    type: 'INVALID_COMMIT_OFFSET_SIZE',\n    code: 28,\n    retriable: false,\n    message: 'The committing offset data size is not valid',\n  },\n  {\n    type: 'TOPIC_AUTHORIZATION_FAILED',\n    code: 29,\n    retriable: false,\n    message: 'Not authorized to access topics: [Topic authorization failed]',\n  },\n  {\n    type: 'GROUP_AUTHORIZATION_FAILED',\n    code: 30,\n    retriable: false,\n    message: 'Not authorized to access group: Group authorization failed',\n  },\n  {\n    type: 'CLUSTER_AUTHORIZATION_FAILED',\n    code: 31,\n    retriable: false,\n    message: 'Cluster authorization failed',\n  },\n  {\n    type: 'INVALID_TIMESTAMP',\n    code: 32,\n    retriable: false,\n    message: 'The timestamp of the message is out of acceptable range',\n  },\n  {\n    type: 'UNSUPPORTED_SASL_MECHANISM',\n    code: 33,\n    retriable: false,\n    message: 'The broker does not support the requested SASL mechanism',\n  },\n  {\n    type: 'ILLEGAL_SASL_STATE',\n    code: 34,\n    retriable: false,\n    message: 'Request is not valid given the current SASL state',\n  },\n  {\n    type: 'UNSUPPORTED_VERSION',\n    code: 35,\n    retriable: false,\n    message: 'The version of API is not supported',\n  },\n  {\n    type: 'TOPIC_ALREADY_EXISTS',\n    code: 36,\n    retriable: false,\n    message: 'Topic with this name already exists',\n  },\n  {\n    type: 'INVALID_PARTITIONS',\n    code: 37,\n    retriable: false,\n    message: 'Number of partitions is invalid',\n  },\n  {\n    type: 'INVALID_REPLICATION_FACTOR',\n    code: 38,\n    retriable: false,\n    message: 'Replication-factor is invalid',\n  },\n  {\n    type: 'INVALID_REPLICA_ASSIGNMENT',\n    code: 39,\n    retriable: false,\n    message: 'Replica assignment is invalid',\n  },\n  {\n    type: 'INVALID_CONFIG',\n    code: 40,\n    retriable: false,\n    message: 'Configuration is invalid',\n  },\n  {\n    type: 'NOT_CONTROLLER',\n    code: 41,\n    retriable: true,\n    message: 'This is not the correct controller for this cluster',\n  },\n  {\n    type: 'INVALID_REQUEST',\n    code: 42,\n    retriable: false,\n    message:\n      \"This most likely occurs because of a request being malformed by the client library or the message was sen't to an incompatible broker. See the broker logs for more details\",\n  },\n  {\n    type: 'UNSUPPORTED_FOR_MESSAGE_FORMAT',\n    code: 43,\n    retriable: false,\n    message: 'The message format version on the broker does not support the request',\n  },\n  {\n    type: 'POLICY_VIOLATION',\n    code: 44,\n    retriable: false,\n    message: 'Request parameters do not satisfy the configured policy',\n  },\n  {\n    type: 'OUT_OF_ORDER_SEQUENCE_NUMBER',\n    code: 45,\n    retriable: false,\n    message: 'The broker received an out of order sequence number',\n  },\n  {\n    type: 'DUPLICATE_SEQUENCE_NUMBER',\n    code: 46,\n    retriable: false,\n    message: 'The broker received a duplicate sequence number',\n  },\n  {\n    type: 'INVALID_PRODUCER_EPOCH',\n    code: 47,\n    retriable: false,\n    message:\n      \"Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker\",\n  },\n  {\n    type: 'INVALID_TXN_STATE',\n    code: 48,\n    retriable: false,\n    message: 'The producer attempted a transactional operation in an invalid state',\n  },\n  {\n    type: 'INVALID_PRODUCER_ID_MAPPING',\n    code: 49,\n    retriable: false,\n    message:\n      'The producer attempted to use a producer id which is not currently assigned to its transactional id',\n  },\n  {\n    type: 'INVALID_TRANSACTION_TIMEOUT',\n    code: 50,\n    retriable: false,\n    message:\n      'The transaction timeout is larger than the maximum value allowed by the broker (as configured by max.transaction.timeout.ms)',\n  },\n  {\n    type: 'CONCURRENT_TRANSACTIONS',\n    code: 51,\n    /**\n     * The concurrent transactions error has \"retriable\" set to false on the protocol documentation (https://kafka.apache.org/protocol.html#protocol_error_codes)\n     * but the server expects the clients to retry. PR #223\n     * @see https://github.com/apache/kafka/blob/12f310d50e7f5b1c18c4f61a119a6cd830da3bc0/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala#L153\n     */\n    retriable: true,\n    message:\n      'The producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing',\n  },\n  {\n    type: 'TRANSACTION_COORDINATOR_FENCED',\n    code: 52,\n    retriable: false,\n    message:\n      'Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer',\n  },\n  {\n    type: 'TRANSACTIONAL_ID_AUTHORIZATION_FAILED',\n    code: 53,\n    retriable: false,\n    message: 'Transactional Id authorization failed',\n  },\n  {\n    type: 'SECURITY_DISABLED',\n    code: 54,\n    retriable: false,\n    message: 'Security features are disabled',\n  },\n  {\n    type: 'OPERATION_NOT_ATTEMPTED',\n    code: 55,\n    retriable: false,\n    message:\n      'The broker did not attempt to execute this operation. This may happen for batched RPCs where some operations in the batch failed, causing the broker to respond without trying the rest',\n  },\n  {\n    type: 'KAFKA_STORAGE_ERROR',\n    code: 56,\n    retriable: true,\n    message: 'Disk error when trying to access log file on the disk',\n  },\n  {\n    type: 'LOG_DIR_NOT_FOUND',\n    code: 57,\n    retriable: false,\n    message: 'The user-specified log directory is not found in the broker config',\n  },\n  {\n    type: 'SASL_AUTHENTICATION_FAILED',\n    code: 58,\n    retriable: false,\n    message: 'SASL Authentication failed',\n    helpUrl: websiteUrl('docs/configuration', 'sasl'),\n  },\n  {\n    type: 'UNKNOWN_PRODUCER_ID',\n    code: 59,\n    retriable: false,\n    message:\n      \"This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception\",\n  },\n  {\n    type: 'REASSIGNMENT_IN_PROGRESS',\n    code: 60,\n    retriable: false,\n    message: 'A partition reassignment is in progress',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTH_DISABLED',\n    code: 61,\n    retriable: false,\n    message: 'Delegation Token feature is not enabled',\n  },\n  {\n    type: 'DELEGATION_TOKEN_NOT_FOUND',\n    code: 62,\n    retriable: false,\n    message: 'Delegation Token is not found on server',\n  },\n  {\n    type: 'DELEGATION_TOKEN_OWNER_MISMATCH',\n    code: 63,\n    retriable: false,\n    message: 'Specified Principal is not valid Owner/Renewer',\n  },\n  {\n    type: 'DELEGATION_TOKEN_REQUEST_NOT_ALLOWED',\n    code: 64,\n    retriable: false,\n    message:\n      'Delegation Token requests are not allowed on PLAINTEXT/1-way SSL channels and on delegation token authenticated channels',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTHORIZATION_FAILED',\n    code: 65,\n    retriable: false,\n    message: 'Delegation Token authorization failed',\n  },\n  {\n    type: 'DELEGATION_TOKEN_EXPIRED',\n    code: 66,\n    retriable: false,\n    message: 'Delegation Token is expired',\n  },\n  {\n    type: 'INVALID_PRINCIPAL_TYPE',\n    code: 67,\n    retriable: false,\n    message: 'Supplied principalType is not supported',\n  },\n  {\n    type: 'NON_EMPTY_GROUP',\n    code: 68,\n    retriable: false,\n    message: 'The group is not empty',\n  },\n  {\n    type: 'GROUP_ID_NOT_FOUND',\n    code: 69,\n    retriable: false,\n    message: 'The group id was not found',\n  },\n  {\n    type: 'FETCH_SESSION_ID_NOT_FOUND',\n    code: 70,\n    retriable: true,\n    message: 'The fetch session ID was not found',\n  },\n  {\n    type: 'INVALID_FETCH_SESSION_EPOCH',\n    code: 71,\n    retriable: true,\n    message: 'The fetch session epoch is invalid',\n  },\n  {\n    type: 'LISTENER_NOT_FOUND',\n    code: 72,\n    retriable: true,\n    message:\n      'There is no listener on the leader broker that matches the listener on which metadata request was processed',\n  },\n  {\n    type: 'TOPIC_DELETION_DISABLED',\n    code: 73,\n    retriable: false,\n    message: 'Topic deletion is disabled',\n  },\n  {\n    type: 'FENCED_LEADER_EPOCH',\n    code: 74,\n    retriable: true,\n    message: 'The leader epoch in the request is older than the epoch on the broker',\n  },\n  {\n    type: 'UNKNOWN_LEADER_EPOCH',\n    code: 75,\n    retriable: true,\n    message: 'The leader epoch in the request is newer than the epoch on the broker',\n  },\n  {\n    type: 'UNSUPPORTED_COMPRESSION_TYPE',\n    code: 76,\n    retriable: false,\n    message: 'The requesting client does not support the compression type of given partition',\n  },\n  {\n    type: 'STALE_BROKER_EPOCH',\n    code: 77,\n    retriable: false,\n    message: 'Broker epoch has changed',\n  },\n  {\n    type: 'OFFSET_NOT_AVAILABLE',\n    code: 78,\n    retriable: true,\n    message:\n      'The leader high watermark has not caught up from a recent leader election so the offsets cannot be guaranteed to be monotonically increasing',\n  },\n  {\n    type: 'MEMBER_ID_REQUIRED',\n    code: 79,\n    retriable: false,\n    message:\n      'The group member needs to have a valid member id before actually entering a consumer group',\n  },\n  {\n    type: 'PREFERRED_LEADER_NOT_AVAILABLE',\n    code: 80,\n    retriable: true,\n    message: 'The preferred leader was not available',\n  },\n  {\n    type: 'GROUP_MAX_SIZE_REACHED',\n    code: 81,\n    retriable: false,\n    message:\n      'The consumer group has reached its max size. It already has the configured maximum number of members',\n  },\n  {\n    type: 'FENCED_INSTANCE_ID',\n    code: 82,\n    retriable: false,\n    message:\n      'The broker rejected this static consumer since another consumer with the same group instance id has registered with a different member id',\n  },\n  {\n    type: 'ELIGIBLE_LEADERS_NOT_AVAILABLE',\n    code: 83,\n    retriable: true,\n    message: 'Eligible topic partition leaders are not available',\n  },\n  {\n    type: 'ELECTION_NOT_NEEDED',\n    code: 84,\n    retriable: true,\n    message: 'Leader election not needed for topic partition',\n  },\n  {\n    type: 'NO_REASSIGNMENT_IN_PROGRESS',\n    code: 85,\n    retriable: false,\n    message: 'No partition reassignment is in progress',\n  },\n  {\n    type: 'GROUP_SUBSCRIBED_TO_TOPIC',\n    code: 86,\n    retriable: false,\n    message:\n      'Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it',\n  },\n  {\n    type: 'INVALID_RECORD',\n    code: 87,\n    retriable: false,\n    message: 'This record has failed the validation on broker and hence be rejected',\n  },\n  {\n    type: 'UNSTABLE_OFFSET_COMMIT',\n    code: 88,\n    retriable: true,\n    message: 'There are unstable offsets that need to be cleared',\n  },\n]\n\nconst unknownErrorCode = errorCode => ({\n  type: 'KAFKAJS_UNKNOWN_ERROR_CODE',\n  code: -99,\n  retriable: false,\n  message: `Unknown error code ${errorCode}`,\n})\n\nconst SUCCESS_CODE = 0\nconst UNSUPPORTED_VERSION_CODE = 35\n\nconst failure = code => code !== SUCCESS_CODE\nconst createErrorFromCode = code => {\n  return new KafkaJSProtocolError(errorCodes.find(e => e.code === code) || unknownErrorCode(code))\n}\n\nconst failIfVersionNotSupported = code => {\n  if (code === UNSUPPORTED_VERSION_CODE) {\n    throw createErrorFromCode(UNSUPPORTED_VERSION_CODE)\n  }\n}\n\nmodule.exports = {\n  failure,\n  errorCodes,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n}\n","/**\n * Enum for isolation levels\n * @readonly\n * @enum {number}\n */\nmodule.exports = {\n  // Makes all records visible\n  READ_UNCOMMITTED: 0,\n\n  // non-transactional and COMMITTED transactional records are visible. It returns all data\n  // from offsets smaller than the current LSO (last stable offset), and enables the inclusion of\n  // the list of aborted transactions in the result, which allows consumers to discard ABORTED\n  // transactional records\n  READ_COMMITTED: 1,\n}\n","const { promisify } = require('util')\nconst zlib = require('zlib')\n\nconst gzip = promisify(zlib.gzip)\nconst unzip = promisify(zlib.unzip)\n\nmodule.exports = {\n  /**\n   * @param {Encoder} encoder\n   * @returns {Promise}\n   */\n  async compress(encoder) {\n    return await gzip(encoder.buffer)\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Promise}\n   */\n  async decompress(buffer) {\n    return await unzip(buffer)\n  },\n}\n","const { KafkaJSNotImplemented } = require('../../../errors')\n\nconst MESSAGE_CODEC_MASK = 0x3\nconst RECORD_BATCH_CODEC_MASK = 0x07\n\nconst Types = {\n  None: 0,\n  GZIP: 1,\n  Snappy: 2,\n  LZ4: 3,\n  ZSTD: 4,\n}\n\nconst Codecs = {\n  [Types.GZIP]: () => require('./gzip'),\n  [Types.Snappy]: () => {\n    throw new KafkaJSNotImplemented('Snappy compression not implemented')\n  },\n  [Types.LZ4]: () => {\n    throw new KafkaJSNotImplemented('LZ4 compression not implemented')\n  },\n  [Types.ZSTD]: () => {\n    throw new KafkaJSNotImplemented('ZSTD compression not implemented')\n  },\n}\n\nconst lookupCodec = type => (Codecs[type] ? Codecs[type]() : null)\nconst lookupCodecByAttributes = attributes => {\n  const codec = Codecs[attributes & MESSAGE_CODEC_MASK]\n  return codec ? codec() : null\n}\nconst lookupCodecByRecordBatchAttributes = attributes => {\n  const codec = Codecs[attributes & RECORD_BATCH_CODEC_MASK]\n  return codec ? codec() : null\n}\n\nmodule.exports = {\n  Types,\n  Codecs,\n  lookupCodec,\n  lookupCodecByAttributes,\n  lookupCodecByRecordBatchAttributes,\n}\n","const {\n  KafkaJSPartialMessageError,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n} = require('../../errors')\n\nconst V0Decoder = require('./v0/decoder')\nconst V1Decoder = require('./v1/decoder')\n\nconst decodeMessage = (decoder, magicByte) => {\n  switch (magicByte) {\n    case 0:\n      return V0Decoder(decoder)\n    case 1:\n      return V1Decoder(decoder)\n    default:\n      throw new KafkaJSUnsupportedMagicByteInMessageSet(\n        `Unsupported MessageSet message version, magic byte: ${magicByte}`\n      )\n  }\n}\n\nmodule.exports = (offset, size, decoder) => {\n  // Don't decrement decoder.offset because slice is already considering the current\n  // offset of the decoder\n  const remainingBytes = Buffer.byteLength(decoder.slice(size).buffer)\n\n  if (remainingBytes < size) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: remainingBytes(${remainingBytes}) < messageSize(${size})`\n    )\n  }\n\n  const crc = decoder.readInt32()\n  const magicByte = decoder.readInt8()\n  const message = decodeMessage(decoder, magicByte)\n  return Object.assign({ offset, size, crc, magicByte }, message)\n}\n","const versions = {\n  0: require('./v0'),\n  1: require('./v1'),\n}\n\nmodule.exports = ({ version = 0 }) => versions[version]\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression } = require('../compression')\n\n/**\n * v0\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, key, value }) => {\n  const content = new Encoder()\n    .writeInt8(0) // magicByte\n    .writeInt8(compression & 0x3)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  timestamp: decoder.readInt64().toString(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression } = require('../compression')\n\n/**\n * v1 (supported since 0.10.0)\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Timestamp => int64\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, timestamp = Date.now(), key, value }) => {\n  const content = new Encoder()\n    .writeInt8(1) // magicByte\n    .writeInt8(compression & 0x3)\n    .writeInt64(timestamp)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","const Long = require('../../utils/long')\nconst Decoder = require('../decoder')\nconst MessageDecoder = require('../message/decoder')\nconst { lookupCodecByAttributes } = require('../message/compression')\nconst { KafkaJSPartialMessageError } = require('../../errors')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = []\n  const messageSetSize = size || primaryDecoder.readInt32()\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize)\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder)\n      const codec = lookupCodecByAttributes(message.attributes)\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value)\n        messages.push(...EntriesDecoder(new Decoder(buffer), message))\n      } else {\n        messages.push(message)\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break\n      }\n\n      throw e\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize)\n  return messages\n}\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = []\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder))\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset)\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset)\n    const baseOffset = compressedOffset - lastMessageOffset\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset)\n        .add(baseOffset)\n        .toString()\n    }\n  }\n\n  return messages\n}\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the offset`\n    )\n  }\n\n  const offset = decoder.readInt64().toString()\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the message size`\n    )\n  }\n\n  const size = decoder.readInt32()\n  return MessageDecoder(offset, size, decoder)\n}\n","const Encoder = require('../encoder')\nconst MessageProtocol = require('../message')\nconst { Types } = require('../message/compression')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\n/**\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n */\nmodule.exports = ({ messageVersion = 0, compression, entries }) => {\n  const isCompressed = compression !== Types.None\n  const Message = MessageProtocol({ version: messageVersion })\n  const encoder = new Encoder()\n\n  // Messages in a message set are __not__ encoded as an array.\n  // They are written in sequence.\n  // https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Messagesets\n\n  entries.forEach((entry, i) => {\n    const message = Message(entry)\n\n    // This is the offset used in kafka as the log sequence number.\n    // When the producer is sending non compressed messages, it can set the offsets to anything\n    // When the producer is sending compressed messages, to avoid server side recompression, each compressed message\n    // should have offset starting from 0 and increasing by one for each inner message in the compressed message\n    encoder.writeInt64(isCompressed ? i : -1)\n    encoder.writeInt32(message.size())\n\n    encoder.writeEncoder(message)\n  })\n\n  return encoder\n}\n","/**\n * A javascript implementation of the CRC32 checksum that uses\n * the CRC32-C polynomial, the same polynomial used by iSCSI\n *\n * also known as CRC32 Castagnoli\n * based on: https://github.com/ashi009/node-fast-crc32c/blob/master/impls/js_crc32c.js\n */\nconst crc32C = buffer => {\n  let crc = 0 ^ -1\n  for (let i = 0; i < buffer.length; i++) {\n    crc = T[(crc ^ buffer[i]) & 0xff] ^ (crc >>> 8)\n  }\n\n  return (crc ^ -1) >>> 0\n}\n\nmodule.exports = crc32C\n\n// prettier-ignore\nvar T = new Int32Array([\n  0x00000000, 0xf26b8303, 0xe13b70f7, 0x1350f3f4,\n  0xc79a971f, 0x35f1141c, 0x26a1e7e8, 0xd4ca64eb,\n  0x8ad958cf, 0x78b2dbcc, 0x6be22838, 0x9989ab3b,\n  0x4d43cfd0, 0xbf284cd3, 0xac78bf27, 0x5e133c24,\n  0x105ec76f, 0xe235446c, 0xf165b798, 0x030e349b,\n  0xd7c45070, 0x25afd373, 0x36ff2087, 0xc494a384,\n  0x9a879fa0, 0x68ec1ca3, 0x7bbcef57, 0x89d76c54,\n  0x5d1d08bf, 0xaf768bbc, 0xbc267848, 0x4e4dfb4b,\n  0x20bd8ede, 0xd2d60ddd, 0xc186fe29, 0x33ed7d2a,\n  0xe72719c1, 0x154c9ac2, 0x061c6936, 0xf477ea35,\n  0xaa64d611, 0x580f5512, 0x4b5fa6e6, 0xb93425e5,\n  0x6dfe410e, 0x9f95c20d, 0x8cc531f9, 0x7eaeb2fa,\n  0x30e349b1, 0xc288cab2, 0xd1d83946, 0x23b3ba45,\n  0xf779deae, 0x05125dad, 0x1642ae59, 0xe4292d5a,\n  0xba3a117e, 0x4851927d, 0x5b016189, 0xa96ae28a,\n  0x7da08661, 0x8fcb0562, 0x9c9bf696, 0x6ef07595,\n  0x417b1dbc, 0xb3109ebf, 0xa0406d4b, 0x522bee48,\n  0x86e18aa3, 0x748a09a0, 0x67dafa54, 0x95b17957,\n  0xcba24573, 0x39c9c670, 0x2a993584, 0xd8f2b687,\n  0x0c38d26c, 0xfe53516f, 0xed03a29b, 0x1f682198,\n  0x5125dad3, 0xa34e59d0, 0xb01eaa24, 0x42752927,\n  0x96bf4dcc, 0x64d4cecf, 0x77843d3b, 0x85efbe38,\n  0xdbfc821c, 0x2997011f, 0x3ac7f2eb, 0xc8ac71e8,\n  0x1c661503, 0xee0d9600, 0xfd5d65f4, 0x0f36e6f7,\n  0x61c69362, 0x93ad1061, 0x80fde395, 0x72966096,\n  0xa65c047d, 0x5437877e, 0x4767748a, 0xb50cf789,\n  0xeb1fcbad, 0x197448ae, 0x0a24bb5a, 0xf84f3859,\n  0x2c855cb2, 0xdeeedfb1, 0xcdbe2c45, 0x3fd5af46,\n  0x7198540d, 0x83f3d70e, 0x90a324fa, 0x62c8a7f9,\n  0xb602c312, 0x44694011, 0x5739b3e5, 0xa55230e6,\n  0xfb410cc2, 0x092a8fc1, 0x1a7a7c35, 0xe811ff36,\n  0x3cdb9bdd, 0xceb018de, 0xdde0eb2a, 0x2f8b6829,\n  0x82f63b78, 0x709db87b, 0x63cd4b8f, 0x91a6c88c,\n  0x456cac67, 0xb7072f64, 0xa457dc90, 0x563c5f93,\n  0x082f63b7, 0xfa44e0b4, 0xe9141340, 0x1b7f9043,\n  0xcfb5f4a8, 0x3dde77ab, 0x2e8e845f, 0xdce5075c,\n  0x92a8fc17, 0x60c37f14, 0x73938ce0, 0x81f80fe3,\n  0x55326b08, 0xa759e80b, 0xb4091bff, 0x466298fc,\n  0x1871a4d8, 0xea1a27db, 0xf94ad42f, 0x0b21572c,\n  0xdfeb33c7, 0x2d80b0c4, 0x3ed04330, 0xccbbc033,\n  0xa24bb5a6, 0x502036a5, 0x4370c551, 0xb11b4652,\n  0x65d122b9, 0x97baa1ba, 0x84ea524e, 0x7681d14d,\n  0x2892ed69, 0xdaf96e6a, 0xc9a99d9e, 0x3bc21e9d,\n  0xef087a76, 0x1d63f975, 0x0e330a81, 0xfc588982,\n  0xb21572c9, 0x407ef1ca, 0x532e023e, 0xa145813d,\n  0x758fe5d6, 0x87e466d5, 0x94b49521, 0x66df1622,\n  0x38cc2a06, 0xcaa7a905, 0xd9f75af1, 0x2b9cd9f2,\n  0xff56bd19, 0x0d3d3e1a, 0x1e6dcdee, 0xec064eed,\n  0xc38d26c4, 0x31e6a5c7, 0x22b65633, 0xd0ddd530,\n  0x0417b1db, 0xf67c32d8, 0xe52cc12c, 0x1747422f,\n  0x49547e0b, 0xbb3ffd08, 0xa86f0efc, 0x5a048dff,\n  0x8ecee914, 0x7ca56a17, 0x6ff599e3, 0x9d9e1ae0,\n  0xd3d3e1ab, 0x21b862a8, 0x32e8915c, 0xc083125f,\n  0x144976b4, 0xe622f5b7, 0xf5720643, 0x07198540,\n  0x590ab964, 0xab613a67, 0xb831c993, 0x4a5a4a90,\n  0x9e902e7b, 0x6cfbad78, 0x7fab5e8c, 0x8dc0dd8f,\n  0xe330a81a, 0x115b2b19, 0x020bd8ed, 0xf0605bee,\n  0x24aa3f05, 0xd6c1bc06, 0xc5914ff2, 0x37faccf1,\n  0x69e9f0d5, 0x9b8273d6, 0x88d28022, 0x7ab90321,\n  0xae7367ca, 0x5c18e4c9, 0x4f48173d, 0xbd23943e,\n  0xf36e6f75, 0x0105ec76, 0x12551f82, 0xe03e9c81,\n  0x34f4f86a, 0xc69f7b69, 0xd5cf889d, 0x27a40b9e,\n  0x79b737ba, 0x8bdcb4b9, 0x988c474d, 0x6ae7c44e,\n  0xbe2da0a5, 0x4c4623a6, 0x5f16d052, 0xad7d5351\n]);\n","const crc32C = require('./crc32C')\nconst unsigned = value => Uint32Array.from([value])[0]\n\nmodule.exports = buffer => unsigned(crc32C(buffer))\n","module.exports = decoder => ({\n  key: decoder.readVarIntString(),\n  value: decoder.readVarIntBytes(),\n})\n","const Encoder = require('../../../encoder')\n\n/**\n * v0\n * Header => Key Value\n *   Key => varInt|string\n *   Value => varInt|bytes\n */\n\nmodule.exports = ({ key, value }) => {\n  return new Encoder().writeVarIntString(key).writeVarIntBytes(value)\n}\n","const Long = require('../../../../utils/long')\nconst HeaderDecoder = require('../../header/v0/decoder')\nconst TimestampTypes = require('../../../timestampTypes')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\nmodule.exports = (decoder, batchContext = {}) => {\n  const {\n    firstOffset,\n    firstTimestamp,\n    magicByte,\n    isControlBatch = false,\n    timestampType,\n    maxTimestamp,\n  } = batchContext\n  const attributes = decoder.readInt8()\n\n  const timestampDelta = decoder.readVarLong()\n  const timestamp =\n    timestampType === TimestampTypes.LOG_APPEND_TIME && maxTimestamp\n      ? maxTimestamp\n      : Long.fromValue(firstTimestamp)\n          .add(timestampDelta)\n          .toString()\n\n  const offsetDelta = decoder.readVarInt()\n  const offset = Long.fromValue(firstOffset)\n    .add(offsetDelta)\n    .toString()\n\n  const key = decoder.readVarIntBytes()\n  const value = decoder.readVarIntBytes()\n  const headers = decoder\n    .readVarIntArray(HeaderDecoder)\n    .reduce((obj, { key, value }) => ({ ...obj, [key]: value }), {})\n\n  return {\n    magicByte,\n    attributes, // Record level attributes are presently unused\n    timestamp,\n    offset,\n    key,\n    value,\n    headers,\n    isControlRecord: isControlBatch,\n    batchContext,\n  }\n}\n","const Encoder = require('../../../encoder')\nconst Header = require('../../header/v0')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\n/**\n * @param [offsetDelta=0] {Integer}\n * @param [timestampDelta=0] {Long}\n * @param key {Buffer}\n * @param value {Buffer}\n * @param [headers={}] {Object}\n */\nmodule.exports = ({ offsetDelta = 0, timestampDelta = 0, key, value, headers = {} }) => {\n  const headersArray = Object.keys(headers).map(headerKey => ({\n    key: headerKey,\n    value: headers[headerKey],\n  }))\n\n  const sizeOfBody =\n    1 + // always one byte for attributes\n    Encoder.sizeOfVarLong(timestampDelta) +\n    Encoder.sizeOfVarInt(offsetDelta) +\n    Encoder.sizeOfVarIntBytes(key) +\n    Encoder.sizeOfVarIntBytes(value) +\n    sizeOfHeaders(headersArray)\n\n  return new Encoder()\n    .writeVarInt(sizeOfBody)\n    .writeInt8(0) // no used record attributes at the moment\n    .writeVarLong(timestampDelta)\n    .writeVarInt(offsetDelta)\n    .writeVarIntBytes(key)\n    .writeVarIntBytes(value)\n    .writeVarIntArray(headersArray.map(Header))\n}\n\nconst sizeOfHeaders = headersArray => {\n  let size = Encoder.sizeOfVarInt(headersArray.length)\n\n  for (const header of headersArray) {\n    const keySize = Buffer.byteLength(header.key)\n    const valueSize = Buffer.byteLength(header.value)\n\n    size += Encoder.sizeOfVarInt(keySize) + keySize\n\n    if (header.value === null) {\n      size += Encoder.sizeOfVarInt(-1)\n    } else {\n      size += Encoder.sizeOfVarInt(valueSize) + valueSize\n    }\n  }\n\n  return size\n}\n","const Decoder = require('../../decoder')\nconst { KafkaJSPartialMessageError } = require('../../../errors')\nconst { lookupCodecByRecordBatchAttributes } = require('../../message/compression')\nconst RecordDecoder = require('../record/v0/decoder')\nconst TimestampTypes = require('../../timestampTypes')\n\nconst TIMESTAMP_TYPE_FLAG_MASK = 0x8\nconst TRANSACTIONAL_FLAG_MASK = 0x10\nconst CONTROL_FLAG_MASK = 0x20\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nmodule.exports = async fetchDecoder => {\n  const firstOffset = fetchDecoder.readInt64().toString()\n  const length = fetchDecoder.readInt32()\n  const decoder = fetchDecoder.slice(length)\n  fetchDecoder.forward(length)\n\n  const remainingBytes = Buffer.byteLength(decoder.buffer)\n\n  if (remainingBytes < length) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`\n    )\n  }\n\n  const partitionLeaderEpoch = decoder.readInt32()\n\n  // The magic byte was read by the Fetch protocol to distinguish between\n  // the record batch and the legacy message set. It's not used here but\n  // it has to be read.\n  const magicByte = decoder.readInt8() // eslint-disable-line no-unused-vars\n\n  // The library is currently not performing CRC validations\n  const crc = decoder.readInt32() // eslint-disable-line no-unused-vars\n\n  const attributes = decoder.readInt16()\n  const lastOffsetDelta = decoder.readInt32()\n  const firstTimestamp = decoder.readInt64().toString()\n  const maxTimestamp = decoder.readInt64().toString()\n  const producerId = decoder.readInt64().toString()\n  const producerEpoch = decoder.readInt16()\n  const firstSequence = decoder.readInt32()\n\n  const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0\n  const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0\n  const timestampType =\n    (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0\n      ? TimestampTypes.LOG_APPEND_TIME\n      : TimestampTypes.CREATE_TIME\n\n  const codec = lookupCodecByRecordBatchAttributes(attributes)\n\n  const recordContext = {\n    firstOffset,\n    firstTimestamp,\n    partitionLeaderEpoch,\n    inTransaction,\n    isControlBatch,\n    lastOffsetDelta,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    maxTimestamp,\n    timestampType,\n  }\n\n  const records = await decodeRecords(codec, decoder, { ...recordContext, magicByte })\n\n  return {\n    ...recordContext,\n    records,\n  }\n}\n\nconst decodeRecords = async (codec, recordsDecoder, recordContext) => {\n  if (!codec) {\n    return recordsDecoder.readArray(decoder => decodeRecord(decoder, recordContext))\n  }\n\n  const length = recordsDecoder.readInt32()\n\n  if (length <= 0) {\n    return []\n  }\n\n  const compressedRecordsBuffer = recordsDecoder.readAll()\n  const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer)\n  const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer)\n  const records = new Array(length)\n\n  for (let i = 0; i < length; i++) {\n    records[i] = decodeRecord(decompressedRecordDecoder, recordContext)\n  }\n\n  return records\n}\n\nconst decodeRecord = (decoder, recordContext) => {\n  const recordBuffer = decoder.readVarIntBytes()\n  return RecordDecoder(new Decoder(recordBuffer), recordContext)\n}\n","const Long = require('../../../utils/long')\nconst Encoder = require('../../encoder')\nconst crc32C = require('../crc32C')\nconst { Types: Compression, lookupCodec } = require('../../message/compression')\n\nconst MAGIC_BYTE = 2\nconst COMPRESSION_MASK = 3 // The lowest 3 bits\nconst TIMESTAMP_MASK = 0 // The fourth lowest bit, always set this bit to 0 (since 0.10.0)\nconst TRANSACTIONAL_MASK = 16 // The fifth lowest bit\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nconst RecordBatch = async ({\n  compression = Compression.None,\n  firstOffset = Long.fromInt(0),\n  firstTimestamp = Date.now(),\n  maxTimestamp = Date.now(),\n  partitionLeaderEpoch = 0,\n  lastOffsetDelta = 0,\n  transactional = false,\n  producerId = Long.fromValue(-1), // for idempotent messages\n  producerEpoch = 0, // for idempotent messages\n  firstSequence = 0, // for idempotent messages\n  records = [],\n}) => {\n  const COMPRESSION_CODEC = compression & COMPRESSION_MASK\n  const IN_TRANSACTION = transactional ? TRANSACTIONAL_MASK : 0\n  const attributes = COMPRESSION_CODEC | TIMESTAMP_MASK | IN_TRANSACTION\n\n  const batchBody = new Encoder()\n    .writeInt16(attributes)\n    .writeInt32(lastOffsetDelta)\n    .writeInt64(firstTimestamp)\n    .writeInt64(maxTimestamp)\n    .writeInt64(producerId)\n    .writeInt16(producerEpoch)\n    .writeInt32(firstSequence)\n\n  if (compression === Compression.None) {\n    if (records.every(v => typeof v === typeof records[0])) {\n      batchBody.writeArray(records, typeof records[0])\n    } else {\n      batchBody.writeArray(records)\n    }\n  } else {\n    const compressedRecords = await compressRecords(compression, records)\n    batchBody.writeInt32(records.length).writeBuffer(compressedRecords)\n  }\n\n  // CRC32C validation is happening here:\n  // https://github.com/apache/kafka/blob/0.11.0.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L148\n\n  const batch = new Encoder()\n    .writeInt32(partitionLeaderEpoch)\n    .writeInt8(MAGIC_BYTE)\n    .writeUInt32(crc32C(batchBody.buffer))\n    .writeEncoder(batchBody)\n\n  return new Encoder().writeInt64(firstOffset).writeBytes(batch.buffer)\n}\n\nconst compressRecords = async (compression, records) => {\n  const codec = lookupCodec(compression)\n  const recordsEncoder = new Encoder()\n\n  recordsEncoder.writeEncoderArray(records)\n\n  return codec.compress(recordsEncoder)\n}\n\nmodule.exports = {\n  RecordBatch,\n  MAGIC_BYTE,\n}\n","const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, groupId }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddOffsetsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddOffsetsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch group_id\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   group_id => STRING\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddOffsetsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * AddOffsetsToTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddPartitionsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddPartitionsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddPartitionsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = partition => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AddPartitionsToTxn Response (Version: 0) => throttle_time_ms [errors]\n *   throttle_time_ms => INT32\n *   errors => topic [partition_errors]\n *     topic => STRING\n *     partition_errors => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errors = await decoder.readArrayAsync(decodeError)\n\n  return {\n    throttleTime,\n    errors,\n  }\n}\n\nconst decodeError = async decoder => ({\n  topic: decoder.readString(),\n  partitionErrors: await decoder.readArrayAsync(decodePartitionError),\n})\n\nconst decodePartitionError = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.errors\n    .map(({ partitionErrors }) => ({\n      partitionsWithErrors: partitionErrors.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ resources, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources, validateOnly }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AlterConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * AlterConfigs Request (Version: 0) => [resources] validate_only\n *   resources => resource_type resource_name [config_entries]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   validate_only => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of resources to change\n * @param {boolean} [validateOnly=false]\n */\nmodule.exports = ({ resources, validateOnly = false }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AlterConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(validateOnly)\n  },\n})\n\nconst encodeResource = ({ type, name, configEntries }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AlterConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n */\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","module.exports = {\n  Produce: 0,\n  Fetch: 1,\n  ListOffsets: 2,\n  Metadata: 3,\n  LeaderAndIsr: 4,\n  StopReplica: 5,\n  UpdateMetadata: 6,\n  ControlledShutdown: 7,\n  OffsetCommit: 8,\n  OffsetFetch: 9,\n  GroupCoordinator: 10,\n  JoinGroup: 11,\n  Heartbeat: 12,\n  LeaveGroup: 13,\n  SyncGroup: 14,\n  DescribeGroups: 15,\n  ListGroups: 16,\n  SaslHandshake: 17,\n  ApiVersions: 18, // ApiVersions v0 on Kafka 0.10\n  CreateTopics: 19,\n  DeleteTopics: 20,\n  DeleteRecords: 21,\n  InitProducerId: 22,\n  OffsetForLeaderEpoch: 23,\n  AddPartitionsToTxn: 24,\n  AddOffsetsToTxn: 25,\n  EndTxn: 26,\n  WriteTxnMarkers: 27,\n  TxnOffsetCommit: 28,\n  DescribeAcls: 29,\n  CreateAcls: 30,\n  DeleteAcls: 31,\n  DescribeConfigs: 32,\n  AlterConfigs: 33, // ApiVersions v0 and v1 on Kafka 0.11\n  AlterReplicaLogDirs: 34,\n  DescribeLogDirs: 35,\n  SaslAuthenticate: 36,\n  CreatePartitions: 37,\n  CreateDelegationToken: 38,\n  RenewDelegationToken: 39,\n  ExpireDelegationToken: 40,\n  DescribeDelegationToken: 41,\n  DeleteGroups: 42, // ApiVersions v2 on Kafka 1.0\n  ElectPreferredLeaders: 43,\n}\n","const logResponseError = false\n\nconst versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response, logResponseError: true }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response, logResponseError }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response, logResponseError }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ApiVersions: apiKey } = require('../../apiKeys')\n\n/**\n * ApiVersionRequest => ApiKeys\n */\n\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ApiVersions',\n  encode: async () => new Encoder(),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * ApiVersionResponse => ApiVersions\n *   ErrorCode = INT16\n *   ApiVersions = [ApiVersion]\n *     ApiVersion = ApiKey MinVersion MaxVersion\n *       ApiKey = INT16\n *       MinVersion = INT16\n *       MaxVersion = INT16\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    apiVersions: decoder.readArray(apiVersion),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * ApiVersions Response (Version: 1) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const apiVersions = decoder.readArray(apiVersion)\n\n  /**\n   * The Java client defaults this value to 0 if not present,\n   * even though it is required in the protocol. This is to\n   * work around https://github.com/tulios/kafkajs/issues/491\n   *\n   * See:\n   * https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java#L23-L25\n   */\n  const throttleTime = decoder.canReadInt32() ? decoder.readInt32() : 0\n\n  return {\n    errorCode,\n    apiVersions,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 2 })\n","const { parse: parseV1, decode: decodeV1 } = require('../v1/response')\n\n/**\n * ApiVersions Response (Version: 2) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  parse: parseV1,\n  decode: decodeV1,\n}\n","const versions = {\n  0: ({ topicPartitions, timeout, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topicPartitions, timeout, validateOnly }), response }\n  },\n  1: ({ topicPartitions, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topicPartitions, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreatePartitions: apiKey } = require('../../apiKeys')\n\n/**\n * CreatePartitions Request (Version: 0) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreatePartitions',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topicPartitions.map(encodeTopicPartitions))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopicPartitions = ({ topic, count, assignments = [] }) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(count)\n    .writeNullableArray(assignments.map(encodeAssignments))\n}\n\nconst encodeAssignments = brokerIds => {\n  return new Encoder().writeNullableArray(brokerIds)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/*\n * CreatePartitions Response (Version: 0) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  return {\n    throttleTime,\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * CreatePartitions Request (Version: 1) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly, timeout }) =>\n  Object.assign(requestV0({ topicPartitions, validateOnly, timeout }), { apiVersion: 1 })\n","module.exports = require('../v0/response')\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n  2: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n * CreateTopics Request (Version: 0) => [create_topic_requests] timeout\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics.map(encodeTopics)).writeInt32(timeout)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * CreateTopics Response (Version: 0) => [topic_errors]\n *   topic_errors => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n *CreateTopics Request (Version: 1) => [create_topic_requests] timeout validate_only\n *  create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *    topic => STRING\n *    num_partitions => INT32\n *    replication_factor => INT16\n *    replica_assignment => partition [replicas]\n *      partition => INT32\n *      replicas => INT32\n *    config_entries => config_name config_value\n *      config_name => STRING\n *      config_value => NULLABLE_STRING\n *  timeout => INT32\n *  validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topics.map(encodeTopics))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * CreateTopics Response (Version: 1) => [topic_errors]\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * CreateTopics Request (Version: 2) => [create_topic_requests] timeout validate_only\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly, timeout }) =>\n  Object.assign(requestV1({ topics, validateOnly, timeout }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\n\n/**\n * CreateTopics Response (Version: 2) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const versions = {\n  0: groupIds => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(groupIds), response }\n  },\n  1: groupIds => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(groupIds), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteGroups Request (Version: 0) => [groups_names]\n *   groups_names => STRING\n */\n\n/**\n */\nmodule.exports = groupIds => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds.map(encodeGroups))\n  },\n})\n\nconst encodeGroups = group => {\n  return new Encoder().writeString(group)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n/**\n * DeleteGroups Response (Version: 0) => throttle_time_ms [results]\n *  throttle_time_ms => INT32\n *  results => group_id error_code\n *    group_id => STRING\n *    error_code => INT16\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTimeMs = decoder.readInt32()\n  const results = decoder.readArray(decodeGroup)\n\n  for (const result of results) {\n    if (failure(result.errorCode)) {\n      result.error = createErrorFromCode(result.errorCode)\n    }\n  }\n  return {\n    throttleTimeMs,\n    results,\n  }\n}\n\nconst parse = async data => {\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteGroups Request (Version: 1)\n */\n\nmodule.exports = groupIds => Object.assign(requestV0(groupIds), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\nmodule.exports = responseV0\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteTopics: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteTopics Request (Version: 0) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics).writeInt32(timeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteTopics Response (Version: 0) => [topic_error_codes]\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteTopics Request (Version: 1) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout }) =>\n  Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * DeleteTopics Response (Version: 1) => throttle_time_ms [topic_error_codes]\n *   throttle_time_ms => INT32\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ resources }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources }), response }\n  },\n  1: ({ resources, includeSynonyms }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ resources, includeSynonyms }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 0) => [resources]\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n */\nmodule.exports = ({ resources }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource))\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only is_default is_sensitive\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       is_default => BOOLEAN\n *       is_sensitive => BOOLEAN\n */\n\nconst decodeConfigEntries = decoder => ({\n  configName: decoder.readString(),\n  configValue: decoder.readString(),\n  readOnly: decoder.readBoolean(),\n  isDefault: decoder.readBoolean(),\n  isSensitive: decoder.readBoolean(),\n})\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  configEntries: decoder.readArray(decodeConfigEntries),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 1) => [resources] include_synonyms\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n *   include_synonyms => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n * @param [includeSynonyms=false]\n */\nmodule.exports = ({ resources, includeSynonyms = false }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(includeSynonyms)\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * DescribeConfigs Response (Version: 1) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only config_source is_sensitive [config_synonyms]\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       config_source => INT8\n *       is_sensitive => BOOLEAN\n *       config_synonyms => config_name config_value config_source\n *         config_name => STRING\n *         config_value => NULLABLE_STRING\n *         config_source => INT8\n */\n\nconst decodeSynonyms = decoder => ({\n  configName: decoder.readString(),\n  configValue: decoder.readString(),\n  configSource: decoder.readInt8(),\n})\n\nconst decodeConfigEntries = decoder => ({\n  configName: decoder.readString(),\n  configValue: decoder.readString(),\n  readOnly: decoder.readBoolean(),\n  isDefault: decoder.readBoolean(),\n  isSensitive: decoder.readBoolean(),\n  configSynonyms: decoder.readArray(decodeSynonyms),\n})\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  configEntries: decoder.readArray(decodeConfigEntries),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ groupIds }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupIds }), response }\n  },\n  1: ({ groupIds }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupIds }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeGroups Request (Version: 0) => [group_ids]\n *   group_ids => STRING\n */\n\n/**\n * @param {Array} groupIds List of groupIds to request metadata for (an empty groupId array will return empty group metadata)\n */\nmodule.exports = ({ groupIds }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeGroups Response (Version: 0) => [groups]\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    groups,\n  }\n}\n\nconst parse = async data => {\n  const groupsWithError = data.groups.filter(({ errorCode }) => failure(errorCode))\n  if (groupsWithError.length > 0) {\n    throw createErrorFromCode(groupsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DescribeGroups Request (Version: 1) => [group_ids]\n *   group_ids => STRING\n */\n\nmodule.exports = ({ groupIds }) => Object.assign(requestV0({ groupIds }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * DescribeGroups Response (Version: 1) => throttle_time_ms [groups]\n *   throttle_time_ms => INT32\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    throttleTime,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, producerId, producerEpoch, transactionResult }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { EndTxn: apiKey } = require('../../apiKeys')\n\n/**\n * EndTxn Request (Version: 0) => transactional_id producer_id producer_epoch transaction_result\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   transaction_result => BOOLEAN\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'EndTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeBoolean(transactionResult)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * EndTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\nconst NETWORK_DELAY = 100\n\n/**\n * The FETCH request can block up to maxWaitTime, which can be bigger than the configured\n * request timeout. It's safer to always use the maxWaitTime\n **/\nconst requestTimeout = timeout =>\n  Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  1: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  2: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  3: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, maxBytes, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  4: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  5: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  6: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  7: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  8: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v8/request')\n    const response = require('./v8/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  9: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v9/request')\n    const response = require('./v9/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  10: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v10/request')\n    const response = require('./v10/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  11: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n    rackId,\n  }) => {\n    const request = require('./v11/request')\n    const response = require('./v11/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n        rackId,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 0) => replica_id max_wait_time min_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { KafkaJSOffsetOutOfRange } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    responses,\n  }\n}\n\nconst { code: OFFSET_OUT_OF_RANGE_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'OFFSET_OUT_OF_RANGE'\n)\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(({ topicName, partitions }) => {\n    return partitions\n      .filter(partition => failure(partition.errorCode))\n      .map(partition => Object.assign({}, partition, { topic: topicName }))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode, topic, partition } = errors[0]\n    if (errorCode === OFFSET_OUT_OF_RANGE_ERROR_CODE) {\n      throw new KafkaJSOffsetOutOfRange(createErrorFromCode(errorCode), { topic, partition })\n    }\n\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 1) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV9 = require('../v9/request')\n\n/**\n * ZStd Compression\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression\n */\n\n/**\n * Fetch Request (Version: 10) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV9({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 10 }\n  )\n","const { decode, parse } = require('../v9/response')\n\n/**\n * Fetch Response (Version: 10) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow consumers to fetch from closest replica\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica\n */\n\n/**\n * Fetch Request (Version: 11) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n *   rack_id => STRING\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  rackId = '',\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 11,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n      .writeString(rackId)\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 11) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *         preferred_read_replica => INT32\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  preferredReadReplica: decoder.readInt32(),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 2 })\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 2) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *      record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 3) => replica_id max_wait_time min_bytes max_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {number} maxBytes Maximum bytes to accumulate in the response. Note that this is not an absolute maximum,\n *                          if the first message in the first non-empty partition of the fetch is larger than this value,\n *                          the message will still be returned to ensure that progress can be made.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, maxBytes, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Decoder = require('../../../decoder')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\nconst RecordBatchDecoder = require('../../../recordBatch/v0/decoder')\nconst { MAGIC_BYTE } = require('../../../recordBatch/v0')\n\n// the magic offset is at the same offset for all current message formats, but the 4 bytes\n// between the size and the magic is dependent on the version.\nconst MAGIC_OFFSET = 16\nconst RECORD_BATCH_OVERHEAD = 49\n\nconst decodeMessages = async decoder => {\n  const messagesSize = decoder.readInt32()\n\n  if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {\n    return []\n  }\n\n  const messagesBuffer = decoder.readBytes(messagesSize)\n  const messagesDecoder = new Decoder(messagesBuffer)\n  const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0)\n\n  if (magicByte === MAGIC_BYTE) {\n    let records = []\n\n    while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {\n      try {\n        const recordBatch = await RecordBatchDecoder(messagesDecoder)\n        records = [...records, ...recordBatch.records]\n      } catch (e) {\n        // The tail of the record batches can have incomplete records\n        // due to how maxBytes works. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI\n        if (e.name === 'KafkaJSPartialMessageError') {\n          break\n        }\n\n        throw e\n      }\n    }\n\n    return records\n  }\n\n  return MessageSetDecoder(messagesDecoder, messagesSize)\n}\n\nmodule.exports = decodeMessages\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 4) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('./decodeMessages')\n\n/**\n * Fetch Response (Version: 4) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 5) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 5) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *        last_stable_offset => INT64\n *        log_start_offset => INT64\n *        aborted_transactions => producer_id first_offset\n *          producer_id => INT64\n *          first_offset => INT64\n *      record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV5 = require('../v5/request')\n\n/**\n * Fetch Request (Version: 6) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) =>\n  Object.assign(\n    requestV5({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n    }),\n    { apiVersion: 6 }\n  )\n","const { decode, parse } = require('../v5/response')\n\n/**\n * Fetch Response (Version: 6) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Sessions are only used by followers\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability\n */\n\n/**\n * Fetch Request (Version: 7) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 7,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 7) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV7 = require('../v7/request')\n\n/**\n * Quota violation brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n */\n\n/**\n * Fetch Request (Version: 8) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV7({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 8 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 8) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow fetchers to detect and handle log truncation\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation\n */\n\n/**\n * Fetch Request (Version: 9) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 9,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v8/response')\n\n/**\n * Fetch Response (Version: 9) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const COORDINATOR_TYPES = require('../../coordinatorTypes')\n\nconst versions = {\n  0: ({ groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId }), response }\n  },\n  1: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ coordinatorKey: groupId, coordinatorType }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 0) => group_id\n *   group_id => STRING\n */\n\nmodule.exports = ({ groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 0) => error_code coordinator\n *  error_code => INT16\n *  coordinator => node_id host port\n *    node_id => INT32\n *    host => STRING\n *    port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    errorCode,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 1) => coordinator_key coordinator_type\n *   coordinator_key => STRING\n *   coordinator_type => INT8\n */\n\nmodule.exports = ({ coordinatorKey, coordinatorType }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(coordinatorKey).writeInt8(coordinatorType)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 1) => throttle_time_ms error_code error_message coordinator\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   coordinator => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const errorMessage = decoder.readString()\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    throttleTime,\n    errorCode,\n    errorMessage,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Heartbeat: apiKey } = require('../../apiKeys')\n\n/**\n * Heartbeat Request (Version: 0) => group_id group_generation_id member_id\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Heartbeat',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * Heartbeat Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Heartbeat Request (Version: 1) => group_id generation_id member_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) =>\n  Object.assign(requestV0({ groupId, groupGenerationId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Heartbeat Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const apiKeys = require('./apiKeys')\nconst { KafkaJSServerDoesNotSupportApiKey } = require('../../errors')\n\nconst requests = {\n  Produce: require('./produce'),\n  Fetch: require('./fetch'),\n  ListOffsets: require('./listOffsets'),\n  Metadata: require('./metadata'),\n  LeaderAndIsr: {},\n  StopReplica: {},\n  UpdateMetadata: {},\n  ControlledShutdown: {},\n  OffsetCommit: require('./offsetCommit'),\n  OffsetFetch: require('./offsetFetch'),\n  GroupCoordinator: require('./findCoordinator'),\n  JoinGroup: require('./joinGroup'),\n  Heartbeat: require('./heartbeat'),\n  LeaveGroup: require('./leaveGroup'),\n  SyncGroup: require('./syncGroup'),\n  DescribeGroups: require('./describeGroups'),\n  ListGroups: require('./listGroups'),\n  SaslHandshake: require('./saslHandshake'),\n  ApiVersions: require('./apiVersions'),\n  CreateTopics: require('./createTopics'),\n  DeleteTopics: require('./deleteTopics'),\n  DeleteRecords: {},\n  InitProducerId: require('./initProducerId'),\n  OffsetForLeaderEpoch: {},\n  AddPartitionsToTxn: require('./addPartitionsToTxn'),\n  AddOffsetsToTxn: require('./addOffsetsToTxn'),\n  EndTxn: require('./endTxn'),\n  WriteTxnMarkers: {},\n  TxnOffsetCommit: require('./txnOffsetCommit'),\n  DescribeAcls: {},\n  CreateAcls: {},\n  DeleteAcls: {},\n  DescribeConfigs: require('./describeConfigs'),\n  AlterConfigs: require('./alterConfigs'),\n  AlterReplicaLogDirs: {},\n  DescribeLogDirs: {},\n  SaslAuthenticate: require('./saslAuthenticate'),\n  CreatePartitions: require('./createPartitions'),\n  CreateDelegationToken: {},\n  RenewDelegationToken: {},\n  ExpireDelegationToken: {},\n  DescribeDelegationToken: {},\n  DeleteGroups: require('./deleteGroups'),\n}\n\nconst names = Object.keys(apiKeys)\nconst keys = Object.values(apiKeys)\nconst findApiName = apiKey => names[keys.indexOf(apiKey)]\n\nconst lookup = versions => (apiKey, definition) => {\n  const version = versions[apiKey]\n  const availableVersions = definition.versions.map(Number)\n  const bestImplementedVersion = Math.max.apply(this, availableVersions)\n\n  if (!version || version.maxVersion == null) {\n    throw new KafkaJSServerDoesNotSupportApiKey(\n      `The Kafka server does not support the requested API version`,\n      { apiKey, apiName: findApiName(apiKey) }\n    )\n  }\n\n  const bestSupportedVersion = Math.min(bestImplementedVersion, version.maxVersion)\n  return definition.protocol({ version: bestSupportedVersion })\n}\n\nmodule.exports = {\n  requests,\n  lookup,\n}\n","const versions = {\n  0: ({ transactionalId, transactionTimeout = 5000 }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, transactionTimeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { InitProducerId: apiKey } = require('../../apiKeys')\n\n/**\n * InitProducerId Request (Version: 0) => transactional_id transaction_timeout_ms\n *   transactional_id => NULLABLE_STRING\n *   transaction_timeout_ms => INT32\n */\n\nmodule.exports = ({ transactionalId, transactionTimeout }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'InitProducerId',\n  encode: async () => {\n    return new Encoder().writeString(transactionalId).writeInt32(transactionTimeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * InitProducerId Response (Version: 0) => throttle_time_ms error_code producer_id producer_epoch\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   producer_id => INT64\n *   producer_epoch => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    producerId: decoder.readInt64().toString(),\n    producerEpoch: decoder.readInt16(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const NETWORK_DELAY = 5000\n\n/**\n * @see https://github.com/apache/kafka/pull/5203\n * The JOIN_GROUP request may block up to sessionTimeout (or rebalanceTimeout in JoinGroupV1),\n * so we should override the requestTimeout to be a bit more than the sessionTimeout\n * NOTE: the sessionTimeout can be configured as Number.MAX_SAFE_INTEGER and overflow when\n * increased, so we have to check for potential overflows\n **/\nconst requestTimeout = ({ rebalanceTimeout, sessionTimeout }) => {\n  const timeout = rebalanceTimeout || sessionTimeout\n  return Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n}\n\nconst logResponseError = memberId => memberId != null && memberId !== ''\n\nconst versions = {\n  0: ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout: null, sessionTimeout }),\n    }\n  },\n  1: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  2: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  3: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  4: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n      logResponseError: logResponseError(memberId),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 0) => group_id session_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 0) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 1) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeInt32(rebalanceTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 1) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * JoinGroup Request (Version: 2) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV1({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 2 }\n  )\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 2) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * JoinGroup Request (Version: 3) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV2({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 3 }\n  )\n","const { parse: parseV0 } = require('../v0/response')\nconst { decode } = require('../v2/response')\n\n/**\n * JoinGroup Response (Version: 3) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * JoinGroup Request (Version: 4) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV3({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode } = require('../v3/response')\nconst { KafkaJSMemberIdRequired } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 4) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'MEMBER_ID_REQUIRED'\n)\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {\n      throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {\n        memberId: data.memberId,\n      })\n    }\n\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { LeaveGroup: apiKey } = require('../../apiKeys')\n\n/**\n * LeaveGroup Request (Version: 0) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'LeaveGroup',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * LeaveGroup Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * LeaveGroup Request (Version: 1) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) =>\n  Object.assign(requestV0({ groupId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * LeaveGroup Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListGroups: apiKey } = require('../../apiKeys')\n\n/**\n * ListGroups Request (Version: 0)\n */\n\n/**\n */\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListGroups',\n  encode: async () => {\n    return new Encoder()\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * ListGroups Response (Version: 0) => error_code [groups]\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  protocolType: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    errorCode,\n    groups,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decodeGroup,\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * ListGroups Request (Version: 1)\n */\n\nmodule.exports = () => Object.assign(requestV0(), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\nconst Decoder = require('../../../decoder')\n\n/**\n * ListGroups Response (Version: 1) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(responseV0.decodeGroup)\n\n  return {\n    throttleTime,\n    errorCode,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: responseV0.parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * ListGroups Request (Version: 2)\n */\n\nmodule.exports = () => Object.assign(requestV1(), { apiVersion: 2 })\n","const responseV1 = require('../v1/response')\n\n/**\n * ListGroups Response (Version: 2) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nmodule.exports = responseV1\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  1: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  2: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ replicaId, isolationLevel, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 0) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp max_num_offsets\n *       partition => INT32\n *       timestamp => INT64\n *       max_num_offsets => INT32\n */\n\n/**\n * @param {number} replicaId\n * @param {object} topics use timestamp=-1 for latest offsets and timestamp=-2 for earliest.\n *                        Default timestamp=-1. Example:\n *                          {\n *                            topics: [\n *                              {\n *                                topic: 'topic-name',\n *                                partitions: [{ partition: 0, timestamp: -1 }]\n *                              }\n *                            ]\n *                          }\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1, maxNumOffsets = 1 }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(timestamp)\n    .writeInt32(maxNumOffsets)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Offsets Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code [offsets]\n *       partition => INT32\n *       error_code => INT16\n *       offsets => INT64\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offsets: decoder.readArray(decodeOffsets),\n})\n\nconst decodeOffsets = decoder => decoder.readInt64().toString()\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 1) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 2) => replica_id isolation_level [topics]\n *   replica_id => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, isolationLevel, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 2) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics }), response }\n  },\n  1: ({ topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics }), response }\n  },\n  2: ({ topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics }), response }\n  },\n  3: ({ topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ topics }), response }\n  },\n  4: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n  5: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 0) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeArray(topics)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Metadata Response (Version: 0) => [brokers] [topic_metadata]\n *   brokers => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *   topic_metadata => topic_error_code topic [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  // leader: The node id for the kafka broker currently acting as leader\n  // for this partition\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithErrors = data.topicMetadata.filter(topic => failure(topic.topicErrorCode))\n  if (topicsWithErrors.length > 0) {\n    const { topicErrorCode } = topicsWithErrors[0]\n    throw createErrorFromCode(topicErrorCode)\n  }\n\n  const partitionsWithErrors = data.topicMetadata.map(topic => {\n    return topic.partitionMetadata.filter(partition => failure(partition.partitionErrorCode))\n  })\n\n  const errors = flatten(partitionsWithErrors)\n  if (errors.length > 0) {\n    const { partitionErrorCode } = errors[0]\n    throw createErrorFromCode(partitionErrorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Metadata Request (Version: 1) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV0({ topics }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 1) => [brokers] controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Metadata Request (Version: 2) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV0({ topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 2) => [brokers] cluster_id controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Metadata Request (Version: 3) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV0({ topics }), { apiVersion: 3 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 3) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 4) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeNullableArray(topics).writeBoolean(allowAutoTopicCreation)\n  },\n})\n","const { parse: parseV3, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Metadata Response (Version: 4) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nmodule.exports = {\n  parse: parseV3,\n  decode: decodeV3,\n}\n","const requestV4 = require('../v4/request')\n\n/**\n * Metadata Request (Version: 5) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) =>\n  Object.assign(requestV4({ topics, allowAutoTopicCreation }), { apiVersion: 5 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 5) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr] [offline_replicas]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n *       offline_replicas => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n  offlineReplicas: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","// This value signals to the broker that its default configuration should be used.\nconst RETENTION_TIME = -1\n\nconst versions = {\n  0: ({ groupId, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  1: ({ groupId, groupGenerationId, memberId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, groupGenerationId, memberId, topics }), response }\n  },\n  2: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  3: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 0) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetCommit Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 1) => group_id group_generation_id member_id [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset timestamp metadata\n *       partition => INT32\n *       offset => INT64\n *       timestamp => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, timestamp = Date.now(), metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeInt64(timestamp)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 2) => group_id group_generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeInt64(retentionTime)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * OffsetCommit Request (Version: 3) => group_id generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) =>\n  Object.assign(requestV2({ groupId, groupGenerationId, memberId, retentionTime, topics }), {\n    apiVersion: 3,\n  })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  1: ({ groupId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  2: ({ groupId, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  3: ({ groupId, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ groupId, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 1) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * OffsetFetch Request (Version: 2) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) =>\n  Object.assign(requestV1({ groupId, topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 2) => [responses] error_code\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 3) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeNullableArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV2 } = require('../v2/response')\n\n/**\n * OffsetFetch Response (Version: 3) => throttle_time_ms [responses] error_code\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV2,\n}\n","const versions = {\n  0: ({ acks, timeout, topicData }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  1: ({ acks, timeout, topicData }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  2: ({ acks, timeout, topicData, compression }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ acks, timeout, compression, topicData }), response }\n  },\n  3: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  4: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  5: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  6: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  7: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\n\n/**\n * Produce Request (Version: 0) => acks timeout [topic_data]\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set record_set_size\n *       partition => INT32\n *       record_set_size => INT32\n *       record_set => RECORDS\n */\n\n/**\n * MessageV0:\n * {\n *   key: bytes,\n *   value: bytes\n * }\n *\n * MessageSet:\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n *\n * TopicData:\n * [\n *   {\n *     topic: 'name1',\n *     partitions: [\n *       {\n *         partition: 0,\n *         messages: [<MessageSet>]\n *       }\n *     ]\n *   }\n * ]\n */\n\n/**\n * @param acks {Integer} This field indicates how many acknowledgements the servers should receive before\n *                       responding to the request. If it is 0 the server will not send any response\n *                       (this is the only case where the server will not reply to a request). If it is 1,\n *                       the server will wait the data is written to the local log before sending a response.\n *                       If it is -1 the server will block until the message is committed by all in sync replicas\n *                       before sending a response.\n *\n * @param timeout {Integer} This provides a maximum time in milliseconds the server can await the receipt of the number\n *                          of acknowledgements in RequiredAcks. The timeout is not an exact limit on the request time\n *                          for a few reasons:\n *                          (1) it does not include network latency,\n *                          (2) the timer begins at the beginning of the processing of this request so if many requests are\n *                              queued due to server overload that wait time will not be included,\n *                          (3) we will not terminate a local write so if the local write time exceeds this timeout it will not\n *                              be respected. To get a hard timeout of this type the client should use the socket timeout.\n *\n * @param topicData {Array}\n */\nmodule.exports = ({ acks, timeout, topicData }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(topicData.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartitions))\n}\n\nconst encodePartitions = ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 0, entries: messages })\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(messageSet.size())\n    .writeEncoder(messageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * v0\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]]\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  return {\n    topics,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(topic => {\n    return topic.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// Produce Request on or after v1 indicates the client can parse the quota throttle time\n// in the Produce Response.\n\nmodule.exports = ({ acks, timeout, topicData }) => {\n  return Object.assign(requestV0({ acks, timeout, topicData }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v1 (supported in 0.9.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\nconst { Types, lookupCodec } = require('../../../message/compression')\n\n// Produce Request on or after v2 indicates the client can parse the timestamp field\n// in the produce Response.\n\nmodule.exports = ({ acks, timeout, compression = Types.None, topicData }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(await encodeTopic(data))\n    }\n\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => {\n  const encodePartitions = partitionsEncoder(compression)\n\n  return async ({ topic, partitions }) => {\n    const encodedPartitions = []\n\n    for (const data of partitions) {\n      encodedPartitions.push(await encodePartitions(data))\n    }\n\n    return new Encoder().writeString(topic).writeArray(encodedPartitions)\n  }\n}\n\nconst partitionsEncoder = compression => async ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 1, compression, entries: messages })\n\n  if (compression === Types.None) {\n    return new Encoder()\n      .writeInt32(partition)\n      .writeInt32(messageSet.size())\n      .writeEncoder(messageSet)\n  }\n\n  const timestamp = messages[0].timestamp || Date.now()\n\n  const codec = lookupCodec(compression)\n  const compressedValue = await codec.compress(messageSet)\n  const compressedMessageSet = MessageSet({\n    messageVersion: 1,\n    entries: [{ compression, timestamp, value: compressedValue }],\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(compressedMessageSet.size())\n    .writeEncoder(compressedMessageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v2 (supported in 0.10.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset Timestamp]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   Timestamp => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n  timestamp: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Long = require('../../../../utils/long')\nconst Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst { Types } = require('../../../message/compression')\nconst Record = require('../../../recordBatch/record/v0')\nconst { RecordBatch } = require('../../../recordBatch/v0')\n\n/**\n * Produce Request (Version: 3) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\n/**\n * @param [transactionalId=null] {String} The transactional id or null if the producer is not transactional\n * @param acks {Integer} See producer request v0\n * @param timeout {Integer} See producer request v0\n * @param [compression=CompressionTypes.None] {CompressionTypes}\n * @param topicData {Array}\n */\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId = null,\n  producerId = Long.fromInt(-1),\n  producerEpoch = 0,\n  compression = Types.None,\n  topicData,\n}) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(\n        await encodeTopic({ ...data, transactionalId, producerId, producerEpoch })\n      )\n    }\n\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => async ({\n  topic,\n  partitions,\n  transactionalId,\n  producerId,\n  producerEpoch,\n}) => {\n  const encodePartitions = partitionsEncoder(compression)\n  const encodedPartitions = []\n\n  for (const data of partitions) {\n    encodedPartitions.push(\n      await encodePartitions({ ...data, transactionalId, producerId, producerEpoch })\n    )\n  }\n\n  return new Encoder().writeString(topic).writeArray(encodedPartitions)\n}\n\nconst partitionsEncoder = compression => async ({\n  partition,\n  messages,\n  transactionalId,\n  firstSequence,\n  producerId,\n  producerEpoch,\n}) => {\n  const dateNow = Date.now()\n  const messageTimestamps = messages\n    .map(m => m.timestamp)\n    .filter(timestamp => timestamp != null)\n    .sort()\n\n  const timestamps = messageTimestamps.length === 0 ? [dateNow] : messageTimestamps\n  const firstTimestamp = timestamps[0]\n  const maxTimestamp = timestamps[timestamps.length - 1]\n\n  const records = messages.map((message, i) =>\n    Record({\n      ...message,\n      offsetDelta: i,\n      timestampDelta: (message.timestamp || dateNow) - firstTimestamp,\n    })\n  )\n\n  const recordBatch = await RecordBatch({\n    compression,\n    records,\n    firstTimestamp,\n    maxTimestamp,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    transactional: !!transactionalId,\n    lastOffsetDelta: records.length - 1,\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(recordBatch.size())\n    .writeEncoder(recordBatch)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Produce Response (Version: 3) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(response => {\n    return response.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 4) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode, parse } = require('../v3/response')\n\n/**\n * Produce Response (Version: 4) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 5) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 5 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV3 } = require('../v3/response')\n\n/**\n * Produce Response (Version: 5) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n  logStartOffset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV3,\n}\n","const requestV5 = require('../v5/request')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L113-L117\n *\n * Produce Request (Version: 6) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV5({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 6 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV5 } = require('../v5/response')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java#L152-L156\n *\n * Produce Response (Version: 6) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n  logStartOffset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n\n  return {\n    topics,\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV5,\n}\n","const requestV6 = require('../v6/request')\n\n/**\n * V7 indicates ZStandard capability (see KIP-110)\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L118-L121\n *\n * Produce Request (Version: 7) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV6({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 7 }\n  )\n","const { decode, parse } = require('../v6/response')\n\n/**\n * Produce Response (Version: 7) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ authBytes }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ authBytes }), response }\n  },\n  1: ({ authBytes }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ authBytes }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslAuthenticate: apiKey } = require('../../apiKeys')\n\n/**\n * SaslAuthenticate Request (Version: 0) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslAuthenticate',\n  encode: async () => {\n    return new Encoder().writeBuffer(authBytes)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst {\n  failure,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n  errorCodes,\n} = require('../../../error')\n\nconst { KafkaJSProtocolError } = require('../../../../errors')\nconst SASL_AUTHENTICATION_FAILED = 58\nconst protocolAuthError = errorCodes.find(e => e.code === SASL_AUTHENTICATION_FAILED)\n\n/**\n * SaslAuthenticate Response (Version: 0) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n  }\n}\n\nconst parse = async data => {\n  if (data.errorCode === SASL_AUTHENTICATION_FAILED && data.errorMessage) {\n    throw new KafkaJSProtocolError({\n      ...protocolAuthError,\n      message: data.errorMessage,\n    })\n  }\n\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SaslAuthenticate Request (Version: 1) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => Object.assign(requestV0({ authBytes }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst { failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslAuthenticate Response (Version: 1) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n *   session_lifetime_ms => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n  const sessionLifetimeMs = decoder.readInt64().toString()\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n    sessionLifetimeMs,\n  }\n}\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ mechanism }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ mechanism }), response }\n  },\n  1: ({ mechanism }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ mechanism }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslHandshake: apiKey } = require('../../apiKeys')\n\n/**\n * SaslHandshake Request (Version: 0) => mechanism\n *    mechanism => STRING\n */\n\n/**\n * @param {string} mechanism - SASL Mechanism chosen by the client\n */\nmodule.exports = ({ mechanism }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslHandshake',\n  encode: async () => new Encoder().writeString(mechanism),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslHandshake Response (Version: 0) => error_code [enabled_mechanisms]\n *    error_code => INT16\n *    enabled_mechanisms => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    enabledMechanisms: decoder.readArray(decoder => decoder.readString()),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ mechanism }) => ({ ...requestV0({ mechanism }), apiVersion: 1 })\n","const { decode: decodeV0, parse: parseV0 } = require('../v0/response')\n\nmodule.exports = {\n  decode: decodeV0,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  1: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SyncGroup: apiKey } = require('../../apiKeys')\n\n/**\n * SyncGroup Request (Version: 0) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SyncGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(generationId)\n      .writeString(memberId)\n      .writeArray(groupAssignment.map(encodeGroupAssignment))\n  },\n})\n\nconst encodeGroupAssignment = ({ memberId, memberAssignment }) => {\n  return new Encoder().writeString(memberId).writeBytes(memberAssignment)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SyncGroup Response (Version: 0) => error_code member_assignment\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SyncGroup Request (Version: 1) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) =>\n  Object.assign(requestV0({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * SyncGroup Response (Version: 1) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, groupId, producerId, producerEpoch, topics }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { TxnOffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * TxnOffsetCommit Request (Version: 0) => transactional_id group_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   group_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'TxnOffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeString(groupId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * TxnOffsetCommit Response (Version: 0) => throttle_time_ms [topics]\n *   throttle_time_ms => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const topics = await decoder.readArrayAsync(decodeTopic)\n\n  return {\n    throttleTime,\n    topics,\n  }\n}\n\nconst decodeTopic = async decoder => ({\n  topic: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decodePartition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.topics\n    .map(({ partitions }) => ({\n      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L31\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg))\n    },\n  }\n}\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage),\n})\n","module.exports = require('../firstMessage/response')\n","/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage),\n})\n","/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n","module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n","/**\n * Enum for timestamp types\n * @readonly\n * @enum {TimestampType}\n */\nmodule.exports = {\n  // Timestamp type is unknown\n  NO_TIMESTAMP: -1,\n\n  // Timestamp relates to message creation time as set by a Kafka client\n  CREATE_TIME: 0,\n\n  // Timestamp relates to the time a message was appended to a Kafka log\n  LOG_APPEND_TIME: 1,\n}\n","module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n","module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n","const { KafkaJSNumberOfRetriesExceeded } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (shouldRetry && isErrorRetriable(e)) {\n          scheduleRetry()\n        } else {\n          reject(new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime }))\n        }\n      })\n  }\n\n  return retry\n}\n\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n","module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n","const defaultErrorHandler = e => {\n  throw e\n}\n\n/**\n * Generator that processes the given promises, and yields their result in the order of them resolving.\n *\n * @template T\n * @param {Promise<T>[]} promises promises to process\n * @param {(err: Error) => any} [handleError] optional error handler\n * @returns {Generator<Promise<T>>}\n */\nfunction* BufferedAsyncIterator(promises, handleError = defaultErrorHandler) {\n  /** Queue of promises in order of resolution */\n  const promisesQueue = []\n  /** Queue of {resolve, reject} in the same order as `promisesQueue` */\n  const resolveRejectQueue = []\n\n  promises.forEach(promise => {\n    // Create a new promise into the promises queue, and keep the {resolve,reject}\n    // in the resolveRejectQueue\n    let resolvePromise\n    let rejectPromise\n    promisesQueue.push(\n      new Promise((resolve, reject) => {\n        resolvePromise = resolve\n        rejectPromise = reject\n      })\n    )\n    resolveRejectQueue.push({ resolve: resolvePromise, reject: rejectPromise })\n\n    // When the promise resolves pick the next available {resolve, reject}, and\n    // through that resolve the next promise in the queue\n    promise.then(\n      result => {\n        const { resolve } = resolveRejectQueue.pop()\n        resolve(result)\n      },\n      async err => {\n        const { reject } = resolveRejectQueue.pop()\n        try {\n          await handleError(err)\n          reject(err)\n        } catch (newError) {\n          reject(newError)\n        }\n      }\n    )\n  })\n\n  // While there are promises left pick the next one to yield\n  // The caller will then wait for the value to resolve.\n  while (promisesQueue.length > 0) {\n    const nextPromise = promisesQueue.pop()\n    yield nextPromise\n  }\n}\n\nmodule.exports = BufferedAsyncIterator\n","const { KafkaJSNonRetriableError } = require('../errors')\n\nconst REJECTED_ERROR = new KafkaJSNonRetriableError(\n  'Queued function aborted due to earlier promise rejection'\n)\nfunction NOOP() {}\n\nconst concurrency = ({ limit, onChange = NOOP } = {}) => {\n  if (isNaN(limit) || typeof limit !== 'number' || limit < 1) {\n    throw new KafkaJSNonRetriableError(`\"limit\" cannot be less than 1`)\n  }\n\n  let waiting = []\n  let semaphore = 0\n\n  const clear = () => {\n    for (const lazyAction of waiting) {\n      lazyAction((_1, _2, reject) => reject(REJECTED_ERROR))\n    }\n    waiting = []\n    semaphore = 0\n  }\n\n  const next = () => {\n    semaphore--\n    onChange(semaphore)\n\n    if (waiting.length > 0) {\n      const lazyAction = waiting.shift()\n      lazyAction()\n    }\n  }\n\n  const invoke = (action, resolve, reject) => {\n    semaphore++\n    onChange(semaphore)\n\n    action()\n      .then(result => {\n        resolve(result)\n        next()\n      })\n      .catch(error => {\n        reject(error)\n        clear()\n      })\n  }\n\n  const push = (action, resolve, reject) => {\n    if (semaphore < limit) {\n      invoke(action, resolve, reject)\n    } else {\n      waiting.push(override => {\n        const execute = override || invoke\n        execute(action, resolve, reject)\n      })\n    }\n  }\n\n  return action => new Promise((resolve, reject) => push(action, resolve, reject))\n}\n\nmodule.exports = concurrency\n","/**\n * Flatten the given arrays into a new array\n *\n * @param {Array<Array<T>>} arrays\n * @returns {Array<T>}\n * @template T\n */\nfunction flatten(arrays) {\n  return [].concat.apply([], arrays)\n}\n\nmodule.exports = flatten\n","const { format } = require('util')\nconst { KafkaJSLockTimeout } = require('../errors')\n\nconst PRIVATE = {\n  LOCKED: Symbol('private:Lock:locked'),\n  TIMEOUT: Symbol('private:Lock:timeout'),\n  WAITING: Symbol('private:Lock:waiting'),\n  TIMEOUT_ERROR_MESSAGE: Symbol('private:Lock:timeoutErrorMessage'),\n}\n\nconst TIMEOUT_MESSAGE = 'Timeout while acquiring lock (%d waiting locks)'\n\nmodule.exports = class Lock {\n  constructor({ timeout, description = null } = {}) {\n    if (typeof timeout !== 'number') {\n      throw new TypeError(`'timeout' is not a number, received '${typeof timeout}'`)\n    }\n\n    this[PRIVATE.LOCKED] = false\n    this[PRIVATE.TIMEOUT] = timeout\n    this[PRIVATE.WAITING] = new Set()\n    this[PRIVATE.TIMEOUT_ERROR_MESSAGE] = () => {\n      const timeoutMessage = format(TIMEOUT_MESSAGE, this[PRIVATE.WAITING].size)\n      return description ? `${timeoutMessage}: \"${description}\"` : timeoutMessage\n    }\n  }\n\n  async acquire() {\n    return new Promise((resolve, reject) => {\n      if (!this[PRIVATE.LOCKED]) {\n        this[PRIVATE.LOCKED] = true\n        return resolve()\n      }\n\n      let timeoutId = null\n      const tryToAcquire = async () => {\n        if (!this[PRIVATE.LOCKED]) {\n          this[PRIVATE.LOCKED] = true\n          clearTimeout(timeoutId)\n          this[PRIVATE.WAITING].delete(tryToAcquire)\n          return resolve()\n        }\n      }\n\n      this[PRIVATE.WAITING].add(tryToAcquire)\n      timeoutId = setTimeout(() => {\n        // The message should contain the number of waiters _including_ this one\n        const error = new KafkaJSLockTimeout(this[PRIVATE.TIMEOUT_ERROR_MESSAGE]())\n        this[PRIVATE.WAITING].delete(tryToAcquire)\n        reject(error)\n      }, this[PRIVATE.TIMEOUT])\n    })\n  }\n\n  async release() {\n    this[PRIVATE.LOCKED] = false\n    const waitingLock = this[PRIVATE.WAITING].values().next().value\n\n    if (waitingLock) {\n      return waitingLock()\n    }\n  }\n}\n","/**\n * @exports Long\n * @class A Long class for representing a 64 bit int (BigInt)\n * @param {bigint} value The value of the 64 bit int\n * @constructor\n */\nclass Long {\n  constructor(value) {\n    this.value = value\n  }\n\n  /**\n   * @function isLong\n   * @param {*} obj Object\n   * @returns {boolean}\n   * @inner\n   */\n  static isLong(obj) {\n    return typeof obj.value === 'bigint'\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromBits(value) {\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromInt(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt.asIntN(64, BigInt(value)))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromNumber(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @function\n   * @param {bigint|number|string|Long} val\n   * @returns {!Long}\n   * @inner\n   */\n  static fromValue(val) {\n    if (typeof val === 'number') return this.fromNumber(val)\n    if (typeof val === 'string') return this.fromString(val)\n    if (typeof val === 'bigint') return new Long(val)\n    if (this.isLong(val)) return new Long(BigInt(val.value))\n\n    return new Long(BigInt(val))\n  }\n\n  /**\n   * @param {string} str\n   * @returns {!Long}\n   * @inner\n   */\n  static fromString(str) {\n    if (str.length === 0) throw Error('empty string')\n    if (str === 'NaN' || str === 'Infinity' || str === '+Infinity' || str === '-Infinity')\n      return Long.ZERO\n    return new Long(BigInt(str))\n  }\n\n  /**\n   * Tests if this Long's value equals zero.\n   * @returns {boolean}\n   */\n  isZero() {\n    return this.value === BigInt(0)\n  }\n\n  /**\n   * Tests if this Long's value is negative.\n   * @returns {boolean}\n   */\n  isNegative() {\n    return this.value < BigInt(0)\n  }\n\n  /**\n   * Converts the Long to a string written in the specified radix.\n   * @returns {string}\n   * @override\n   */\n  toString() {\n    return String(this.value)\n  }\n\n  /**\n   * Converts the Long to the nearest floating-point representation (double, 53-bit mantissa)\n   * @returns {number}\n   * @override\n   */\n  toNumber() {\n    return Number(this.value)\n  }\n\n  /**\n   * Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.\n   * @returns {number}\n   */\n  toInt() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n\n  /**\n   * Returns this Long with bits shifted to the left by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftLeft(numBits) {\n    return new Long(this.value << BigInt(numBits))\n  }\n\n  /**\n   * Returns this Long with bits arithmetically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRight(numBits) {\n    return new Long(this.value >> BigInt(numBits))\n  }\n\n  /**\n   * Returns the bitwise OR of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  or(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return Long.fromBits(this.value | other.value)\n  }\n\n  /**\n   * Returns the bitwise XOR of this Long and the given one.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  xor(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value ^ other.value)\n  }\n\n  /**\n   * Returns the bitwise AND of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  and(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value & other.value)\n  }\n\n  /**\n   * Returns the bitwise NOT of this Long.\n   * @returns {!Long}\n   */\n  not() {\n    return new Long(~this.value)\n  }\n\n  /**\n   * Returns this Long with bits logically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRightUnsigned(numBits) {\n    return new Long(this.value >> BigInt.asUintN(64, BigInt(numBits)))\n  }\n\n  /**\n   * Tests if this Long's value equals the specified's.\n   * @param {bigint|number|string} other Other value\n   * @returns {boolean}\n   */\n  equals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value === other.value\n  }\n\n  /**\n   * Tests if this Long's value is greater than or equal the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  greaterThanOrEqual(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value >= other.value\n  }\n\n  gte(other) {\n    return this.greaterThanOrEqual(other)\n  }\n\n  notEquals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return !this.equals(/* validates */ other)\n  }\n\n  /**\n   * Returns the sum of this and the specified Long.\n   * @param {!Long|number|string} addend Addend\n   * @returns {!Long} Sum\n   */\n  add(addend) {\n    if (!Long.isLong(addend)) addend = Long.fromValue(addend)\n    return new Long(this.value + addend.value)\n  }\n\n  /**\n   * Returns the difference of this and the specified Long.\n   * @param {!Long|number|string} subtrahend Subtrahend\n   * @returns {!Long} Difference\n   */\n  subtract(subtrahend) {\n    if (!Long.isLong(subtrahend)) subtrahend = Long.fromValue(subtrahend)\n    return this.add(subtrahend.negate())\n  }\n\n  /**\n   * Returns the product of this and the specified Long.\n   * @param {!Long|number|string} multiplier Multiplier\n   * @returns {!Long} Product\n   */\n  multiply(multiplier) {\n    if (this.isZero()) return Long.ZERO\n    if (!Long.isLong(multiplier)) multiplier = Long.fromValue(multiplier)\n    return new Long(this.value * multiplier.value)\n  }\n\n  /**\n   * Returns this Long divided by the specified. The result is signed if this Long is signed or\n   *  unsigned if this Long is unsigned.\n   * @param {!Long|number|string} divisor Divisor\n   * @returns {!Long} Quotient\n   */\n  divide(divisor) {\n    if (!Long.isLong(divisor)) divisor = Long.fromValue(divisor)\n    if (divisor.isZero()) throw Error('division by zero')\n    return new Long(this.value / divisor.value)\n  }\n\n  /**\n   * Compares this Long's value with the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {number} 0 if they are the same, 1 if the this is greater and -1\n   *  if the given one is greater\n   */\n  compare(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    if (this.value === other.value) return 0\n    if (this.value > other.value) return 1\n    if (other.value > this.value) return -1\n  }\n\n  /**\n   * Tests if this Long's value is less than the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  lessThan(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value < other.value\n  }\n\n  /**\n   * Negates this Long's value.\n   * @returns {!Long} Negated Long\n   */\n  negate() {\n    if (this.equals(Long.MIN_VALUE)) {\n      return Long.MIN_VALUE\n    }\n    return this.not().add(Long.ONE)\n  }\n\n  /**\n   * Gets the high 32 bits as a signed integer.\n   * @returns {number} Signed high bits\n   */\n  getHighBits() {\n    return Number(BigInt.asIntN(32, this.value >> BigInt(32)))\n  }\n\n  /**\n   * Gets the low 32 bits as a signed integer.\n   * @returns {number} Signed low bits\n   */\n  getLowBits() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n}\n\n/**\n * Minimum signed value.\n * @type {bigint}\n */\nLong.MIN_VALUE = new Long(BigInt('-9223372036854775808'))\n\n/**\n * Maximum signed value.\n * @type {bigint}\n */\nLong.MAX_VALUE = new Long(BigInt('9223372036854775807'))\n\n/**\n * Signed zero.\n * @type {Long}\n */\nLong.ZERO = Long.fromInt(0)\n\n/**\n * Signed one.\n * @type {!Long}\n */\nLong.ONE = Long.fromInt(1)\n\nmodule.exports = Long\n","module.exports = array => {\n  if (!Array.isArray(array)) {\n    throw new TypeError(\"'array' is not an array\")\n  }\n\n  if (array.length < 2) {\n    return array\n  }\n\n  const copy = array.slice()\n\n  for (let i = copy.length - 1; i > 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1))\n    const temp = copy[i]\n    copy[i] = copy[j]\n    copy[j] = temp\n  }\n\n  return copy\n}\n","module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n","const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n","const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n","const BASE_URL = 'https://kafka.js.org'\n\nmodule.exports = (path, hash) => `${BASE_URL}/${path}${hash ? '#' + hash : ''}`\n","'use strict';\n\nmodule.exports = require('./lib')\n","'use strict';\n\nvar asap = require('asap/raw');\n\nfunction noop() {}\n\n// States:\n//\n// 0 - pending\n// 1 - fulfilled with _value\n// 2 - rejected with _value\n// 3 - adopted the state of another promise, _value\n//\n// once the state is no longer pending (0) it is immutable\n\n// All `_` prefixed properties will be reduced to `_{random number}`\n// at build time to obfuscate them and discourage their use.\n// We don't use symbols or Object.defineProperty to fully hide them\n// because the performance isn't good enough.\n\n\n// to avoid using try/catch inside critical functions, we\n// extract them to here.\nvar LAST_ERROR = null;\nvar IS_ERROR = {};\nfunction getThen(obj) {\n  try {\n    return obj.then;\n  } catch (ex) {\n    LAST_ERROR = ex;\n    return IS_ERROR;\n  }\n}\n\nfunction tryCallOne(fn, a) {\n  try {\n    return fn(a);\n  } catch (ex) {\n    LAST_ERROR = ex;\n    return IS_ERROR;\n  }\n}\nfunction tryCallTwo(fn, a, b) {\n  try {\n    fn(a, b);\n  } catch (ex) {\n    LAST_ERROR = ex;\n    return IS_ERROR;\n  }\n}\n\nmodule.exports = Promise;\n\nfunction Promise(fn) {\n  if (typeof this !== 'object') {\n    throw new TypeError('Promises must be constructed via new');\n  }\n  if (typeof fn !== 'function') {\n    throw new TypeError('Promise constructor\\'s argument is not a function');\n  }\n  this._U = 0;\n  this._V = 0;\n  this._W = null;\n  this._X = null;\n  if (fn === noop) return;\n  doResolve(fn, this);\n}\nPromise._Y = null;\nPromise._Z = null;\nPromise._0 = noop;\n\nPromise.prototype.then = function(onFulfilled, onRejected) {\n  if (this.constructor !== Promise) {\n    return safeThen(this, onFulfilled, onRejected);\n  }\n  var res = new Promise(noop);\n  handle(this, new Handler(onFulfilled, onRejected, res));\n  return res;\n};\n\nfunction safeThen(self, onFulfilled, onRejected) {\n  return new self.constructor(function (resolve, reject) {\n    var res = new Promise(noop);\n    res.then(resolve, reject);\n    handle(self, new Handler(onFulfilled, onRejected, res));\n  });\n}\nfunction handle(self, deferred) {\n  while (self._V === 3) {\n    self = self._W;\n  }\n  if (Promise._Y) {\n    Promise._Y(self);\n  }\n  if (self._V === 0) {\n    if (self._U === 0) {\n      self._U = 1;\n      self._X = deferred;\n      return;\n    }\n    if (self._U === 1) {\n      self._U = 2;\n      self._X = [self._X, deferred];\n      return;\n    }\n    self._X.push(deferred);\n    return;\n  }\n  handleResolved(self, deferred);\n}\n\nfunction handleResolved(self, deferred) {\n  asap(function() {\n    var cb = self._V === 1 ? deferred.onFulfilled : deferred.onRejected;\n    if (cb === null) {\n      if (self._V === 1) {\n        resolve(deferred.promise, self._W);\n      } else {\n        reject(deferred.promise, self._W);\n      }\n      return;\n    }\n    var ret = tryCallOne(cb, self._W);\n    if (ret === IS_ERROR) {\n      reject(deferred.promise, LAST_ERROR);\n    } else {\n      resolve(deferred.promise, ret);\n    }\n  });\n}\nfunction resolve(self, newValue) {\n  // Promise Resolution Procedure: https://github.com/promises-aplus/promises-spec#the-promise-resolution-procedure\n  if (newValue === self) {\n    return reject(\n      self,\n      new TypeError('A promise cannot be resolved with itself.')\n    );\n  }\n  if (\n    newValue &&\n    (typeof newValue === 'object' || typeof newValue === 'function')\n  ) {\n    var then = getThen(newValue);\n    if (then === IS_ERROR) {\n      return reject(self, LAST_ERROR);\n    }\n    if (\n      then === self.then &&\n      newValue instanceof Promise\n    ) {\n      self._V = 3;\n      self._W = newValue;\n      finale(self);\n      return;\n    } else if (typeof then === 'function') {\n      doResolve(then.bind(newValue), self);\n      return;\n    }\n  }\n  self._V = 1;\n  self._W = newValue;\n  finale(self);\n}\n\nfunction reject(self, newValue) {\n  self._V = 2;\n  self._W = newValue;\n  if (Promise._Z) {\n    Promise._Z(self, newValue);\n  }\n  finale(self);\n}\nfunction finale(self) {\n  if (self._U === 1) {\n    handle(self, self._X);\n    self._X = null;\n  }\n  if (self._U === 2) {\n    for (var i = 0; i < self._X.length; i++) {\n      handle(self, self._X[i]);\n    }\n    self._X = null;\n  }\n}\n\nfunction Handler(onFulfilled, onRejected, promise){\n  this.onFulfilled = typeof onFulfilled === 'function' ? onFulfilled : null;\n  this.onRejected = typeof onRejected === 'function' ? onRejected : null;\n  this.promise = promise;\n}\n\n/**\n * Take a potentially misbehaving resolver function and make sure\n * onFulfilled and onRejected are only called once.\n *\n * Makes no guarantees about asynchrony.\n */\nfunction doResolve(fn, promise) {\n  var done = false;\n  var res = tryCallTwo(fn, function (value) {\n    if (done) return;\n    done = true;\n    resolve(promise, value);\n  }, function (reason) {\n    if (done) return;\n    done = true;\n    reject(promise, reason);\n  });\n  if (!done && res === IS_ERROR) {\n    done = true;\n    reject(promise, LAST_ERROR);\n  }\n}\n","'use strict';\n\nvar Promise = require('./core.js');\n\nmodule.exports = Promise;\nPromise.prototype.done = function (onFulfilled, onRejected) {\n  var self = arguments.length ? this.then.apply(this, arguments) : this;\n  self.then(null, function (err) {\n    setTimeout(function () {\n      throw err;\n    }, 0);\n  });\n};\n","'use strict';\n\n//This file contains the ES6 extensions to the core Promises/A+ API\n\nvar Promise = require('./core.js');\n\nmodule.exports = Promise;\n\n/* Static Functions */\n\nvar TRUE = valuePromise(true);\nvar FALSE = valuePromise(false);\nvar NULL = valuePromise(null);\nvar UNDEFINED = valuePromise(undefined);\nvar ZERO = valuePromise(0);\nvar EMPTYSTRING = valuePromise('');\n\nfunction valuePromise(value) {\n  var p = new Promise(Promise._0);\n  p._V = 1;\n  p._W = value;\n  return p;\n}\nPromise.resolve = function (value) {\n  if (value instanceof Promise) return value;\n\n  if (value === null) return NULL;\n  if (value === undefined) return UNDEFINED;\n  if (value === true) return TRUE;\n  if (value === false) return FALSE;\n  if (value === 0) return ZERO;\n  if (value === '') return EMPTYSTRING;\n\n  if (typeof value === 'object' || typeof value === 'function') {\n    try {\n      var then = value.then;\n      if (typeof then === 'function') {\n        return new Promise(then.bind(value));\n      }\n    } catch (ex) {\n      return new Promise(function (resolve, reject) {\n        reject(ex);\n      });\n    }\n  }\n  return valuePromise(value);\n};\n\nvar iterableToArray = function (iterable) {\n  if (typeof Array.from === 'function') {\n    // ES2015+, iterables exist\n    iterableToArray = Array.from;\n    return Array.from(iterable);\n  }\n\n  // ES5, only arrays and array-likes exist\n  iterableToArray = function (x) { return Array.prototype.slice.call(x); };\n  return Array.prototype.slice.call(iterable);\n}\n\nPromise.all = function (arr) {\n  var args = iterableToArray(arr);\n\n  return new Promise(function (resolve, reject) {\n    if (args.length === 0) return resolve([]);\n    var remaining = args.length;\n    function res(i, val) {\n      if (val && (typeof val === 'object' || typeof val === 'function')) {\n        if (val instanceof Promise && val.then === Promise.prototype.then) {\n          while (val._V === 3) {\n            val = val._W;\n          }\n          if (val._V === 1) return res(i, val._W);\n          if (val._V === 2) reject(val._W);\n          val.then(function (val) {\n            res(i, val);\n          }, reject);\n          return;\n        } else {\n          var then = val.then;\n          if (typeof then === 'function') {\n            var p = new Promise(then.bind(val));\n            p.then(function (val) {\n              res(i, val);\n            }, reject);\n            return;\n          }\n        }\n      }\n      args[i] = val;\n      if (--remaining === 0) {\n        resolve(args);\n      }\n    }\n    for (var i = 0; i < args.length; i++) {\n      res(i, args[i]);\n    }\n  });\n};\n\nPromise.reject = function (value) {\n  return new Promise(function (resolve, reject) {\n    reject(value);\n  });\n};\n\nPromise.race = function (values) {\n  return new Promise(function (resolve, reject) {\n    iterableToArray(values).forEach(function(value){\n      Promise.resolve(value).then(resolve, reject);\n    });\n  });\n};\n\n/* Prototype Methods */\n\nPromise.prototype['catch'] = function (onRejected) {\n  return this.then(null, onRejected);\n};\n","'use strict';\n\nvar Promise = require('./core.js');\n\nmodule.exports = Promise;\nPromise.prototype.finally = function (f) {\n  return this.then(function (value) {\n    return Promise.resolve(f()).then(function () {\n      return value;\n    });\n  }, function (err) {\n    return Promise.resolve(f()).then(function () {\n      throw err;\n    });\n  });\n};\n","'use strict';\n\nmodule.exports = require('./core.js');\nrequire('./done.js');\nrequire('./finally.js');\nrequire('./es6-extensions.js');\nrequire('./node-extensions.js');\nrequire('./synchronous.js');\n","'use strict';\n\n// This file contains then/promise specific extensions that are only useful\n// for node.js interop\n\nvar Promise = require('./core.js');\nvar asap = require('asap');\n\nmodule.exports = Promise;\n\n/* Static Functions */\n\nPromise.denodeify = function (fn, argumentCount) {\n  if (\n    typeof argumentCount === 'number' && argumentCount !== Infinity\n  ) {\n    return denodeifyWithCount(fn, argumentCount);\n  } else {\n    return denodeifyWithoutCount(fn);\n  }\n};\n\nvar callbackFn = (\n  'function (err, res) {' +\n  'if (err) { rj(err); } else { rs(res); }' +\n  '}'\n);\nfunction denodeifyWithCount(fn, argumentCount) {\n  var args = [];\n  for (var i = 0; i < argumentCount; i++) {\n    args.push('a' + i);\n  }\n  var body = [\n    'return function (' + args.join(',') + ') {',\n    'var self = this;',\n    'return new Promise(function (rs, rj) {',\n    'var res = fn.call(',\n    ['self'].concat(args).concat([callbackFn]).join(','),\n    ');',\n    'if (res &&',\n    '(typeof res === \"object\" || typeof res === \"function\") &&',\n    'typeof res.then === \"function\"',\n    ') {rs(res);}',\n    '});',\n    '};'\n  ].join('');\n  return Function(['Promise', 'fn'], body)(Promise, fn);\n}\nfunction denodeifyWithoutCount(fn) {\n  var fnLength = Math.max(fn.length - 1, 3);\n  var args = [];\n  for (var i = 0; i < fnLength; i++) {\n    args.push('a' + i);\n  }\n  var body = [\n    'return function (' + args.join(',') + ') {',\n    'var self = this;',\n    'var args;',\n    'var argLength = arguments.length;',\n    'if (arguments.length > ' + fnLength + ') {',\n    'args = new Array(arguments.length + 1);',\n    'for (var i = 0; i < arguments.length; i++) {',\n    'args[i] = arguments[i];',\n    '}',\n    '}',\n    'return new Promise(function (rs, rj) {',\n    'var cb = ' + callbackFn + ';',\n    'var res;',\n    'switch (argLength) {',\n    args.concat(['extra']).map(function (_, index) {\n      return (\n        'case ' + (index) + ':' +\n        'res = fn.call(' + ['self'].concat(args.slice(0, index)).concat('cb').join(',') + ');' +\n        'break;'\n      );\n    }).join(''),\n    'default:',\n    'args[argLength] = cb;',\n    'res = fn.apply(self, args);',\n    '}',\n    \n    'if (res &&',\n    '(typeof res === \"object\" || typeof res === \"function\") &&',\n    'typeof res.then === \"function\"',\n    ') {rs(res);}',\n    '});',\n    '};'\n  ].join('');\n\n  return Function(\n    ['Promise', 'fn'],\n    body\n  )(Promise, fn);\n}\n\nPromise.nodeify = function (fn) {\n  return function () {\n    var args = Array.prototype.slice.call(arguments);\n    var callback =\n      typeof args[args.length - 1] === 'function' ? args.pop() : null;\n    var ctx = this;\n    try {\n      return fn.apply(this, arguments).nodeify(callback, ctx);\n    } catch (ex) {\n      if (callback === null || typeof callback == 'undefined') {\n        return new Promise(function (resolve, reject) {\n          reject(ex);\n        });\n      } else {\n        asap(function () {\n          callback.call(ctx, ex);\n        })\n      }\n    }\n  }\n};\n\nPromise.prototype.nodeify = function (callback, ctx) {\n  if (typeof callback != 'function') return this;\n\n  this.then(function (value) {\n    asap(function () {\n      callback.call(ctx, null, value);\n    });\n  }, function (err) {\n    asap(function () {\n      callback.call(ctx, err);\n    });\n  });\n};\n","'use strict';\n\nvar Promise = require('./core.js');\n\nmodule.exports = Promise;\nPromise.enableSynchronous = function () {\n  Promise.prototype.isPending = function() {\n    return this.getState() == 0;\n  };\n\n  Promise.prototype.isFulfilled = function() {\n    return this.getState() == 1;\n  };\n\n  Promise.prototype.isRejected = function() {\n    return this.getState() == 2;\n  };\n\n  Promise.prototype.getValue = function () {\n    if (this._V === 3) {\n      return this._W.getValue();\n    }\n\n    if (!this.isFulfilled()) {\n      throw new Error('Cannot get a value of an unfulfilled promise.');\n    }\n\n    return this._W;\n  };\n\n  Promise.prototype.getReason = function () {\n    if (this._V === 3) {\n      return this._W.getReason();\n    }\n\n    if (!this.isRejected()) {\n      throw new Error('Cannot get a rejection reason of a non-rejected promise.');\n    }\n\n    return this._W;\n  };\n\n  Promise.prototype.getState = function () {\n    if (this._V === 3) {\n      return this._W.getState();\n    }\n    if (this._V === -1 || this._V === -2) {\n      return 0;\n    }\n\n    return this._V;\n  };\n};\n\nPromise.disableSynchronous = function() {\n  Promise.prototype.isPending = undefined;\n  Promise.prototype.isFulfilled = undefined;\n  Promise.prototype.isRejected = undefined;\n  Promise.prototype.getValue = undefined;\n  Promise.prototype.getReason = undefined;\n  Promise.prototype.getState = undefined;\n};\n","module.exports = {\n\tcore: {\n\t\tBatch: require(\"./src/Batch\"),\n\t\tClientBuilder: require(\"./src/ClientBuilder\"),\n\t\tbuildClient: require(\"./src/util/buildClients\"),\n\t\tSharedCredentials: require(\"./src/SharedCredentials\"),\n\t\tStaticCredentials: require(\"./src/StaticCredentials\"),\n\t\tErrors: require(\"./src/Errors\"),\n\t},\n\tusStreet: {\n\t\tLookup: require(\"./src/us_street/Lookup\"),\n\t\tCandidate: require(\"./src/us_street/Candidate\")\n\t},\n\tusZipcode: {\n\t\tLookup: require(\"./src/us_zipcode/Lookup\"),\n\t\tResult: require(\"./src/us_zipcode/Result\")\n\t},\n\tusAutocomplete: {\n\t\tLookup: require(\"./src/us_autocomplete/Lookup\"),\n\t\tSuggestion: require(\"./src/us_autocomplete/Suggestion\")\n\t},\n\tusAutocompletePro: {\n\t\tLookup: require(\"./src/us_autocomplete_pro/Lookup\"),\n\t\tSuggestion: require(\"./src/us_autocomplete_pro/Suggestion\")\n\t},\n\tusExtract: {\n\t\tLookup: require(\"./src/us_extract/Lookup\"),\n\t\tResult: require(\"./src/us_extract/Result\")\n\t},\n\tinternationalStreet: {\n\t\tLookup: require(\"./src/international_street/Lookup\"),\n\t\tCandidate: require(\"./src/international_street/Candidate\")\n\t},\n};","const Promise = require(\"promise\");\n\nclass AgentSender {\n\tconstructor(innerSender) {\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\trequest.parameters.agent = \"smartystreets (sdk:javascript@\" + require(\"../package.json\").version + \")\";\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = AgentSender;","const Promise = require(\"promise\");\n\nclass BaseUrlSender {\n\tconstructor(innerSender, urlOverride) {\n\t\tthis.urlOverride = urlOverride;\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\trequest.baseUrl = this.urlOverride;\n\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = BaseUrlSender;","const BatchFullError = require(\"./Errors\").BatchFullError;\n\n/**\n * This class contains a collection of up to 100 lookups to be sent to one of the SmartyStreets APIs<br>\n *     all at once. This is more efficient than sending them one at a time.\n */\nclass Batch {\n\tconstructor () {\n\t\tthis.lookups = [];\n\t}\n\n\tadd (lookup) {\n\t\tif (this.lookupsHasRoomForLookup()) this.lookups.push(lookup);\n\t\telse throw new BatchFullError();\n\t}\n\n\tlookupsHasRoomForLookup() {\n\t\tconst maxNumberOfLookups = 100;\n\t\treturn this.lookups.length < maxNumberOfLookups;\n\t}\n\n\tlength() {\n\t\treturn this.lookups.length;\n\t}\n\n\tgetByIndex(index) {\n\t\treturn this.lookups[index];\n\t}\n\n\tgetByInputId(inputId) {\n\t\treturn this.lookups.filter(lookup => {\n\t\t\treturn lookup.inputId === inputId;\n\t\t})[0];\n\t}\n\n\t/**\n\t * Clears the lookups stored in the batch so it can be used again.<br>\n\t *     This helps avoid the overhead of building a new Batch object for each group of lookups.\n\t */\n\tclear () {\n\t\tthis.lookups = [];\n\t}\n\n\tisEmpty () {\n\t\treturn this.length() === 0;\n\t}\n}\n\nmodule.exports = Batch;","const HttpSender = require(\"./HttpSender\");\nconst SigningSender = require(\"./SigningSender\");\nconst BaseUrlSender = require(\"./BaseUrlSender\");\nconst AgentSender = require(\"./AgentSender\");\nconst StaticCredentials = require(\"./StaticCredentials\");\nconst SharedCredentials = require(\"./SharedCredentials\");\nconst CustomHeaderSender = require(\"./CustomHeaderSender\");\nconst StatusCodeSender = require(\"./StatusCodeSender\");\nconst LicenseSender = require(\"./LicenseSender\");\nconst BadCredentialsError = require(\"./Errors\").BadCredentialsError;\n\n//TODO: refactor this to work more cleanly with a bundler.\nconst UsStreetClient = require(\"./us_street/Client\");\nconst UsZipcodeClient = require(\"./us_zipcode/Client\");\nconst UsAutocompleteClient = require(\"./us_autocomplete/Client\");\nconst UsAutocompleteProClient = require(\"./us_autocomplete_pro/Client\");\nconst UsExtractClient = require(\"./us_extract/Client\");\nconst InternationalStreetClient = require(\"./international_street/Client\");\nconst UsReverseGeoClient = require(\"./us_reverse_geo/Client\");\n\nconst INTERNATIONAL_STREET_API_URI = \"https://international-street.api.smartystreets.com/verify\";\nconst US_AUTOCOMPLETE_API_URL = \"https://us-autocomplete.api.smartystreets.com/suggest\";\nconst US_AUTOCOMPLETE_PRO_API_URL = \"https://us-autocomplete-pro.api.smartystreets.com/lookup\";\nconst US_EXTRACT_API_URL = \"https://us-extract.api.smartystreets.com/\";\nconst US_STREET_API_URL = \"https://us-street.api.smartystreets.com/street-address\";\nconst US_ZIP_CODE_API_URL = \"https://us-zipcode.api.smartystreets.com/lookup\";\nconst US_REVERSE_GEO_API_URL = \"https://us-reverse-geo.api.smartystreets.com/lookup\";\n\n/**\n * The ClientBuilder class helps you build a client object for one of the supported SmartyStreets APIs.<br>\n * You can use ClientBuilder's methods to customize settings like maximum retries or timeout duration. These methods<br>\n * are chainable, so you can usually get set up with one line of code.\n */\nclass ClientBuilder {\n\tconstructor(signer) {\n\t\tif (noCredentialsProvided()) throw new BadCredentialsError();\n\n\t\tthis.signer = signer;\n\t\tthis.httpSender = undefined;\n\t\tthis.maxRetries = 5;\n\t\tthis.maxTimeout = 10000;\n\t\tthis.baseUrl = undefined;\n\t\tthis.proxy = undefined;\n\t\tthis.customHeaders = {};\n\t\tthis.debug = undefined;\n\t\tthis.licenses = [];\n\n\t\tfunction noCredentialsProvided() {\n\t\t\treturn !signer instanceof StaticCredentials || !signer instanceof SharedCredentials;\n\t\t}\n\t}\n\n\t/**\n\t * @param retries The maximum number of times to retry sending the request to the API. (Default is 5)\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithMaxRetries(retries) {\n\t\tthis.maxRetries = retries;\n\t\treturn this;\n\t}\n\n\t/**\n\t * @param timeout The maximum time (in milliseconds) to wait for a connection, and also to wait for <br>\n\t *                   the response to be read. (Default is 10000)\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithMaxTimeout(timeout) {\n\t\tthis.maxTimeout = timeout;\n\t\treturn this;\n\t}\n\n\t/**\n\t * @param sender Default is a series of nested senders. See <b>buildSender()</b>.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithSender(sender) {\n\t\tthis.httpSender = sender;\n\t\treturn this;\n\t}\n\n\t/**\n\t * This may be useful when using a local installation of the SmartyStreets APIs.\n\t * @param url Defaults to the URL for the API corresponding to the <b>Client</b> object being built.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithBaseUrl(url) {\n\t\tthis.baseUrl = url;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use this to specify a proxy through which to send all lookups.\n\t * @param host The host of the proxy server (do not include the port).\n\t * @param port The port on the proxy server to which you wish to connect.\n\t * @param username The username to login to the proxy.\n\t * @param password The password to login to the proxy.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithProxy(host, port, username, password) {\n\t\tthis.proxy = {\n\t\t\thost: host,\n\t\t\tport: port\n\t\t};\n\n\t\tif (username && password) {\n\t\t\tthis.proxy.auth = {\n\t\t\t\tusername: username,\n\t\t\t\tpassword: password\n\t\t\t};\n\t\t}\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use this to add any additional headers you need.\n\t * @param customHeaders A String to Object <b>Map</b> of header name/value pairs.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithCustomHeaders(customHeaders) {\n\t\tthis.customHeaders = customHeaders;\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Enables debug mode, which will print information about the HTTP request and response to console.log\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithDebug() {\n\t\tthis.debug = true;\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Allows the caller to specify the subscription license (aka \"track\") they wish to use.\n\t * @param licenses A String Array of licenses.\n\t * @returns Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithLicenses(licenses) {\n\t\tfor (const license in licenses) {\n\t\t\tthis.licenses.push(license);\n\t\t}\n\n\t\treturn this;\n\t}\n\n\n\tbuildSender() {\n\t\tif (this.httpSender) return this.httpSender;\n\n\t\tconst httpSender = new HttpSender(this.maxTimeout, this.maxRetries, this.proxy, this.debug);\n\t\tconst statusCodeSender = new StatusCodeSender(httpSender);\n\t\tconst signingSender = new SigningSender(statusCodeSender, this.signer);\n\t\tconst agentSender = new AgentSender(signingSender);\n\t\tconst customHeaderSender = new CustomHeaderSender(agentSender, this.customHeaders);\n\t\tconst baseUrlSender = new BaseUrlSender(customHeaderSender, this.baseUrl);\n\t\tconst licenseSender = new LicenseSender(baseUrlSender, this.licenses);\n\n\t\treturn licenseSender;\n\t}\n\n\tbuildClient(baseUrl, Client) {\n\t\tif (!this.baseUrl) {\n\t\t\tthis.baseUrl = baseUrl;\n\t\t}\n\n\t\treturn new Client(this.buildSender());\n\t}\n\n\tbuildUsStreetApiClient() {\n\t\treturn this.buildClient(US_STREET_API_URL, UsStreetClient);\n\t}\n\n\tbuildUsZipcodeClient() {\n\t\treturn this.buildClient(US_ZIP_CODE_API_URL, UsZipcodeClient);\n\t}\n\n\tbuildUsAutocompleteClient() {\n\t\treturn this.buildClient(US_AUTOCOMPLETE_API_URL, UsAutocompleteClient);\n\t}\n\n\tbuildUsAutocompleteProClient() {\n\t\treturn this.buildClient(US_AUTOCOMPLETE_PRO_API_URL, UsAutocompleteProClient);\n\t}\n\n\tbuildUsExtractClient() {\n\t\treturn this.buildClient(US_EXTRACT_API_URL, UsExtractClient);\n\t}\n\n\tbuildInternationalStreetClient() {\n\t\treturn this.buildClient(INTERNATIONAL_STREET_API_URI, InternationalStreetClient);\n\t}\n\n\tbuildUsReverseGeoClient() {\n\t\treturn this.buildClient(US_REVERSE_GEO_API_URL, UsReverseGeoClient);\n\t}\n}\n\nmodule.exports = ClientBuilder;","const Promise = require(\"promise\");\n\nclass CustomHeaderSender {\n\tconstructor(innerSender, customHeaders) {\n\t\tthis.sender = innerSender;\n\t\tthis.customHeaders = customHeaders;\n\t}\n\n\tsend(request) {\n\t\tfor (let key in this.customHeaders) {\n\t\t\trequest.headers[key] = this.customHeaders[key];\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = CustomHeaderSender;","class SmartyError extends Error {\n\tconstructor(message) {\n\t\tsuper(message);\n\t}\n}\n\nclass BatchFullError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"A batch can contain a max of 100 lookups.\");\n\t}\n}\n\nclass BatchEmptyError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"A batch must contain at least 1 lookup.\");\n\t}\n}\n\nclass UndefinedLookupError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"The lookup provided is missing or undefined. Make sure you're passing a Lookup object.\");\n\t}\n}\n\nclass BadCredentialsError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Unauthorized: The credentials were provided incorrectly or did not match any existing active credentials.\");\n\t}\n}\n\nclass PaymentRequiredError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Payment Required: There is no active subscription for the account associated with the credentials submitted with the request.\");\n\t}\n}\n\nclass RequestEntityTooLargeError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Request Entity Too Large: The request body has exceeded the maximum size.\");\n\t}\n}\n\nclass BadRequestError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Bad Request (Malformed Payload): A GET request lacked a street field or the request body of a POST request contained malformed JSON.\");\n\t}\n}\n\nclass UnprocessableEntityError extends SmartyError {\n\tconstructor(message) {\n\t\tsuper(message);\n\t}\n}\n\nclass TooManyRequestsError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"When using the public 'website key' authentication, we restrict the number of requests coming from a given source over too short of a time.\");\n\t}\n}\n\nclass InternalServerError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Internal Server Error.\");\n\t}\n}\n\nclass ServiceUnavailableError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Service Unavailable. Try again later.\");\n\t}\n}\n\nclass GatewayTimeoutError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"The upstream data provider did not respond in a timely fashion and the request failed. A serious, yet rare occurrence indeed.\");\n\t}\n}\n\nmodule.exports = {\n\tBatchFullError: BatchFullError,\n\tBatchEmptyError: BatchEmptyError,\n\tUndefinedLookupError: UndefinedLookupError,\n\tBadCredentialsError: BadCredentialsError,\n\tPaymentRequiredError: PaymentRequiredError,\n\tRequestEntityTooLargeError: RequestEntityTooLargeError,\n\tBadRequestError: BadRequestError,\n\tUnprocessableEntityError: UnprocessableEntityError,\n\tTooManyRequestsError: TooManyRequestsError,\n\tInternalServerError: InternalServerError,\n\tServiceUnavailableError: ServiceUnavailableError,\n\tGatewayTimeoutError: GatewayTimeoutError\n};","const Response = require(\"./Response\");\nconst Axios = require(\"axios-proxy-fix\");\nconst axiosRetry = require(\"axios-retry\");\nconst Promise = require(\"promise\");\n\nclass HttpSender {\n\tconstructor(timeout = 10000, retries = 5, proxyConfig, debug = false) {\n\t\taxiosRetry(Axios, {\n\t\t\tretries: retries,\n\t\t});\n\t\tthis.timeout = timeout;\n\t\tthis.proxyConfig = proxyConfig;\n\t\tif (debug) this.enableDebug();\n\t}\n\n\tbuildRequestConfig({payload, parameters, headers, baseUrl}) {\n\t\tlet config = {\n\t\t\tmethod: \"GET\",\n\t\t\ttimeout: this.timeout,\n\t\t\tparams: parameters,\n\t\t\theaders: headers,\n\t\t\tbaseURL: baseUrl,\n\t\t\tvalidateStatus: function (status) {\n\t\t\t\treturn status < 500;\n\t\t\t},\n\t\t};\n\n\t\tif (payload) {\n\t\t\tconfig.method = \"POST\";\n\t\t\tconfig.data = payload;\n\t\t}\n\n\t\tif (this.proxyConfig) config.proxy = this.proxyConfig;\n\t\treturn config;\n\t}\n\n\tbuildSmartyResponse(response, error) {\n\t\tif (response) return new Response(response.status, response.data);\n\t\treturn new Response(undefined, undefined, error)\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tlet requestConfig = this.buildRequestConfig(request);\n\n\t\t\tAxios(requestConfig)\n\t\t\t\t.then(response => {\n\t\t\t\t\tlet smartyResponse = this.buildSmartyResponse(response);\n\n\t\t\t\t\tif (smartyResponse.statusCode >= 400) reject(smartyResponse);\n\n\t\t\t\t\tresolve(smartyResponse);\n\t\t\t\t})\n\t\t\t\t.catch(error => reject(this.buildSmartyResponse(undefined, error)));\n\t\t});\n\t}\n\n\tenableDebug() {\n\t\tAxios.interceptors.request.use(request => {\n\t\t\tconsole.log('Request:\\r\\n', request);\n\t\t\tconsole.log('\\r\\n*******************************************\\r\\n');\n\t\t\treturn request\n\t\t});\n\n\t\tAxios.interceptors.response.use(response => {\n\t\t\tconsole.log('Response:\\r\\n');\n\t\t\tconsole.log('Status:', response.status, response.statusText);\n\t\t\tconsole.log('Headers:', response.headers);\n\t\t\tconsole.log('Data:', response.data);\n\t\t\treturn response\n\t\t})\n\t}\n}\n\nmodule.exports = HttpSender;","class InputData {\n\tconstructor(lookup) {\n\t\tthis.lookup = lookup;\n\t\tthis.data = {};\n\t}\n\n\tadd(apiField, lookupField) {\n\t\tif (this.lookupFieldIsPopulated(lookupField)) this.data[apiField] = this.lookup[lookupField];\n\t}\n\n\tlookupFieldIsPopulated(lookupField) {\n\t\treturn this.lookup[lookupField] !== \"\" && this.lookup[lookupField] !== undefined;\n\t}\n}\n\nmodule.exports = InputData;","const Promise = require(\"promise\");\n\nclass LicenseSender {\n\tconstructor(innerSender, licenses) {\n\t\tthis.sender = innerSender;\n\t\tthis.licenses = licenses;\n\t}\n\n\tsend(request) {\n\t\tif (this.licenses.length !== 0) {\n\t\t\trequest.parameters[\"license\"] = this.licenses.join(\",\");\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = LicenseSender;","class Request {\n\tconstructor(payload) {\n\t\tthis.baseUrl = \"\";\n\t\tthis.payload = payload;\n\t\tthis.headers = {\n\t\t\t\"Content-Type\": \"application/json; charset=utf-8\",\n\t\t};\n\n\t\tthis.parameters = {};\n\t}\n}\n\nmodule.exports = Request;","class Response {\n\tconstructor (statusCode, payload, error = undefined) {\n\t\tthis.statusCode = statusCode;\n\t\tthis.payload = payload;\n\t\tthis.error = error;\n\t}\n}\n\nmodule.exports = Response;","class SharedCredentials {\n\tconstructor(authId, hostName) {\n\t\tthis.authId = authId;\n\t\tthis.hostName = hostName;\n\t}\n\n\tsign(request) {\n\t\trequest.parameters[\"auth-id\"] = this.authId;\n\t\tif (this.hostName) request.headers[\"Referer\"] = this.hostName;\n\t}\n}\n\nmodule.exports = SharedCredentials;","const Promise = require(\"promise\");\nconst UnprocessableEntityError = require(\"./Errors\").UnprocessableEntityError;\nconst SharedCredentials = require(\"./SharedCredentials\");\n\nclass SigningSender {\n\tconstructor(innerSender, signer) {\n\t\tthis.signer = signer;\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\tconst sendingPostWithSharedCredentials = request.payload && this.signer instanceof SharedCredentials;\n\t\tif (sendingPostWithSharedCredentials) {\n\t\t\tconst message = \"Shared credentials cannot be used in batches with a length greater than 1 or when using the US Extract API.\";\n\t\t\tthrow new UnprocessableEntityError(message);\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.signer.sign(request);\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = SigningSender;","class StaticCredentials {\n\tconstructor (authId, authToken) {\n\t\tthis.authId = authId;\n\t\tthis.authToken = authToken;\n\t}\n\n\tsign (request) {\n\t\trequest.parameters[\"auth-id\"] = this.authId;\n\t\trequest.parameters[\"auth-token\"] = this.authToken;\n\t}\n}\n\nmodule.exports = StaticCredentials;","const Promise = require(\"promise\");\nconst Errors = require(\"./Errors\");\n\nclass StatusCodeSender {\n\tconstructor(innerSender) {\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(error => {\n\t\t\t\t\tswitch (error.statusCode) {\n\t\t\t\t\t\tcase 400:\n\t\t\t\t\t\t\terror.error = new Errors.BadRequestError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 401:\n\t\t\t\t\t\t\terror.error = new Errors.BadCredentialsError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 402:\n\t\t\t\t\t\t\terror.error = new Errors.PaymentRequiredError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 413:\n\t\t\t\t\t\t\terror.error = new Errors.RequestEntityTooLargeError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 422:\n\t\t\t\t\t\t\terror.error = new Errors.UnprocessableEntityError(\"GET request lacked required fields.\");\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 429:\n\t\t\t\t\t\t\terror.error = new Errors.TooManyRequestsError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 500:\n\t\t\t\t\t\t\terror.error = new Errors.InternalServerError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 503:\n\t\t\t\t\t\t\terror.error = new Errors.ServiceUnavailableError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 504:\n\t\t\t\t\t\t\terror.error = new Errors.GatewayTimeoutError();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\treject(error);\n\t\t\t\t});\n\t\t});\n\t}\n}\n\nmodule.exports = StatusCodeSender;","/**\n * A candidate is a possible match for an address that was submitted.<br>\n *     A lookup can have multiple candidates if the address was ambiguous.\n *\n * @see \"https://smartystreets.com/docs/cloud/international-street-api#root\"\n */\nclass Candidate {\n\tconstructor(responseData) {\n\t\tthis.organization = responseData.organization;\n\t\tthis.address1 = responseData.address1;\n\t\tthis.address2 = responseData.address2;\n\t\tthis.address3 = responseData.address3;\n\t\tthis.address4 = responseData.address4;\n\t\tthis.address5 = responseData.address5;\n\t\tthis.address6 = responseData.address6;\n\t\tthis.address7 = responseData.address7;\n\t\tthis.address8 = responseData.address8;\n\t\tthis.address9 = responseData.address9;\n\t\tthis.address10 = responseData.address10;\n\t\tthis.address11 = responseData.address11;\n\t\tthis.address12 = responseData.address12;\n\n\t\tthis.components = {};\n\t\tif (responseData.components !== undefined) {\n\t\t\tthis.components.countryIso3 = responseData.components.country_iso_3;\n\t\t\tthis.components.superAdministrativeArea = responseData.components.super_administrative_area;\n\t\t\tthis.components.administrativeArea = responseData.components.administrative_area;\n\t\t\tthis.components.subAdministrativeArea = responseData.components.sub_administrative_area;\n\t\t\tthis.components.dependentLocality = responseData.components.dependent_locality;\n\t\t\tthis.components.dependentLocalityName = responseData.components.dependent_locality_name;\n\t\t\tthis.components.doubleDependentLocality = responseData.components.double_dependent_locality;\n\t\t\tthis.components.locality = responseData.components.locality;\n\t\t\tthis.components.postalCode = responseData.components.postal_code;\n\t\t\tthis.components.postalCodeShort = responseData.components.postal_code_short;\n\t\t\tthis.components.postalCodeExtra = responseData.components.postal_code_extra;\n\t\t\tthis.components.premise = responseData.components.premise;\n\t\t\tthis.components.premiseExtra = responseData.components.premise_extra;\n\t\t\tthis.components.premisePrefixNumber = responseData.components.premise_prefix_number;\n\t\t\tthis.components.premiseNumber = responseData.components.premise_number;\n\t\t\tthis.components.premiseType = responseData.components.premise_type;\n\t\t\tthis.components.thoroughfare = responseData.components.thoroughfare;\n\t\t\tthis.components.thoroughfarePredirection = responseData.components.thoroughfare_predirection;\n\t\t\tthis.components.thoroughfarePostdirection = responseData.components.thoroughfare_postdirection;\n\t\t\tthis.components.thoroughfareName = responseData.components.thoroughfare_name;\n\t\t\tthis.components.thoroughfareTrailingType = responseData.components.thoroughfare_trailing_type;\n\t\t\tthis.components.thoroughfareType = responseData.components.thoroughfare_type;\n\t\t\tthis.components.dependentThoroughfare = responseData.components.dependent_thoroughfare;\n\t\t\tthis.components.dependentThoroughfarePredirection = responseData.components.dependent_thoroughfare_predirection;\n\t\t\tthis.components.dependentThoroughfarePostdirection = responseData.components.dependent_thoroughfare_postdirection;\n\t\t\tthis.components.dependentThoroughfareName = responseData.components.dependent_thoroughfare_name;\n\t\t\tthis.components.dependentThoroughfareTrailingType = responseData.components.dependent_thoroughfare_trailing_type;\n\t\t\tthis.components.dependentThoroughfareType = responseData.components.dependent_thoroughfare_type;\n\t\t\tthis.components.building = responseData.components.building;\n\t\t\tthis.components.buildingLeadingType = responseData.components.building_leading_type;\n\t\t\tthis.components.buildingName = responseData.components.building_name;\n\t\t\tthis.components.buildingTrailingType = responseData.components.building_trailing_type;\n\t\t\tthis.components.subBuildingType = responseData.components.sub_building_type;\n\t\t\tthis.components.subBuildingNumber = responseData.components.sub_building_number;\n\t\t\tthis.components.subBuildingName = responseData.components.sub_building_name;\n\t\t\tthis.components.subBuilding = responseData.components.sub_building;\n\t\t\tthis.components.postBox = responseData.components.post_box;\n\t\t\tthis.components.postBoxType = responseData.components.post_box_type;\n\t\t\tthis.components.postBoxNumber = responseData.components.post_box_number;\n\t\t}\n\n\t\tthis.analysis = {};\n\t\tif (responseData.analysis !== undefined) {\n\t\t\tthis.analysis.verificationStatus = responseData.analysis.verification_status;\n\t\t\tthis.analysis.addressPrecision = responseData.analysis.address_precision;\n\t\t\tthis.analysis.maxAddressPrecision = responseData.analysis.max_address_precision;\n\n\t\t\tthis.analysis.changes = {};\n\t\t\tif (responseData.analysis.changes !== undefined) {\n\t\t\t\tthis.analysis.changes.organization = responseData.analysis.changes.organization;\n\t\t\t\tthis.analysis.changes.address1 = responseData.analysis.changes.address1;\n\t\t\t\tthis.analysis.changes.address2 = responseData.analysis.changes.address2;\n\t\t\t\tthis.analysis.changes.address3 = responseData.analysis.changes.address3;\n\t\t\t\tthis.analysis.changes.address4 = responseData.analysis.changes.address4;\n\t\t\t\tthis.analysis.changes.address5 = responseData.analysis.changes.address5;\n\t\t\t\tthis.analysis.changes.address6 = responseData.analysis.changes.address6;\n\t\t\t\tthis.analysis.changes.address7 = responseData.analysis.changes.address7;\n\t\t\t\tthis.analysis.changes.address8 = responseData.analysis.changes.address8;\n\t\t\t\tthis.analysis.changes.address9 = responseData.analysis.changes.address9;\n\t\t\t\tthis.analysis.changes.address10 = responseData.analysis.changes.address10;\n\t\t\t\tthis.analysis.changes.address11 = responseData.analysis.changes.address11;\n\t\t\t\tthis.analysis.changes.address12 = responseData.analysis.changes.address12;\n\n\t\t\t\tthis.analysis.changes.components = {};\n\t\t\t\tif (responseData.analysis.changes.components !== undefined) {\n\t\t\t\t\tthis.analysis.changes.components.countryIso3 = responseData.analysis.changes.components.country_iso_3;\n\t\t\t\t\tthis.analysis.changes.components.superAdministrativeArea = responseData.analysis.changes.components.super_administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.administrativeArea = responseData.analysis.changes.components.administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.subAdministrativeArea = responseData.analysis.changes.components.sub_administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.dependentLocality = responseData.analysis.changes.components.dependent_locality;\n\t\t\t\t\tthis.analysis.changes.components.dependentLocalityName = responseData.analysis.changes.components.dependent_locality_name;\n\t\t\t\t\tthis.analysis.changes.components.doubleDependentLocality = responseData.analysis.changes.components.double_dependent_locality;\n\t\t\t\t\tthis.analysis.changes.components.locality = responseData.analysis.changes.components.locality;\n\t\t\t\t\tthis.analysis.changes.components.postalCode = responseData.analysis.changes.components.postal_code;\n\t\t\t\t\tthis.analysis.changes.components.postalCodeShort = responseData.analysis.changes.components.postal_code_short;\n\t\t\t\t\tthis.analysis.changes.components.postalCodeExtra = responseData.analysis.changes.components.postal_code_extra;\n\t\t\t\t\tthis.analysis.changes.components.premise = responseData.analysis.changes.components.premise;\n\t\t\t\t\tthis.analysis.changes.components.premiseExtra = responseData.analysis.changes.components.premise_extra;\n\t\t\t\t\tthis.analysis.changes.components.premisePrefixNumber = responseData.analysis.changes.components.premise_prefix_number;\n\t\t\t\t\tthis.analysis.changes.components.premiseNumber = responseData.analysis.changes.components.premise_number;\n\t\t\t\t\tthis.analysis.changes.components.premiseType = responseData.analysis.changes.components.premise_type;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfare = responseData.analysis.changes.components.thoroughfare;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfarePredirection = responseData.analysis.changes.components.thoroughfare_predirection;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfarePostdirection = responseData.analysis.changes.components.thoroughfare_postdirection;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareName = responseData.analysis.changes.components.thoroughfare_name;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareTrailingType = responseData.analysis.changes.components.thoroughfare_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareType = responseData.analysis.changes.components.thoroughfare_type;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfare = responseData.analysis.changes.components.dependent_thoroughfare;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfarePredirection = responseData.analysis.changes.components.dependent_thoroughfare_predirection;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfarePostdirection = responseData.analysis.changes.components.dependent_thoroughfare_postdirection;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareName = responseData.analysis.changes.components.dependent_thoroughfare_name;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareTrailingType = responseData.analysis.changes.components.dependent_thoroughfare_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareType = responseData.analysis.changes.components.dependent_thoroughfare_type;\n\t\t\t\t\tthis.analysis.changes.components.building = responseData.analysis.changes.components.building;\n\t\t\t\t\tthis.analysis.changes.components.buildingLeadingType = responseData.analysis.changes.components.building_leading_type;\n\t\t\t\t\tthis.analysis.changes.components.buildingName = responseData.analysis.changes.components.building_name;\n\t\t\t\t\tthis.analysis.changes.components.buildingTrailingType = responseData.analysis.changes.components.building_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingType = responseData.analysis.changes.components.sub_building_type;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingNumber = responseData.analysis.changes.components.sub_building_number;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingName = responseData.analysis.changes.components.sub_building_name;\n\t\t\t\t\tthis.analysis.changes.components.subBuilding = responseData.analysis.changes.components.sub_building;\n\t\t\t\t\tthis.analysis.changes.components.postBox = responseData.analysis.changes.components.post_box;\n\t\t\t\t\tthis.analysis.changes.components.postBoxType = responseData.analysis.changes.components.post_box_type;\n\t\t\t\t\tthis.analysis.changes.components.postBoxNumber = responseData.analysis.changes.components.post_box_number;\n\t\t\t\t}\n\t\t\t\t//TODO: Fill in the rest of these fields and their corresponding tests.\n\t\t\t}\n\t\t}\n\n\t\tthis.metadata = {};\n\t\tif (responseData.metadata !== undefined) {\n\t\t\tthis.metadata.latitude = responseData.metadata.latitude;\n\t\t\tthis.metadata.longitude = responseData.metadata.longitude;\n\t\t\tthis.metadata.geocodePrecision = responseData.metadata.geocode_precision;\n\t\t\tthis.metadata.maxGeocodePrecision = responseData.metadata.max_geocode_precision;\n\t\t\tthis.metadata.addressFormat = responseData.metadata.address_format;\n\t\t}\n\t}\n}\n\nmodule.exports = Candidate;","const Request = require(\"../Request\");\nconst Errors = require(\"../Errors\");\nconst Candidate = require(\"./Candidate\");\nconst Promise = require(\"promise\");\nconst buildInputData = require(\"../util/buildInputData\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").internationalStreet;\n\n/**\n * This client sends lookups to the SmartyStreets International Street API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildInputData(lookup, keyTranslationFormat);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tresolve(attachLookupCandidates(response, lookup));\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction attachLookupCandidates(response, lookup) {\n\t\t\tresponse.payload.map(rawCandidate => {\n\t\t\t\tlookup.result.push(new Candidate(rawCandidate));\n\t\t\t});\n\n\t\t\treturn lookup;\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","const UnprocessableEntityError = require(\"../Errors\").UnprocessableEntityError;\nconst messages = {\n\tcountryRequired: \"Country field is required.\",\n\tfreeformOrAddress1Required: \"Either freeform or address1 is required.\",\n\tinsufficientInformation: \"Insufficient information: One or more required fields were not set on the lookup.\",\n\tbadGeocode: \"Invalid input: geocode can only be set to 'true' (default is 'false'.\",\n\tinvalidLanguage: \"Invalid input: language can only be set to 'latin' or 'native'. When not set, the the output language will match the language of the input values.\"\n};\n\n\n/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     <p><b>Note: </b><i>Lookups must have certain required fields set with non-blank values. <br>\n *         These can be found at the URL below.</i></p>\n *     @see \"https://smartystreets.com/docs/cloud/international-street-api#http-input-fields\"\n */\nclass Lookup {\n\tconstructor(country, freeform) {\n\t\tthis.result = [];\n\n\t\tthis.country = country;\n\t\tthis.freeform = freeform;\n\t\tthis.address1 = undefined;\n\t\tthis.address2 = undefined;\n\t\tthis.address3 = undefined;\n\t\tthis.address4 = undefined;\n\t\tthis.organization = undefined;\n\t\tthis.locality = undefined;\n\t\tthis.administrativeArea = undefined;\n\t\tthis.postalCode = undefined;\n\t\tthis.geocode = undefined;\n\t\tthis.language = undefined;\n\t\tthis.inputId = undefined;\n\n\t\tthis.ensureEnoughInfo = this.ensureEnoughInfo.bind(this);\n\t\tthis.ensureValidData = this.ensureValidData.bind(this);\n\t}\n\n\tensureEnoughInfo() {\n\t\tif (fieldIsMissing(this.country)) throw new UnprocessableEntityError(messages.countryRequired);\n\n\t\tif (fieldIsSet(this.freeform)) return true;\n\n\t\tif (fieldIsMissing(this.address1)) throw new UnprocessableEntityError(messages.freeformOrAddress1Required);\n\n\t\tif (fieldIsSet(this.postalCode)) return true;\n\n\t\tif (fieldIsMissing(this.locality) || fieldIsMissing(this.administrativeArea)) throw new UnprocessableEntityError(messages.insufficientInformation);\n\n\t\treturn true;\n\t}\n\n\tensureValidData() {\n\t\tlet languageIsSetIncorrectly = () => {\n\t\t\tlet isLanguage = language => this.language.toLowerCase() === language;\n\n\t\t\treturn fieldIsSet(this.language) && !(isLanguage(\"latin\") || isLanguage(\"native\"));\n\t\t};\n\n\t\tlet geocodeIsSetIncorrectly = () => {\n\t\t\treturn fieldIsSet(this.geocode) && this.geocode.toLowerCase() !== \"true\";\n\t\t};\n\n\t\tif (geocodeIsSetIncorrectly()) throw new UnprocessableEntityError(messages.badGeocode);\n\n\t\tif (languageIsSetIncorrectly()) throw new UnprocessableEntityError(messages.invalidLanguage);\n\n\t\treturn true;\n\t}\n}\n\nfunction fieldIsMissing (field) {\n\tif (!field) return true;\n\n\tconst whitespaceCharacters = /\\s/g;\n\n\treturn field.replace(whitespaceCharacters, \"\").length < 1;\n}\n\nfunction fieldIsSet (field) {\n\treturn !fieldIsMissing(field);\n}\n\nmodule.exports = Lookup;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Suggestion = require(\"./Suggestion\");\nconst Promise = require(\"promise\");\n\n/**\n * This client sends lookups to the SmartyStreets US Autocomplete API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildRequestParameters(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = buildSuggestionsFromResponse(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParameters(lookup) {\n\t\t\treturn {\n\t\t\t\tprefix: lookup.prefix,\n\t\t\t\tsuggestions: lookup.maxSuggestions,\n\t\t\t\tcity_filter: joinFieldWith(lookup.cityFilter, \",\"),\n\t\t\t\tstate_filter: joinFieldWith(lookup.stateFilter, \",\"),\n\t\t\t\tprefer: joinFieldWith(lookup.prefer, \";\"),\n\t\t\t\tprefer_ratio: lookup.preferRatio,\n\t\t\t\tgeolocate: lookup.geolocate,\n\t\t\t\tgeolocate_precision: lookup.geolocatePrecision,\n\t\t\t};\n\n\t\t\tfunction joinFieldWith(field, delimiter) {\n\t\t\t\tif (field.length) return field.join(delimiter);\n\t\t\t}\n\t\t}\n\n\t\tfunction buildSuggestionsFromResponse(payload) {\n\t\t\tif (payload.suggestions === null) return [];\n\n\t\t\treturn payload.suggestions.map(suggestion => new Suggestion(suggestion));\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://smartystreets.com/docs/cloud/us-autocomplete-api#http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param prefix The beginning of an address. This is required to be set.\n\t */\n\tconstructor(prefix) {\n\t\tthis.result = [];\n\n\t\tthis.prefix = prefix;\n\t\tthis.maxSuggestions = undefined;\n\t\tthis.cityFilter = [];\n\t\tthis.stateFilter = [];\n\t\tthis.prefer = [];\n\t\tthis.preferRatio = undefined;\n\t\tthis.geolocate = undefined;\n\t\tthis.geolocatePrecision = undefined;\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://smartystreets.com/docs/cloud/us-autocomplete-api#http-response\"\n */\nclass Suggestion {\n\tconstructor(responseData) {\n\t\tthis.text = responseData.text;\n\t\tthis.streetLine = responseData.street_line;\n\t\tthis.city = responseData.city;\n\t\tthis.state = responseData.state;\n\t}\n}\n\nmodule.exports = Suggestion;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Suggestion = require(\"./Suggestion\");\nconst Promise = require(\"promise\");\n\n/**\n * This client sends lookups to the SmartyStreets US Autocomplete Pro API, <br>\n *     and attaches the suggestions to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildRequestParameters(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = buildSuggestionsFromResponse(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParameters(lookup) {\n\t\t\treturn {\n\t\t\t\tsearch: lookup.search,\n\t\t\t\tselected: lookup.selected,\n\t\t\t\tmax_results: lookup.maxResults,\n\t\t\t\tinclude_only_cities: joinFieldWith(lookup.includeOnlyCities, \";\"),\n\t\t\t\tinclude_only_states: joinFieldWith(lookup.includeOnlyStates, \";\"),\n\t\t\t\tinclude_only_zip_codes: joinFieldWith(lookup.includeOnlyZIPCodes, \";\"),\n\t\t\t\texclude_states: joinFieldWith(lookup.excludeStates, \";\"),\n\t\t\t\tprefer_cities: joinFieldWith(lookup.preferCities, \";\"),\n\t\t\t\tprefer_states: joinFieldWith(lookup.preferStates, \";\"),\n\t\t\t\tprefer_zip_codes: joinFieldWith(lookup.preferZIPCodes, \";\"),\n\t\t\t\tprefer_ratio: lookup.preferRatio,\n\t\t\t\tprefer_geolocation: lookup.preferGeolocation,\n\t\t\t};\n\n\t\t\tfunction joinFieldWith(field, delimiter) {\n\t\t\t\tif (field.length) return field.join(delimiter);\n\t\t\t}\n\t\t}\n\n\t\tfunction buildSuggestionsFromResponse(payload) {\n\t\t\tif (payload.suggestions === null) return [];\n\n\t\t\treturn payload.suggestions.map(suggestion => new Suggestion(suggestion));\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://smartystreets.com/docs/cloud/us-autocomplete-api#pro-http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param search The beginning of an address. This is required to be set.\n\t */\n\tconstructor(search) {\n\t\tthis.result = [];\n\n\t\tthis.search = search;\n\t\tthis.selected = undefined;\n\t\tthis.maxResults = undefined;\n\t\tthis.includeOnlyCities = [];\n\t\tthis.includeOnlyStates = [];\n\t\tthis.includeOnlyZIPCodes = [];\n\t\tthis.excludeStates = [];\n\t\tthis.preferCities = [];\n\t\tthis.preferStates = [];\n\t\tthis.preferZIPCodes = [];\n\t\tthis.preferRatio = undefined;\n\t\tthis.preferGeolocation = undefined;\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://smartystreets.com/docs/cloud/us-autocomplete-api#pro-http-response\"\n */\nclass Suggestion {\n\tconstructor(responseData) {\n\t\tthis.streetLine = responseData.street_line;\n\t\tthis.secondary = responseData.secondary;\n\t\tthis.city = responseData.city;\n\t\tthis.state = responseData.state;\n\t\tthis.zipcode = responseData.zipcode;\n\t\tthis.entries = responseData.entries;\n\t}\n}\n\nmodule.exports = Suggestion;","const Candidate = require(\"../us_street/Candidate\");\n\n/**\n * @see <a href=\"https://smartystreets.com/docs/cloud/us-extract-api#http-response-status\">SmartyStreets US Extract API docs</a>\n */\nclass Address {\n\tconstructor (responseData) {\n\t\tthis.text = responseData.text;\n\t\tthis.verified = responseData.verified;\n\t\tthis.line = responseData.line;\n\t\tthis.start = responseData.start;\n\t\tthis.end = responseData.end;\n\t\tthis.candidates = responseData.api_output.map(rawAddress => new Candidate(rawAddress));\n\t}\n}\n\nmodule.exports = Address;","const Errors = require(\"../Errors\");\nconst Promise = require(\"promise\");\nconst Request = require(\"../Request\");\nconst Result = require(\"./Result\");\n\n/**\n * This client sends lookups to the SmartyStreets US Extract API, <br>\n *     and attaches the results to the Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request(lookup.text);\n\t\trequest.parameters = buildRequestParams(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = new Result(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParams(lookup) {\n\t\t\treturn {\n\t\t\t\thtml: lookup.html,\n\t\t\t\taggressive: lookup.aggressive,\n\t\t\t\taddr_line_breaks: lookup.addressesHaveLineBreaks,\n\t\t\t\taddr_per_line: lookup.addressesPerLine,\n\t\t\t};\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://smartystreets.com/docs/cloud/us-extract-api#http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param text The text that is to have addresses extracted out of it for verification (required)\n\t */\n\tconstructor(text) {\n\t\tthis.result = {\n\t\t\tmeta: {},\n\t\t\taddresses: [],\n\t\t};\n\t\t//TODO: require the text field.\n\t\tthis.text = text;\n\t\tthis.html = undefined;\n\t\tthis.aggressive = undefined;\n\t\tthis.addressesHaveLineBreaks = undefined;\n\t\tthis.addressesPerLine = undefined;\n\t}\n}\n\nmodule.exports = Lookup;","const Address = require(\"./Address\");\n\n/**\n * @see <a href=\"https://smartystreets.com/docs/cloud/us-extract-api#http-response-status\">SmartyStreets US Extract API docs</a>\n */\nclass Result {\n\tconstructor({meta, addresses}) {\n\t\tthis.meta = {\n\t\t\tlines: meta.lines,\n\t\t\tunicode: meta.unicode,\n\t\t\taddressCount: meta.address_count,\n\t\t\tverifiedCount: meta.verified_count,\n\t\t\tbytes: meta.bytes,\n\t\t\tcharacterCount: meta.character_count,\n\t\t};\n\n\t\tthis.addresses = addresses.map(rawAddress => new Address(rawAddress));\n\t}\n}\n\nmodule.exports = Result;","const Request = require(\"../Request\");\nconst buildInputData = require(\"../util/buildInputData\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usReverseGeo;\nconst {UndefinedLookupError} = require(\"../Errors.js\");\n\n/**\n * This client sends lookups to the SmartyStreets US Reverse Geo API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildInputData(lookup, keyTranslationFormat);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tresolve(attachLookupResults(response, lookup));\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction attachLookupResults(response, lookup) {\n\t\t\tlookup.response = response.payload;\n\n\t\t\treturn lookup;\n\t\t}\n\t}\n}\n\nmodule.exports = Client;\n","/**\n * A candidate is a possible match for an address that was submitted.<br>\n *     A lookup can have multiple candidates if the address was ambiguous, and<br>\n *     the maxCandidates field is set higher than 1.\n *\n * @see \"https://smartystreets.com/docs/cloud/us-street-api#root\"\n */\nclass Candidate {\n\tconstructor(responseData) {\n\t\tthis.inputIndex = responseData.input_index;\n\t\tthis.candidateIndex = responseData.candidate_index;\n\t\tthis.addressee = responseData.addressee;\n\t\tthis.deliveryLine1 = responseData.delivery_line_1;\n\t\tthis.deliveryLine2 = responseData.delivery_line_2;\n\t\tthis.lastLine = responseData.last_line;\n\t\tthis.deliveryPointBarcode = responseData.delivery_point_barcode;\n\n\t\tthis.components = {};\n\t\tif (responseData.components !== undefined) {\n\t\t\tthis.components.urbanization = responseData.components.urbanization;\n\t\t\tthis.components.primaryNumber = responseData.components.primary_number;\n\t\t\tthis.components.streetName = responseData.components.street_name;\n\t\t\tthis.components.streetPredirection = responseData.components.street_predirection;\n\t\t\tthis.components.streetPostdirection = responseData.components.street_postdirection;\n\t\t\tthis.components.streetSuffix = responseData.components.street_suffix;\n\t\t\tthis.components.secondaryNumber = responseData.components.secondary_number;\n\t\t\tthis.components.secondaryDesignator = responseData.components.secondary_designator;\n\t\t\tthis.components.extraSecondaryNumber = responseData.components.extra_secondary_number;\n\t\t\tthis.components.extraSecondaryDesignator = responseData.components.extra_secondary_designator;\n\t\t\tthis.components.pmbDesignator = responseData.components.pmb_designator;\n\t\t\tthis.components.pmbNumber = responseData.components.pmb_number;\n\t\t\tthis.components.cityName = responseData.components.city_name;\n\t\t\tthis.components.defaultCityName = responseData.components.default_city_name;\n\t\t\tthis.components.state = responseData.components.state_abbreviation;\n\t\t\tthis.components.zipCode = responseData.components.zipcode;\n\t\t\tthis.components.plus4Code = responseData.components.plus4_code;\n\t\t\tthis.components.deliveryPoint = responseData.components.delivery_point;\n\t\t\tthis.components.deliveryPointCheckDigit = responseData.components.delivery_point_check_digit;\n\t\t}\n\n\t\tthis.metadata = {};\n\t\tif (responseData.metadata !== undefined) {\n\t\t\tthis.metadata.recordType = responseData.metadata.record_type;\n\t\t\tthis.metadata.zipType = responseData.metadata.zip_type;\n\t\t\tthis.metadata.countyFips = responseData.metadata.county_fips;\n\t\t\tthis.metadata.countyName = responseData.metadata.county_name;\n\t\t\tthis.metadata.carrierRoute = responseData.metadata.carrier_route;\n\t\t\tthis.metadata.congressionalDistrict = responseData.metadata.congressional_district;\n\t\t\tthis.metadata.buildingDefaultIndicator = responseData.metadata.building_default_indicator;\n\t\t\tthis.metadata.rdi = responseData.metadata.rdi;\n\t\t\tthis.metadata.elotSequence = responseData.metadata.elot_sequence;\n\t\t\tthis.metadata.elotSort = responseData.metadata.elot_sort;\n\t\t\tthis.metadata.latitude = responseData.metadata.latitude;\n\t\t\tthis.metadata.longitude = responseData.metadata.longitude;\n\t\t\tthis.metadata.precision = responseData.metadata.precision;\n\t\t\tthis.metadata.timeZone = responseData.metadata.time_zone;\n\t\t\tthis.metadata.utcOffset = responseData.metadata.utc_offset;\n\t\t\tthis.metadata.obeysDst = responseData.metadata.dst;\n\t\t\tthis.metadata.isEwsMatch = responseData.metadata.ews_match;\n\t\t}\n\n\t\tthis.analysis = {};\n\t\tif (responseData.analysis !== undefined) {\n\t\t\tthis.analysis.dpvMatchCode = responseData.analysis.dpv_match_code;\n\t\t\tthis.analysis.dpvFootnotes = responseData.analysis.dpv_footnotes;\n\t\t\tthis.analysis.cmra = responseData.analysis.dpv_cmra;\n\t\t\tthis.analysis.vacant = responseData.analysis.dpv_vacant;\n\t\t\tthis.analysis.active = responseData.analysis.active;\n\t\t\tthis.analysis.isEwsMatch = responseData.analysis.ews_match; // Deprecated, refer to metadata.ews_match\n\t\t\tthis.analysis.footnotes = responseData.analysis.footnotes;\n\t\t\tthis.analysis.lacsLinkCode = responseData.analysis.lacslink_code;\n\t\t\tthis.analysis.lacsLinkIndicator = responseData.analysis.lacslink_indicator;\n\t\t\tthis.analysis.isSuiteLinkMatch = responseData.analysis.suitelink_match;\n\t\t}\n\t}\n}\n\nmodule.exports = Candidate;","const Candidate = require(\"./Candidate\");\nconst Lookup = require(\"./Lookup\");\nconst Batch = require(\"../Batch\");\nconst UndefinedLookupError = require(\"../Errors\").UndefinedLookupError;\nconst sendBatch = require(\"../util/sendBatch\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usStreet;\n\n/**\n * This client sends lookups to the SmartyStreets US Street API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\t/**\n\t * Sends up to 100 lookups for validation.\n\t * @param data may be a Lookup object, or a Batch which must contain between 1 and 100 Lookup objects\n\t * @throws SmartyException\n\t */\n\tsend(data) {\n\t\tconst dataIsBatch = data instanceof Batch;\n\t\tconst dataIsLookup = data instanceof Lookup;\n\n\t\tif (!dataIsLookup && !dataIsBatch) throw new UndefinedLookupError;\n\n\t\tlet batch;\n\n\t\tif (dataIsLookup) {\n\t\t\tbatch = new Batch();\n\t\t\tbatch.add(data);\n\t\t} else {\n\t\t\tbatch = data;\n\t\t}\n\n\t\treturn sendBatch(batch, this.sender, Candidate, keyTranslationFormat);\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://smartystreets.com/docs/cloud/us-street-api#input-fields\"\n */\nclass Lookup {\n\tconstructor(street, street2, secondary, city, state, zipCode, lastLine, addressee, urbanization, match, maxCandidates, inputId) {\n\t\tthis.street = street;\n\t\tthis.street2 = street2;\n\t\tthis.secondary = secondary;\n\t\tthis.city = city;\n\t\tthis.state = state;\n\t\tthis.zipCode = zipCode;\n\t\tthis.lastLine = lastLine;\n\t\tthis.addressee = addressee;\n\t\tthis.urbanization = urbanization;\n\t\tthis.match = match;\n\t\tthis.maxCandidates = maxCandidates;\n\t\tthis.inputId = inputId;\n\t\tthis.result = [];\n\t}\n}\n\nmodule.exports = Lookup;\n","const Lookup = require(\"./Lookup\");\nconst Result = require(\"./Result\");\nconst Batch = require(\"../Batch\");\nconst UndefinedLookupError = require(\"../Errors\").UndefinedLookupError;\nconst sendBatch = require(\"../util/sendBatch\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usZipcode;\n\n/**\n * This client sends lookups to the SmartyStreets US ZIP Code API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\t/**\n\t * Sends up to 100 lookups for validation.\n\t * @param data May be a Lookup object, or a Batch which must contain between 1 and 100 Lookup objects\n\t * @throws SmartyException\n\t */\n\tsend(data) {\n\t\tconst dataIsBatch = data instanceof Batch;\n\t\tconst dataIsLookup = data instanceof Lookup;\n\n\t\tif (!dataIsLookup && !dataIsBatch) throw new UndefinedLookupError;\n\n\t\tlet batch;\n\n\t\tif (dataIsLookup) {\n\t\t\tbatch = new Batch();\n\t\t\tbatch.add(data);\n\t\t} else batch = data;\n\n\t\treturn sendBatch(batch, this.sender, Result, keyTranslationFormat);\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://smartystreets.com/docs/cloud/us-zipcode-api#http-request-input-fields\"\n */\nclass Lookup {\n\tconstructor(city, state, zipCode, inputId) {\n\t\tthis.city = city;\n\t\tthis.state = state;\n\t\tthis.zipCode = zipCode;\n\t\tthis.inputId = inputId;\n\t\tthis.result = [];\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://smartystreets.com/docs/cloud/us-zipcode-api#root\"\n */\nclass Result {\n\tconstructor(responseData) {\n\t\tthis.inputIndex = responseData.input_index;\n\t\tthis.status = responseData.status;\n\t\tthis.reason = responseData.reason;\n\t\tthis.valid = this.status === undefined && this.reason === undefined;\n\n\t\tthis.cities = !responseData.city_states ? [] : responseData.city_states.map(city => {\n\t\t\treturn {\n\t\t\t\tcity: city.city,\n\t\t\t\tstateAbbreviation: city.state_abbreviation,\n\t\t\t\tstate: city.state,\n\t\t\t\tmailableCity: city.mailable_city,\n\t\t\t};\n\t\t});\n\n\t\tthis.zipcodes = !responseData.zipcodes ? [] : responseData.zipcodes.map(zipcode => {\n\t\t\treturn {\n\t\t\t\tzipcode: zipcode.zipcode,\n\t\t\t\tzipcodeType: zipcode.zipcode_type,\n\t\t\t\tdefaultCity: zipcode.default_city,\n\t\t\t\tcountyFips: zipcode.county_fips,\n\t\t\t\tcountyName: zipcode.county_name,\n\t\t\t\tlatitude: zipcode.latitude,\n\t\t\t\tlongitude: zipcode.longitude,\n\t\t\t\tprecision: zipcode.precision,\n\t\t\t\tstateAbbreviation: zipcode.state_abbreviation,\n\t\t\t\tstate: zipcode.state,\n\t\t\t\talternateCounties: !zipcode.alternate_counties ? [] : zipcode.alternate_counties.map(county => {\n\t\t\t\t\treturn {\n\t\t\t\t\t\tcountyFips: county.county_fips,\n\t\t\t\t\t\tcountyName: county.county_name,\n\t\t\t\t\t\tstateAbbreviation: county.state_abbreviation,\n\t\t\t\t\t\tstate: county.state,\n\t\t\t\t\t}\n\t\t\t\t}),\n\t\t\t};\n\t\t});\n\t}\n}\n\nmodule.exports = Result;","module.exports = {\n\tusStreet: {\n\t\t\"street\": \"street\",\n\t\t\"street2\": \"street2\",\n\t\t\"secondary\": \"secondary\",\n\t\t\"city\": \"city\",\n\t\t\"state\": \"state\",\n\t\t\"zipcode\": \"zipCode\",\n\t\t\"lastline\": \"lastLine\",\n\t\t\"addressee\": \"addressee\",\n\t\t\"urbanization\": \"urbanization\",\n\t\t\"match\": \"match\",\n\t\t\"candidates\": \"maxCandidates\",\n\t},\n\tusZipcode: {\n\t\t\"city\": \"city\",\n\t\t\"state\": \"state\",\n\t\t\"zipcode\": \"zipCode\",\n\t},\n\tinternationalStreet: {\n\t\t\"country\": \"country\",\n\t\t\"freeform\": \"freeform\",\n\t\t\"address1\": \"address1\",\n\t\t\"address2\": \"address2\",\n\t\t\"address3\": \"address3\",\n\t\t\"address4\": \"address4\",\n\t\t\"organization\": \"organization\",\n\t\t\"locality\": \"locality\",\n\t\t\"administrative_area\": \"administrativeArea\",\n\t\t\"postal_code\": \"postalCode\",\n\t\t\"geocode\": \"geocode\",\n\t\t\"language\": \"language\",\n\t},\n\tusReverseGeo: {\n\t\t\"latitude\": \"latitude\",\n\t\t\"longitude\": \"longitude\",\n\t}\n};","const ClientBuilder = require(\"../ClientBuilder\");\n\nfunction instantiateClientBuilder(credentials) {\n\treturn new ClientBuilder(credentials);\n}\n\nfunction buildUsStreetApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsStreetApiClient();\n}\n\nfunction buildUsAutocompleteApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsAutocompleteClient();\n}\n\nfunction buildUsAutocompleteProApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsAutocompleteProClient();\n}\n\nfunction buildUsExtractApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsExtractClient();\n}\n\nfunction buildUsZipcodeApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsZipcodeClient();\n}\n\nfunction buildInternationalStreetApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildInternationalStreetClient();\n}\n\nfunction buildUsReverseGeoApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsReverseGeoClient();\n}\n\nmodule.exports = {\n\tusStreet: buildUsStreetApiClient,\n\tusAutocomplete: buildUsAutocompleteApiClient,\n\tusAutocompletePro: buildUsAutocompleteProApiClient,\n\tusExtract: buildUsExtractApiClient,\n\tusZipcode: buildUsZipcodeApiClient,\n\tinternationalStreet: buildInternationalStreetApiClient,\n\tusReverseGeo: buildUsReverseGeoApiClient,\n};","const InputData = require(\"../InputData\");\n\nmodule.exports = (lookup, keyTranslationFormat) => {\n\tlet inputData = new InputData(lookup);\n\n\tfor (let key in keyTranslationFormat) {\n\t\tinputData.add(key, keyTranslationFormat[key]);\n\t}\n\n\treturn inputData.data;\n};\n","const Request = require(\"../Request\");\nconst Promise = require(\"promise\");\nconst Errors = require(\"../Errors\");\nconst buildInputData = require(\"../util/buildInputData\");\n\nmodule.exports = (batch, sender, Result, keyTranslationFormat) => {\n\tif (batch.isEmpty()) throw new Errors.BatchEmptyError;\n\n\tlet request = new Request();\n\n\tif (batch.length() === 1) request.parameters = generateRequestPayload(batch)[0];\n\telse request.payload = generateRequestPayload(batch);\n\n\treturn new Promise((resolve, reject) => {\n\t\tsender.send(request)\n\t\t\t.then(response => {\n\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\tresolve(assignResultsToLookups(batch, response));\n\t\t\t})\n\t\t\t.catch(reject);\n\t});\n\n\tfunction generateRequestPayload(batch) {\n\t\treturn batch.lookups.map((lookup) => {\n\t\t\treturn buildInputData(lookup, keyTranslationFormat);\n\t\t});\n\t}\n\n\tfunction assignResultsToLookups(batch, response) {\n\t\tresponse.payload.map(rawResult => {\n\t\t\tlet result = new Result(rawResult);\n\t\t\tlet lookup = batch.getByIndex(result.inputIndex);\n\n\t\t\tlookup.result.push(result);\n\t\t});\n\n\t\treturn batch;\n\t}\n};\n","var moduleMap = {\n\t\"./model-cache\": () => {\n\t\treturn Promise.all([__webpack_require__.e(\"src_adapters_index_js\"), __webpack_require__.e(\"src_domain_index_js\")]).then(() => () => (__webpack_require__(/*! ./src/domain */ \"./src/domain/index.js\")));\n\t},\n\t\"./adapter-cache\": () => {\n\t\treturn __webpack_require__.e(\"src_adapters_index_js\").then(() => () => (__webpack_require__(/*! ./src/adapters */ \"./src/adapters/index.js\")));\n\t},\n\t\"./service-cache\": () => {\n\t\treturn Promise.all([__webpack_require__.e(\"vendors-node_modules_nanoid_index_js-node_modules_ws_index_js\"), __webpack_require__.e(\"src_adapters_index_js\"), __webpack_require__.e(\"src_services_index_js\")]).then(() => () => (__webpack_require__(/*! ./src/services */ \"./src/services/index.js\")));\n\t},\n\t\"./event-bus\": () => {\n\t\treturn Promise.all([__webpack_require__.e(\"src_adapters_index_js\"), __webpack_require__.e(\"src_services_event-bus_js\")]).then(() => () => (__webpack_require__(/*! ./src/services/event-bus */ \"./src/services/event-bus.js\")));\n\t}\n};\nvar get = (module) => {\n\treturn (\n\t\t__webpack_require__.o(moduleMap, module)\n\t\t\t? moduleMap[module]()\n\t\t\t: Promise.resolve().then(() => {\n\t\t\t\tthrow new Error('Module \"' + module + '\" does not exist in container.');\n\t\t\t})\n\t);\n};\nvar init = (shareScope) => {\n\tvar oldScope = __webpack_require__.S[\"default\"];\n\tvar name = \"default\"\n\tif(oldScope && oldScope !== shareScope) throw new Error(\"Container initialization failed as it has already been initialized with a different share scope\");\n\t__webpack_require__.S[name] = shareScope;\n\treturn __webpack_require__.I(name);\n};\n\n// This exports getters to disallow modifications\n__webpack_require__.d(exports, {\n\tget: () => get,\n\tinit: () => init\n});","module.exports = require(\"assert\");","module.exports = require(\"crypto\");","module.exports = require(\"dns/promises\");","module.exports = require(\"domain\");","module.exports = require(\"events\");","module.exports = require(\"fs\");","module.exports = require(\"http\");","module.exports = require(\"https\");","module.exports = require(\"net\");","module.exports = require(\"stream\");","module.exports = require(\"tls\");","module.exports = require(\"tty\");","module.exports = require(\"url\");","module.exports = require(\"util\");","module.exports = require(\"zlib\");","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tif(__webpack_module_cache__[moduleId]) {\n\t\treturn __webpack_module_cache__[moduleId].exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = (module) => {\n\tvar getter = module && module.__esModule ?\n\t\t() => module['default'] :\n\t\t() => module;\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = (chunkId) => {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks\n__webpack_require__.u = (chunkId) => {\n\t// return url for filenames based on template\n\treturn \"\" + chunkId + \".js\";\n};","__webpack_require__.o = (obj, prop) => Object.prototype.hasOwnProperty.call(obj, prop)","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"http://cache.aegis.dev:8060\";","__webpack_require__.S = {};\nvar initPromises = {};\n__webpack_require__.I = (name) => {\n\t// only runs once\n\tif(initPromises[name]) return initPromises[name];\n\t// handling circular init calls\n\tinitPromises[name] = 1;\n\t// creates a new share scope if needed\n\tif(!__webpack_require__.o(__webpack_require__.S, name)) __webpack_require__.S[name] = {};\n\t// runs all init snippets from all modules reachable\n\tvar scope = __webpack_require__.S[name];\n\tvar warn = (msg) => typeof console !== \"undefined\" && console.warn && console.warn(msg);;\n\tvar uniqueName = \"microlib-example\";\n\tvar register = (name, version, factory) => {\n\t\tvar versions = scope[name] = scope[name] || {};\n\t\tvar activeVersion = versions[version];\n\t\tif(!activeVersion || !activeVersion.loaded && uniqueName > activeVersion.from) versions[version] = { get: factory, from: uniqueName };\n\t};\n\tvar initExternal = (id) => {\n\t\tvar handleError = (err) => warn(\"Initialization of sharing external failed: \" + err);\n\t\ttry {\n\t\t\tvar module = __webpack_require__(id);\n\t\t\tif(!module) return;\n\t\t\tvar initFn = (module) => module && module.init && module.init(__webpack_require__.S[name])\n\t\t\tif(module.then) return promises.push(module.then(initFn, handleError));\n\t\t\tvar initResult = initFn(module);\n\t\t\tif(initResult && initResult.then) return promises.push(initResult.catch(handleError));\n\t\t} catch(err) { handleError(err); }\n\t}\n\tvar promises = [];\n\tswitch(name) {\n\t\tcase \"default\": {\n\t\t\tregister(\"axios\", \"0.21.1\", () => () => __webpack_require__(/*! ./node_modules/axios/index.js */ \"./node_modules/axios/index.js\"));\n\t\t\tregister(\"kafkajs\", \"1.14.0\", () => () => __webpack_require__(/*! ./node_modules/kafkajs/index.js */ \"./node_modules/kafkajs/index.js\"));\n\t\t\tregister(\"smartystreets-javascript-sdk\", \"1.6.0\", () => () => __webpack_require__(/*! ./node_modules/smartystreets-javascript-sdk/index.js */ \"./node_modules/smartystreets-javascript-sdk/index.js\"));\n\t\t}\n\t\tbreak;\n\t}\n\treturn promises.length && (initPromises[name] = Promise.all(promises).then(() => initPromises[name] = 1));\n};","var parseVersion = (str) => {\n\t// see webpack/lib/util/semver.js for original code\n\tvar p=p=>{return p.split(\".\").map((p=>{return+p==p?+p:p}))},n=/^([^-+]+)?(?:-([^+]+))?(?:\\+(.+))?$/.exec(str),r=n[1]?p(n[1]):[];return n[2]&&(r.length++,r.push.apply(r,p(n[2]))),n[3]&&(r.push([]),r.push.apply(r,p(n[3]))),r;\n}\nvar versionLt = (a, b) => {\n\t// see webpack/lib/util/semver.js for original code\n\ta=parseVersion(a),b=parseVersion(b);for(var r=0;;){if(r>=a.length)return r<b.length&&\"u\"!=(typeof b[r])[0];var e=a[r],n=(typeof e)[0];if(r>=b.length)return\"u\"==n;var t=b[r],f=(typeof t)[0];if(n!=f)return\"o\"==n&&\"n\"==f||(\"s\"==f||\"u\"==n);if(\"o\"!=n&&\"u\"!=n&&e!=t)return e<t;r++}\n}\nvar rangeToString = (range) => {\n\t// see webpack/lib/util/semver.js for original code\n\tif(1===range.length)return\"*\";if(0 in range){var r=\"\",n=range[0];r+=0==n?\">=\":-1==n?\"<\":1==n?\"^\":2==n?\"~\":n>0?\"=\":\"!=\";for(var e=1,a=1;a<range.length;a++){e--,r+=\"u\"==(typeof(t=range[a]))[0]?\"-\":(e>0?\".\":\"\")+(e=2,t)}return r}var g=[];for(a=1;a<range.length;a++){var t=range[a];g.push(0===t?\"not(\"+o()+\")\":1===t?\"(\"+o()+\" || \"+o()+\")\":2===t?g.pop()+\" \"+g.pop():rangeToString(t))}return o();function o(){return g.pop().replace(/^\\((.+)\\)$/,\"$1\")}\n}\nvar satisfy = (range, version) => {\n\t// see webpack/lib/util/semver.js for original code\n\tif(0 in range){version=parseVersion(version);var e=range[0],r=e<0;r&&(e=-e-1);for(var n=0,i=1,a=!0;;i++,n++){var f,s,g=i<range.length?(typeof range[i])[0]:\"\";if(n>=version.length||\"o\"==(s=(typeof(f=version[n]))[0]))return!a||(\"u\"==g?i>e&&!r:\"\"==g!=r);if(\"u\"==s){if(!a||\"u\"!=g)return!1}else if(a)if(g==s)if(i<=e){if(f!=range[i])return!1}else{if(r?f>range[i]:f<range[i])return!1;f!=range[i]&&(a=!1)}else if(\"s\"!=g&&\"n\"!=g){if(r||i<=e)return!1;a=!1,i--}else{if(i<=e||s<g!=r)return!1;a=!1}else\"s\"!=g&&\"n\"!=g&&(a=!1,i--)}}var t=[],o=t.pop.bind(t);for(n=1;n<range.length;n++){var u=range[n];t.push(1==u?o()|o():2==u?o()&o():u?satisfy(u,version):!o())}return!!o();\n}\nvar ensureExistence = (scopeName, key) => {\n\tvar scope = __webpack_require__.S[scopeName];\n\tif(!scope || !__webpack_require__.o(scope, key)) throw new Error(\"Shared module \" + key + \" doesn't exist in shared scope \" + scopeName);\n\treturn scope;\n};\nvar findVersion = (scope, key) => {\n\tvar versions = scope[key];\n\tvar key = Object.keys(versions).reduce((a, b) => {\n\t\treturn !a || versionLt(a, b) ? b : a;\n\t}, 0);\n\treturn key && versions[key]\n};\nvar findSingletonVersionKey = (scope, key) => {\n\tvar versions = scope[key];\n\treturn Object.keys(versions).reduce((a, b) => {\n\t\treturn !a || (!versions[a].loaded && versionLt(a, b)) ? b : a;\n\t}, 0);\n};\nvar getInvalidSingletonVersionMessage = (key, version, requiredVersion) => {\n\treturn \"Unsatisfied version \" + version + \" of shared singleton module \" + key + \" (required \" + rangeToString(requiredVersion) + \")\"\n};\nvar getSingletonVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar version = findSingletonVersionKey(scope, key);\n\tif (!satisfy(requiredVersion, version)) typeof console !== \"undefined\" && console.warn && console.warn(getInvalidSingletonVersionMessage(key, version, requiredVersion));\n\treturn get(scope[key][version]);\n};\nvar getStrictSingletonVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar version = findSingletonVersionKey(scope, key);\n\tif (!satisfy(requiredVersion, version)) throw new Error(getInvalidSingletonVersionMessage(key, version, requiredVersion));\n\treturn get(scope[key][version]);\n};\nvar findValidVersion = (scope, key, requiredVersion) => {\n\tvar versions = scope[key];\n\tvar key = Object.keys(versions).reduce((a, b) => {\n\t\tif (!satisfy(requiredVersion, b)) return a;\n\t\treturn !a || versionLt(a, b) ? b : a;\n\t}, 0);\n\treturn key && versions[key]\n};\nvar getInvalidVersionMessage = (scope, scopeName, key, requiredVersion) => {\n\tvar versions = scope[key];\n\treturn \"No satisfying version (\" + rangeToString(requiredVersion) + \") of shared module \" + key + \" found in shared scope \" + scopeName + \".\\n\" +\n\t\t\"Available versions: \" + Object.keys(versions).map((key) => {\n\t\treturn key + \" from \" + versions[key].from;\n\t}).join(\", \");\n};\nvar getValidVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar entry = findValidVersion(scope, key, requiredVersion);\n\tif(entry) return get(entry);\n\tthrow new Error(getInvalidVersionMessage(scope, scopeName, key, requiredVersion));\n};\nvar warnInvalidVersion = (scope, scopeName, key, requiredVersion) => {\n\ttypeof console !== \"undefined\" && console.warn && console.warn(getInvalidVersionMessage(scope, scopeName, key, requiredVersion));\n};\nvar get = (entry) => {\n\tentry.loaded = 1;\n\treturn entry.get()\n};\nvar init = (fn) => function(scopeName, a, b, c) {\n\tvar promise = __webpack_require__.I(scopeName);\n\tif (promise.then) return promise.then(fn.bind(fn, scopeName, __webpack_require__.S[scopeName], a, b, c));\n\treturn fn(scopeName, __webpack_require__.S[scopeName], a, b, c);\n};\n\nvar load = /*#__PURE__*/ init((scopeName, scope, key) => {\n\tensureExistence(scopeName, key);\n\treturn get(findVersion(scope, key));\n});\nvar loadFallback = /*#__PURE__*/ init((scopeName, scope, key, fallback) => {\n\treturn scope && __webpack_require__.o(scope, key) ? get(findVersion(scope, key)) : fallback();\n});\nvar loadVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn get(findValidVersion(scope, key, version) || warnInvalidVersion(scope, scopeName, key, version) || findVersion(scope, key));\n});\nvar loadSingletonVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getSingletonVersion(scope, scopeName, key, version);\n});\nvar loadStrictVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getValidVersion(scope, scopeName, key, version);\n});\nvar loadStrictSingletonVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getStrictSingletonVersion(scope, scopeName, key, version);\n});\nvar loadVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn get(findValidVersion(scope, key, version) || warnInvalidVersion(scope, scopeName, key, version) || findVersion(scope, key));\n});\nvar loadSingletonVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn getSingletonVersion(scope, scopeName, key, version);\n});\nvar loadStrictVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tvar entry = scope && __webpack_require__.o(scope, key) && findValidVersion(scope, key, version);\n\treturn entry ? get(entry) : fallback();\n});\nvar loadStrictSingletonVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn getStrictSingletonVersion(scope, scopeName, key, version);\n});\nvar installedModules = {};\nvar moduleToHandlerMapping = {\n\t\"webpack/sharing/consume/default/kafkajs/kafkajs\": () => loadStrictVersionCheckFallback(\"default\", \"kafkajs\", [1,1,14,0], () => () => __webpack_require__(/*! kafkajs */ \"./node_modules/kafkajs/index.js\")),\n\t\"webpack/sharing/consume/default/smartystreets-javascript-sdk/smartystreets-javascript-sdk\": () => loadStrictVersionCheckFallback(\"default\", \"smartystreets-javascript-sdk\", [1,1,6,0], () => () => __webpack_require__(/*! smartystreets-javascript-sdk */ \"./node_modules/smartystreets-javascript-sdk/index.js\"))\n};\n// no consumes in initial chunks\nvar chunkMapping = {\n\t\"src_adapters_index_js\": [\n\t\t\"webpack/sharing/consume/default/kafkajs/kafkajs\"\n\t],\n\t\"src_services_index_js\": [\n\t\t\"webpack/sharing/consume/default/smartystreets-javascript-sdk/smartystreets-javascript-sdk\"\n\t]\n};\n__webpack_require__.f.consumes = (chunkId, promises) => {\n\tif(__webpack_require__.o(chunkMapping, chunkId)) {\n\t\tchunkMapping[chunkId].forEach((id) => {\n\t\t\tif(__webpack_require__.o(installedModules, id)) return promises.push(installedModules[id]);\n\t\t\tvar onFactory = (factory) => {\n\t\t\t\tinstalledModules[id] = 0;\n\t\t\t\t__webpack_modules__[id] = (module) => {\n\t\t\t\t\tdelete __webpack_module_cache__[id];\n\t\t\t\t\tmodule.exports = factory();\n\t\t\t\t}\n\t\t\t};\n\t\t\tvar onError = (error) => {\n\t\t\t\tdelete installedModules[id];\n\t\t\t\t__webpack_modules__[id] = (module) => {\n\t\t\t\t\tdelete __webpack_module_cache__[id];\n\t\t\t\t\tthrow error;\n\t\t\t\t}\n\t\t\t};\n\t\t\ttry {\n\t\t\t\tvar promise = moduleToHandlerMapping[id]();\n\t\t\t\tif(promise.then) {\n\t\t\t\t\tpromises.push(installedModules[id] = promise.then(onFactory).catch(onError));\n\t\t\t\t} else onFactory(promise);\n\t\t\t} catch(e) { onError(e); }\n\t\t});\n\t}\n}","\nconst { Octokit } = require(\"@octokit/rest\");\nconst fs = require(\"fs\");\nconst path = require(\"path\");\nconst token = process.env.GITHUB_TOKEN;\n\nconst octokit = new Octokit({ auth: token });\n\nfunction githubFetch(url) {\n  console.info(\"github url\", url);\n  const owner = url.searchParams.get(\"owner\");\n  const repo = url.searchParams.get(\"repo\");\n  const filedir = url.searchParams.get(\"filedir\");\n  const branch = url.searchParams.get(\"branch\");\n  return new Promise(function (resolve, reject) {\n    octokit\n      .request(\n        \"GET /repos/{owner}/{repo}/contents/{filedir}?ref={branch}\",\n        {\n          owner,\n          repo,\n          filedir,\n          branch\n        }\n      )\n      .then(function (rest) {\n        const file = rest.data.find(d => \"/\" + d.name === url.pathname);\n        return file.sha;\n      })\n      .then(function (sha) {\n        console.log(sha);\n        return octokit.request(\n          \"GET /repos/{owner}/{repo}/git/blobs/{sha}\",\n          {\n            owner,\n            repo,\n            sha,\n          }\n        );\n      })\n      .then(function (rest) {\n        resolve(Buffer.from(rest.data.content, \"base64\").toString(\"utf-8\"));\n      });\n  });\n}\n\nfunction httpRequest(url) {\n  if (/github/i.test(url.hostname)) \n    return githubFetch(url)\n  return httpGet(url)\n}\n\nfunction httpGet(params) {\n  return new Promise(function(resolve, reject) {\n    var req = require(params.protocol.slice(0, params.protocol.length - 1)).request(params, function(res) {\n      if (res.statusCode < 200 || res.statusCode >= 300) {\n        return reject(new Error('statusCode=' + res.statusCode));\n      }\n      var body = [];\n      res.on('data', function(chunk) {\n        body.push(chunk);\n      });\n      res.on('end', function() {\n        try {\n          body = Buffer.concat(body).toString();\n        } catch(e) {\n          reject(e);\n        }\n        resolve(body);\n      });\n    });\n    req.on('error', function(err) {\n      reject(err);\n    });\n    req.end();\n  });\n}\n\n// object to store loaded chunks\n// \"0\" means \"already loaded\", Promise means loading\nvar installedChunks = {\n\t\"distributed-cache\": 0\n};\n\nvar installChunk = (chunk) => {\n\tvar moreModules = chunk.modules, chunkIds = chunk.ids, runtime = chunk.runtime;\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\tvar callbacks = [];\n\tfor(var i = 0; i < chunkIds.length; i++) {\n\t\tif(installedChunks[chunkIds[i]])\n\t\t\tcallbacks = callbacks.concat(installedChunks[chunkIds[i]][0]);\n\t\tinstalledChunks[chunkIds[i]] = 0;\n\t}\n\tfor(i = 0; i < callbacks.length; i++)\n\t\tcallbacks[i]();\n};\n\n// ReadFile + VM.run chunk loading for javascript\n__webpack_require__.f.readFileVm = function(chunkId, promises) { console.log(\">>>>>>>>>chunkId\",chunkId);\n\n\tvar installedChunkData = installedChunks[chunkId];\n\tif(installedChunkData !== 0) { // 0 means \"already installed\".\n\t\t// array of [resolve, reject, promise] means \"currently loading\"\n\t\tif(installedChunkData) {\n\t\t\tpromises.push(installedChunkData[2]);\n\t\t} else {\n\t\t\tif(true) { // all chunks have JS\n\t\t\t\t// load the chunk and return promise to it\n\t\t\t\tvar promise = new Promise(function(resolve, reject) {\n\t\t\t\t\tinstalledChunkData = installedChunks[chunkId] = [resolve, reject];\n\t\t\t\t\tvar chunkFileName = \"/\" + __webpack_require__.u(chunkId);\n\t\t\t\t\tvar url = new (require(\"url\").URL)(__webpack_require__.p)\n\t\t\t\t\turl.pathname = chunkFileName;\n\t\t\t\t\thttpRequest(url)\n\t\t\t\t\t\t.then((content) => {\n\t\t\t\t\t\t\tvar chunk = {};\n\t\t\t\t\t\t\trequire('vm').runInThisContext('(function(exports, require, __dirname, __filename) {' + content + '\\n})', chunkFileName)(chunk, require, __dirname, chunkFileName);\n\t\t\t\t\t\t\tinstallChunk(chunk);\n\t\t\t\t\t\t}).catch((err) => {\n\t\t\t\t\t\t\treject(err);\n\t\t\t\t\t\t});\n\t\t\t\t});\n\t\t\t\tpromises.push(installedChunkData[2] = promise);\n\t\t\t} else installedChunks[chunkId] = 0;\n\t\t}\n\t}\n};\n\n// no external install chunk\n\n// no HMR\n\n// no HMR manifest","// module exports must be returned from runtime so entry inlining is disabled\n// startup\n// Load entry module and return exports\nreturn __webpack_require__(\"webpack/container/entry/distributed-cache\");\n"],"sourceRoot":""}