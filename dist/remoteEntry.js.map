{"version":3,"sources":["webpack://aegis-app/./node_modules/@leichtgewicht/ip-codec/index.cjs","webpack://aegis-app/./node_modules/axios-retry/index.js","webpack://aegis-app/./node_modules/axios-retry/lib/index.js","webpack://aegis-app/./node_modules/axios/index.js","webpack://aegis-app/./node_modules/axios/lib/adapters/http.js","webpack://aegis-app/./node_modules/axios/lib/adapters/xhr.js","webpack://aegis-app/./node_modules/axios/lib/axios.js","webpack://aegis-app/./node_modules/axios/lib/cancel/Cancel.js","webpack://aegis-app/./node_modules/axios/lib/cancel/CancelToken.js","webpack://aegis-app/./node_modules/axios/lib/cancel/isCancel.js","webpack://aegis-app/./node_modules/axios/lib/core/Axios.js","webpack://aegis-app/./node_modules/axios/lib/core/InterceptorManager.js","webpack://aegis-app/./node_modules/axios/lib/core/buildFullPath.js","webpack://aegis-app/./node_modules/axios/lib/core/createError.js","webpack://aegis-app/./node_modules/axios/lib/core/dispatchRequest.js","webpack://aegis-app/./node_modules/axios/lib/core/enhanceError.js","webpack://aegis-app/./node_modules/axios/lib/core/mergeConfig.js","webpack://aegis-app/./node_modules/axios/lib/core/settle.js","webpack://aegis-app/./node_modules/axios/lib/core/transformData.js","webpack://aegis-app/./node_modules/axios/lib/defaults.js","webpack://aegis-app/./node_modules/axios/lib/helpers/bind.js","webpack://aegis-app/./node_modules/axios/lib/helpers/buildURL.js","webpack://aegis-app/./node_modules/axios/lib/helpers/combineURLs.js","webpack://aegis-app/./node_modules/axios/lib/helpers/cookies.js","webpack://aegis-app/./node_modules/axios/lib/helpers/isAbsoluteURL.js","webpack://aegis-app/./node_modules/axios/lib/helpers/isAxiosError.js","webpack://aegis-app/./node_modules/axios/lib/helpers/isURLSameOrigin.js","webpack://aegis-app/./node_modules/axios/lib/helpers/normalizeHeaderName.js","webpack://aegis-app/./node_modules/axios/lib/helpers/parseHeaders.js","webpack://aegis-app/./node_modules/axios/lib/helpers/spread.js","webpack://aegis-app/./node_modules/axios/lib/helpers/validator.js","webpack://aegis-app/./node_modules/axios/lib/utils.js","webpack://aegis-app/./node_modules/debug/node_modules/ms/index.js","webpack://aegis-app/./node_modules/debug/src/browser.js","webpack://aegis-app/./node_modules/debug/src/debug.js","webpack://aegis-app/./node_modules/debug/src/index.js","webpack://aegis-app/./node_modules/debug/src/node.js","webpack://aegis-app/./node_modules/dns-packet/classes.js","webpack://aegis-app/./node_modules/dns-packet/index.js","webpack://aegis-app/./node_modules/dns-packet/opcodes.js","webpack://aegis-app/./node_modules/dns-packet/optioncodes.js","webpack://aegis-app/./node_modules/dns-packet/rcodes.js","webpack://aegis-app/./node_modules/dns-packet/types.js","webpack://aegis-app/./node_modules/follow-redirects/debug.js","webpack://aegis-app/./node_modules/follow-redirects/index.js","webpack://aegis-app/./node_modules/is-retry-allowed/index.js","webpack://aegis-app/./node_modules/kafkajs/index.js","webpack://aegis-app/./node_modules/kafkajs/src/admin/index.js","webpack://aegis-app/./node_modules/kafkajs/src/admin/instrumentationEvents.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/index.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/awsIam.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/index.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/oauthBearer.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/plain.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/scram.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/scram256.js","webpack://aegis-app/./node_modules/kafkajs/src/broker/saslAuthenticator/scram512.js","webpack://aegis-app/./node_modules/kafkajs/src/cluster/brokerPool.js","webpack://aegis-app/./node_modules/kafkajs/src/cluster/connectionBuilder.js","webpack://aegis-app/./node_modules/kafkajs/src/cluster/index.js","webpack://aegis-app/./node_modules/kafkajs/src/constants.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/assignerProtocol.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/assigners/index.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/assigners/roundRobinAssigner/index.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/barrier.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/batch.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/consumerGroup.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/filterAbortedMessages.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/index.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/instrumentationEvents.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/offsetManager/index.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/offsetManager/initializeConsumerOffsets.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/offsetManager/isInvalidOffset.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/runner.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/seekOffsets.js","webpack://aegis-app/./node_modules/kafkajs/src/consumer/subscriptionState.js","webpack://aegis-app/./node_modules/kafkajs/src/env.js","webpack://aegis-app/./node_modules/kafkajs/src/errors.js","webpack://aegis-app/./node_modules/kafkajs/src/index.js","webpack://aegis-app/./node_modules/kafkajs/src/instrumentation/emitter.js","webpack://aegis-app/./node_modules/kafkajs/src/instrumentation/event.js","webpack://aegis-app/./node_modules/kafkajs/src/instrumentation/eventType.js","webpack://aegis-app/./node_modules/kafkajs/src/loggers/console.js","webpack://aegis-app/./node_modules/kafkajs/src/loggers/index.js","webpack://aegis-app/./node_modules/kafkajs/src/network/connection.js","webpack://aegis-app/./node_modules/kafkajs/src/network/connectionStatus.js","webpack://aegis-app/./node_modules/kafkajs/src/network/instrumentationEvents.js","webpack://aegis-app/./node_modules/kafkajs/src/network/requestQueue/index.js","webpack://aegis-app/./node_modules/kafkajs/src/network/requestQueue/socketRequest.js","webpack://aegis-app/./node_modules/kafkajs/src/network/socket.js","webpack://aegis-app/./node_modules/kafkajs/src/network/socketFactory.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/createTopicData.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/eosManager/index.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/eosManager/transactionStateMachine.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/eosManager/transactionStates.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/groupMessagesPerPartition.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/index.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/instrumentationEvents.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/messageProducer.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/default/index.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/default/murmur2.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/default/partitioner.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/default/randomBytes.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/defaultJava/index.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/defaultJava/murmur2.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/partitioners/index.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/responseSerializer.js","webpack://aegis-app/./node_modules/kafkajs/src/producer/sendMessages.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/aclOperationTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/aclPermissionTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/aclResourceTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/configResourceTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/configSource.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/coordinatorTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/crc32.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/encoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/error.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/isolationLevel.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/compression/gzip.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/compression/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/v0/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/v0/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/v1/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/message/v1/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/messageSet/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/messageSet/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/crc32C.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/crc32C/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/header/v0/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/record/v0/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/v0/decoder.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/recordBatch/v0/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/alterConfigs/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiKeys.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/apiVersions/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createAcls/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createAcls/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createAcls/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createAcls/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createAcls/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createPartitions/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createPartitions/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createPartitions/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/createTopics/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteAcls/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteGroups/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteRecords/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteTopics/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeAcls/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeAcls/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeAcls/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeAcls/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeAcls/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/describeGroups/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/endTxn/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/endTxn/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/endTxn/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/endTxn/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v10/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v10/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v11/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v11/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v4/decodeMessages.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v5/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v5/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v6/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v6/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v7/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v7/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v8/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v8/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v9/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/fetch/v9/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/heartbeat/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/initProducerId/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/initProducerId/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/initProducerId/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/initProducerId/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v5/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/joinGroup/v5/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listGroups/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/listOffsets/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v5/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v5/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v6/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/metadata/v6/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v4/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v4/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v5/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v5/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v6/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v6/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v7/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/produce/v7/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslHandshake/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v2/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v2/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v3/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/syncGroup/v3/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/resourcePatternTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/resourceTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/awsIam/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/awsIam/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/awsIam/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/oauthBearer/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/plain/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/plain/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/plain/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/request.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/response.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/sasl/scram/index.js","webpack://aegis-app/./node_modules/kafkajs/src/protocol/timestampTypes.js","webpack://aegis-app/./node_modules/kafkajs/src/retry/defaults.js","webpack://aegis-app/./node_modules/kafkajs/src/retry/defaults.test.js","webpack://aegis-app/./node_modules/kafkajs/src/retry/index.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/arrayDiff.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/bufferedAsyncIterator.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/concurrency.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/flatten.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/groupBy.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/lock.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/long.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/sharedPromiseTo.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/shuffle.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/sleep.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/swapObject.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/waitFor.js","webpack://aegis-app/./node_modules/kafkajs/src/utils/websiteUrl.js","webpack://aegis-app/./node_modules/multicast-dns/index.js","webpack://aegis-app/./node_modules/nanoid/index.js","webpack://aegis-app/./node_modules/nanoid/url-alphabet/index.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/index.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/index.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/adapters/http.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/adapters/xhr.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/axios.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/cancel/Cancel.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/cancel/CancelToken.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/cancel/isCancel.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/Axios.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/InterceptorManager.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/buildFullPath.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/createError.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/dispatchRequest.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/enhanceError.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/mergeConfig.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/settle.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/core/transformData.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/defaults/index.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/defaults/transitional.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/env/data.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/bind.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/buildURL.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/combineURLs.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/cookies.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/isAbsoluteURL.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/isAxiosError.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/isURLSameOrigin.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/normalizeHeaderName.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/parseHeaders.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/spread.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/helpers/validator.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/node_modules/axios/lib/utils.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/AgentSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/BaseUrlSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/Batch.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/ClientBuilder.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/CustomHeaderSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/Errors.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/HttpSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/InputData.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/LicenseSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/Request.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/Response.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/SharedCredentials.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/SigningSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/StaticCredentials.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/StatusCodeSender.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_address_autocomplete/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_address_autocomplete/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_address_autocomplete/Suggestion.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_street/Candidate.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_street/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/international_street/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete/Suggestion.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_autocomplete_pro/Suggestion.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_extract/Address.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_extract/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_extract/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_extract/Result.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_reverse_geo/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_reverse_geo/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_reverse_geo/Response.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_reverse_geo/Result.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_street/Candidate.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_street/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_street/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Client.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Lookup.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/us_zipcode/Result.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/util/apiToSDKKeyMap.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/util/buildClients.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/util/buildInputData.js","webpack://aegis-app/./node_modules/smartystreets-javascript-sdk/src/util/sendBatch.js","webpack://aegis-app/./node_modules/thunky/index.js","webpack://aegis-app/webpack/container-entry","webpack://aegis-app/external \"assert\"","webpack://aegis-app/external \"buffer\"","webpack://aegis-app/external \"crypto\"","webpack://aegis-app/external \"dgram\"","webpack://aegis-app/external \"events\"","webpack://aegis-app/external \"fs\"","webpack://aegis-app/external \"http\"","webpack://aegis-app/external \"https\"","webpack://aegis-app/external \"net\"","webpack://aegis-app/external \"os\"","webpack://aegis-app/external \"path\"","webpack://aegis-app/external \"stream\"","webpack://aegis-app/external \"tls\"","webpack://aegis-app/external \"tty\"","webpack://aegis-app/external \"url\"","webpack://aegis-app/external \"util\"","webpack://aegis-app/external \"zlib\"","webpack://aegis-app/webpack/bootstrap","webpack://aegis-app/webpack/runtime/compat get default export","webpack://aegis-app/webpack/runtime/define property getters","webpack://aegis-app/webpack/runtime/ensure chunk","webpack://aegis-app/webpack/runtime/get javascript chunk filename","webpack://aegis-app/webpack/runtime/hasOwnProperty shorthand","webpack://aegis-app/webpack/runtime/make namespace object","webpack://aegis-app/webpack/runtime/publicPath","webpack://aegis-app/webpack/runtime/sharing","webpack://aegis-app/webpack/runtime/consumes","webpack://aegis-app/webpack/runtime/readFile chunk loading","webpack://aegis-app/webpack/startup"],"names":[],"mappings":";;;;;;;;;;;;AAAA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,IAAI,IAAI,IAAI,GAAG,IAAI;AAC3C;AACA,+BAA+B,IAAI,IAAI,EAAE,IAAI,IAAI,EAAE,EAAE,aAAa,IAAI,EAAE,IAAI,EAAE,IAAI;AAClF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,SAAS;AAC9B;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,KAAK;;AAEL;AACA;AACA,gBAAgB,eAAe,GAAG,eAAe,GAAG,eAAe,GAAG,aAAa;AACnF;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qDAAqD;AACrD;AACA;AACA;AACA;;AAEA,qBAAqB,eAAe;AACpC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,oBAAoB;AACpB,WAAW;AACX,oBAAoB;AACpB,WAAW;AACX,oBAAoB;;AAEpB;AACA,WAAW;;;AAGX;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA,mDAAmD,eAAe;AAClE;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;;AAEA,qBAAqB,YAAY;AACjC;AACA;AACA;;AAEA;AACA;;AAEA,uEAAuE,IAAI;AAC3E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,uCAAuC,GAAG;AAC1C;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,mDAAmD,QAAQ,aAAa,QAAQ;AAChF;AACA;AACA,CAAC,IAAI;AACL,IAAI,IAA0C,EAAE,iCAAO,EAAE,mCAAE,YAAY,gBAAgB,EAAE;AAAA,kGAAC;AAC1F,KAAK,EAAsF;;;;;;;;;;;;;;ACzP3F,0GAA+C,C;;;;;;;;;;;;;;;;;;;;;;ACAlC;;AAEb,8CAA6C;AAC7C;AACA,CAAC,EAAC;;AAEF,oGAAoG,mBAAmB,EAAE,mBAAmB,8HAA8H;;AAE1Q,sBAAsB;AACtB,wBAAwB;AACxB,0BAA0B;AAC1B,gCAAgC;AAChC,yCAAyC;AACzC,wBAAwB;AACxB,eAAe;;AAEf,sBAAsB,mBAAO,CAAC,kEAAkB;;AAEhD;;AAEA,sCAAsC,uCAAuC,gBAAgB;;AAE7F;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA,uCAAuC;AACvC;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;AACA;;AAEA;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;;AAEA;AACA,YAAY,OAAO;AACnB,YAAY,OAAO;AACnB;AACA;AACA;;AAEA;AACA,8CAA8C;AAC9C;AACA;;AAEA;AACA;AACA,YAAY,mBAAmB;AAC/B,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,mBAAmB;AAC/B,YAAY,iBAAiB;AAC7B,YAAY;AACZ;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA,YAAY,MAAM;AAClB,YAAY,mBAAmB;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,YAAY,OAAO;AACnB,YAAY,SAAS;AACrB,YAAY,OAAO;AACnB,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB,aAAa;AACnC;AACA;AACA;AACA,mBAAmB;AACnB,MAAM;AACN;AACA;AACA,sBAAsB,0CAA0C;AAChE;AACA;AACA,sBAAsB;AACtB;AACA,KAAK;AACL;AACA;AACA,gCAAgC,gCAAgC;AAChE,uBAAuB,aAAa;AACpC;AACA;AACA;AACA,mBAAmB;AACnB,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,sBAAsB;AACtB;AACA,MAAM;AACN;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA,WAAW,SAAS;AACpB;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iC;;;;;;;;;;;;;AClRA,4FAAuC,C;;;;;;;;;;;;;;ACA1B;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,aAAa,mBAAO,CAAC,iEAAkB;AACvC,oBAAoB,mBAAO,CAAC,6EAAuB;AACnD,eAAe,mBAAO,CAAC,2EAAuB;AAC9C,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,iBAAiB,4FAAgC;AACjD,kBAAkB,6FAAiC;AACnD,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;AACzB,UAAU,mBAAO,CAAC,+DAAsB;AACxC,kBAAkB,mBAAO,CAAC,yEAAqB;AAC/C,mBAAmB,mBAAO,CAAC,2EAAsB;;AAEjD;;AAEA;AACA;AACA,WAAW,uBAAuB;AAClC,WAAW,iBAAiB;AAC5B,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mDAAmD;AAClE;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;AC1Ua;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,aAAa,mBAAO,CAAC,iEAAkB;AACvC,cAAc,mBAAO,CAAC,yEAAsB;AAC5C,eAAe,mBAAO,CAAC,2EAAuB;AAC9C,oBAAoB,mBAAO,CAAC,6EAAuB;AACnD,mBAAmB,mBAAO,CAAC,mFAA2B;AACtD,sBAAsB,mBAAO,CAAC,yFAA8B;AAC5D,kBAAkB,mBAAO,CAAC,yEAAqB;;AAE/C;AACA;AACA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;AC5La;;AAEb,YAAY,mBAAO,CAAC,kDAAS;AAC7B,WAAW,mBAAO,CAAC,gEAAgB;AACnC,YAAY,mBAAO,CAAC,4DAAc;AAClC,kBAAkB,mBAAO,CAAC,wEAAoB;AAC9C,eAAe,mBAAO,CAAC,wDAAY;;AAEnC;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,kEAAiB;AACxC,oBAAoB,mBAAO,CAAC,4EAAsB;AAClD,iBAAiB,mBAAO,CAAC,sEAAmB;;AAE5C;AACA;AACA;AACA;AACA,eAAe,mBAAO,CAAC,oEAAkB;;AAEzC;AACA,qBAAqB,mBAAO,CAAC,gFAAwB;;AAErD;;AAEA;AACA,sBAAsB;;;;;;;;;;;;;;;ACvDT;;AAEb;AACA;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;;;;;;;;;;;;;;AClBa;;AAEb,aAAa,mBAAO,CAAC,2DAAU;;AAE/B;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;ACxDa;;AAEb;AACA;AACA;;;;;;;;;;;;;;;ACJa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,eAAe,mBAAO,CAAC,yEAAqB;AAC5C,yBAAyB,mBAAO,CAAC,iFAAsB;AACvD,sBAAsB,mBAAO,CAAC,2EAAmB;AACjD,kBAAkB,mBAAO,CAAC,mEAAe;AACzC,gBAAgB,mBAAO,CAAC,2EAAsB;;AAE9C;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA,yBAAyB;AACzB,KAAK;AACL;AACA,CAAC;;AAED;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA,CAAC;;AAED;;;;;;;;;;;;;;;ACnJa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;;;;;;;;;;;;;;ACrDa;;AAEb,oBAAoB,mBAAO,CAAC,mFAA0B;AACtD,kBAAkB,mBAAO,CAAC,+EAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACnBa;;AAEb,mBAAmB,mBAAO,CAAC,qEAAgB;;AAE3C;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACjBa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,oBAAoB,mBAAO,CAAC,uEAAiB;AAC7C,eAAe,mBAAO,CAAC,uEAAoB;AAC3C,eAAe,mBAAO,CAAC,yDAAa;;AAEpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,+BAA+B;AAC/B,uCAAuC;AACvC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACjFa;;AAEb;AACA;AACA;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACzCa;;AAEb,YAAY,mBAAO,CAAC,mDAAU;;AAE9B;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,2BAA2B;AAC3B,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;;;;;;;;;;;;;;;ACtFa;;AAEb,kBAAkB,mBAAO,CAAC,mEAAe;;AAEzC;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACxBa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;AAChC,eAAe,mBAAO,CAAC,2DAAe;;AAEtC;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,MAAM;AACjB,WAAW,eAAe;AAC1B,aAAa,EAAE;AACf;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACrBa;;AAEb,YAAY,mBAAO,CAAC,kDAAS;AAC7B,0BAA0B,mBAAO,CAAC,8FAA+B;AACjE,mBAAmB,mBAAO,CAAC,0EAAqB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc,mBAAO,CAAC,gEAAgB;AACtC,GAAG;AACH;AACA,cAAc,mBAAO,CAAC,kEAAiB;AACvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wEAAwE;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA,CAAC;;AAED;;;;;;;;;;;;;;;ACrIa;;AAEb;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;;;;;;;;;;;;;ACrEa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0CAA0C;AAC1C,SAAS;;AAET;AACA,4DAA4D,wBAAwB;AACpF;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,kCAAkC;AAClC,+BAA+B,aAAa,EAAE;AAC9C;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb;AACA;AACA;AACA,WAAW,EAAE;AACb,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB;AAChB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACnEa;;AAEb,YAAY,mBAAO,CAAC,mDAAU;;AAE9B;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACXa;;AAEb,YAAY,mBAAO,CAAC,qDAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,eAAe;;AAEhC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA,WAAW,SAAS;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC1Ba;;AAEb,UAAU,mBAAO,CAAC,+DAAsB;;AAExC;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB,aAAa;AACb;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,kBAAkB;AAC7B,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,aAAa;AACb;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,SAAS;AACpB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACxGa;;AAEb,WAAW,mBAAO,CAAC,gEAAgB;;AAEnC;;AAEA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,OAAO;AAC1C;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,SAAS,GAAG,SAAS;AAC5C,2BAA2B;AAC3B;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,4BAA4B;AAC5B,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;;AAEA,uCAAuC,OAAO;AAC9C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC5VA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;ACvJA;AACA;AACA;AACA;AACA;;AAEA,UAAU,wFAAmC;AAC7C,WAAW;AACX,kBAAkB;AAClB,YAAY;AACZ,YAAY;AACZ,iBAAiB;AACjB,eAAe;AACf;AACA;AACA;;AAEA;AACA;AACA;;AAEA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA,GAAG;AACH;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,MAAM,qBAAqB;AAC3B;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;;;;;;;;ACvLA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd,eAAe;AACf,cAAc;AACd,eAAe;AACf,iGAAgC;;AAEhC;AACA;AACA;;AAEA,aAAa;AACb,aAAa;;AAEb;AACA;AACA;AACA;AACA;;AAEA,kBAAkB;;AAElB;AACA;AACA;;AAEA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;;AAEA;AACA;AACA,cAAc;AACd;;AAEA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;;AAEA,EAAE,aAAa;AACf,EAAE,aAAa;;AAEf;AACA;;AAEA,iBAAiB,SAAS;AAC1B,4BAA4B;AAC5B;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY;AACZ;AACA;;AAEA;AACA;AACA,yCAAyC,SAAS;AAClD;AACA;AACA;AACA;AACA,yCAAyC,SAAS;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,MAAM;AACjB,YAAY;AACZ;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzMA;AACA;AACA;AACA;;AAEA;AACA,EAAE,+FAAwC;AAC1C,CAAC;AACD,EAAE,yFAAqC;AACvC;;;;;;;;;;;;;;;;ACTA;AACA;AACA;;AAEA,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;;AAEzB;AACA;AACA;AACA;AACA;;AAEA,UAAU,wFAAmC;AAC7C,YAAY;AACZ,WAAW;AACX,kBAAkB;AAClB,YAAY;AACZ,YAAY;AACZ,iBAAiB;;AAEjB;AACA;AACA;;AAEA,cAAc;;AAEd;AACA;AACA;AACA;AACA;;AAEA,mBAAmB;AACnB;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,2CAA2C,yBAAyB;;AAEpE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC,IAAI;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,6BAA6B;AAC7B;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,sCAAsC;;AAEtC;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY,OAAO;AACnB;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,cAAI;AAC3B,2CAA2C,mBAAmB;AAC9D;AACA;;AAEA;AACA;AACA,gBAAgB,mBAAO,CAAC,gBAAK;AAC7B;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,iBAAiB,iBAAiB;AAClC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;;;ACvPY;;AAEZ,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACtBY;;AAEZ,eAAe,kDAAwB;AACvC,cAAc,mBAAO,CAAC,mDAAS;AAC/B,eAAe,mBAAO,CAAC,qDAAU;AACjC,gBAAgB,mBAAO,CAAC,uDAAW;AACnC,gBAAgB,mBAAO,CAAC,uDAAW;AACnC,oBAAoB,mBAAO,CAAC,+DAAe;AAC3C,WAAW,mBAAO,CAAC,iFAAyB;;AAE5C;AACA;AACA;AACA;AACA;AACA;;AAEA,aAAa,YAAY;;AAEzB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,iBAAiB,eAAe;;AAEhC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,YAAY,UAAU;;AAEtB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,WAAW;;AAExB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,WAAW;;AAExB;AACA;AACA,iBAAiB,iBAAiB;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA,cAAc,YAAY;;AAE1B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,eAAe,aAAa;;AAE5B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,WAAW;AACxB,eAAe,aAAa;AAC5B,eAAe,aAAa;;AAE5B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,WAAW;;AAExB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,WAAW;;AAExB;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,YAAY,UAAU;;AAEtB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,SAAS;;AAEpB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,cAAc,YAAY;;AAE1B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,gBAAgB,cAAc;;AAE9B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iDAAiD,YAAY;AAC7D;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,SAAS;AAC9B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C,YAAY;AACvD;;AAEA,aAAa,WAAW;;AAExB;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,gBAAgB,cAAc;;AAE9B;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,eAAe,aAAa;;AAE5B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,YAAY,UAAU;;AAEtB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,iBAAiB,qBAAqB;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA,aAAa,0BAA0B;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,kBAAkB;AACrC;AACA,qBAAqB,OAAO;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,iBAAiB,qBAAqB;AACtC;AACA;AACA;;AAEA;AACA,aAAa,oBAAoB;AACjC;AACA;AACA;AACA;;AAEA;AACA;;AAEA,cAAc,YAAY;;AAE1B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,eAAe,aAAa;;AAE5B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,YAAY,UAAU;;AAEtB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,eAAe,aAAa;;AAE5B;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,aAAa,cAAc;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,eAAe,cAAc;;AAE7B;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,gBAAgB;;AAEjC;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,4BAA4B;AAC5B,0BAA0B;AAC1B,yBAAyB;AACzB,2BAA2B;AAC3B,sBAAsB;AACtB,yBAAyB;AACzB,iBAAiB;;AAEjB,cAAc;AACd;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,EAAE,oBAAoB;;AAEtB;AACA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB;;AAEpB,cAAc;AACd;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,EAAE,oBAAoB;;AAEtB;AACA;;AAEA,oBAAoB;;AAEpB,sBAAsB;AACtB;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB;AACpB;AACA;AACA;AACA;AACA,EAAE,0BAA0B;AAC5B;AACA;;AAEA,0BAA0B;;AAE1B,oBAAoB;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,EAAE,0BAA0B;AAC5B;AACA;;AAEA,0BAA0B;;AAE1B;AACA;AACA,iBAAiB,iBAAiB;AAClC;AACA;;AAEA;AACA,iBAAiB,iBAAiB;AAClC;AACA;AACA;AACA;AACA;;AAEA;AACA,iBAAiB,iBAAiB;AAClC;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;AC5lDY;;AAEZ;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;ACjDY;;AAEZ,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,KAAK;AACxB;;AAEA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;AC1DY;;AAEZ;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;ACjDY;;AAEZ,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtGA;;AAEA;AACA;AACA;AACA;AACA,cAAc,mBAAO,CAAC,gDAAO;AAC7B;AACA,mBAAmB;AACnB;AACA,2BAA2B;AAC3B;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,UAAU,mBAAO,CAAC,gBAAK;AACvB;AACA,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,eAAe,oDAA0B;AACzC,aAAa,mBAAO,CAAC,sBAAQ;AAC7B,YAAY,mBAAO,CAAC,yDAAS;;AAE7B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mCAAmC,iCAAiC;AACpE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,sBAAsB,uCAAuC,EAAE;AAC/D,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,uCAAuC;AACvC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,+CAA+C,oBAAoB;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,gBAAgB,uEAAuE;AACvF,YAAY,mEAAmE;AAC/E,KAAK;AACL,GAAG;AACH;AACA;;AAEA;AACA,iBAAiB;;AAEjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,2BAA2B;AAClD,mBAAmB;;;;;;;;;;;;;;;ACrlBN;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC7DA,cAAc,mBAAO,CAAC,kDAAO;AAC7B,2BAA2B,mBAAO,CAAC,wFAA0B;AAC7D,yBAAyB,mBAAO,CAAC,gGAAiC;AAClE,qBAAqB,mBAAO,CAAC,8FAA6B;AAC1D,oBAAoB,mBAAO,CAAC,4GAAoC;AAChE,sBAAsB,mBAAO,CAAC,0FAA8B;AAC5D,4BAA4B,mBAAO,CAAC,sGAAoC;AACxE,qBAAqB,mBAAO,CAAC,wFAA6B;AAC1D,yBAAyB,mBAAO,CAAC,gGAAiC;AAClE,0BAA0B,mBAAO,CAAC,kGAAkC;AACpE,2BAA2B,mBAAO,CAAC,oGAAmC;AACtE,6BAA6B,mBAAO,CAAC,wGAAqC;AAC1E,eAAe,mBAAO,CAAC,0DAAc;AACrC,OAAO,SAAS,GAAG,mBAAO,CAAC,kEAAe;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACrCA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,uBAAuB,mBAAO,CAAC,iEAAa;AAC5C,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,+CAA+C,GAAG,mBAAO,CAAC,0FAAyB;AAC1F,OAAO,SAAS,GAAG,mBAAO,CAAC,+DAAY;AACvC;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;AACvB,OAAO,gBAAgB,GAAG,mBAAO,CAAC,uEAAmB;AACrD,8BAA8B,mBAAO,CAAC,mGAAiC;AACvE,2BAA2B,mBAAO,CAAC,6FAA8B;AACjE,4BAA4B,mBAAO,CAAC,+FAA+B;AACnE,6BAA6B,mBAAO,CAAC,iGAAgC;AACrE,+BAA+B,mBAAO,CAAC,qGAAkC;AACzE,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;;AAEjE,OAAO,sBAAsB;;AAE7B;;AAEA,OAAO,wBAAwB;AAC/B;AACA;AACA,8BAA8B,IAAI;AAClC;;AAEA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,cAAc;AACzB;AACA;AACA;AACA;AACA,WAAW,sBAAsB,yBAAyB,eAAe,WAAW,EAAE;AACtF;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,6BAA6B;AACxC,WAAW,4BAA4B;AACvC,WAAW,mCAAmC;AAC9C,WAAW,8BAA8B;AACzC;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;;AAEA;AACA,cAAc;AACd;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,uDAAuD;AACtF;AACA,iEAAiE,OAAO;AACxE;;AAEA,wBAAwB,QAAQ;AAChC;AACA;AACA;AACA;;AAEA,4CAA4C,QAAQ;AACpD;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,gCAAgC;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA,aAAa,MAAM;AACnB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,mCAAmC,yCAAyC;AAC5E;AACA,2EAA2E,gBAAgB;AAC3F;AACA;AACA;AACA;;AAEA,iCAAiC,QAAQ;AACzC;AACA;AACA;AACA;;AAEA,qDAAqD,QAAQ;AAC7D;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,uCAAuC,yCAAyC;AAChF,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB,cAAc;AACd;AACA,+BAA+B,kBAAkB;AACjD;AACA,iEAAiE,OAAO;AACxE;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,mCAAmC,kBAAkB;;AAErD;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA,4CAA4C,2BAA2B;AACvE,WAAW;AACX;;AAEA,eAAe,6BAA6B;AAC5C,eAAe,4BAA4B;AAC3C,oCAAoC,oBAAoB;AACxD;AACA;AACA;AACA,oCAAoC,0BAA0B;AAC9D;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;;AAEA;AACA;AACA,0DAA0D,MAAM;AAChE;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,2BAA2B;;AAE1E;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,6BAA6B;;AAE5C;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,eAAe,4BAA4B;;AAE3C,mCAAmC,oBAAoB;AACvD;AACA;AACA;AACA;AACA,sCAAsC,2BAA2B;AACjE;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,iDAAiD;AAChF;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,4DAA4D,UAAU;AACtE;AACA;AACA;AACA,gEAAgE,YAAY;AAC5E,gBAAgB;AAChB,OAAO;AACP;AACA,SAAS,6BAA6B;AACtC;AACA;AACA,KAAK;;AAEL;AACA;AACA,oCAAoC,oBAAoB;AACxD;AACA,0DAA0D,8BAA8B;AACxF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX,4BAA4B,qDAAqD;;AAEjF;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA,yCAAyC,oBAAoB;AAC7D,kDAAkD,8BAA8B;AAChF;AACA;AACA;AACA,OAAO;;AAEP,cAAc;AACd,KAAK;;AAEL;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,+BAA+B,mCAAmC;AAClE;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,qCAAqC,0BAA0B;AAC/D,KAAK;;AAEL,uBAAuB,+CAA+C;AACtE;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,6BAA6B,6BAA6B;AAC1D;AACA,4DAA4D,QAAQ;AACpE;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL,8BAA8B,6BAA6B;AAC3D;;AAEA;AACA;AACA,6EAA6E,kBAAkB;AAC/F;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,uBAAuB,QAAQ;;AAE/B;AACA,uBAAuB,qBAAqB;AAC5C;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA,mCAAmC,2BAA2B;AAC9D,+BAA+B,qBAAqB;AACpD;AACA,oCAAoC,yBAAyB;AAC7D;AACA,KAAK;;AAEL;AACA,aAAa,2BAA2B;AACxC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,mBAAmB;AACnC,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B;AACA,kCAAkC,6BAA6B;AAC/D;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA,wCAAwC,YAAY,IAAI,+BAA+B;AACvF;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,oBAAoB,YAAY;AAChC;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,qDAAqD,0CAA0C;AAC/F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,sBAAsB;AACnC,aAAa,QAAQ;AACrB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,mBAAmB;AACnC,gBAAgB,OAAO;AACvB,gBAAgB,2BAA2B;AAC3C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,+BAA+B,0BAA0B;AACzD;AACA,oEAAoE,UAAU;AAC9E;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA;AACA,iCAAiC,iBAAiB,IAAI,4BAA4B;AAClF;AACA;;AAEA;;AAEA;AACA,aAAa,gBAAgB;AAC7B;AACA,0CAA0C,cAAc,IAAI,+BAA+B;AAC3F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0CAA0C,mCAAmC;AAC7E;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,oBAAoB,YAAY;AAChC;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA;AACA,WAAW,SAAS;;AAEpB;AACA;AACA;AACA;AACA,gEAAgE,MAAM;AACtE;;AAEA;AACA;AACA,WAAW;AACX,sDAAsD,MAAM,IAAI,UAAU;AAC1E;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,yBAAyB;AACzC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,cAAc;AAC9B,gBAAgB,cAAc;AAC9B;AACA,qCAAqC,cAAc,KAAK;AACxD;AACA;AACA;AACA,8DAA8D,MAAM;AACpE;AACA,OAAO;AACP;;AAEA,6CAA6C,SAAS;;AAEtD;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,cAAc;AAC9B,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,WAAW,0CAA0C,2BAA2B,aAAa;AAC7F,gCAAgC,qBAAqB;AACrD;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,iBAAiB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA;AACA,+CAA+C,SAAS;AACxD;AACA;AACA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA,eAAe,OAAO;AACtB,gBAAgB,wBAAwB;AACxC;AACA,cAAc;AACd;AACA;AACA;AACA;AACA,gEAAgE,UAAU;AAC1E;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,kDAAkD,uBAAuB;AACzE;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,8CAA8C;AAC9C;AACA;AACA,OAAO,IAAI;AACX;;AAEA;AACA,sCAAsC,wBAAwB;AAC9D;AACA,eAAe,SAAS,mDAAmD,WAAW;AACtF;AACA,OAAO;AACP;;AAEA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA,eAAe,MAAM;AACrB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,mEAAmE,SAAS;AAC5E;;AAEA;;AAEA;AACA,kEAAkE,+BAA+B;AACjG;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB,UAAU;AAC9B,0BAA0B,4BAA4B;AACtD,sBAAsB;AACtB,aAAa;AACb;AACA,mBAAmB,YAAY;;AAE/B,sCAAsC,UAAU;;AAEhD;;AAEA,oCAAoC,UAAU;;AAE9C;AACA,OAAO;AACP;AACA,kDAAkD,0CAA0C;AAC5F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,qCAAqC,oBAAoB;AACzD;AACA,2DAA2D,MAAM;AACjE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,yBAAyB,oBAAoB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,gDAAgD,0CAA0C;AAC1F;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,qBAAqB,oBAAoB;AACzC,qDAAqD,SAAS;AAC9D,wCAAwC,WAAW,oBAAoB,GAAG;AAC1E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,uCAAuC,2BAA2B;AAClE;AACA;AACA;AACA;AACA,mBAAmB;AACnB,iBAAiB;AACjB,eAAe;AACf;AACA;AACA;AACA,aAAa;AACb;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,OAAO;AACP;AACA;AACA;AACA,cAAc,QAAQ;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,gBAAgB;AAC7B,cAAc;AACd;AACA,eAAe,OAAO;AACtB;AACA,6BAA6B,MAAM;AACnC;AACA,8DAA8D,IAAI;AAClE;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;;AAEA;AACA,mBAAmB,OAAO;AAC1B;AACA;;AAEA;AACA,mBAAmB,eAAe;AAClC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,kCAAkC,sBAAsB,IAAI,4BAA4B;AACxF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yCAAyC,gCAAgC,IAAI;AAC7E;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,2BAA2B,IAAI,4BAA4B;AAC9F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC,yBAAyB,IAAI,4BAA4B;AAC1F;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,iCAAiC,MAAM;;AAEvC;AACA,OAAO;AACP;AACA,+CAA+C,0CAA0C;AACzF;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,iBAAiB;AAC9B,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,kBAAkB;AAC/B,aAAa,mBAAmB;AAChC,cAAc;AACd;AACA,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,mEAAmE,UAAU;AAC7E;;AAEA;AACA;AACA;AACA;AACA,gDAAgD,oBAAoB;AACpE;AACA;;AAEA;AACA;AACA;AACA,oEAAoE,eAAe;AACnF;;AAEA;AACA;AACA;AACA,kEAAkE,aAAa;AAC/E;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,YAAY;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,gBAAgB;AAChB,OAAO;AACP;AACA,iDAAiD,0CAA0C;AAC3F;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,iBAAiB;AAC9B,cAAc;AACd;AACA,eAAe,OAAO;AACtB;AACA,6BAA6B,UAAU;AACvC;AACA,qEAAqE,QAAQ;AAC7E;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU,YAAY;AACtB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,OAAO;AAC9B;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU,eAAe;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,kCAAkC,sBAAsB,IAAI,4BAA4B;AACxF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yCAAyC,gCAAgC,IAAI;AAC7E;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,2BAA2B,IAAI,4BAA4B;AAC9F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC,yBAAyB,IAAI,4BAA4B;AAC1F;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,kBAAkB,4BAA4B,UAAU;AACvE,gBAAgB;AAChB,OAAO;AACP;AACA,+CAA+C,0CAA0C;AACzF;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,kCAAkC;AAC/C;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACr+CA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,aAAa,mBAAO,CAAC,+DAAe;AACpC,aAAa,mBAAO,CAAC,+DAAe;AACpC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,yGAAiC;AACxE,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAsB;AAC3D,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,gBAAgB,mBAAO,CAAC,6FAA8B;AACtD,0BAA0B,mBAAO,CAAC,yFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6FAA8B;AACjF,wBAAwB,mBAAO,CAAC,qFAA0B;;AAE1D;AACA;AACA;AACA;AACA;;AAEA,WAAW,sCAAsC;AACjD;AACA;AACA;;AAEA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,gCAAgC;AAC7C,aAAa,6BAA6B;AAC1C,aAAa,OAAO;AACpB,aAAa,kCAAkC;AAC/C;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB;AACA;AACA,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,4BAA4B,qBAAqB,GAAG,qBAAqB;;AAEzE;AACA;AACA,wCAAwC,mBAAmB;AAC3D,KAAK;;AAEL;;AAEA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,kBAAkB;AAC7B;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,2DAA2D,4BAA4B;AACvF;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,YAAY;AACZ,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,8EAA8E;AAC9F;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE,yCAAyC,uBAAuB;AAChE;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA,yCAAyC,uBAAuB;AAChE;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA,qCAAqC;AACrC;AACA,iCAAiC;AACjC;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uCAAuC;AACpD,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA,sEAAsE,oBAAoB;AAC1F;AACA,8BAA8B,mBAAmB;AACjD,OAAO;AACP;AACA,KAAK;;AAEL;;AAEA;AACA;AACA,yBAAyB,mBAAmB;AAC5C;;AAEA;AACA;AACA,SAAS;AACT,gCAAgC,iCAAiC;AACjE;;AAEA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,uCAAuC;AAC1D;AACA,uDAAuD,uCAAuC;AAC9F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,8BAA8B,2BAA2B;AACzD;AACA;AACA,6DAA6D,2BAA2B;AACxF;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,sCAAsC,mCAAmC,2BAA2B,GAAG;AACvG,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,oBAAoB;AACxC;AACA,wDAAwD,oBAAoB;AAC5E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uBAAuB;AACpC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA,eAAe;AACf;AACA,qBAAqB,oCAAoC;AACzD;AACA;AACA,mBAAmB,oCAAoC;AACvD;;AAEA;AACA;AACA;AACA,sDAAsD,4BAA4B;AAClF,0BAA0B,0CAA0C;AACpE,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,sBAAsB,8DAA8D;AACpF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA,6BAA6B;AAC7B;AACA;AACA;AACA,eAAe;AACf;AACA,qBAAqB,kBAAkB;AACvC;AACA,yDAAyD,kBAAkB;AAC3E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB,eAAe;AACf;AACA,wBAAwB,WAAW;AACnC;AACA,4DAA4D,WAAW;AACvE;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,sBAAsB,+CAA+C;AACrE;AACA,0DAA0D,gCAAgC;AAC1F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,QAAQ;AACrB;AACA,aAAa,OAAO;AACpB;AACA,eAAe;AACf;AACA,0BAA0B,wDAAwD;AAClF;AACA;AACA,wBAAwB,yCAAyC;AACjE;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,OAAO;AACpB;AACA;AACA,eAAe;AACf;AACA,sBAAsB,yBAAyB;AAC/C;AACA,0DAA0D,kBAAkB;AAC5E;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,4CAA4C;AACzD;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,yBAAyB,qCAAqC;AAC9D;AACA,6DAA6D,6BAA6B;AAC1F;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC;AACtC,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,sBAAsB,kCAAkC;AACxD;AACA,0DAA0D,0BAA0B;AACpF;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,wBAAwB,sCAAsC;AAC9D;AACA,4DAA4D,sCAAsC;AAClG;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,4BAA4B,qDAAqD;AACjF;AACA;AACA;AACA;AACA;AACA,0BAA0B,qDAAqD;AAC/E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,yBAAyB,sDAAsD;AAC/E;AACA;AACA,uBAAuB,sDAAsD;AAC7E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,oBAAoB;AACjC,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,oBAAoB;AACjC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,6BAA6B;AAC7C;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,eAAe;AACf;AACA,yBAAyB,8DAA8D;AACvF;AACA;AACA,uBAAuB,8DAA8D;AACrF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe;AACf;AACA,gBAAgB,gEAAgE;AAChF;AACA;AACA,cAAc,gEAAgE;AAC9E;AACA;;AAEA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS;AACtB;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC;AACA;AACA;AACA;AACA,qCAAqC,yBAAyB;AAC9D,qCAAqC,yBAAyB;AAC9D;AACA;AACA;AACA,eAAe,eAAe;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC,iDAAiD;AACvF,sCAAsC,iDAAiD;AACvF;AACA,kCAAkC;AAClC;AACA;AACA;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA,uBAAuB,SAAS;AAChC;AACA,2DAA2D,SAAS;AACpE;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,oBAAoB,MAAM;AAC1B;AACA,wDAAwD,iBAAiB;AACzE;;AAEA;AACA;AACA,aAAa,+BAA+B;AAC5C,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C,eAAe;AACf;AACA,oBAAoB,UAAU;AAC9B;AACA,wDAAwD,UAAU;AAClE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC37BA,eAAe,mBAAO,CAAC,4FAA4B;AACnD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,2DAA2D,SAAS;AACpE,mCAAmC,oBAAoB;AACvD,mEAAmE,SAAS;AAC5E,KAAK;AACL;AACA,+CAA+C,UAAU;AACzD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,OAAO,mBAAmB,GAAG,mBAAO,CAAC,sFAAyB;AAC9D,gBAAgB,mBAAO,CAAC,gGAAiC;AACzD,2BAA2B,mBAAO,CAAC,6EAAS;AAC5C,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,8BAA8B,mBAAO,CAAC,mFAAY;AAClD,4BAA4B,mBAAO,CAAC,+EAAU;AAC9C,iCAAiC,mBAAO,CAAC,yFAAe;AACxD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qEAAqE,YAAY;AACjF;AACA;AACA,gBAAgB,UAAU;AAC1B;AACA;;AAEA,qCAAqC,wCAAwC;AAC7E;AACA,eAAe,2BAA2B;AAC1C;AACA,uCAAuC,8BAA8B;AACrE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,eAAe,+BAA+B;AAC9C;AACA;AACA;;AAEA,2CAA2C,wCAAwC;AACnF;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,mBAAO,CAAC,sGAAiC;AAC7D,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;;AAEA,WAAW,sBAAsB;;AAEjC;;AAEA;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,+DAA+D,SAAS;AACxE,mCAAmC,oBAAoB;AACvD,uEAAuE,SAAS;AAChF,KAAK;AACL;AACA,mDAAmD,UAAU;AAC7D;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,iCAAiC,GAAG,mBAAO,CAAC,0DAAc;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,yDAAyD,SAAS;AAClE,mCAAmC,oBAAoB;AACvD,iEAAiE,SAAS;AAC1E,KAAK;AACL;AACA,6CAA6C,UAAU;AACvD;AACA,wCAAwC,SAAS;AACjD;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA,eAAe,mBAAO,CAAC,sBAAQ;AAC/B,cAAc,mBAAO,CAAC,0FAA2B;AACjD,OAAO,2DAA2D,GAAG,mBAAO,CAAC,0DAAc;;AAE3F;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,WAAW;AACxB,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,aAAa,iBAAiB;AAC9B;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gCAAgC,WAAW;;AAE3C;AACA;;AAEA;AACA,WAAW,SAAS;AACpB,WAAW,mBAAmB;AAC9B,sBAAsB,KAAK,GAAG,KAAK;;AAEnC;AACA,kDAAkD,YAAY;AAC9D;;AAEA;AACA,4DAA4D,SAAS;AACrE;;AAEA,kDAAkD,SAAS;AAC3D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,2BAA2B,OAAO,eAAe,SAAS;AAC1D,KAAK;AACL,0DAA0D,OAAO,WAAW,UAAU;AACtF,wCAAwC,SAAS;AACjD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,kCAAkC,WAAW,EAAE,wBAAwB;AACvE,gDAAgD,qBAAqB;AACrE;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA,WAAW,gBAAgB;;AAE3B;AACA;AACA,WAAW,OAAO;AAClB;AACA;;AAEA;AACA;AACA,WAAW,OAAO,gCAAgC,WAAW,4BAA4B,cAAc;AACvG;AACA;;AAEA;AACA;AACA,4BAA4B,yBAAyB,KAAK,YAAY;AACtE,gDAAgD,eAAe;AAC/D;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB,uBAAuB,KAAK,kBAAkB;AAC9D;;AAEA;AACA;AACA;AACA;AACA;AACA,gBAAgB,qBAAqB,KAAK,OAAO;AACjD;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,WAAW;AACtB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrUA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6EAAS;;AAE5C;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,eAAe,mBAAO,CAAC,6DAAW;AAClC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,kBAAkB,mBAAO,CAAC,yEAAoB;AAC9C,OAAO,8CAA8C,GAAG,mBAAO,CAAC,uDAAW;;AAE3E,OAAO,uBAAuB;AAC9B,wCAAwC,mBAAmB;AAC3D;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,gDAAgD;AAC7D,aAAa,6BAA6B;AAC1C,aAAa,mCAAmC;AAChD,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,wCAAwC;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,eAAe,mBAAmB;AAClC;AACA,eAAe,4CAA4C;AAC3D;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,sFAAsF,UAAU;AAChG,aAAa;AACb;AACA,SAAS;AACT,wCAAwC,wBAAwB;AAChE;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA,WAAW,iCAAiC;;AAE5C;AACA;AACA;AACA;;AAEA;;AAEA;AACA,iCAAiC,2BAA2B;AAC5D;;AAEA;AACA,0DAA0D,mBAAmB;AAC7E;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,mBAAmB;AACnF;AACA,eAAe;AACf,aAAa;AACb,WAAW;AACX;AACA;;AAEA,2DAA2D,SAAS,QAAQ,OAAO;AACnF;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;;AAEA;AACA,gDAAgD,OAAO;AACvD;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,UAAU,iCAAiC,gBAAgB;AACxE,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4CAA4C,SAAS;AACrD;AACA,+BAA+B,iBAAiB;AAChD,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA,oCAAoC,4BAA4B;AAChE;;AAEA;AACA;AACA;AACA,sCAAsC,SAAS;AAC/C,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX,0EAA0E,wBAAwB;AAClG,6CAA6C,kBAAkB;AAC/D;;AAEA;AACA,8BAA8B,wCAAwC;AACtE;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC3VA,mBAAmB,mBAAO,CAAC,+EAAuB;AAClD,OAAO,mDAAmD,GAAG,mBAAO,CAAC,uDAAW;;AAEhF;AACA,aAAa,OAAO;AACpB,cAAc,gBAAgB,8CAA8C,yBAAyB;AACrG;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD,WAAW,0BAA0B;AACrC,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,mCAAmC;AAC9C,WAAW,6BAA6B;AACxC,WAAW,qCAAqC;AAChD,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kDAAkD,MAAM,eAAe,cAAc;AACrF;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,wDAAwD,UAAU;AAClE;AACA,gCAAgC,kBAAkB,iBAAiB,QAAQ;AAC3E;AACA;AACA,KAAK;AACL;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA,mBAAmB,mBAAmB,KAAK;AAC3C;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACjHA,mBAAmB,mBAAO,CAAC,sEAAc;AACzC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,0BAA0B,mBAAO,CAAC,oFAAqB;AACvD,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;AACvB,0BAA0B,mBAAO,CAAC,6FAA8B;;AAEhE,OAAO,OAAO;;AAEd,2BAA2B,oBAAoB;AAC/C;AACA;AACA,CAAC;;AAED;AACA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,mCAAmC;AAChD,aAAa,6BAA6B;AAC1C,aAAa,qCAAqC;AAClD,aAAa,IAAI;AACjB,aAAa,qCAAqC;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,gBAAgB,aAAa;AAC7B,kCAAkC,aAAa;AAC/C;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,kBAAkB,cAAc,KAAK;AACrC;AACA;AACA;AACA,kDAAkD,SAAS;AAC3D,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,cAAc;AACd;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,SAAS;AACtB,cAAc;AACd;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,SAAS;AAC7B;AACA,+CAA+C,SAAS;AACxD,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,WAAW,WAAW;;AAEtB;AACA;AACA;;AAEA,0CAA0C,gCAAgC;;AAE1E;AACA;AACA,qCAAqC,sBAAsB;AAC3D;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,eAAe,0CAA0C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA,WAAW,WAAW;AACtB;AACA,4EAA4E,QAAQ;AACpF;;AAEA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,kBAAkB;AAC/B,eAAe,OAAO;AACtB;AACA,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA,aAAa,SAAS;AACtB;AACA,cAAc;AACd,KAAK,IAAI;AACT;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,8BAA8B,qDAAqD;AACnF;AACA;AACA,eAAe,cAAc;AAC7B;AACA;AACA,SAAS;AACT,sCAAsC,6BAA6B;AACnE,OAAO;AACP;AACA;AACA;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;;AAEA;AACA,oEAAoE;AACpE;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,uDAAuD;AACpE,eAAe;AACf;AACA,sCAAsC,2BAA2B;AACjE,oEAAoE,iBAAiB;AACrF;AACA;AACA,oEAAoE,2BAA2B;AAC/F;AACA;AACA;AACA,WAAW;AACX;AACA,SAAS;AACT;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA;;AAEA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA,iBAAiB,gBAAgB;AACjC;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;AACA,gDAAgD,eAAe;AAC/D;AACA;AACA;AACA,eAAe,8CAA8C;AAC7D;AACA;AACA;AACA;AACA,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE,qCAAqC,4BAA4B;AACjE;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,YAAY;AACzB,cAAc;AACd;;AAEA;AACA;AACA,aAAa,kDAAkD;AAC/D;AACA;AACA;AACA;AACA;AACA,oEAAoE,gBAAgB;;AAEpF,oCAAoC;;AAEpC;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,4CAA4C,SAAS;AACrD;;AAEA,aAAa,0BAA0B;AACvC;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA,KAAK;;AAEL;AACA;AACA,wEAAwE;;AAExE;AACA;AACA,kDAAkD,oBAAoB;AACtE;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA,oBAAoB,UAAU;AAC9B;AACA,kDAAkD;AAClD;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA,yBAAyB,oCAAoC;AAC7D,oDAAoD,UAAU;;AAE9D;AACA;AACA;AACA;;;;;;;;;;;;;;ACzfA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,gBAAgB,mBAAO,CAAC,2EAAqB;;AAE7C;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,8CAA8C;AACxD;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,qBAAqB;AAClC;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU,kDAAkD;AAC5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA,gFAAgF;AAChF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,mBAAmB,mBAAO,CAAC,uGAAsB;;AAEjD;AACA;AACA;;;;;;;;;;;;;;ACJA,OAAO,mCAAmC,GAAG,mBAAO,CAAC,uFAAwB;AAC7E,gBAAgB,mBAAO,CAAC,2EAAwB;;AAEhD;AACA;AACA,WAAW,QAAQ;AACnB,aAAa;AACb;AACA,mBAAmB,UAAU;AAC7B;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,MAAM;AACnB,gCAAgC,oDAAoD;AACpF,aAAa,MAAM;AACnB,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;AACA,gBAAgB,kBAAkB;AAClC;AACA,wCAAwC,WAAW;AACnD;;AAEA;AACA;AACA,0CAA0C,2CAA2C;AACrF,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL,GAAG;;AAEH,YAAY,SAAS;AACrB;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClFD;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,UAAU;AACV;;;;;;;;;;;;;;ACbA,aAAa,mBAAO,CAAC,+DAAe;AACpC,8BAA8B,mBAAO,CAAC,6FAAyB;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,gCAAgC;;AAE3C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,yDAAyD,kBAAkB;AAC3E;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/GA,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,cAAc,mBAAO,CAAC,iEAAgB;AACtC,8BAA8B,mBAAO,CAAC,iGAAgC;AACtE,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,kBAAkB,mBAAO,CAAC,yEAAoB;AAC9C,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,wBAAwB,mBAAO,CAAC,qFAA0B;;AAE1D,sBAAsB,mBAAO,CAAC,mFAAiB;AAC/C,cAAc,mBAAO,CAAC,6DAAS;AAC/B,oBAAoB,mBAAO,CAAC,yEAAe;AAC3C,0BAA0B,mBAAO,CAAC,qFAAqB;AACvD;AACA,WAAW,+DAA+D;AAC1E,CAAC,GAAG,mBAAO,CAAC,6FAAyB;AACrC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,mFAAoB;AACzD;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,uDAAW;;AAEvB,OAAO,OAAO;;AAEd;AACA;AACA;AACA;AACA,0BAA0B,eAAe;AACzC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,eAAe,8BAA8B;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,sBAAsB,sBAAsB;AAC3D;AACA;AACA;AACA;;AAEA;;AAEA,4DAA4D,WAAW;AACvE,aAAa,kCAAkC;AAC/C;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,4CAA4C;;AAEvD,gEAAgE,UAAU;;AAE1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,oBAAoB;AAC/B;AACA,yCAAyC,oBAAoB;AAC7D;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,mDAAmD,0CAA0C;AAC7F,6CAA6C,OAAO;;AAEpD;AACA;AACA,6CAA6C,cAAc;AAC3D;AACA;;AAEA;AACA,0CAA0C,oCAAoC;;AAE9E;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA,WAAW,mBAAmB;AAC9B;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gDAAgD,QAAQ;AACxD;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,oBAAoB;AACjD;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB,oBAAoB,OAAO,iCAAiC;AAChF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,qCAAqC;AAClD;AACA,eAAe,mBAAmB;AAClC,oCAAoC,mBAAmB;AACvD;;AAEA;AACA,aAAa,2CAA2C;AACxD;AACA,iBAAiB,2BAA2B;AAC5C,sCAAsC,2BAA2B;AACjE;;AAEA;AACA;AACA;AACA;AACA;AACA,aAAa,2CAA2C;AACxD;AACA,QAAQ,2BAA2B;AACnC;AACA;;AAEA;AACA,8CAA8C,uBAAuB;AACrE;AACA,KAAK;AACL;AACA;;AAEA;AACA,+CAA+C,uBAAuB;AACtE;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,mBAAmB,WAAW;AAC9B,0CAA0C,WAAW;AACrD;;AAEA;AACA;AACA,aAAa,gEAAgE;AAC7E,kBAAkB,mBAAmB,4BAA4B,mBAAmB,qBAAqB,mBAAmB,GAAG,IAAI;AACnI;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;;AAEA,mEAAmE,aAAa;AAChF;AACA,kBAAkB,aAAa;AAC/B,eAAe,QAAQ;;AAEvB;AACA,sEAAsE,iBAAiB;AACvF;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;;AAEA;AACA,yBAAyB,wBAAwB,kBAAkB,oBAAoB,UAAU,cAAc;AAC/G;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;;AAEb;AACA,wCAAwC,0CAA0C;AAClF;AACA;;AAEA;AACA,sDAAsD,SAAS;AAC/D,eAAe,YAAY;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET,oDAAoD,wBAAwB;AAC5E,kEAAkE,QAAQ;AAC1E;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,kCAAkC;AACvD;AACA,uBAAuB,sCAAsC;AAC7D;AACA;AACA,oEAAoE,qBAAqB;AACzF;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,YAAY;AAC9B;;AAEA;AACA;AACA,aAAa;AACb,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;;AAEA;AACA,0BAA0B,UAAU;AACpC;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;;AAEA;AACA,iCAAiC,6BAA6B;AAC9D;;AAEA;AACA,2BAA2B,UAAU;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,SAAS;AACtB,gBAAgB,4BAA4B;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,8DAA8D,+BAA+B;AAC7F;;AAEA;AACA;AACA;AACA,eAAe,yCAAyC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA,cAAc;AACd,KAAK,IAAI;AACT;AACA;;;;;;;;;;;;;;AC5tBA,aAAa,mBAAO,CAAC,+DAAe;AACpC;;AAEA,wBAAwB,MAAM;AAC9B;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW,UAAU;AACrB,WAAW,cAAc;AACzB,aAAa,UAAU;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,WAAW;AACtB,WAAW,YAAY;AACvB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA,aAAa,OAAO;AACpB,WAAW,OAAO;AAClB,WAAW,QAAQ;AACnB;AACA,mBAAmB,gCAAgC;AACnD;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;;AAEA,WAAW,4BAA4B;;AAEvC;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC/DA,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,mBAAmB,GAAG,mBAAO,CAAC,uEAAmB;AACxD,sBAAsB,mBAAO,CAAC,6EAAiB;AAC/C,eAAe,mBAAO,CAAC,+DAAU;AACjC,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,OAAO,aAAa,GAAG,mBAAO,CAAC,2EAAa;AAC5C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,6DAAc;AACjE,wBAAwB,mBAAO,CAAC,yFAA4B;;AAE5D,OAAO,eAAe;AACtB,OAAO,mCAAmC;;AAE1C;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,OAAO;AAClB,WAAW,mCAAmC;AAC9C,WAAW,6BAA6B;AACxC,WAAW,0CAA0C;AACrD,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,4BAA4B;AACvC,WAAW,OAAO;AAClB;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA,oBAAoB,2BAA2B;AAC/C;;AAEA;AACA;AACA;;AAEA;AACA;AACA,qCAAqC,kBAAkB,uCAAuC,eAAe;AAC7G;AACA;;AAEA,gCAAgC,sDAAsD;AACtF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,0CAA0C;AACvD;AACA;AACA;AACA;;AAEA,aAAa,6CAA6C;AAC1D;AACA;AACA;AACA,2DAA2D,UAAU;AACrE;AACA;AACA,KAAK;AACL,oEAAoE,UAAU;AAC9E;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA,aAAa,uCAAuC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,UAAU;AACxC,KAAK;AACL,+DAA+D,UAAU;AACzE;AACA;AACA,OAAO;;AAEP;AACA;AACA;;AAEA,aAAa,4CAA4C;AACzD,4BAA4B,+BAA+B;AAC3D;AACA;AACA;;AAEA;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA;AACA,yBAAyB,MAAM,IAAI,aAAa;AAChD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mBAAmB;AAClC;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;;AAEA;AACA,mBAAmB;AACnB;;AAEA;AACA;;AAEA,aAAa,sCAAsC;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA,mFAAmF,UAAU;AAC7F;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,+BAA+B,UAAU;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA,6BAA6B,OAAO,IAAI,UAAU;AAClD;AACA;AACA;AACA,OAAO;;AAEP;AACA,8BAA8B,6BAA6B;AAC3D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,mDAAmD,UAAU;AAC7D;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;AACA;;AAEA,aAAa,qCAAqC;AAClD;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,yEAAyE;AAC3F;AACA;AACA;AACA,iBAAiB,4CAA4C;AAC7D;AACA,8DAA8D,MAAM;AACpE;;AAEA;AACA;AACA,6DAA6D,UAAU;AACvE;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,yFAAyF,OAAO;AAChG;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0EAA0E,SAAS;AACnF;AACA;;AAEA;;AAEA,2BAA2B,4CAA4C;;AAEvE,gBAAgB;AAChB,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA,aAAa,uCAAuC;AACpD,iBAAiB,2BAA2B;AAC5C;AACA,0DAA0D,MAAM;AAChE;;AAEA;AACA;AACA,yDAAyD,UAAU;AACnE;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,qFAAqF,OAAO;AAC5F;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kDAAkD;AAC1E;;AAEA,aAAa,gDAAgD;AAC7D;AACA,4DAA4D,UAAU;AACtE;AACA;AACA,aAAa,SAAS,qCAAqC,sBAAsB;AACjF;AACA,KAAK;AACL;;AAEA;AACA,YAAY;AACZ;AACA,kBAAkB,0CAA0C;AAC5D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,wFAAwF,0BAA0B;AAClH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,YAAY;AACZ;AACA,iBAAiB,0CAA0C;AAC3D;AACA;AACA;AACA;AACA;AACA,2BAA2B,2DAA2D;AACtF;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,yFAAyF,0BAA0B;AACnH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,cAAc,OAAO;AACrB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjhBA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,wEAAqB;AAC7C,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,kCAAkC,mBAAO,CAAC,mHAA6B;AACvE;AACA,WAAW,iBAAiB;AAC5B,CAAC,GAAG,mBAAO,CAAC,8FAA0B;;AAEtC,OAAO,eAAe;AACtB,yEAAyE,YAAY,EAAE,KAAK;;AAE5F;AACA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,iCAAiC;AAC9C,aAAa,gCAAgC;AAC7C,aAAa,2CAA2C;AACxD,aAAa,QAAQ;AACrB,aAAa,cAAc;AAC3B,aAAa,cAAc;AAC3B,cAAc,kBAAkB,2BAA2B;AAC3D,aAAa,wCAAwC;AACrD,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,wCAAwC;AACrD;AACA,eAAe,mBAAmB;AAClC;AACA;;AAEA;AACA,aAAa,8CAA8C;AAC3D;AACA,iBAAiB,2BAA2B;AAC5C;AACA;AACA;AACA;;AAEA;AACA,eAAe;AACf;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,wCAAwC;AACrD;AACA,0BAA0B,mBAAmB;AAC7C,WAAW,kCAAkC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mCAAmC;AAC3D,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;AACA;AACA;AACA,aAAa,8CAA8C;AAC3D;AACA,cAAc,2BAA2B;AACzC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA,WAAW,kCAAkC;AAC7C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,oBAAoB;AAC5C,SAAS;AACT;AACA,KAAK;;AAEL,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,eAAe;AAC/B;AACA,eAAe,OAAO;AACtB,gBAAgB,kBAAkB;AAClC;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,8BAA8B,aAAa;AAC3C;AACA;AACA;AACA,KAAK;AACL,sCAAsC,oBAAoB;AAC1D;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,YAAY;AACZ;;AAEA,kCAAkC;AAClC,WAAW,kCAAkC;AAC7C,WAAW,4CAA4C;;AAEvD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,oBAAoB;AAC3C;AACA,iBAAiB,oBAAoB,kBAAkB,sBAAsB;AAC7E;AACA;;AAEA;AACA,YAAY;AACZ;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,UAAU;AACrB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,YAAY;AAC1C,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,6BAA6B;AACxC;AACA;AACA,KAAK;;AAEL,uDAAuD,oBAAoB;AAC3E;AACA;AACA;AACA;AACA,sBAAsB,SAAS;AAC/B,mBAAmB,YAAY,aAAa,YAAY;AACxD,SAAS;AACT;AACA;AACA;;AAEA,mCAAmC,oBAAoB;AACvD,0BAA0B,sBAAsB;AAChD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA,aAAa,wCAAwC;AACrD;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,wBAAwB;AACjE;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1YA,wBAAwB,mBAAO,CAAC,+FAAmB;AACnD,OAAO,eAAe;;AAEtB,+BAA+B,oBAAoB,kBAAkB,sBAAsB;AAC3F,2BAA2B,oBAAoB;AAC/C,eAAe,+CAA+C,GAAG;;AAEjE;AACA,uEAAuE;AACvE,iEAAiE;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB;AAChB,OAAO;AACP;AACA,GAAG;AACH;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;;AAEvC;;;;;;;;;;;;;;ACFA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,aAAa,mBAAO,CAAC,+DAAe;AACpC,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,yBAAyB,mBAAO,CAAC,6EAAsB;AACvD,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;AAC5C,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC;AACA,WAAW,0EAA0E;AACrF,CAAC,GAAG,mBAAO,CAAC,6FAAyB;;AAErC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,6BAA6B;AAC1C,aAAa,0BAA0B;AACvC,aAAa,qCAAqC;AAClD,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,mEAAmE;AAChF,aAAa,qEAAqE;AAClF,aAAa,OAAO;AACpB,aAAa,wBAAwB;AACrC,aAAa,mCAAmC;AAChD,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA,WAAW,mBAAmB;;AAE9B;AACA,6DAA6D,mBAAmB;AAChF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gDAAgD,mCAAmC;AACnF,WAAW;AACX,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;;AAEA,wCAAwC,2CAA2C;AACnF,0CAA0C,mCAAmC;AAC7E;AACA;AACA;;AAEA;AACA,WAAW,mBAAmB;AAC9B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C,4CAA4C;AACxF,SAAS;AACT;AACA,8CAA8C,mCAAmC;AACjF,SAAS;AACT;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,yDAAyD,mBAAmB;AAC5E,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,+CAA+C;AACvF;AACA;;AAEA;AACA;;AAEA,oDAAoD;;AAEpD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP,0CAA0C,mCAAmC;AAC7E;;AAEA,WAAW,gCAAgC;AAC3C,2CAA2C,6CAA6C;;AAExF;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa;AACb;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wCAAwC,mCAAmC;AAC3E;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;ACrkBA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C,gBAAgB,wBAAwB,oBAAoB;AAC5D,OAAO;AACP;AACA;AACA;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,uBAAuB,4BAA4B,0CAA0C;AAC1G;AACA;AACA,8BAA8B,oBAAoB;AAClD;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA,+DAA+D,oBAAoB;AACnF;AACA;AACA,KAAK;AACL;;AAEA;AACA,eAAe,6CAA6C;AAC5D,gBAAgB,0CAA0C;AAC1D;AACA;AACA;AACA,aAAa,oBAAoB;AACjC;AACA;AACA,OAAO;AACP,gBAAgB,aAAa;AAC7B;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1HA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD,gBAAgB,mBAAO,CAAC,4DAAiB;AACzC,OAAO,OAAO;;AAEd;AACA,kBAAkB,mBAAmB,KAAK;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc,mBAAmB;AACjC;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,0BAA0B,KAAK;AACjD,cAAc,YAAY;AAC1B;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,mBAAmB;AACrC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,WAAW;AAC7B;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,wBAAwB;AAC1C;AACA,oBAAoB,UAAU,iBAAiB,QAAQ;AACvD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,eAAe,KAAK;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,aAAa,KAAK;AACpC,cAAc,YAAY,KAAK,GAAG,KAAK,GAAG;AAC1C;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,4DAA4D,KAAK;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,QAAQ,KAAK;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,2BAA2B,KAAK;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,kBAAkB,KAAK;AACzC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,aAAa;AAC5B;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB,eAAe,QAAQ;;AAEvB,2CAA2C,YAAY;AACvD;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,iCAAiC,QAAQ;AACzC;;AAEA;AACA,oEAAoE,QAAQ;AAC5E,wBAAwB,SAAS,0DAA0D,WAAW;AACtG;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChRA;AACA;AACA,WAAW,OAAO;AAClB,CAAC,GAAG,mBAAO,CAAC,8DAAW;;AAEvB,oCAAoC,mBAAO,CAAC,wFAA2B;AACvE,sBAAsB,mBAAO,CAAC,wEAAmB;AACjD,gBAAgB,mBAAO,CAAC,8DAAW;AACnC,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,uBAAuB,mBAAO,CAAC,gEAAY;AAC3C,oBAAoB,mBAAO,CAAC,0DAAS;AACrC,wBAAwB,mBAAO,CAAC,wFAA2B;AAC3D,6BAA6B,mBAAO,CAAC,oFAAyB;;AAE9D;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,cAAc;AAC3B,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,gCAAgC;AAC7C,aAAa,kCAAkC;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,yCAAyC,8BAA8B;AACvE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa,aAAa;AAC1B;AACA;AACA;AACA;AACA,GAAG,KAAK;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,SAAS,QAAQ,KAAK;AACtB;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA,cAAc,2CAA2C;AACzD;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnMA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,6BAA6B,mBAAO,CAAC,oEAAS;AAC9C,OAAO,eAAe,GAAG,mBAAO,CAAC,uDAAW;;AAE5C;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,oDAAoD,mBAAmB;AACvE;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,aAAa,yBAAyB;AACtC,eAAe,iEAAiE;AAChF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACtBA,yCAAyC,UAAU,GAAG,KAAK;;;;;;;;;;;;;;ACA3D,OAAO,mBAAmB,GAAG,mBAAO,CAAC,4DAAS;;AAE9C,yBAAyB,+BAA+B;AACxD,iCAAiC,UAAU;AAC3C;AACA,mBAAmB,eAAe;AAClC,kBAAkB,OAAO,EAAE,YAAY;AACvC,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpBA,OAAO,SAAS;;AAEhB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA,uBAAuB,kCAAkC,KAAK;AAC9D;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnEA,qBAAqB,mBAAO,CAAC,8DAAU;AACvC,sBAAsB,mBAAO,CAAC,2EAAqB;AACnD,gBAAgB,mBAAO,CAAC,2EAAqB;AAC7C,OAAO,uDAAuD,GAAG,mBAAO,CAAC,uDAAW;AACpF,OAAO,mBAAmB,GAAG,mBAAO,CAAC,6DAAc;AACnD,eAAe,mBAAO,CAAC,iDAAQ;AAC/B,qBAAqB,mBAAO,CAAC,gFAAgB;AAC7C,OAAO,sCAAsC,GAAG,mBAAO,CAAC,kFAAoB;;AAE5E,sBAAsB,8BAA8B;AACpD,KAAK,QAAQ,QAAQ,OAAO,aAAa,WAAW;;AAEpD;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,6BAA6B;AAC1C,aAAa,qCAAqC;AAClD,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB;AACA,aAAa,qCAAqC;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA,6CAA6C;AAC7C;AACA,sBAAsB,0CAA0C;AAChE;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sEAAsE,UAAU;AAChF,qBAAqB,UAAU,GAAG,UAAU;AAC5C;AACA,SAAS;;AAET,sCAAsC,iBAAiB;AACvD;AACA;;AAEA;AACA;;AAEA;AACA;AACA,qBAAqB,UAAU,GAAG,UAAU;AAC5C,SAAS;;AAET;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA,2DAA2D,UAAU;AACrE,uBAAuB,UAAU,GAAG,UAAU;AAC9C,WAAW;AACX;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA,gBAAgB,gDAAgD;AAChE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA,yBAAyB,UAAU,GAAG,UAAU;AAChD,aAAa;AACb;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,eAAe,cAAc;AAC7B;AACA,cAAc,oEAAoE;AAClF;;AAEA;AACA;AACA,aAAa,WAAW;AACxB;;AAEA,kDAAkD,mCAAmC;AACrF,aAAa,8BAA8B;AAC3C,+BAA+B,qBAAqB;AACpD;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,yBAAyB;;AAEzB;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX,SAAS;AACT;AACA;AACA,OAAO;AACP;;AAEA,WAAW,sCAAsC;;AAEjD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;AACA,kCAAkC,mBAAmB;AACrD;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,gCAAgC,mBAAmB;AACnD;AACA;AACA;AACA,gDAAgD,qCAAqC;AACrF,OAAO;;AAEP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,UAAU,GAAG,UAAU;AAC1C,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1bA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACPA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,eAAe,mBAAO,CAAC,6FAA0B;AACjD,OAAO,4BAA4B,GAAG,mBAAO,CAAC,0DAAc;;AAE5D;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,QAAQ;AACrB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,gCAAgC;AAC7C,aAAa,wCAAwC;AACrD,aAAa,cAAc;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,uCAAuC;AACvD,gBAAgB,QAAQ;AACxB,gBAAgB,SAAS;AACzB,gBAAgB,OAAO;AACvB;AACA;AACA,aAAa,cAAc;AAC3B;AACA;AACA,WAAW,gBAAgB;AAC3B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA,aAAa,cAAc;AAC3B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,+BAA+B,yBAAyB;AACxD;AACA;;AAEA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB;AACA,kBAAkB,+BAA+B;AACjD;AACA;AACA;;AAEA;AACA,+BAA+B,gBAAgB;AAC/C,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,MAAM;AACnB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;;;;;;;;;;;;;ACtTA,OAAO,uDAAuD,GAAG,mBAAO,CAAC,0DAAc;AACvF,eAAe,mBAAO,CAAC,6FAA0B;;AAEjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,aAAa;AAC3B,cAAc,QAAQ;AACtB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA,aAAa,OAAO;AACpB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,OAAO;AACrB,cAAc,SAAS;AACvB,cAAc,SAAS;AACvB;AACA;AACA;AACA,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,OAAO;AACpB,aAAa,aAAa;AAC1B,aAAa,QAAQ;AACrB,aAAa,SAAS;AACtB,aAAa,WAAW;AACxB,aAAa,wCAAwC;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,8BAA8B;AACzC,2BAA2B,QAAQ,QAAQ,OAAO,aAAa,WAAW;AAC1E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4DAA4D,YAAY;AACxE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA,aAAa,gBAAgB;AAC7B;AACA;AACA;AACA,KAAK;;AAEL,WAAW,6EAA6E;;AAExF;AACA;AACA,mBAAmB,sCAAsC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,uBAAuB,iBAAiB;AACxC;AACA;AACA;;AAEA;;AAEA;AACA,8CAA8C,QAAQ,MAAM,gBAAgB;AAC5E;AACA;AACA;;;;;;;;;;;;;;ACvKA;AACA,WAAW,OAAO;AAClB,WAAW,qCAAqC;AAChD,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,WAAW;AACtB,WAAW,uBAAuB;AAClC,WAAW,WAAW;AACtB,WAAW,qBAAqB;AAChC,WAAW,WAAW;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,gCAAgC,6BAA6B;;AAE7D;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC/BA;;AAEA;AACA,aAAa;AACb;AACA;AACA,cAAc,mBAAO,CAAC,gBAAK;AAC3B,cAAc,mBAAO,CAAC,gBAAK;;AAE3B,WAAW,6BAA6B;AACxC;AACA,mCAAmC,+BAA+B;AAClE,qBAAqB,aAAa;;AAElC;;AAEA;AACA;AACA;;;;;;;;;;;;;;AClBA;AACA;AACA,MAAM,gEAAgE;AACtE;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;;;;;;;;;;;;;;ACXA,oBAAoB,mBAAO,CAAC,8DAAa;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,0BAA0B,mBAAO,CAAC,gGAAiC;AACnE,2BAA2B,mBAAO,CAAC,4GAA2B;AAC9D,eAAe,mBAAO,CAAC,sBAAQ;;AAE/B,eAAe,mBAAO,CAAC,gGAAqB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yEAAyE;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;;AAEA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2CAA2C,SAAS;AACpD,kCAAkC,KAAK;AACvC;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,mBAAmB;AACnB;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa;;AAEb;AACA;AACA;AACA;;AAEA,6DAA6D,4BAA4B;AACzF,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,mBAAmB;AACnB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,4BAA4B,MAAM,GAAG,UAAU,sBAAsB,SAAS;AAC9E;AACA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,YAAY;AAC7B;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,SAAS;AAC7B,oBAAoB,OAAO;AAC3B;AACA;AACA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;;AAEA,+BAA+B,YAAY;AAC3C;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA,2CAA2C,qDAAqD;AAChG;;AAEA,yBAAyB,oBAAoB;AAC7C;AACA;AACA,WAAW;AACX,SAAS;AACT,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA,iBAAiB,OAAO;AACxB,iBAAiB,oBAAoB;AACrC,mBAAmB;AACnB;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,6BAA6B;AACjD;AACA,mBAAmB,OAAO;AAC1B,oBAAoB,OAAO;AAC3B,oBAAoB,OAAO;AAC3B;AACA,yBAAyB,0BAA0B;AACnD;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,eAAe;;AAEf;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA;AACA,uBAAuB,oDAAoD;AAC3E,yBAAyB,4CAA4C;AACrE,mCAAmC,oCAAoC;AACvE,oBAAoB,oCAAoC;AACxD,eAAe,oCAAoC;AACnD,cAAc,oCAAoC;AAClD;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACtZA,OAAO,eAAe,GAAG,mBAAO,CAAC,sBAAQ;AACzC,OAAO,2BAA2B,GAAG,mBAAO,CAAC,0DAAc;AAC3D,eAAe,mBAAO,CAAC,gGAAqB;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8CAA8C;AACjE;;AAEA,kCAAkC,qCAAqC;AACvE;AACA,gFAAgF,OAAO;AACvF;;AAEA;AACA;;AAEA;AACA;AACA,uDAAuD,OAAO,cAAc,aAAa;AACzF;;AAEA;AACA;AACA,SAAS;AACT;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,eAAe,OAAO;AACtB,eAAe,OAAO;AACtB,eAAe,SAAS;AACxB,eAAe,aAAa;AAC5B;AACA;AACA;AACA;AACA;AACA,OAAO,IAAI;;AAEX,cAAc;AACd,KAAK;AACL;AACA;AACA;AACA;AACA,mDAAmD,aAAa,OAAO,MAAM;;AAE7E;AACA;AACA,6DAA6D,aAAa,OAAO,MAAM;AACvF;AACA;;AAEA,uCAAuC,gCAAgC;AACvE;AACA,KAAK;;AAEL;AACA;AACA,KAAK;AACL,GAAG;;AAEH;AACA;;;;;;;;;;;;;;AC9EA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,mBAAmB,kDAAkD;AACrE;AACA;AACA;;AAEA;AACA,mCAAmC,oCAAoC;AACvE;AACA,kCAAkC,qCAAqC;AACvE,GAAG,IAAI;AACP;;;;;;;;;;;;;;ACVA,oBAAoB,mBAAO,CAAC,2DAAU;AACtC,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;AACnE,OAAO,qBAAqB,GAAG,mBAAO,CAAC,kFAAiB;AACxD,oCAAoC,mBAAO,CAAC,yFAA4B;AACxE,yBAAyB,mBAAO,CAAC,6EAAc;AAC/C,8BAA8B,mBAAO,CAAC,iFAAmB;AACzD,OAAO,+CAA+C,GAAG,mBAAO,CAAC,6FAAyB;AAC1F,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD,OAAO,eAAe;AACtB;AACA;AACA,iCAAiC,IAAI;AACrC;;AAEA,OAAO,sBAAsB;;AAE7B;AACA;AACA,WAAW,OAAO;AAClB,WAAW,8BAA8B;AACzC,WAAW,6BAA6B;AACxC,WAAW,yCAAyC;AACpD,WAAW,mCAAmC;AAC9C,WAAW,QAAQ;AACnB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,4BAA4B;AACvC;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,oBAAoB;;AAEpB;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,8CAA8C;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH,SAAS,kBAAkB;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA,aAAa,qCAAqC;AAClD;AACA;AACA,wEAAwE,UAAU;AAClF;;AAEA;AACA;AACA;AACA,oDAAoD,UAAU;AAC9D;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB,gBAAgB,SAAS;AACzB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,WAAW,yCAAyC;AACpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,kBAAkB,yBAAyB;AAC3C;AACA,4CAA4C,0BAA0B;AACtE,mDAAmD,0BAA0B;;AAE7E;AACA,iBAAiB,oBAAoB;AACrC,sBAAsB,oBAAoB;AAC1C;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB;AACA;;AAEA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrPA,mBAAmB,mBAAO,CAAC,2EAAqB;AAChD,sBAAsB,mBAAO,CAAC,qGAAkC;AAChE,iCAAiC,mBAAO,CAAC,6FAA8B;AACvE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,2BAA2B,mBAAO,CAAC,2EAAgB;AACnD,OAAO,yCAAyC,GAAG,mBAAO,CAAC,uDAAW;AACtE,OAAO,oBAAoB,GAAG,mBAAO,CAAC,2FAA6B;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA,yCAAyC;AACzC;AACA;AACA;AACA;AACA;;AAEA;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE;AACA,eAAe,OAAO;AACtB,gBAAgB,qBAAqB;AACrC,gBAAgB,OAAO;AACvB;AACA;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,aAAa;AACb,eAAe;AACf;AACA,4BAA4B,sDAAsD;AAClF,6BAA6B,QAAQ;AACrC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,kBAAkB;AAClC;AACA;AACA,qCAAqC,SAAS,eAAe,MAAM;AACnE;AACA;;AAEA;AACA;AACA;AACA,sDAAsD,MAAM,KAAK;AACjE;AACA,YAAY;AACZ;AACA;AACA;;AAEA;AACA,+DAA+D,kBAAkB;AACjF,uCAAuC,qBAAqB;;AAE5D;AACA,qBAAqB,kBAAkB;AACvC,OAAO;AACP;AACA;;AAEA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,aAAa,eAAe;AAC5B,eAAe;AACf;AACA,eAAe,OAAO;AACtB,gBAAgB,OAAO;AACvB,gBAAgB,MAAM;AACtB,+BAA+B,kCAAkC;AACjE,gBAAgB,OAAO;AACvB;AACA;AACA;AACA,gBAAgB,OAAO;AACvB,gBAAgB,kBAAkB;AAClC;AACA,uBAAuB,8CAA8C;AACrE,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnIA,gBAAgB,mBAAO,CAAC,sFAAW;AACnC,iCAAiC,mBAAO,CAAC,8FAAe;;AAExD;;;;;;;;;;;;;;ACHA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AClDA,oBAAoB,mBAAO,CAAC,8FAAe;;AAE3C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,WAAW,oCAAoC;AAC/C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9CA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,6DAAiB;;AAE9D;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD,yBAAyB,mBAAO,CAAC,sBAAQ;AACzC;;AAEA;;AAEA;AACA;AACA;AACA,sBAAsB,KAAK,0DAA0D,UAAU;AAC/F;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,0FAAW;AACnC,iCAAiC,mBAAO,CAAC,uGAAwB;;AAEjE;;;;;;;;;;;;;;ACHA;AACA,aAAa,mBAAO,CAAC,qEAAqB;;AAE1C;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,aAAa;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACpDA,2BAA2B,mBAAO,CAAC,oFAAW;AAC9C,kCAAkC,mBAAO,CAAC,4FAAe;;AAEzD;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,gBAAgB,mBAAO,CAAC,qEAAkB;;AAE1C,mBAAmB,SAAS;AAC5B,kCAAkC,wBAAwB;AAC1D,kCAAkC,0BAA0B;AAC5D;;AAEA;AACA;;;;;;;;;;;;;;ACRA,gBAAgB,mBAAO,CAAC,qEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;AACxD,OAAO,gBAAgB,GAAG,mBAAO,CAAC,uEAAmB;AACrD,kCAAkC,mBAAO,CAAC,qGAA6B;AACvE,wBAAwB,mBAAO,CAAC,iFAAmB;AACnD,2BAA2B,mBAAO,CAAC,uFAAsB;;AAEzD,OAAO,OAAO;;AAEd;AACA,WAAW,OAAO;AAClB,WAAW,6BAA6B;AACxC,WAAW,8BAA8B;AACzC,WAAW,qDAAqD;AAChE,WAAW,kCAAkC;AAC7C,WAAW,2BAA2B;AACtC;AACA,mBAAmB,oDAAoD;AACvE,iBAAiB,4CAA4C;AAC7D,eAAe,yCAAyC;AACxD;;AAEA,gBAAgB,yCAAyC;AACzD;AACA;;AAEA;;AAEA,kBAAkB,kBAAkB;AACpC;;AAEA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,SAAS,IAAI;;AAEb;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA,mDAAmD,SAAS;AAC5D;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,sBAAsB;AAC9C,yBAAyB,kEAAkE;AAC3F;AACA;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;AACA;;AAEA,sCAAsC,uBAAuB;AAC7D;;AAEA;AACA,WAAW;;AAEX;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,yCAAyC,QAAQ;AACjD;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,gCAAgC,6BAA6B;AAC7D;;AAEA;AACA,kEAAkE,UAAU;AAC5E;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,UAAU,IAAI,wBAAwB;AACzF;AACA;AACA;;AAEA,wBAAwB,UAAU,IAAI,wBAAwB;AAC9D;AACA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;ACpKA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,SAAS,SAAS;AAClB;AACA;AACA,iBAAiB,OAAO;AACxB;AACA;AACA;AACA;;;;;;;;;;;;;;AC7QA,OAAO,qDAAqD,GAAG,mBAAO,CAAC,uDAAW;AAClF,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,gBAAgB;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mBAAmB,YAAY;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/RA,aAAa,mBAAO,CAAC,+DAAe;;AAEpC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,MAAM;AACnB,aAAa,mCAAmC;AAChD,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClZA,OAAO,uBAAuB,GAAG,mBAAO,CAAC,uDAAW;AACpD,mBAAmB,mBAAO,CAAC,2EAAqB;;AAEh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iCAAiC,UAAU;AAC3C,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxlBA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,OAAO,YAAY,GAAG,mBAAO,CAAC,kBAAM;AACpC,aAAa,mBAAO,CAAC,kBAAM;;AAE3B;AACA;;AAEA;AACA;AACA,aAAa,QAAQ;AACrB,eAAe;AACf;AACA;AACA;AACA,GAAG;;AAEH;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;ACtBA,OAAO,wBAAwB,GAAG,mBAAO,CAAC,6DAAiB;;AAE3D;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB,mBAAO,CAAC,+EAAQ;AACtC;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrCA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,0DAAc;;AAE1B,kBAAkB,mBAAO,CAAC,+EAAc;AACxC,kBAAkB,mBAAO,CAAC,+EAAc;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+DAA+D,UAAU;AACzE;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,2DAA2D,eAAe,kBAAkB,KAAK;AACjG;AACA;;AAEA;AACA;AACA;AACA,wBAAwB,+BAA+B;AACvD;;;;;;;;;;;;;;ACpCA;AACA,KAAK,mBAAO,CAAC,qEAAM;AACnB,KAAK,mBAAO,CAAC,qEAAM;AACnB;;AAEA,mBAAmB,cAAc;;;;;;;;;;;;;;ACLjC;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACJD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,6CAA6C,GAAG,mBAAO,CAAC,wFAAgB;;AAE/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,6CAA6C;AAChE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACLD,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,cAAc,mBAAO,CAAC,iEAAa;AACnC,OAAO,6CAA6C,GAAG,mBAAO,CAAC,wFAAgB;;AAE/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qEAAqE;AACxF;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACzBA,aAAa,mBAAO,CAAC,kEAAkB;AACvC,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,uBAAuB,mBAAO,CAAC,kFAAoB;AACnD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAwB;AACpE,OAAO,6BAA6B,GAAG,mBAAO,CAAC,0DAAc;;AAE7D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AC1FA,gBAAgB,mBAAO,CAAC,kEAAY;AACpC,wBAAwB,mBAAO,CAAC,wEAAY;AAC5C,OAAO,QAAQ,GAAG,mBAAO,CAAC,gGAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA,mCAAmC,0BAA0B;AAC7D;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;ACxCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,mBAAmB;AACpC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpFA,eAAe,mBAAO,CAAC,kFAAU;AACjC;;AAEA;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACHD,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,aAAa;AAChC;AACA;;;;;;;;;;;;;;ACXA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,sBAAsB,mBAAO,CAAC,qGAAyB;AACvD,uBAAuB,mBAAO,CAAC,sFAAyB;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4CAA4C;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mBAAmB,aAAa,OAAO,uBAAuB,KAAK;;AAEnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,eAAe,mBAAO,CAAC,2FAAiB;;AAExC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,2BAA2B;AAC3B,8BAA8B;AAC9B,eAAe;AACf,iBAAiB;AACjB,qBAAqB,GAAG;AACxB;AACA,mBAAmB,8DAA8D,EAAE;AACnF;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,OAAO,6BAA6B,GAAG,mBAAO,CAAC,6DAAiB;AAChE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAA2B;AACvE,sBAAsB,mBAAO,CAAC,kGAAsB;AACpD,uBAAuB,mBAAO,CAAC,mFAAsB;;AAErD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,gEAAgE,eAAe,wBAAwB,OAAO;AAC9G;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uDAAuD,8BAA8B;;AAErF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,YAAY;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrHA,aAAa,mBAAO,CAAC,qEAAqB;AAC1C,gBAAgB,mBAAO,CAAC,qEAAe;AACvC,eAAe,mBAAO,CAAC,kFAAW;AAClC;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,mGAA2B;;AAEvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5FA,gBAAgB,mBAAO,CAAC,iEAAW;;AAEnC,yBAAyB,oCAAoC,6BAA6B,EAAE;AAC5F;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACZA;AACA,OAAO,sDAAsD;AAC7D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,sDAAsD;AACrF,GAAG;AACH,OAAO,sDAAsD;AAC7D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,sDAAsD;AACrF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sDAAsD;AACzE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sDAAsD;AACzE;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACnBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA;AACA,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,mGAAc;AAC1C,qBAAqB,mBAAO,CAAC,qGAAe;AAC5C,YAAY,mBAAmB,qDAAqD;AACpF,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,mGAAc;AAC1C,qBAAqB,mBAAO,CAAC,qGAAe;AAC5C,YAAY,mBAAmB,qDAAqD;AACpF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,6BAA6B,GAAG,mBAAO,CAAC,8EAAe;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,kBAAkB;AAC7B,qDAAqD,YAAY;AACjE,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,kBAAkB,mBAAO,CAAC,oGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,sGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,0BAA0B;AACjC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,0BAA0B;AACzD,GAAG;AACH,OAAO,0BAA0B;AACjC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,0BAA0B;AACzD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB,WAAW,QAAQ;AACnB;AACA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,4BAA4B;AACrD;AACA;AACA;AACA;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB,WAAW,QAAQ;AACnB;AACA,mBAAmB,0BAA0B;AAC7C;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACxBA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA;;AAEA;AACA;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;ACZD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;;AAEA,yBAAyB,gCAAgC;;;;;;;;;;;;;;ACJzD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qEAAqE,YAAY;AACjF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzCD,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;AACA,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH,OAAO,yCAAyC;AAChD,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,yCAAyC;AACxE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wDAAwD;AAC3E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,iCAAiC;AACjE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACnCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yCAAyC;AAC5D,2BAA2B,yCAAyC,IAAI,gBAAgB;;;;;;;;;;;;;;ACdxF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,oGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH,OAAO,gCAAgC;AACvC,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,gCAAgC;AAC/D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;AChDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,OAAO,iDAAiD,GAAG,mBAAO,CAAC,gEAAoB;;AAEvF;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,sBAAsB;AACxD;AACA;;AAEA,8BAA8B,cAAc;AAC5C;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gCAAgC;AACnD,2BAA2B,gCAAgC,IAAI,gBAAgB;;;;;;;;;;;;;;ACnB/E,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gCAAgC;AACnD,2BAA2B,gCAAgC,IAAI,gBAAgB;;;;;;;;;;;;;;ACnB/E,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA;AACA,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,iEAAiE,YAAY;AAC7E;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,wDAAwD,YAAY;;AAEpE;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzCD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA;AACA;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;;AAEA,iEAAiE,gBAAgB;;;;;;;;;;;;;;ACNjF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,kBAAkB,uBAAuB,SAAS;AACjF,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,kBAAkB,uBAAuB,SAAS;AACjF,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,8EAAe;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,oBAAoB;AACzC;AACA,6BAA6B,oBAAoB;AACjD;AACA,aAAa;AACb;AACA,SAAS;AACT;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC7BD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iCAAiC,GAAG,mBAAO,CAAC,gEAAoB;AACvE,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,WAAW,aAAa;AACxB,gDAAgD,YAAY;AAC5D,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA,YAAY,QAAQ;AACpB,YAAY,gCAAgC;AAC5C,YAAY,uBAAuB;;AAEnC;AACA;AACA,6CAA6C,uBAAuB;AACpE;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA,CAAC;;;;;;;;;;;;;;AChED,kBAAkB,mBAAO,CAAC,+FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,mBAAmB,mBAAO,CAAC,iGAAgB;;AAE3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B,SAAS,0BAA0B,eAAe,SAAS;;AAE3D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjCA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA,mBAAmB,yBAAyB;AAC5C;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oDAAoD,YAAY;AAChE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnCA;AACA,OAAO,yEAAyE;AAChF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA,wBAAwB,yEAAyE;AACjG;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,yEAAyE;AAC5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC1BD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACpCD,OAAO,QAAQ,GAAG,mBAAO,CAAC,gGAAgB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,6BAA6B;AACpC,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,6BAA6B;AAC5D,GAAG;AACH,OAAO,6BAA6B;AACpC,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,6BAA6B;AAC5D,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,qBAAqB,mBAAO,CAAC,kFAAuB;AACpD,4BAA4B,mBAAO,CAAC,gGAA8B;;AAElE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,qDAAqD,YAAY;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjGA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;AACA,mBAAmB,qCAAqC;AACxD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,yBAAyB,+BAA+B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,mGAAgB;AACnD,OAAO,iBAAiB,GAAG,mBAAO,CAAC,kFAAuB;;AAE1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvEA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;AACA,mBAAmB,6BAA6B;AAChD,2BAA2B,6BAA6B,IAAI,gBAAgB;;;;;;;;;;;;;;AChB5E,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA;AACA,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH,OAAO,WAAW;AAClB,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,WAAW;AAC1C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA,mBAAmB,WAAW;AAC9B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,+CAA+C,YAAY;AAC3D;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,WAAW,8BAA8B,WAAW,IAAI,gBAAgB;;;;;;;;;;;;;;ACP3F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,kGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,WAAW,8BAA8B,WAAW,IAAI,gBAAgB;;;;;;;;;;;;;;ACP3F,OAAO,0BAA0B,GAAG,mBAAO,CAAC,kGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnCA;AACA,OAAO,gEAAgE;AACvE,oBAAoB,mBAAO,CAAC,uFAAc;AAC1C,qBAAqB,mBAAO,CAAC,yFAAe;AAC5C;AACA,wBAAwB,gEAAgE;AACxF;AACA;AACA,GAAG;AACH,OAAO,gEAAgE;AACvE,oBAAoB,mBAAO,CAAC,uFAAc;AAC1C,qBAAqB,mBAAO,CAAC,yFAAe;AAC5C;AACA,wBAAwB,gEAAgE;AACxF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8EAAe;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gEAAgE;AACnF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACtBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,wFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,gEAAgE;AACnF,2BAA2B,gEAAgE;AAC3F;AACA,GAAG;;;;;;;;;;;;;;ACbH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,0FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxBA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA,GAAG;AACH,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qDAAqD;AAC7E;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA,wBAAwB,qEAAqE;AAC7F;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,sFAAc;AAC1C,qBAAqB,mBAAO,CAAC,wFAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,wFAAe;AAC3C,qBAAqB,mBAAO,CAAC,0FAAgB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1PA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,2CAA2C;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;AAC7E,gBAAgB,mBAAO,CAAC,8EAA2B;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,OAAO,uCAAuC;AAC9C;AACA;;AAEA;AACA,mDAAmD,wBAAwB;AAC3E;AACA;AACA,wCAAwC,cAAc,mBAAmB;AACzE,GAAG;;AAEH;AACA;AACA,WAAW,8BAA8B;AACzC;AACA,yEAAyE,mBAAmB;AAC5F;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,0BAA0B,mBAAO,CAAC,8FAA6B;;AAE/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3CA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACtDA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpEA,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC,mBAAmB,2CAA2C;AAC9D,kCAAkC,2CAA2C,IAAI,gBAAgB;AACjG;;;;;;;;;;;;;;ACJA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA,WAAW,MAAM;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,0BAA0B,mBAAO,CAAC,8FAA6B;AAC/D,2BAA2B,mBAAO,CAAC,sGAAiC;AACpE,OAAO,aAAa,GAAG,mBAAO,CAAC,4FAAyB;;AAExD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,iGAAkB;;AAEjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxDA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,wDAAwD;AAClF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA,wBAAwB,mBAAO,CAAC,sFAAyB;AACzD,kBAAkB,mBAAO,CAAC,uFAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,yFAAgB;AACnD,uBAAuB,mBAAO,CAAC,qGAAsB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,gBAAgB,GAAG,mBAAO,CAAC,8EAAe;AACjD,wBAAwB,mBAAO,CAAC,sFAAyB;;AAEzD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChFA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,yFAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,0BAA0B,mBAAO,CAAC,uFAAwB;;AAE1D;AACA,OAAO,UAAU;AACjB,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,UAAU;AACzC,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,2CAA2C;AAC1E,GAAG;AACH,OAAO,qDAAqD;AAC5D,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C,YAAY,mBAAmB,2CAA2C;AAC1E,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA,mBAAmB,UAAU;AAC7B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kCAAkC;AACrD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/CA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kCAAkC;AACrD,2BAA2B,kCAAkC,IAAI,gBAAgB;;;;;;;;;;;;;;ACTjF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA;AACA,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,uCAAuC;AAC9C,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,uCAAuC;AAC/D;AACA;AACA,GAAG;AACH,OAAO,wDAAwD;AAC/D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,wDAAwD;AAChF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACpBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D,2BAA2B,uCAAuC,IAAI,gBAAgB;;;;;;;;;;;;;;ACVtF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,uCAAuC;AAC1D,2BAA2B,uCAAuC,IAAI,gBAAgB;;;;;;;;;;;;;;ACVtF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wDAAwD;AAC3E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACzBD,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACVA,gBAAgB,mBAAO,CAAC,0EAAW;AACnC,OAAO,2DAA2D,GAAG,mBAAO,CAAC,0DAAc;;AAE3F;AACA,aAAa,uBAAuB,4DAA4D;AAChG;;AAEA;AACA,aAAa,OAAO;AACpB,cAAc,SAAS;AACvB,cAAc,EAAE,kBAAkB,aAAa;AAC/C;;AAEA;AACA,aAAa,6DAA6D;AAC1E;;AAEA,WAAW,mBAAmB;AAC9B;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA,WAAW;AACX;AACA;AACA,WAAW,mBAAO,CAAC,gFAAW;AAC9B,SAAS,mBAAO,CAAC,4EAAS;AAC1B,eAAe,mBAAO,CAAC,wFAAe;AACtC,YAAY,mBAAO,CAAC,kFAAY;AAChC;AACA;AACA;AACA;AACA,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,eAAe,mBAAO,CAAC,wFAAe;AACtC,oBAAoB,mBAAO,CAAC,gGAAmB;AAC/C,aAAa,mBAAO,CAAC,oFAAa;AAClC,aAAa,mBAAO,CAAC,oFAAa;AAClC,cAAc,mBAAO,CAAC,sFAAc;AACpC,aAAa,mBAAO,CAAC,oFAAa;AAClC,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C,cAAc,mBAAO,CAAC,sFAAc;AACpC,iBAAiB,mBAAO,CAAC,4FAAiB;AAC1C,eAAe,mBAAO,CAAC,wFAAe;AACtC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,iBAAiB,mBAAO,CAAC,4FAAiB;AAC1C,kBAAkB,mBAAO,CAAC,8FAAkB;AAC5C;AACA,sBAAsB,mBAAO,CAAC,sGAAsB;AACpD,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,UAAU,mBAAO,CAAC,8EAAU;AAC5B;AACA,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC,cAAc,mBAAO,CAAC,sFAAc;AACpC,cAAc,mBAAO,CAAC,sFAAc;AACpC,mBAAmB,mBAAO,CAAC,gGAAmB;AAC9C,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC;AACA;AACA,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD,oBAAoB,mBAAO,CAAC,kGAAoB;AAChD;AACA;AACA;AACA;AACA,gBAAgB,mBAAO,CAAC,0FAAgB;AACxC;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,qCAAqC;AAChD,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA,8BAA8B,gCAAgC;AAC9D;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrGA;AACA,OAAO,6CAA6C;AACpD,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,sCAAsC;AACrE,GAAG;AACH,OAAO,6CAA6C;AACpD,oBAAoB,mBAAO,CAAC,+FAAc;AAC1C,qBAAqB,mBAAO,CAAC,iGAAe;AAC5C,YAAY,mBAAmB,sCAAsC;AACrE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,yBAAyB,GAAG,mBAAO,CAAC,8EAAe;;AAE1D;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sCAAsC;AACzD;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA,kBAAkB,mBAAO,CAAC,gGAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,sCAAsC;AACzD,2BAA2B,sCAAsC,IAAI,gBAAgB;;;;;;;;;;;;;;ACTrF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,kGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB,mCAAmC;AAC5D;AACA;AACA;;AAEA;;AAEA;AACA,OAAO,kEAAkE;AACzE,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,yCAAyC;AAC/E;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;;AAE5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,sCAAsC,mCAAmC;AACzE;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtIA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kEAAkE;AACrF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;AC9BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;ACvCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AChCA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACnCA,OAAO,SAAS,GAAG,mBAAO,CAAC,6FAAgB;AAC3C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE,OAAO,2CAA2C,GAAG,mBAAO,CAAC,oEAAgB;;AAE7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,OAAO,sCAAsC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,+BAA+B,mCAAmC;AAClE;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gEAAoB;AAChE;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,oEAAgB;;AAE5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,sCAAsC;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClEA;AACA,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,oBAAoB;AAC3B,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,oBAAoB;AAC5C;AACA;AACA,GAAG;AACH,OAAO,qCAAqC;AAC5C,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C;AACA,wBAAwB,qBAAqB,4BAA4B,GAAG;AAC5E;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC,2BAA2B,oBAAoB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTnE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,oBAAoB;AACvC,2BAA2B,oBAAoB,IAAI,gBAAgB;;;;;;;;;;;;;;ACTnE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mBAAmB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,uBAAuB,mCAAmC;AAC1D;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;AAC5F,OAAO,iBAAiB,GAAG,mBAAO,CAAC,8FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,UAAU;AACV;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA;AACA;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;AACA,oBAAoB,mBAAO,CAAC,2FAAc;AAC1C,qBAAqB,mBAAO,CAAC,6FAAe;AAC5C,YAAY;AACZ,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,qBAAqB,GAAG,mBAAO,CAAC,8EAAe;;AAEtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,mBAAmB,mBAAO,CAAC,8FAAgB;;AAE3C,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA,kBAAkB,mBAAO,CAAC,4FAAe;;AAEzC;AACA;AACA;;AAEA,mDAAmD,gBAAgB;;;;;;;;;;;;;;ACNnE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1BA,wBAAwB,mBAAO,CAAC,mFAAsB;;AAEtD;AACA;;AAEA;AACA,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oBAAoB;AACnD,GAAG;AACH,OAAO,kFAAkF;AACzF,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oCAAoC;AACnE,GAAG;AACH,OAAO,kFAAkF;AACzF,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,oCAAoC;AACnE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC/BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,gDAAgD,8BAA8B;AAC9E;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,+CAA+C;AACzE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oBAAoB;AACvC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oCAAoC;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;;;;;;;;;;;;;AC/BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACnDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,oCAAoC;AACvD,2BAA2B,oCAAoC,IAAI,gBAAgB;;;;;;;;;;;;;;ACbnF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7BA;AACA,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,SAAS;AAChB,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,SAAS;AACxC,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH,OAAO,iCAAiC;AACxC,oBAAoB,mBAAO,CAAC,yFAAc;AAC1C,qBAAqB,mBAAO,CAAC,2FAAe;AAC5C,YAAY,mBAAmB,iCAAiC;AAChE,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACzCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,iBAAiB;AAC5B;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,qBAAqB;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACfD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzDA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3DA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;;AAEA,mBAAmB,SAAS,8BAA8B,SAAS,IAAI,gBAAgB;;;;;;;;;;;;;;ACPvF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7DA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,mBAAmB,GAAG,mBAAO,CAAC,8EAAe;;AAEpD;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AChBD,OAAO,mCAAmC,GAAG,mBAAO,CAAC,4FAAgB;;AAErE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC3BA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D,2BAA2B,iCAAiC,IAAI,gBAAgB;;;;;;;;;;;;;;ACThF,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,4FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/DA,kBAAkB,mBAAO,CAAC,0FAAe;;AAEzC;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,wCAAwC;AAC3D,2BAA2B,iCAAiC,IAAI,gBAAgB;;;;;;;;;;;;;;ACThF,OAAO,0BAA0B,GAAG,mBAAO,CAAC,4FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA;AACA;;AAEA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,+CAA+C;AACtD,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C,YAAY,mBAAmB,+CAA+C;AAC9E,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+EAA+E;AACtF,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,+CAA+C;AACtD,oBAAoB,mBAAO,CAAC,6FAAc;AAC1C,qBAAqB,mBAAO,CAAC,+FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1EA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,6DAA6D;AACvF;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,gGAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;AClCA,kBAAkB,mBAAO,CAAC,8FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,gGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,uBAAuB,GAAG,mBAAO,CAAC,8EAAe;;AAExD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,+CAA+C;AAClE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,qCAAqC;AAC/D;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,gGAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACdA;AACA,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH,OAAO,kBAAkB;AACzB,oBAAoB,mBAAO,CAAC,4FAAc;AAC1C,qBAAqB,mBAAO,CAAC,8FAAe;AAC5C,YAAY,mBAAmB,kBAAkB;AACjD,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AC1BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjDA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,sBAAsB,GAAG,mBAAO,CAAC,8EAAe;;AAEvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;;;;;;;;;;;;;AC3BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,+FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,kBAAkB,mBAAO,CAAC,6FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,kBAAkB;AACrC,2BAA2B,kBAAkB,IAAI,gBAAgB;;;;;;;;;;;;;;ACZjE,OAAO,0BAA0B,GAAG,mBAAO,CAAC,+FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA;AACA,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,2BAA2B;AAClC,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,2BAA2B;AAC1D,GAAG;AACH,OAAO,wCAAwC;AAC/C,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C,YAAY,mBAAmB,wCAAwC;AACvE,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH,OAAO,oFAAoF;AAC3F,oBAAoB,mBAAO,CAAC,wFAAc;AAC1C,qBAAqB,mBAAO,CAAC,0FAAe;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACrGA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;;AAEhD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM,mCAAmC;AACzC,MAAM,mCAAmC;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,QAAQ;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA,mBAAmB,2BAA2B;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,2BAA2B,sBAAsB;AACjD,iCAAiC,uCAAuC;AACxE;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrFA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;;AAEA,mBAAmB,2BAA2B;AAC9C,kCAAkC,2BAA2B,IAAI,gBAAgB;AACjF;;;;;;;;;;;;;;ACPA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACrCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,mBAAmB,mBAAO,CAAC,oFAAqB;AAChD,OAAO,qBAAqB,GAAG,mBAAO,CAAC,sGAA8B;;AAErE;AACA;;AAEA,mBAAmB,qDAAqD;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;;AAEA,iBAAiB,oBAAoB;AACrC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iDAAiD,sBAAsB;AACvE,iCAAiC,oDAAoD;;AAErF;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,eAAe,iDAAiD;AAChE,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjEA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvCA,aAAa,mBAAO,CAAC,wEAAwB;AAC7C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,kBAAkB,GAAG,mBAAO,CAAC,8EAAe;AACnD,OAAO,QAAQ,GAAG,mBAAO,CAAC,sGAA8B;AACxD,eAAe,mBAAO,CAAC,0GAAgC;AACvD,OAAO,cAAc,GAAG,mBAAO,CAAC,4FAAyB;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kCAAkC,OAAO;AACzC,gBAAgB,QAAQ;AACxB,mBAAmB,QAAQ;AAC3B,+CAA+C;AAC/C,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,2BAA2B,sDAAsD;AACjF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA,8BAA8B,sDAAsD;AACpF;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACtHA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;AACjE,gBAAgB,mBAAO,CAAC,8EAA2B;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,WAAW,YAAY;AACvB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACvDA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;AClCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,2FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1CA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,0BAA0B,GAAG,mBAAO,CAAC,2FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC/BA,kBAAkB,mBAAO,CAAC,yFAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,KAAK;AACL;;;;;;;;;;;;;;ACrCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,2FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClBA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,iGAAc;AAC1C,qBAAqB,mBAAO,CAAC,mGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,2BAA2B,GAAG,mBAAO,CAAC,8EAAe;;AAE5D;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AClBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C;AACA;AACA;AACA;AACA;AACA,CAAC,GAAG,mBAAO,CAAC,oEAAgB;;AAE5B,OAAO,uBAAuB,GAAG,mBAAO,CAAC,gEAAoB;AAC7D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC1DA,kBAAkB,mBAAO,CAAC,kGAAe;;AAEzC;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY,8BAA8B,YAAY,IAAI,gBAAgB;;;;;;;;;;;;;;ACV7F,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,iBAAiB,GAAG,mBAAO,CAAC,oGAAgB;AACnD,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;;AAE9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACpCA;AACA,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH,OAAO,YAAY;AACnB,oBAAoB,mBAAO,CAAC,8FAAc;AAC1C,qBAAqB,mBAAO,CAAC,gGAAe;AAC5C,YAAY,mBAAmB,YAAY;AAC3C,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;AChBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,wBAAwB,GAAG,mBAAO,CAAC,8EAAe;;AAEzD;AACA;AACA;AACA;;AAEA;AACA,WAAW,OAAO;AAClB;AACA,mBAAmB,YAAY;AAC/B;AACA;AACA;AACA;AACA,CAAC;;;;;;;;;;;;;;AChBD,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,+FAAe;;AAEzC,mBAAmB,YAAY,OAAO,eAAe,YAAY,kBAAkB;;;;;;;;;;;;;;ACFnF,OAAO,mCAAmC,GAAG,mBAAO,CAAC,iGAAgB;;AAErE;AACA;AACA;AACA;;;;;;;;;;;;;;ACLA;AACA,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,mDAAmD;AAC1D,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,mDAAmD;AAC3E;AACA;AACA,GAAG;AACH,OAAO,oEAAoE;AAC3E,oBAAoB,mBAAO,CAAC,0FAAc;AAC1C,qBAAqB,mBAAO,CAAC,4FAAe;AAC5C;AACA,wBAAwB,oEAAoE;AAC5F;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,6BAA6B;AAC7D;AACA;;;;;;;;;;;;;;AC5BA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0DAA0D,GAAG,mBAAO,CAAC,oEAAgB;;AAE5F;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AChCA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE,2BAA2B,mDAAmD,IAAI,gBAAgB;;;;;;;;;;;;;;ACblG,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,4BAA4B,GAAG,mBAAO,CAAC,oEAAgB;AAC9D,OAAO,iBAAiB,GAAG,mBAAO,CAAC,6FAAgB;;AAEnD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA,kBAAkB,mBAAO,CAAC,2FAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,mDAAmD;AACtE,2BAA2B,mDAAmD,IAAI,gBAAgB;;;;;;;;;;;;;;ACblG,OAAO,0BAA0B,GAAG,mBAAO,CAAC,6FAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;ACzBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,oBAAoB,GAAG,mBAAO,CAAC,8EAAe;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,gCAAgC,6BAA6B;AAC7D;AACA;;;;;;;;;;;;;;ACvCA,OAAO,gBAAgB,GAAG,mBAAO,CAAC,6FAAgB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACXA;AACA,OAAO,8DAA8D;AACrE,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C;AACA,wBAAwB,8DAA8D;AACtF;AACA;AACA,GAAG;AACH,OAAO,8DAA8D;AACrE,oBAAoB,mBAAO,CAAC,gGAAc;AAC1C,qBAAqB,mBAAO,CAAC,kGAAe;AAC5C;AACA,wBAAwB,8DAA8D;AACtF;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,cAAc,UAAU;AACxB;;;;;;;;;;;;;;ACtBA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,0BAA0B,GAAG,mBAAO,CAAC,8EAAe;;AAE3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;AAED,sBAAsB,oBAAoB;AAC1C;AACA;;AAEA,0BAA0B,8BAA8B;AACxD;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACxCA,gBAAgB,mBAAO,CAAC,wEAAkB;AAC1C,OAAO,+BAA+B,GAAG,mBAAO,CAAC,oEAAgB;;AAEjE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA,WAAW,aAAa;AACxB,gDAAgD,YAAY;AAC5D,KAAK;AACL,cAAc,uBAAuB;;AAErC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AClDA,kBAAkB,mBAAO,CAAC,iGAAe;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mBAAmB,8DAA8D;AACjF,2BAA2B,8DAA8D;AACzF;AACA,GAAG;;;;;;;;;;;;;;ACnBH,OAAO,0BAA0B,GAAG,mBAAO,CAAC,mGAAgB;;AAE5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;AC5BA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,gEAAgE;AAC7F,UAAU,sBAAsB;AAChC;AACA,kEAAkE,2DAA2D;AAC7H,0DAA0D,2CAA2C;AACrG,kGAAkG,oDAAoD;AACtJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqC,QAAQ;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC7CA,yBAAyB,mBAAO,CAAC,mFAAoB;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACRA;AACA,WAAW,mBAAO,CAAC,6EAAW;AAC9B,YAAY,mBAAO,CAAC,+EAAY;AAChC;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,yEAAyE;AAC5F;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;ACVD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,kFAAW;AAC9B,YAAY,mBAAO,CAAC,oFAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,cAAc,OAAO,EAAE,EAAE,GAAG,cAAc;AAC1C;AACA;;AAEA;AACA;;AAEA,yBAAyB,+BAA+B;AACxD,6DAA6D,sBAAsB;AACnF;AACA;AACA,aAAa,UAAU,EAAE,IAAI;AAC7B;;AAEA,wBAAwB,QAAQ,GAAG,UAAU,cAAc,uBAAuB,EAAE,IAAI,EAAE,UAAU,EAAE,UAAU;;AAEhH;AACA;AACA;AACA,KAAK;AACL;AACA;;;;;;;;;;;;;;AC7DA;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA;AACA,WAAW,mBAAO,CAAC,4EAAW;AAC9B,YAAY,mBAAO,CAAC,8EAAY;AAChC;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,qEAAe;;AAEvC;;AAEA,mBAAmB,mDAAmD;AACtE;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;;;;;;;;;;;;;;AC3BD;AACA;AACA;AACA;;;;;;;;;;;;;;ACHA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,eAAe;AAClC;AACA,CAAC;;;;;;;;;;;;;;ACJD,+IAAoD;;;;;;;;;;;;;;ACApD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C,mBAAmB,qBAAqB;AACxC;AACA,CAAC;;;;;;;;;;;;;;ACrBD,qCAAqC,2BAA2B;;AAEhE,gBAAgB,mBAAO,CAAC,wEAAkB;;AAE1C;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,gCAAgC,+BAA+B,KAAK;;AAEpE,YAAY;AACZ,GAAG;AACH;;;;;;;;;;;;;;ACtBA;AACA;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;AACA,aAAa,mBAAO,CAAC,sGAAwB;AAC7C,cAAc,mBAAO,CAAC,wGAAyB;AAC/C,GAAG;AACH;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;;;;;;;;;;;;ACdA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA,OAAO,2DAA2D,GAAG,mBAAO,CAAC,uDAAW;;AAExF,mBAAmB,aAAoB;AACvC,mCAAmC,mBAAO,CAAC,0EAAiB,IAAI,mBAAO,CAAC,gEAAY;;AAEpF;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS,4CAA4C;;AAErD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,0DAA0D,wBAAwB;AAClF;AACA,SAAS;AACT;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA,aAAa,4GAA4G;AACzH;;AAEA;AACA,WAAW,mCAAmC;AAC9C,aAAa;AACb;AACA,2BAA2B;AAC3B;AACA,oCAAoC;AACpC;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;AC1EA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACbA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,oBAAoB;AAC/B,aAAa;AACb;AACA;AACA;AACA;AACA,gBAAgB,gBAAgB;AAChC;;AAEA;AACA,mEAAmE;AACnE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,6BAA6B,iDAAiD;;AAE9E,0DAA0D,gBAAgB;AAC1E;AACA;AACA;AACA,eAAe,UAAU;AACzB;AACA,OAAO;AACP;AACA,eAAe,SAAS;AACxB;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;AC1DA,OAAO,2BAA2B,GAAG,mBAAO,CAAC,uDAAW;;AAExD;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB,KAAK;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;;AAEA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,gBAAgB;AAC3B,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACXA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACTA,OAAO,SAAS,GAAG,mBAAO,CAAC,kBAAM;AACjC,OAAO,qBAAqB,GAAG,mBAAO,CAAC,uDAAW;;AAElD;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,eAAe,8BAA8B,KAAK;AAClD;AACA,kEAAkE,eAAe;AACjF;;AAEA;AACA;AACA;AACA;AACA;AACA,8BAA8B,eAAe,KAAK,YAAY;AAC9D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;AC9DA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,EAAE;AACf,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,aAAa,0BAA0B;AACvC,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,aAAa,OAAO;AACpB,eAAe;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,cAAc;AAC3B,eAAe,MAAM;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,qBAAqB;AAClC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,aAAa,oBAAoB;AACjC,eAAe;AACf;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,MAAM;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;;AAEA;AACA;AACA,eAAe,OAAO;AACtB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;;AAEA;;;;;;;;;;;;;;ACtVA;AACA;AACA,WAAW,+BAA+B;AAC1C;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACjBA;AACA,WAAW,IAAI;AACf;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,+BAA+B,OAAO;AACtC;AACA;AACA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACxBA;AACA;AACA;AACA,GAAG;;;;;;;;;;;;;;ACHH,OAAO,OAAO;AACd;AACA,yCAAyC,gCAAgC,KAAK;;;;;;;;;;;;;;ACF9E,cAAc,mBAAO,CAAC,0DAAS;AAC/B,OAAO,iBAAiB,GAAG,mBAAO,CAAC,uDAAW;;AAE9C;AACA;AACA,GAAG,iFAAiF;AACpF;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;;;;;;;;;;;;;AC5CA;;AAEA,oCAAoC,SAAS,GAAG,KAAK,EAAE,uBAAuB;;;;;;;;;;;;;;ACF9E,aAAa,mBAAO,CAAC,sDAAY;AACjC,YAAY,mBAAO,CAAC,oBAAO;AAC3B,aAAa,mBAAO,CAAC,8CAAQ;AAC7B,aAAa,mBAAO,CAAC,sBAAQ;AAC7B,SAAS,mBAAO,CAAC,cAAI;;AAErB;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,YAAY;AACZ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;;AAEH;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC;;AAEnC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qCAAqC,6BAA6B;AAClE,+BAA+B;;AAE/B;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,mBAAmB,mBAAmB;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,iBAAiB,kBAAkB;AACnC;AACA,mBAAmB,gBAAgB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,kBAAkB;AACnC;AACA,mBAAmB,gBAAgB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B;AAC1B;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC5M2B;AAC0B;AACrD;AACA;AACA;AACA;AACA;AACA,IAAI,4DAAqB;AACzB;AACA,GAAG;AACH,IAAI,4DAAqB;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC,gBAAgB;AACjD,UAAU,+DAAW;AACrB;AACA;AACA;AACoE;;;;;;;;;;;;;;;;;;;;AC5CpE;AACA;AACsB;;;;;;;;;;;;;;ACFtB;AACA;AACA,SAAS,mBAAO,CAAC,6EAAa;AAC9B,iBAAiB,mBAAO,CAAC,6FAAqB;AAC9C,eAAe,mBAAO,CAAC,qGAAyB;AAChD,qBAAqB,mBAAO,CAAC,qGAAyB;AACtD,qBAAqB,mBAAO,CAAC,qGAAyB;AACtD,UAAU,mBAAO,CAAC,+EAAc;AAChC,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,mGAAwB;AAC1C,aAAa,mBAAO,CAAC,yGAA2B;AAChD,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,+GAA8B;AAChD,cAAc,mBAAO,CAAC,uHAAkC;AACxD,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,uHAAkC;AACpD,cAAc,mBAAO,CAAC,+HAAsC;AAC5D,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,UAAU,mBAAO,CAAC,qGAAyB;AAC3C,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,yHAAmC;AACrD,aAAa,mBAAO,CAAC,+HAAsC;AAC3D,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,6GAA6B;AAC/C,EAAE;AACF;AACA,UAAU,mBAAO,CAAC,qJAAiD;AACnE,cAAc,mBAAO,CAAC,6JAAqD;AAC3E,EAAE;AACF;;;;;;;;;;;;;;ACxCA,sIAAuC,C;;;;;;;;;;;;;;ACA1B;;AAEb,YAAY,mBAAO,CAAC,+FAAY;AAChC,aAAa,mBAAO,CAAC,2GAAkB;AACvC,oBAAoB,mBAAO,CAAC,uHAAuB;AACnD,eAAe,mBAAO,CAAC,qHAAuB;AAC9C,WAAW,mBAAO,CAAC,kBAAM;AACzB,YAAY,mBAAO,CAAC,oBAAO;AAC3B,iBAAiB,4FAAgC;AACjD,kBAAkB,6FAAiC;AACnD,UAAU,mBAAO,CAAC,gBAAK;AACvB,WAAW,mBAAO,CAAC,kBAAM;AACzB,cAAc,kIAAgC;AAC9C,kBAAkB,mBAAO,CAAC,mHAAqB;AAC/C,mBAAmB,mBAAO,CAAC,qHAAsB;AACjD,2BAA2B,mBAAO,CAAC,6HAA0B;AAC7D,aAAa,mBAAO,CAAC,6GAAkB;;AAEvC;;AAEA;AACA;AACA,WAAW,uBAAuB;AAClC,WAAW,iBAAiB;AAC5B,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,eAAe,mDAAmD;AAClE;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW;;AAEX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA,SAAS;AACT;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACnZa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;AAChC,aAAa,mBAAO,CAAC,2GAAkB;AACvC,cAAc,mBAAO,CAAC,mHAAsB;AAC5C,eAAe,mBAAO,CAAC,qHAAuB;AAC9C,oBAAoB,mBAAO,CAAC,uHAAuB;AACnD,mBAAmB,mBAAO,CAAC,6HAA2B;AACtD,sBAAsB,mBAAO,CAAC,mIAA8B;AAC5D,kBAAkB,mBAAO,CAAC,mHAAqB;AAC/C,2BAA2B,mBAAO,CAAC,6HAA0B;AAC7D,aAAa,mBAAO,CAAC,6GAAkB;;AAEvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;;AAEP;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACnNa;;AAEb,YAAY,mBAAO,CAAC,4FAAS;AAC7B,WAAW,mBAAO,CAAC,0GAAgB;AACnC,YAAY,mBAAO,CAAC,sGAAc;AAClC,kBAAkB,mBAAO,CAAC,kHAAoB;AAC9C,eAAe,mBAAO,CAAC,wGAAY;;AAEnC;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,MAAM;AAClB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,eAAe,mBAAO,CAAC,4GAAiB;AACxC,oBAAoB,mBAAO,CAAC,sHAAsB;AAClD,iBAAiB,mBAAO,CAAC,gHAAmB;AAC5C,gBAAgB,+HAA6B;;AAE7C;AACA;AACA;AACA;AACA,eAAe,mBAAO,CAAC,8GAAkB;;AAEzC;AACA,qBAAqB,mBAAO,CAAC,0HAAwB;;AAErD;;AAEA;AACA,sBAAsB;;;;;;;;;;;;;;;ACxDT;;AAEb;AACA;AACA;AACA;AACA,WAAW,QAAQ;AACnB;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;;;;;;;;;;;;;;AClBa;;AAEb,aAAa,mBAAO,CAAC,qGAAU;;AAE/B;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,eAAe,OAAO;AACtB;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;;ACtHa;;AAEb;AACA;AACA;;;;;;;;;;;;;;;ACJa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;AAChC,eAAe,mBAAO,CAAC,mHAAqB;AAC5C,yBAAyB,mBAAO,CAAC,2HAAsB;AACvD,sBAAsB,mBAAO,CAAC,qHAAmB;AACjD,kBAAkB,mBAAO,CAAC,6GAAe;AACzC,gBAAgB,mBAAO,CAAC,qHAAsB;;AAE9C;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA,yBAAyB;AACzB,KAAK;AACL;AACA,CAAC;;AAED;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,KAAK;AACL;AACA,CAAC;;AAED;;;;;;;;;;;;;;;ACnJa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB;AACA,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;;;;;;;;;;;;;;ACrDa;;AAEb,oBAAoB,mBAAO,CAAC,6HAA0B;AACtD,kBAAkB,mBAAO,CAAC,yHAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACnBa;;AAEb,mBAAmB,mBAAO,CAAC,+GAAgB;;AAE3C;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACjBa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;AAChC,oBAAoB,mBAAO,CAAC,iHAAiB;AAC7C,eAAe,mBAAO,CAAC,iHAAoB;AAC3C,eAAe,mBAAO,CAAC,yGAAa;AACpC,aAAa,mBAAO,CAAC,6GAAkB;;AAEvC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,+BAA+B;AAC/B,uCAAuC;AACvC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACtFa;;AAEb;AACA;AACA;AACA,WAAW,MAAM;AACjB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,MAAM;AACnB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC1Ca;;AAEb,YAAY,mBAAO,CAAC,6FAAU;;AAE9B;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL,2BAA2B;AAC3B,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;AClGa;;AAEb,kBAAkB,mBAAO,CAAC,6GAAe;;AAEzC;AACA;AACA;AACA,WAAW,SAAS;AACpB,WAAW,SAAS;AACpB,WAAW,OAAO;AAClB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACxBa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;AAChC,eAAe,mBAAO,CAAC,yGAAa;;AAEpC;AACA;AACA;AACA,WAAW,cAAc;AACzB,WAAW,MAAM;AACjB,WAAW,eAAe;AAC1B,aAAa,EAAE;AACf;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACrBa;;AAEb,YAAY,mBAAO,CAAC,6FAAU;AAC9B,0BAA0B,mBAAO,CAAC,yIAAgC;AAClE,mBAAmB,mBAAO,CAAC,qHAAsB;AACjD,2BAA2B,mBAAO,CAAC,mHAAgB;;AAEnD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc,mBAAO,CAAC,2GAAiB;AACvC,GAAG;AACH;AACA,cAAc,mBAAO,CAAC,6GAAkB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wEAAwE;AACxE;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA,CAAC;;AAED;;;;;;;;;;;;;;;AClIa;;AAEb;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;ACNA;AACA;AACA,E;;;;;;;;;;;;;;ACFa;;AAEb;AACA;AACA;AACA,mBAAmB,iBAAiB;AACpC;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACVa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;;;;;;;;;;;;;;ACrEa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0CAA0C;AAC1C,SAAS;;AAET;AACA,4DAA4D,wBAAwB;AACpF;AACA,SAAS;;AAET;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,kCAAkC;AAClC,+BAA+B,aAAa,EAAE;AAC9C;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACba;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;AACA;AACA,WAAW,EAAE;AACb,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;;;;;;;;;;;;;;ACZa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB;AAChB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,cAAc,OAAO;AACrB,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;;;;;;;;;;;;;;ACnEa;;AAEb,YAAY,mBAAO,CAAC,6FAAU;;AAE9B;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;;ACXa;;AAEb,YAAY,mBAAO,CAAC,+FAAY;;AAEhC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,eAAe;;AAEhC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,GAAG;;AAEH;AACA;;;;;;;;;;;;;;;ACpDa;;AAEb;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA,WAAW,SAAS;AACpB,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC1Ba;;AAEb,cAAc,gIAA8B;;AAE5C;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;;AAEA;AACA;AACA,WAAW,kBAAkB;AAC7B,WAAW,QAAQ;AACnB,WAAW,QAAQ;AACnB,aAAa;AACb;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,SAAS;AACpB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACjFa;;AAEb,WAAW,mBAAO,CAAC,0GAAgB;;AAEnC;;AAEA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,QAAQ;AACpB;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,QAAQ;AACrB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB,WAAW,SAAS;AACpB;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,OAAO;AAC1C;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB,SAAS,GAAG,SAAS;AAC5C,2BAA2B;AAC3B;AACA;AACA,WAAW,OAAO;AAClB,aAAa,OAAO;AACpB;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,4BAA4B;AAC5B,KAAK;AACL;AACA,KAAK;AACL;AACA;AACA;;AAEA,uCAAuC,OAAO;AAC9C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA,WAAW,OAAO;AAClB,YAAY,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC5VA;AACA;AACA;AACA;;AAEA;AACA,yDAAyD,8GAAkC;AAC3F;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,6B;;;;;;;;;;;;;ACfA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;ACjBA,uBAAuB,+GAAkC;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,uB;;;;;;;;;;;;;AChDA,mBAAmB,mBAAO,CAAC,mFAAc;AACzC,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,oBAAoB,mBAAO,CAAC,qFAAe;AAC3C,0BAA0B,mBAAO,CAAC,iGAAqB;AACvD,0BAA0B,mBAAO,CAAC,iGAAqB;AACvD,2BAA2B,mBAAO,CAAC,mGAAsB;AACzD,yBAAyB,mBAAO,CAAC,+FAAoB;AACrD,sBAAsB,mBAAO,CAAC,yFAAiB;AAC/C,4BAA4B,oHAAuC;;AAEnE;AACA,uBAAuB,mBAAO,CAAC,+FAAoB;AACnD,wBAAwB,mBAAO,CAAC,iGAAqB;AACrD,6BAA6B,mBAAO,CAAC,2GAA0B;AAC/D,gCAAgC,mBAAO,CAAC,mHAA8B;AACtE,wBAAwB,mBAAO,CAAC,iGAAqB;AACrD,kCAAkC,mBAAO,CAAC,qHAA+B;AACzE,2BAA2B,mBAAO,CAAC,yGAAyB;AAC5D,+CAA+C,mBAAO,CAAC,iJAA6C;;AAEpG;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B;AAC9B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,+B;;;;;;;;;;;;;AC7MA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,oC;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;AC3FA,iBAAiB,mBAAO,CAAC,+EAAY;AACrC,cAAc,mBAAO,CAAC,+DAAO;AAC7B,mBAAmB,mBAAO,CAAC,wDAAa;;AAExC;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,qBAAqB,sCAAsC;AAC3D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,4B;;;;;;;;;;;;;ACzEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;ACfA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;ACnBA;AACA;AACA;AACA;AACA;AACA,qCAAqC;AACrC;;AAEA;AACA;AACA;;AAEA,yB;;;;;;;;;;;;;ACZA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0B;;;;;;;;;;;;;ACRA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,mC;;;;;;;;;;;;;ACZA,iCAAiC,yHAA4C;AAC7E,0BAA0B,mBAAO,CAAC,iGAAqB;;AAEvD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA,+B;;;;;;;;;;;;;ACzBA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,mC;;;;;;;;;;;;;ACZA,eAAe,mBAAO,CAAC,2EAAU;;AAEjC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL,GAAG;AACH;AACA;;AAEA,kC;;;;;;;;;;;;;ACxDA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,mBAAmB,mBAAO,CAAC,sHAAc;;AAEzC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACzCA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACbA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4B;;;;;;;;;;;;;ACVA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;AChJA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,eAAe,mBAAO,CAAC,4EAAW;AAClC,kBAAkB,mBAAO,CAAC,sGAAa;AACvC,uBAAuB,mBAAO,CAAC,sGAAwB;AACvD,6BAA6B,+IAAqD;;AAElF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACzCA,iCAAiC,0HAA6C;AAC9E;AACA;AACA;AACA;AACA;AACA;AACA;;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACpFA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,mBAAmB,mBAAO,CAAC,mGAAc;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C;AAC3C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACvDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4B;;;;;;;;;;;;;ACZA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,mBAAmB,mBAAO,CAAC,uGAAc;;AAEzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,mEAAmE;AACnE,mEAAmE;AACnE,wEAAwE;AACxE,0DAA0D;AAC1D,wDAAwD;AACxD,wDAAwD;AACxD,6DAA6D;AAC7D;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC5DA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC5BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4B;;;;;;;;;;;;;ACdA,kBAAkB,mBAAO,CAAC,sGAAwB;;AAElD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yB;;;;;;;;;;;;;AChBA,eAAe,mBAAO,CAAC,4EAAW;AAClC,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,eAAe,mBAAO,CAAC,sFAAU;;AAEjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACzCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACvBA,gBAAgB,mBAAO,CAAC,wFAAW;;AAEnC;AACA;AACA;AACA;AACA,cAAc,gBAAgB;AAC9B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACpBA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,iBAAiB,mBAAO,CAAC,8FAAY;AACrC,uBAAuB,mBAAO,CAAC,sGAAwB;AACvD,6BAA6B,wIAA8C;AAC3E,OAAO,qBAAqB,GAAG,mBAAO,CAAC,+EAAc;;AAErD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACvCA,iBAAiB,mBAAO,CAAC,8FAAY;;AAErC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACfA,eAAe,mBAAO,CAAC,0FAAU;;AAEjC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;;;;;;;;;;;;;;AChBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AClCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2B;;;;;;;;;;;;;ACvFA,kBAAkB,mBAAO,CAAC,2FAAa;AACvC,eAAe,mBAAO,CAAC,qFAAU;AACjC,cAAc,mBAAO,CAAC,0EAAU;AAChC,6BAA6B,sHAAyC;AACtE,kBAAkB,mBAAO,CAAC,4FAAmB;AAC7C,6BAA6B,oIAA0C;;AAEvE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;AC1CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;;;;;;;;;;;;;ACvBA,eAAe,mBAAO,CAAC,sFAAU;AACjC,eAAe,mBAAO,CAAC,sFAAU;AACjC,cAAc,mBAAO,CAAC,0EAAU;AAChC,6BAA6B,sHAAyC;AACtE,kBAAkB,mBAAO,CAAC,4FAAmB;AAC7C,6BAA6B,qIAA2C;;AAExE;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACtCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wB;;;;;;;;;;;;;ACfA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA,wB;;;;;;;;;;;;;AC5CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;ACrCA,sBAAsB,mBAAO,CAAC,0FAAkB;;AAEhD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,E;;;;;;;;;;;;;AC/CA,kBAAkB,mBAAO,CAAC,kFAAc;;AAExC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;;;;;;;;;;;;;ACVA,gBAAgB,mBAAO,CAAC,8EAAY;AACpC,eAAe,mBAAO,CAAC,4EAAW;AAClC,uBAAuB,mBAAO,CAAC,sGAAwB;;AAEvD;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,IAAI;AACJ;AACA,EAAE;;AAEF;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA;;;;;;;;;;;;;;;ACtCY;;AAEZ;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB;AACxB;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;;;;;;;;;;;;;ACtDA;AACA;AACA;AACA,EAAE;AACF;AACA;AACA,EAAE;AACF;AACA;AACA,EAAE;AACF;AACA;AACA,EAAE;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC,E;;;;;;;;;;;;;ACtCD,mC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,kC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,+B;;;;;;;;;;;;;ACAA,iC;;;;;;;;;;;;;ACAA,kC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,+B;;;;;;;;;;;;;ACAA,iC;;;;;;;;;;;;;ACAA,mC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,gC;;;;;;;;;;;;;ACAA,iC;;;;;;;;;;;;;ACAA,iC;;;;;;UCAA;UACA;;UAEA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;UACA;;UAEA;UACA;;UAEA;UACA;UACA;;UAEA;UACA;;;;;WCxBA;WACA;WACA;WACA;WACA;WACA,gCAAgC,YAAY;WAC5C;WACA,E;;;;;WCPA;WACA;WACA;WACA;WACA,wCAAwC,yCAAyC;WACjF;WACA;WACA,E;;;;;WCPA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF,E;;;;;WCRA;WACA;WACA;WACA;WACA,E;;;;;WCJA,sF;;;;;WCAA;WACA;WACA;WACA,sDAAsD,kBAAkB;WACxE;WACA,+CAA+C,cAAc;WAC7D,E;;;;;WCNA,iD;;;;;WCAA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,sGAAsG;WACtG;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,GAAG,aAAa,kBAAkB;WAClC;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,E;;;;;WC1CA;WACA;WACA,WAAW,6BAA6B,iBAAiB,GAAG,qEAAqE;WACjI;WACA;WACA;WACA,qCAAqC,aAAa,EAAE,wDAAwD,2BAA2B,4BAA4B,2BAA2B,+CAA+C,mCAAmC;WAChR;WACA;WACA;WACA,+BAA+B,eAAe,oBAAoB,sDAAsD,gBAAgB,eAAe,KAAK,6DAA6D,SAAS,SAAS,QAAQ,eAAe,KAAK,eAAe,qGAAqG,WAAW,aAAa;WACnZ;WACA;WACA;WACA,gBAAgB,8BAA8B,qBAAqB,YAAY,sBAAsB,SAAS,iDAAiD,6FAA6F,WAAW,uBAAuB,2BAA2B,wBAAwB,KAAK,oCAAoC,oBAAoB,wBAAwB,oBAAoB,SAAS,KAAK,yBAAyB,KAAK,gCAAgC,yBAAyB,QAAQ,eAAe,KAAK,eAAe,4DAA4D;WACtoB;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA,EAAE;WACF;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA,CAAC;WACD;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,CAAC;WACD;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,KAAK;WACL,IAAI,WAAW,YAAY;WAC3B,GAAG;WACH;WACA,C;;;;;;WCpLA,OAAO,UAAU;WACjB;WACA;WACA;;WAEA,6BAA6B,cAAc;;WAE3C;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,qBAAqB,MAAM,EAAE,KAAK,WAAW,QAAQ,MAAM,OAAO;WAClE;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA;WACA,uBAAuB,MAAM,EAAE,KAAK,YAAY,IAAI;WACpD;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA,OAAO;WACP,GAAG;WACH;;WAEA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,OAAO;WACP;WACA;WACA;WACA,SAAS;WACT;WACA;WACA;WACA,OAAO;WACP,KAAK;WACL;WACA;WACA,KAAK;WACL;WACA,GAAG;WACH;;WAEA;WACA;WACA;WACA;WACA;;WAEA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,eAAe,qBAAqB;WACpC;WACA;WACA;WACA;WACA,WAAW,sBAAsB;WACjC;WACA;;WAEA;WACA,gEAAgE;;WAEhE;WACA,+BAA+B;WAC/B;WACA;WACA;WACA,GAAG;WACH,aAAa;WACb;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA;WACA,2FAA2F,kBAAkB;WAC7G;WACA,OAAO;WACP;WACA,OAAO;WACP,KAAK;WACL;WACA,IAAI;WACJ;WACA;WACA;;WAEA;;WAEA;;WAEA,kB;;;;UCzIA;UACA;UACA;UACA","file":"remoteEntry.js","sourcesContent":["// GENERATED FILE. DO NOT EDIT.\nvar ipCodec = (function(exports) {\n  \"use strict\";\n  \n  Object.defineProperty(exports, \"__esModule\", {\n    value: true\n  });\n  exports.decode = decode;\n  exports.encode = encode;\n  exports.familyOf = familyOf;\n  exports.name = void 0;\n  exports.sizeOf = sizeOf;\n  exports.v6 = exports.v4 = void 0;\n  const v4Regex = /^(\\d{1,3}\\.){3,3}\\d{1,3}$/;\n  const v4Size = 4;\n  const v6Regex = /^(::)?(((\\d{1,3}\\.){3}(\\d{1,3}){1})?([0-9a-f]){0,4}:{0,2}){1,8}(::)?$/i;\n  const v6Size = 16;\n  const v4 = {\n    name: 'v4',\n    size: v4Size,\n    isFormat: ip => v4Regex.test(ip),\n  \n    encode(ip, buff, offset) {\n      offset = ~~offset;\n      buff = buff || new Uint8Array(offset + v4Size);\n      const max = ip.length;\n      let n = 0;\n  \n      for (let i = 0; i < max;) {\n        const c = ip.charCodeAt(i++);\n  \n        if (c === 46) {\n          // \".\"\n          buff[offset++] = n;\n          n = 0;\n        } else {\n          n = n * 10 + (c - 48);\n        }\n      }\n  \n      buff[offset] = n;\n      return buff;\n    },\n  \n    decode(buff, offset) {\n      offset = ~~offset;\n      return `${buff[offset++]}.${buff[offset++]}.${buff[offset++]}.${buff[offset]}`;\n    }\n  \n  };\n  exports.v4 = v4;\n  const v6 = {\n    name: 'v6',\n    size: v6Size,\n    isFormat: ip => ip.length > 0 && v6Regex.test(ip),\n  \n    encode(ip, buff, offset) {\n      offset = ~~offset;\n      let end = offset + v6Size;\n      let fill = -1;\n      let hexN = 0;\n      let decN = 0;\n      let prevColon = true;\n      let useDec = false;\n      buff = buff || new Uint8Array(offset + v6Size); // Note: This algorithm needs to check if the offset\n      // could exceed the buffer boundaries as it supports\n      // non-standard compliant encodings that may go beyond\n      // the boundary limits. if (offset < end) checks should\n      // not be necessary...\n  \n      for (let i = 0; i < ip.length; i++) {\n        let c = ip.charCodeAt(i);\n  \n        if (c === 58) {\n          // :\n          if (prevColon) {\n            if (fill !== -1) {\n              // Not Standard! (standard doesn't allow multiple ::)\n              // We need to treat\n              if (offset < end) buff[offset] = 0;\n              if (offset < end - 1) buff[offset + 1] = 0;\n              offset += 2;\n            } else if (offset < end) {\n              // :: in the middle\n              fill = offset;\n            }\n          } else {\n            // : ends the previous number\n            if (useDec === true) {\n              // Non-standard! (ipv4 should be at end only)\n              // A ipv4 address should not be found anywhere else but at\n              // the end. This codec also support putting characters\n              // after the ipv4 address..\n              if (offset < end) buff[offset] = decN;\n              offset++;\n            } else {\n              if (offset < end) buff[offset] = hexN >> 8;\n              if (offset < end - 1) buff[offset + 1] = hexN & 0xff;\n              offset += 2;\n            }\n  \n            hexN = 0;\n            decN = 0;\n          }\n  \n          prevColon = true;\n          useDec = false;\n        } else if (c === 46) {\n          // . indicates IPV4 notation\n          if (offset < end) buff[offset] = decN;\n          offset++;\n          decN = 0;\n          hexN = 0;\n          prevColon = false;\n          useDec = true;\n        } else {\n          prevColon = false;\n  \n          if (c >= 97) {\n            c -= 87; // a-f ... 97~102 -87 => 10~15\n          } else if (c >= 65) {\n            c -= 55; // A-F ... 65~70 -55 => 10~15\n          } else {\n            c -= 48; // 0-9 ... starting from charCode 48\n  \n            decN = decN * 10 + c;\n          } // We don't know yet if its a dec or hex number\n  \n  \n          hexN = (hexN << 4) + c;\n        }\n      }\n  \n      if (prevColon === false) {\n        // Commiting last number\n        if (useDec === true) {\n          if (offset < end) buff[offset] = decN;\n          offset++;\n        } else {\n          if (offset < end) buff[offset] = hexN >> 8;\n          if (offset < end - 1) buff[offset + 1] = hexN & 0xff;\n          offset += 2;\n        }\n      } else if (fill === 0) {\n        // Not Standard! (standard doesn't allow multiple ::)\n        // This means that a : was found at the start AND end which means the\n        // end needs to be treated as 0 entry...\n        if (offset < end) buff[offset] = 0;\n        if (offset < end - 1) buff[offset + 1] = 0;\n        offset += 2;\n      } else if (fill !== -1) {\n        // Non-standard! (standard doens't allow multiple ::)\n        // Here we find that there has been a :: somewhere in the middle\n        // and the end. To treat the end with priority we need to move all\n        // written data two bytes to the right.\n        offset += 2;\n  \n        for (let i = Math.min(offset - 1, end - 1); i >= fill + 2; i--) {\n          buff[i] = buff[i - 2];\n        }\n  \n        buff[fill] = 0;\n        buff[fill + 1] = 0;\n        fill = offset;\n      }\n  \n      if (fill !== offset && fill !== -1) {\n        // Move the written numbers to the end while filling the everything\n        // \"fill\" to the bytes with zeros.\n        if (offset > end - 2) {\n          // Non Standard support, when the cursor exceeds bounds.\n          offset = end - 2;\n        }\n  \n        while (end > fill) {\n          buff[--end] = offset < end && offset > fill ? buff[--offset] : 0;\n        }\n      } else {\n        // Fill the rest with zeros\n        while (offset < end) {\n          buff[offset++] = 0;\n        }\n      }\n  \n      return buff;\n    },\n  \n    decode(buff, offset) {\n      offset = ~~offset;\n      let result = '';\n  \n      for (let i = 0; i < v6Size; i += 2) {\n        if (i !== 0) {\n          result += ':';\n        }\n  \n        result += (buff[offset + i] << 8 | buff[offset + i + 1]).toString(16);\n      }\n  \n      return result.replace(/(^|:)0(:0)*:0(:|$)/, '$1::$3').replace(/:{3,4}/, '::');\n    }\n  \n  };\n  exports.v6 = v6;\n  const name = 'ip';\n  exports.name = name;\n  \n  function sizeOf(ip) {\n    if (v4.isFormat(ip)) return v4.size;\n    if (v6.isFormat(ip)) return v6.size;\n    throw Error(`Invalid ip address: ${ip}`);\n  }\n  \n  function familyOf(string) {\n    return sizeOf(string) === v4.size ? 1 : 2;\n  }\n  \n  function encode(ip, buff, offset) {\n    offset = ~~offset;\n    const size = sizeOf(ip);\n  \n    if (typeof buff === 'function') {\n      buff = buff(offset + size);\n    }\n  \n    if (size === v4.size) {\n      return v4.encode(ip, buff, offset);\n    }\n  \n    return v6.encode(ip, buff, offset);\n  }\n  \n  function decode(buff, offset, length) {\n    offset = ~~offset;\n    length = length || buff.length - offset;\n  \n    if (length === v4.size) {\n      return v4.decode(buff, offset, length);\n    }\n  \n    if (length === v6.size) {\n      return v6.decode(buff, offset, length);\n    }\n  \n    throw Error(`Invalid buffer size needs to be ${v4.size} for v4 or ${v6.size} for v6.`);\n  }\n  return \"default\" in exports ? exports.default : exports;\n})({});\nif (typeof define === 'function' && define.amd) define([], function() { return ipCodec; });\nelse if (typeof module === 'object' && typeof exports==='object') module.exports = ipCodec;\n","module.exports = require('./lib/index').default;","'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar _typeof = typeof Symbol === \"function\" && typeof Symbol.iterator === \"symbol\" ? function (obj) { return typeof obj; } : function (obj) { return obj && typeof Symbol === \"function\" && obj.constructor === Symbol && obj !== Symbol.prototype ? \"symbol\" : typeof obj; };\n\nexports.isNetworkError = isNetworkError;\nexports.isRetryableError = isRetryableError;\nexports.isSafeRequestError = isSafeRequestError;\nexports.isIdempotentRequestError = isIdempotentRequestError;\nexports.isNetworkOrIdempotentRequestError = isNetworkOrIdempotentRequestError;\nexports.exponentialDelay = exponentialDelay;\nexports.default = axiosRetry;\n\nvar _isRetryAllowed = require('is-retry-allowed');\n\nvar _isRetryAllowed2 = _interopRequireDefault(_isRetryAllowed);\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nvar namespace = 'axios-retry';\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isNetworkError(error) {\n  return !error.response && Boolean(error.code) && // Prevents retrying cancelled requests\n  error.code !== 'ECONNABORTED' && // Prevents retrying timed out requests\n  (0, _isRetryAllowed2.default)(error); // Prevents retrying unsafe errors\n}\n\nvar SAFE_HTTP_METHODS = ['get', 'head', 'options'];\nvar IDEMPOTENT_HTTP_METHODS = SAFE_HTTP_METHODS.concat(['put', 'delete']);\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isRetryableError(error) {\n  return error.code !== 'ECONNABORTED' && (!error.response || error.response.status >= 500 && error.response.status <= 599);\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isSafeRequestError(error) {\n  if (!error.config) {\n    // Cannot determine if the request can be retried\n    return false;\n  }\n\n  return isRetryableError(error) && SAFE_HTTP_METHODS.indexOf(error.config.method) !== -1;\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean}\n */\nfunction isIdempotentRequestError(error) {\n  if (!error.config) {\n    // Cannot determine if the request can be retried\n    return false;\n  }\n\n  return isRetryableError(error) && IDEMPOTENT_HTTP_METHODS.indexOf(error.config.method) !== -1;\n}\n\n/**\n * @param  {Error}  error\n * @return {boolean | Promise}\n */\nfunction isNetworkOrIdempotentRequestError(error) {\n  return isNetworkError(error) || isIdempotentRequestError(error);\n}\n\n/**\n * @return {number} - delay in milliseconds, always 0\n */\nfunction noDelay() {\n  return 0;\n}\n\n/**\n * @param  {number} [retryNumber=0]\n * @return {number} - delay in milliseconds\n */\nfunction exponentialDelay() {\n  var retryNumber = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n\n  var delay = Math.pow(2, retryNumber) * 100;\n  var randomSum = delay * 0.2 * Math.random(); // 0-20% of the delay\n  return delay + randomSum;\n}\n\n/**\n * Initializes and returns the retry state for the given request/config\n * @param  {AxiosRequestConfig} config\n * @return {Object}\n */\nfunction getCurrentState(config) {\n  var currentState = config[namespace] || {};\n  currentState.retryCount = currentState.retryCount || 0;\n  config[namespace] = currentState;\n  return currentState;\n}\n\n/**\n * Returns the axios-retry options for the current request\n * @param  {AxiosRequestConfig} config\n * @param  {AxiosRetryConfig} defaultOptions\n * @return {AxiosRetryConfig}\n */\nfunction getRequestOptions(config, defaultOptions) {\n  return Object.assign({}, defaultOptions, config[namespace]);\n}\n\n/**\n * @param  {Axios} axios\n * @param  {AxiosRequestConfig} config\n */\nfunction fixConfig(axios, config) {\n  if (axios.defaults.agent === config.agent) {\n    delete config.agent;\n  }\n  if (axios.defaults.httpAgent === config.httpAgent) {\n    delete config.httpAgent;\n  }\n  if (axios.defaults.httpsAgent === config.httpsAgent) {\n    delete config.httpsAgent;\n  }\n}\n\n/**\n * Checks retryCondition if request can be retried. Handles it's retruning value or Promise.\n * @param  {number} retries\n * @param  {Function} retryCondition\n * @param  {Object} currentState\n * @param  {Error} error\n * @return {boolean}\n */\nasync function shouldRetry(retries, retryCondition, currentState, error) {\n  var shouldRetryOrPromise = currentState.retryCount < retries && retryCondition(error);\n\n  // This could be a promise\n  if ((typeof shouldRetryOrPromise === 'undefined' ? 'undefined' : _typeof(shouldRetryOrPromise)) === 'object') {\n    try {\n      await shouldRetryOrPromise;\n      return true;\n    } catch (_err) {\n      return false;\n    }\n  }\n  return shouldRetryOrPromise;\n}\n\n/**\n * Adds response interceptors to an axios instance to retry requests failed due to network issues\n *\n * @example\n *\n * import axios from 'axios';\n *\n * axiosRetry(axios, { retries: 3 });\n *\n * axios.get('http://example.com/test') // The first request fails and the second returns 'ok'\n *   .then(result => {\n *     result.data; // 'ok'\n *   });\n *\n * // Exponential back-off retry delay between requests\n * axiosRetry(axios, { retryDelay : axiosRetry.exponentialDelay});\n *\n * // Custom retry delay\n * axiosRetry(axios, { retryDelay : (retryCount) => {\n *   return retryCount * 1000;\n * }});\n *\n * // Also works with custom axios instances\n * const client = axios.create({ baseURL: 'http://example.com' });\n * axiosRetry(client, { retries: 3 });\n *\n * client.get('/test') // The first request fails and the second returns 'ok'\n *   .then(result => {\n *     result.data; // 'ok'\n *   });\n *\n * // Allows request-specific configuration\n * client\n *   .get('/test', {\n *     'axios-retry': {\n *       retries: 0\n *     }\n *   })\n *   .catch(error => { // The first request fails\n *     error !== undefined\n *   });\n *\n * @param {Axios} axios An axios instance (the axios object or one created from axios.create)\n * @param {Object} [defaultOptions]\n * @param {number} [defaultOptions.retries=3] Number of retries\n * @param {boolean} [defaultOptions.shouldResetTimeout=false]\n *        Defines if the timeout should be reset between retries\n * @param {Function} [defaultOptions.retryCondition=isNetworkOrIdempotentRequestError]\n *        A function to determine if the error can be retried\n * @param {Function} [defaultOptions.retryDelay=noDelay]\n *        A function to determine the delay between retry requests\n */\nfunction axiosRetry(axios, defaultOptions) {\n  axios.interceptors.request.use(function (config) {\n    var currentState = getCurrentState(config);\n    currentState.lastRequestTime = Date.now();\n    return config;\n  });\n\n  axios.interceptors.response.use(null, async function (error) {\n    var config = error.config;\n\n    // If we have no information to retry the request\n    if (!config) {\n      return Promise.reject(error);\n    }\n\n    var _getRequestOptions = getRequestOptions(config, defaultOptions),\n        _getRequestOptions$re = _getRequestOptions.retries,\n        retries = _getRequestOptions$re === undefined ? 3 : _getRequestOptions$re,\n        _getRequestOptions$re2 = _getRequestOptions.retryCondition,\n        retryCondition = _getRequestOptions$re2 === undefined ? isNetworkOrIdempotentRequestError : _getRequestOptions$re2,\n        _getRequestOptions$re3 = _getRequestOptions.retryDelay,\n        retryDelay = _getRequestOptions$re3 === undefined ? noDelay : _getRequestOptions$re3,\n        _getRequestOptions$sh = _getRequestOptions.shouldResetTimeout,\n        shouldResetTimeout = _getRequestOptions$sh === undefined ? false : _getRequestOptions$sh;\n\n    var currentState = getCurrentState(config);\n\n    if (await shouldRetry(retries, retryCondition, currentState, error)) {\n      currentState.retryCount += 1;\n      var delay = retryDelay(currentState.retryCount, error);\n\n      // Axios fails merging this configuration to the default configuration because it has an issue\n      // with circular structures: https://github.com/mzabriskie/axios/issues/370\n      fixConfig(axios, config);\n\n      if (!shouldResetTimeout && config.timeout && currentState.lastRequestTime) {\n        var lastRequestDuration = Date.now() - currentState.lastRequestTime;\n        // Minimum 1ms timeout (passing 0 or less to XHR means no timeout)\n        config.timeout = Math.max(config.timeout - lastRequestDuration - delay, 1);\n      }\n\n      config.transformRequest = [function (data) {\n        return data;\n      }];\n\n      return new Promise(function (resolve) {\n        return setTimeout(function () {\n          return resolve(axios(config));\n        }, delay);\n      });\n    }\n\n    return Promise.reject(error);\n  });\n}\n\n// Compatibility with CommonJS\naxiosRetry.isNetworkError = isNetworkError;\naxiosRetry.isSafeRequestError = isSafeRequestError;\naxiosRetry.isIdempotentRequestError = isIdempotentRequestError;\naxiosRetry.isNetworkOrIdempotentRequestError = isNetworkOrIdempotentRequestError;\naxiosRetry.exponentialDelay = exponentialDelay;\naxiosRetry.isRetryableError = isRetryableError;\n//# sourceMappingURL=index.js.map","module.exports = require('./lib/axios');","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar buildFullPath = require('../core/buildFullPath');\nvar buildURL = require('./../helpers/buildURL');\nvar http = require('http');\nvar https = require('https');\nvar httpFollow = require('follow-redirects').http;\nvar httpsFollow = require('follow-redirects').https;\nvar url = require('url');\nvar zlib = require('zlib');\nvar pkg = require('./../../package.json');\nvar createError = require('../core/createError');\nvar enhanceError = require('../core/enhanceError');\n\nvar isHttps = /https:?/;\n\n/**\n *\n * @param {http.ClientRequestArgs} options\n * @param {AxiosProxyConfig} proxy\n * @param {string} location\n */\nfunction setProxy(options, proxy, location) {\n  options.hostname = proxy.host;\n  options.host = proxy.host;\n  options.port = proxy.port;\n  options.path = location;\n\n  // Basic proxy authorization\n  if (proxy.auth) {\n    var base64 = Buffer.from(proxy.auth.username + ':' + proxy.auth.password, 'utf8').toString('base64');\n    options.headers['Proxy-Authorization'] = 'Basic ' + base64;\n  }\n\n  // If a proxy is used, any redirects must also pass through the proxy\n  options.beforeRedirect = function beforeRedirect(redirection) {\n    redirection.headers.host = redirection.host;\n    setProxy(redirection, proxy, redirection.href);\n  };\n}\n\n/*eslint consistent-return:0*/\nmodule.exports = function httpAdapter(config) {\n  return new Promise(function dispatchHttpRequest(resolvePromise, rejectPromise) {\n    var resolve = function resolve(value) {\n      resolvePromise(value);\n    };\n    var reject = function reject(value) {\n      rejectPromise(value);\n    };\n    var data = config.data;\n    var headers = config.headers;\n\n    // Set User-Agent (required by some servers)\n    // See https://github.com/axios/axios/issues/69\n    if ('User-Agent' in headers || 'user-agent' in headers) {\n      // User-Agent is specified; handle case where no UA header is desired\n      if (!headers['User-Agent'] && !headers['user-agent']) {\n        delete headers['User-Agent'];\n        delete headers['user-agent'];\n      }\n      // Otherwise, use specified value\n    } else {\n      // Only set header if it hasn't been set in config\n      headers['User-Agent'] = 'axios/' + pkg.version;\n    }\n\n    if (data && !utils.isStream(data)) {\n      if (Buffer.isBuffer(data)) {\n        // Nothing to do...\n      } else if (utils.isArrayBuffer(data)) {\n        data = Buffer.from(new Uint8Array(data));\n      } else if (utils.isString(data)) {\n        data = Buffer.from(data, 'utf-8');\n      } else {\n        return reject(createError(\n          'Data after transformation must be a string, an ArrayBuffer, a Buffer, or a Stream',\n          config\n        ));\n      }\n\n      // Add Content-Length header if data exists\n      headers['Content-Length'] = data.length;\n    }\n\n    // HTTP basic authentication\n    var auth = undefined;\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password || '';\n      auth = username + ':' + password;\n    }\n\n    // Parse url\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    var parsed = url.parse(fullPath);\n    var protocol = parsed.protocol || 'http:';\n\n    if (!auth && parsed.auth) {\n      var urlAuth = parsed.auth.split(':');\n      var urlUsername = urlAuth[0] || '';\n      var urlPassword = urlAuth[1] || '';\n      auth = urlUsername + ':' + urlPassword;\n    }\n\n    if (auth) {\n      delete headers.Authorization;\n    }\n\n    var isHttpsRequest = isHttps.test(protocol);\n    var agent = isHttpsRequest ? config.httpsAgent : config.httpAgent;\n\n    var options = {\n      path: buildURL(parsed.path, config.params, config.paramsSerializer).replace(/^\\?/, ''),\n      method: config.method.toUpperCase(),\n      headers: headers,\n      agent: agent,\n      agents: { http: config.httpAgent, https: config.httpsAgent },\n      auth: auth\n    };\n\n    if (config.socketPath) {\n      options.socketPath = config.socketPath;\n    } else {\n      options.hostname = parsed.hostname;\n      options.port = parsed.port;\n    }\n\n    var proxy = config.proxy;\n    if (!proxy && proxy !== false) {\n      var proxyEnv = protocol.slice(0, -1) + '_proxy';\n      var proxyUrl = process.env[proxyEnv] || process.env[proxyEnv.toUpperCase()];\n      if (proxyUrl) {\n        var parsedProxyUrl = url.parse(proxyUrl);\n        var noProxyEnv = process.env.no_proxy || process.env.NO_PROXY;\n        var shouldProxy = true;\n\n        if (noProxyEnv) {\n          var noProxy = noProxyEnv.split(',').map(function trim(s) {\n            return s.trim();\n          });\n\n          shouldProxy = !noProxy.some(function proxyMatch(proxyElement) {\n            if (!proxyElement) {\n              return false;\n            }\n            if (proxyElement === '*') {\n              return true;\n            }\n            if (proxyElement[0] === '.' &&\n                parsed.hostname.substr(parsed.hostname.length - proxyElement.length) === proxyElement) {\n              return true;\n            }\n\n            return parsed.hostname === proxyElement;\n          });\n        }\n\n        if (shouldProxy) {\n          proxy = {\n            host: parsedProxyUrl.hostname,\n            port: parsedProxyUrl.port,\n            protocol: parsedProxyUrl.protocol\n          };\n\n          if (parsedProxyUrl.auth) {\n            var proxyUrlAuth = parsedProxyUrl.auth.split(':');\n            proxy.auth = {\n              username: proxyUrlAuth[0],\n              password: proxyUrlAuth[1]\n            };\n          }\n        }\n      }\n    }\n\n    if (proxy) {\n      options.headers.host = parsed.hostname + (parsed.port ? ':' + parsed.port : '');\n      setProxy(options, proxy, protocol + '//' + parsed.hostname + (parsed.port ? ':' + parsed.port : '') + options.path);\n    }\n\n    var transport;\n    var isHttpsProxy = isHttpsRequest && (proxy ? isHttps.test(proxy.protocol) : true);\n    if (config.transport) {\n      transport = config.transport;\n    } else if (config.maxRedirects === 0) {\n      transport = isHttpsProxy ? https : http;\n    } else {\n      if (config.maxRedirects) {\n        options.maxRedirects = config.maxRedirects;\n      }\n      transport = isHttpsProxy ? httpsFollow : httpFollow;\n    }\n\n    if (config.maxBodyLength > -1) {\n      options.maxBodyLength = config.maxBodyLength;\n    }\n\n    // Create the request\n    var req = transport.request(options, function handleResponse(res) {\n      if (req.aborted) return;\n\n      // uncompress the response body transparently if required\n      var stream = res;\n\n      // return the last request in case of redirects\n      var lastRequest = res.req || req;\n\n\n      // if no content, is HEAD request or decompress disabled we should not decompress\n      if (res.statusCode !== 204 && lastRequest.method !== 'HEAD' && config.decompress !== false) {\n        switch (res.headers['content-encoding']) {\n        /*eslint default-case:0*/\n        case 'gzip':\n        case 'compress':\n        case 'deflate':\n        // add the unzipper to the body stream processing pipeline\n          stream = stream.pipe(zlib.createUnzip());\n\n          // remove the content-encoding in order to not confuse downstream operations\n          delete res.headers['content-encoding'];\n          break;\n        }\n      }\n\n      var response = {\n        status: res.statusCode,\n        statusText: res.statusMessage,\n        headers: res.headers,\n        config: config,\n        request: lastRequest\n      };\n\n      if (config.responseType === 'stream') {\n        response.data = stream;\n        settle(resolve, reject, response);\n      } else {\n        var responseBuffer = [];\n        var totalResponseBytes = 0;\n        stream.on('data', function handleStreamData(chunk) {\n          responseBuffer.push(chunk);\n          totalResponseBytes += chunk.length;\n\n          // make sure the content length is not over the maxContentLength if specified\n          if (config.maxContentLength > -1 && totalResponseBytes > config.maxContentLength) {\n            stream.destroy();\n            reject(createError('maxContentLength size of ' + config.maxContentLength + ' exceeded',\n              config, null, lastRequest));\n          }\n        });\n\n        stream.on('error', function handleStreamError(err) {\n          if (req.aborted) return;\n          reject(enhanceError(err, config, null, lastRequest));\n        });\n\n        stream.on('end', function handleStreamEnd() {\n          var responseData = Buffer.concat(responseBuffer);\n          if (config.responseType !== 'arraybuffer') {\n            responseData = responseData.toString(config.responseEncoding);\n            if (!config.responseEncoding || config.responseEncoding === 'utf8') {\n              responseData = utils.stripBOM(responseData);\n            }\n          }\n\n          response.data = responseData;\n          settle(resolve, reject, response);\n        });\n      }\n    });\n\n    // Handle errors\n    req.on('error', function handleRequestError(err) {\n      if (req.aborted && err.code !== 'ERR_FR_TOO_MANY_REDIRECTS') return;\n      reject(enhanceError(err, config, null, req));\n    });\n\n    // Handle request timeout\n    if (config.timeout) {\n      // This is forcing a int timeout to avoid problems if the `req` interface doesn't handle other types.\n      var timeout = parseInt(config.timeout, 10);\n\n      if (isNaN(timeout)) {\n        reject(createError(\n          'error trying to parse `config.timeout` to int',\n          config,\n          'ERR_PARSE_TIMEOUT',\n          req\n        ));\n\n        return;\n      }\n\n      // Sometime, the response will be very slow, and does not respond, the connect event will be block by event loop system.\n      // And timer callback will be fired, and abort() will be invoked before connection, then get \"socket hang up\" and code ECONNRESET.\n      // At this time, if we have a large number of request, nodejs will hang up some socket on background. and the number will up and up.\n      // And then these socket which be hang up will devoring CPU little by little.\n      // ClientRequest.setTimeout will be fired on the specify milliseconds, and can make sure that abort() will be fired after connect.\n      req.setTimeout(timeout, function handleRequestTimeout() {\n        req.abort();\n        reject(createError(\n          'timeout of ' + timeout + 'ms exceeded',\n          config,\n          config.transitional && config.transitional.clarifyTimeoutError ? 'ETIMEDOUT' : 'ECONNABORTED',\n          req\n        ));\n      });\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (req.aborted) return;\n\n        req.abort();\n        reject(cancel);\n      });\n    }\n\n    // Send the request\n    if (utils.isStream(data)) {\n      data.on('error', function handleStreamError(err) {\n        reject(enhanceError(err, config, null, req));\n      }).pipe(req);\n    } else {\n      req.end(data);\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar cookies = require('./../helpers/cookies');\nvar buildURL = require('./../helpers/buildURL');\nvar buildFullPath = require('../core/buildFullPath');\nvar parseHeaders = require('./../helpers/parseHeaders');\nvar isURLSameOrigin = require('./../helpers/isURLSameOrigin');\nvar createError = require('../core/createError');\n\nmodule.exports = function xhrAdapter(config) {\n  return new Promise(function dispatchXhrRequest(resolve, reject) {\n    var requestData = config.data;\n    var requestHeaders = config.headers;\n    var responseType = config.responseType;\n\n    if (utils.isFormData(requestData)) {\n      delete requestHeaders['Content-Type']; // Let the browser set it\n    }\n\n    var request = new XMLHttpRequest();\n\n    // HTTP basic authentication\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password ? unescape(encodeURIComponent(config.auth.password)) : '';\n      requestHeaders.Authorization = 'Basic ' + btoa(username + ':' + password);\n    }\n\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    request.open(config.method.toUpperCase(), buildURL(fullPath, config.params, config.paramsSerializer), true);\n\n    // Set the request timeout in MS\n    request.timeout = config.timeout;\n\n    function onloadend() {\n      if (!request) {\n        return;\n      }\n      // Prepare the response\n      var responseHeaders = 'getAllResponseHeaders' in request ? parseHeaders(request.getAllResponseHeaders()) : null;\n      var responseData = !responseType || responseType === 'text' ||  responseType === 'json' ?\n        request.responseText : request.response;\n      var response = {\n        data: responseData,\n        status: request.status,\n        statusText: request.statusText,\n        headers: responseHeaders,\n        config: config,\n        request: request\n      };\n\n      settle(resolve, reject, response);\n\n      // Clean up request\n      request = null;\n    }\n\n    if ('onloadend' in request) {\n      // Use onloadend if available\n      request.onloadend = onloadend;\n    } else {\n      // Listen for ready state to emulate onloadend\n      request.onreadystatechange = function handleLoad() {\n        if (!request || request.readyState !== 4) {\n          return;\n        }\n\n        // The request errored out and we didn't get a response, this will be\n        // handled by onerror instead\n        // With one exception: request that using file: protocol, most browsers\n        // will return status as 0 even though it's a successful request\n        if (request.status === 0 && !(request.responseURL && request.responseURL.indexOf('file:') === 0)) {\n          return;\n        }\n        // readystate handler is calling before onerror or ontimeout handlers,\n        // so we should call onloadend on the next 'tick'\n        setTimeout(onloadend);\n      };\n    }\n\n    // Handle browser request cancellation (as opposed to a manual cancellation)\n    request.onabort = function handleAbort() {\n      if (!request) {\n        return;\n      }\n\n      reject(createError('Request aborted', config, 'ECONNABORTED', request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle low level network errors\n    request.onerror = function handleError() {\n      // Real errors are hidden from us by the browser\n      // onerror should only fire if it's a network error\n      reject(createError('Network Error', config, null, request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle timeout\n    request.ontimeout = function handleTimeout() {\n      var timeoutErrorMessage = 'timeout of ' + config.timeout + 'ms exceeded';\n      if (config.timeoutErrorMessage) {\n        timeoutErrorMessage = config.timeoutErrorMessage;\n      }\n      reject(createError(\n        timeoutErrorMessage,\n        config,\n        config.transitional && config.transitional.clarifyTimeoutError ? 'ETIMEDOUT' : 'ECONNABORTED',\n        request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Add xsrf header\n    // This is only done if running in a standard browser environment.\n    // Specifically not if we're in a web worker, or react-native.\n    if (utils.isStandardBrowserEnv()) {\n      // Add xsrf header\n      var xsrfValue = (config.withCredentials || isURLSameOrigin(fullPath)) && config.xsrfCookieName ?\n        cookies.read(config.xsrfCookieName) :\n        undefined;\n\n      if (xsrfValue) {\n        requestHeaders[config.xsrfHeaderName] = xsrfValue;\n      }\n    }\n\n    // Add headers to the request\n    if ('setRequestHeader' in request) {\n      utils.forEach(requestHeaders, function setRequestHeader(val, key) {\n        if (typeof requestData === 'undefined' && key.toLowerCase() === 'content-type') {\n          // Remove Content-Type if data is undefined\n          delete requestHeaders[key];\n        } else {\n          // Otherwise add header to the request\n          request.setRequestHeader(key, val);\n        }\n      });\n    }\n\n    // Add withCredentials to request if needed\n    if (!utils.isUndefined(config.withCredentials)) {\n      request.withCredentials = !!config.withCredentials;\n    }\n\n    // Add responseType to request if needed\n    if (responseType && responseType !== 'json') {\n      request.responseType = config.responseType;\n    }\n\n    // Handle progress if needed\n    if (typeof config.onDownloadProgress === 'function') {\n      request.addEventListener('progress', config.onDownloadProgress);\n    }\n\n    // Not all browsers support upload events\n    if (typeof config.onUploadProgress === 'function' && request.upload) {\n      request.upload.addEventListener('progress', config.onUploadProgress);\n    }\n\n    if (config.cancelToken) {\n      // Handle cancellation\n      config.cancelToken.promise.then(function onCanceled(cancel) {\n        if (!request) {\n          return;\n        }\n\n        request.abort();\n        reject(cancel);\n        // Clean up request\n        request = null;\n      });\n    }\n\n    if (!requestData) {\n      requestData = null;\n    }\n\n    // Send the request\n    request.send(requestData);\n  });\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar bind = require('./helpers/bind');\nvar Axios = require('./core/Axios');\nvar mergeConfig = require('./core/mergeConfig');\nvar defaults = require('./defaults');\n\n/**\n * Create an instance of Axios\n *\n * @param {Object} defaultConfig The default config for the instance\n * @return {Axios} A new instance of Axios\n */\nfunction createInstance(defaultConfig) {\n  var context = new Axios(defaultConfig);\n  var instance = bind(Axios.prototype.request, context);\n\n  // Copy axios.prototype to instance\n  utils.extend(instance, Axios.prototype, context);\n\n  // Copy context to instance\n  utils.extend(instance, context);\n\n  return instance;\n}\n\n// Create the default instance to be exported\nvar axios = createInstance(defaults);\n\n// Expose Axios class to allow class inheritance\naxios.Axios = Axios;\n\n// Factory for creating new instances\naxios.create = function create(instanceConfig) {\n  return createInstance(mergeConfig(axios.defaults, instanceConfig));\n};\n\n// Expose Cancel & CancelToken\naxios.Cancel = require('./cancel/Cancel');\naxios.CancelToken = require('./cancel/CancelToken');\naxios.isCancel = require('./cancel/isCancel');\n\n// Expose all/spread\naxios.all = function all(promises) {\n  return Promise.all(promises);\n};\naxios.spread = require('./helpers/spread');\n\n// Expose isAxiosError\naxios.isAxiosError = require('./helpers/isAxiosError');\n\nmodule.exports = axios;\n\n// Allow use of default import syntax in TypeScript\nmodule.exports.default = axios;\n","'use strict';\n\n/**\n * A `Cancel` is an object that is thrown when an operation is canceled.\n *\n * @class\n * @param {string=} message The message.\n */\nfunction Cancel(message) {\n  this.message = message;\n}\n\nCancel.prototype.toString = function toString() {\n  return 'Cancel' + (this.message ? ': ' + this.message : '');\n};\n\nCancel.prototype.__CANCEL__ = true;\n\nmodule.exports = Cancel;\n","'use strict';\n\nvar Cancel = require('./Cancel');\n\n/**\n * A `CancelToken` is an object that can be used to request cancellation of an operation.\n *\n * @class\n * @param {Function} executor The executor function.\n */\nfunction CancelToken(executor) {\n  if (typeof executor !== 'function') {\n    throw new TypeError('executor must be a function.');\n  }\n\n  var resolvePromise;\n  this.promise = new Promise(function promiseExecutor(resolve) {\n    resolvePromise = resolve;\n  });\n\n  var token = this;\n  executor(function cancel(message) {\n    if (token.reason) {\n      // Cancellation has already been requested\n      return;\n    }\n\n    token.reason = new Cancel(message);\n    resolvePromise(token.reason);\n  });\n}\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nCancelToken.prototype.throwIfRequested = function throwIfRequested() {\n  if (this.reason) {\n    throw this.reason;\n  }\n};\n\n/**\n * Returns an object that contains a new `CancelToken` and a function that, when called,\n * cancels the `CancelToken`.\n */\nCancelToken.source = function source() {\n  var cancel;\n  var token = new CancelToken(function executor(c) {\n    cancel = c;\n  });\n  return {\n    token: token,\n    cancel: cancel\n  };\n};\n\nmodule.exports = CancelToken;\n","'use strict';\n\nmodule.exports = function isCancel(value) {\n  return !!(value && value.__CANCEL__);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar buildURL = require('../helpers/buildURL');\nvar InterceptorManager = require('./InterceptorManager');\nvar dispatchRequest = require('./dispatchRequest');\nvar mergeConfig = require('./mergeConfig');\nvar validator = require('../helpers/validator');\n\nvar validators = validator.validators;\n/**\n * Create a new instance of Axios\n *\n * @param {Object} instanceConfig The default config for the instance\n */\nfunction Axios(instanceConfig) {\n  this.defaults = instanceConfig;\n  this.interceptors = {\n    request: new InterceptorManager(),\n    response: new InterceptorManager()\n  };\n}\n\n/**\n * Dispatch a request\n *\n * @param {Object} config The config specific for this request (merged with this.defaults)\n */\nAxios.prototype.request = function request(config) {\n  /*eslint no-param-reassign:0*/\n  // Allow for axios('example/url'[, config]) a la fetch API\n  if (typeof config === 'string') {\n    config = arguments[1] || {};\n    config.url = arguments[0];\n  } else {\n    config = config || {};\n  }\n\n  config = mergeConfig(this.defaults, config);\n\n  // Set config.method\n  if (config.method) {\n    config.method = config.method.toLowerCase();\n  } else if (this.defaults.method) {\n    config.method = this.defaults.method.toLowerCase();\n  } else {\n    config.method = 'get';\n  }\n\n  var transitional = config.transitional;\n\n  if (transitional !== undefined) {\n    validator.assertOptions(transitional, {\n      silentJSONParsing: validators.transitional(validators.boolean, '1.0.0'),\n      forcedJSONParsing: validators.transitional(validators.boolean, '1.0.0'),\n      clarifyTimeoutError: validators.transitional(validators.boolean, '1.0.0')\n    }, false);\n  }\n\n  // filter out skipped interceptors\n  var requestInterceptorChain = [];\n  var synchronousRequestInterceptors = true;\n  this.interceptors.request.forEach(function unshiftRequestInterceptors(interceptor) {\n    if (typeof interceptor.runWhen === 'function' && interceptor.runWhen(config) === false) {\n      return;\n    }\n\n    synchronousRequestInterceptors = synchronousRequestInterceptors && interceptor.synchronous;\n\n    requestInterceptorChain.unshift(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  var responseInterceptorChain = [];\n  this.interceptors.response.forEach(function pushResponseInterceptors(interceptor) {\n    responseInterceptorChain.push(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  var promise;\n\n  if (!synchronousRequestInterceptors) {\n    var chain = [dispatchRequest, undefined];\n\n    Array.prototype.unshift.apply(chain, requestInterceptorChain);\n    chain = chain.concat(responseInterceptorChain);\n\n    promise = Promise.resolve(config);\n    while (chain.length) {\n      promise = promise.then(chain.shift(), chain.shift());\n    }\n\n    return promise;\n  }\n\n\n  var newConfig = config;\n  while (requestInterceptorChain.length) {\n    var onFulfilled = requestInterceptorChain.shift();\n    var onRejected = requestInterceptorChain.shift();\n    try {\n      newConfig = onFulfilled(newConfig);\n    } catch (error) {\n      onRejected(error);\n      break;\n    }\n  }\n\n  try {\n    promise = dispatchRequest(newConfig);\n  } catch (error) {\n    return Promise.reject(error);\n  }\n\n  while (responseInterceptorChain.length) {\n    promise = promise.then(responseInterceptorChain.shift(), responseInterceptorChain.shift());\n  }\n\n  return promise;\n};\n\nAxios.prototype.getUri = function getUri(config) {\n  config = mergeConfig(this.defaults, config);\n  return buildURL(config.url, config.params, config.paramsSerializer).replace(/^\\?/, '');\n};\n\n// Provide aliases for supported request methods\nutils.forEach(['delete', 'get', 'head', 'options'], function forEachMethodNoData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: (config || {}).data\n    }));\n  };\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, data, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: data\n    }));\n  };\n});\n\nmodule.exports = Axios;\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction InterceptorManager() {\n  this.handlers = [];\n}\n\n/**\n * Add a new interceptor to the stack\n *\n * @param {Function} fulfilled The function to handle `then` for a `Promise`\n * @param {Function} rejected The function to handle `reject` for a `Promise`\n *\n * @return {Number} An ID used to remove interceptor later\n */\nInterceptorManager.prototype.use = function use(fulfilled, rejected, options) {\n  this.handlers.push({\n    fulfilled: fulfilled,\n    rejected: rejected,\n    synchronous: options ? options.synchronous : false,\n    runWhen: options ? options.runWhen : null\n  });\n  return this.handlers.length - 1;\n};\n\n/**\n * Remove an interceptor from the stack\n *\n * @param {Number} id The ID that was returned by `use`\n */\nInterceptorManager.prototype.eject = function eject(id) {\n  if (this.handlers[id]) {\n    this.handlers[id] = null;\n  }\n};\n\n/**\n * Iterate over all the registered interceptors\n *\n * This method is particularly useful for skipping over any\n * interceptors that may have become `null` calling `eject`.\n *\n * @param {Function} fn The function to call for each interceptor\n */\nInterceptorManager.prototype.forEach = function forEach(fn) {\n  utils.forEach(this.handlers, function forEachHandler(h) {\n    if (h !== null) {\n      fn(h);\n    }\n  });\n};\n\nmodule.exports = InterceptorManager;\n","'use strict';\n\nvar isAbsoluteURL = require('../helpers/isAbsoluteURL');\nvar combineURLs = require('../helpers/combineURLs');\n\n/**\n * Creates a new URL by combining the baseURL with the requestedURL,\n * only when the requestedURL is not already an absolute URL.\n * If the requestURL is absolute, this function returns the requestedURL untouched.\n *\n * @param {string} baseURL The base URL\n * @param {string} requestedURL Absolute or relative URL to combine\n * @returns {string} The combined full path\n */\nmodule.exports = function buildFullPath(baseURL, requestedURL) {\n  if (baseURL && !isAbsoluteURL(requestedURL)) {\n    return combineURLs(baseURL, requestedURL);\n  }\n  return requestedURL;\n};\n","'use strict';\n\nvar enhanceError = require('./enhanceError');\n\n/**\n * Create an Error with the specified message, config, error code, request and response.\n *\n * @param {string} message The error message.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The created error.\n */\nmodule.exports = function createError(message, config, code, request, response) {\n  var error = new Error(message);\n  return enhanceError(error, config, code, request, response);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar transformData = require('./transformData');\nvar isCancel = require('../cancel/isCancel');\nvar defaults = require('../defaults');\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nfunction throwIfCancellationRequested(config) {\n  if (config.cancelToken) {\n    config.cancelToken.throwIfRequested();\n  }\n}\n\n/**\n * Dispatch a request to the server using the configured adapter.\n *\n * @param {object} config The config that is to be used for the request\n * @returns {Promise} The Promise to be fulfilled\n */\nmodule.exports = function dispatchRequest(config) {\n  throwIfCancellationRequested(config);\n\n  // Ensure headers exist\n  config.headers = config.headers || {};\n\n  // Transform request data\n  config.data = transformData.call(\n    config,\n    config.data,\n    config.headers,\n    config.transformRequest\n  );\n\n  // Flatten headers\n  config.headers = utils.merge(\n    config.headers.common || {},\n    config.headers[config.method] || {},\n    config.headers\n  );\n\n  utils.forEach(\n    ['delete', 'get', 'head', 'post', 'put', 'patch', 'common'],\n    function cleanHeaderConfig(method) {\n      delete config.headers[method];\n    }\n  );\n\n  var adapter = config.adapter || defaults.adapter;\n\n  return adapter(config).then(function onAdapterResolution(response) {\n    throwIfCancellationRequested(config);\n\n    // Transform response data\n    response.data = transformData.call(\n      config,\n      response.data,\n      response.headers,\n      config.transformResponse\n    );\n\n    return response;\n  }, function onAdapterRejection(reason) {\n    if (!isCancel(reason)) {\n      throwIfCancellationRequested(config);\n\n      // Transform response data\n      if (reason && reason.response) {\n        reason.response.data = transformData.call(\n          config,\n          reason.response.data,\n          reason.response.headers,\n          config.transformResponse\n        );\n      }\n    }\n\n    return Promise.reject(reason);\n  });\n};\n","'use strict';\n\n/**\n * Update an Error with the specified config, error code, and response.\n *\n * @param {Error} error The error to update.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The error.\n */\nmodule.exports = function enhanceError(error, config, code, request, response) {\n  error.config = config;\n  if (code) {\n    error.code = code;\n  }\n\n  error.request = request;\n  error.response = response;\n  error.isAxiosError = true;\n\n  error.toJSON = function toJSON() {\n    return {\n      // Standard\n      message: this.message,\n      name: this.name,\n      // Microsoft\n      description: this.description,\n      number: this.number,\n      // Mozilla\n      fileName: this.fileName,\n      lineNumber: this.lineNumber,\n      columnNumber: this.columnNumber,\n      stack: this.stack,\n      // Axios\n      config: this.config,\n      code: this.code\n    };\n  };\n  return error;\n};\n","'use strict';\n\nvar utils = require('../utils');\n\n/**\n * Config-specific merge-function which creates a new config-object\n * by merging two configuration objects together.\n *\n * @param {Object} config1\n * @param {Object} config2\n * @returns {Object} New object resulting from merging config2 to config1\n */\nmodule.exports = function mergeConfig(config1, config2) {\n  // eslint-disable-next-line no-param-reassign\n  config2 = config2 || {};\n  var config = {};\n\n  var valueFromConfig2Keys = ['url', 'method', 'data'];\n  var mergeDeepPropertiesKeys = ['headers', 'auth', 'proxy', 'params'];\n  var defaultToConfig2Keys = [\n    'baseURL', 'transformRequest', 'transformResponse', 'paramsSerializer',\n    'timeout', 'timeoutMessage', 'withCredentials', 'adapter', 'responseType', 'xsrfCookieName',\n    'xsrfHeaderName', 'onUploadProgress', 'onDownloadProgress', 'decompress',\n    'maxContentLength', 'maxBodyLength', 'maxRedirects', 'transport', 'httpAgent',\n    'httpsAgent', 'cancelToken', 'socketPath', 'responseEncoding'\n  ];\n  var directMergeKeys = ['validateStatus'];\n\n  function getMergedValue(target, source) {\n    if (utils.isPlainObject(target) && utils.isPlainObject(source)) {\n      return utils.merge(target, source);\n    } else if (utils.isPlainObject(source)) {\n      return utils.merge({}, source);\n    } else if (utils.isArray(source)) {\n      return source.slice();\n    }\n    return source;\n  }\n\n  function mergeDeepProperties(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(config1[prop], config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  }\n\n  utils.forEach(valueFromConfig2Keys, function valueFromConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(undefined, config2[prop]);\n    }\n  });\n\n  utils.forEach(mergeDeepPropertiesKeys, mergeDeepProperties);\n\n  utils.forEach(defaultToConfig2Keys, function defaultToConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      config[prop] = getMergedValue(undefined, config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  });\n\n  utils.forEach(directMergeKeys, function merge(prop) {\n    if (prop in config2) {\n      config[prop] = getMergedValue(config1[prop], config2[prop]);\n    } else if (prop in config1) {\n      config[prop] = getMergedValue(undefined, config1[prop]);\n    }\n  });\n\n  var axiosKeys = valueFromConfig2Keys\n    .concat(mergeDeepPropertiesKeys)\n    .concat(defaultToConfig2Keys)\n    .concat(directMergeKeys);\n\n  var otherKeys = Object\n    .keys(config1)\n    .concat(Object.keys(config2))\n    .filter(function filterAxiosKeys(key) {\n      return axiosKeys.indexOf(key) === -1;\n    });\n\n  utils.forEach(otherKeys, mergeDeepProperties);\n\n  return config;\n};\n","'use strict';\n\nvar createError = require('./createError');\n\n/**\n * Resolve or reject a Promise based on response status.\n *\n * @param {Function} resolve A function that resolves the promise.\n * @param {Function} reject A function that rejects the promise.\n * @param {object} response The response.\n */\nmodule.exports = function settle(resolve, reject, response) {\n  var validateStatus = response.config.validateStatus;\n  if (!response.status || !validateStatus || validateStatus(response.status)) {\n    resolve(response);\n  } else {\n    reject(createError(\n      'Request failed with status code ' + response.status,\n      response.config,\n      null,\n      response.request,\n      response\n    ));\n  }\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar defaults = require('./../defaults');\n\n/**\n * Transform the data for a request or a response\n *\n * @param {Object|String} data The data to be transformed\n * @param {Array} headers The headers for the request or response\n * @param {Array|Function} fns A single function or Array of functions\n * @returns {*} The resulting transformed data\n */\nmodule.exports = function transformData(data, headers, fns) {\n  var context = this || defaults;\n  /*eslint no-param-reassign:0*/\n  utils.forEach(fns, function transform(fn) {\n    data = fn.call(context, data, headers);\n  });\n\n  return data;\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar normalizeHeaderName = require('./helpers/normalizeHeaderName');\nvar enhanceError = require('./core/enhanceError');\n\nvar DEFAULT_CONTENT_TYPE = {\n  'Content-Type': 'application/x-www-form-urlencoded'\n};\n\nfunction setContentTypeIfUnset(headers, value) {\n  if (!utils.isUndefined(headers) && utils.isUndefined(headers['Content-Type'])) {\n    headers['Content-Type'] = value;\n  }\n}\n\nfunction getDefaultAdapter() {\n  var adapter;\n  if (typeof XMLHttpRequest !== 'undefined') {\n    // For browsers use XHR adapter\n    adapter = require('./adapters/xhr');\n  } else if (typeof process !== 'undefined' && Object.prototype.toString.call(process) === '[object process]') {\n    // For node use HTTP adapter\n    adapter = require('./adapters/http');\n  }\n  return adapter;\n}\n\nfunction stringifySafely(rawValue, parser, encoder) {\n  if (utils.isString(rawValue)) {\n    try {\n      (parser || JSON.parse)(rawValue);\n      return utils.trim(rawValue);\n    } catch (e) {\n      if (e.name !== 'SyntaxError') {\n        throw e;\n      }\n    }\n  }\n\n  return (encoder || JSON.stringify)(rawValue);\n}\n\nvar defaults = {\n\n  transitional: {\n    silentJSONParsing: true,\n    forcedJSONParsing: true,\n    clarifyTimeoutError: false\n  },\n\n  adapter: getDefaultAdapter(),\n\n  transformRequest: [function transformRequest(data, headers) {\n    normalizeHeaderName(headers, 'Accept');\n    normalizeHeaderName(headers, 'Content-Type');\n\n    if (utils.isFormData(data) ||\n      utils.isArrayBuffer(data) ||\n      utils.isBuffer(data) ||\n      utils.isStream(data) ||\n      utils.isFile(data) ||\n      utils.isBlob(data)\n    ) {\n      return data;\n    }\n    if (utils.isArrayBufferView(data)) {\n      return data.buffer;\n    }\n    if (utils.isURLSearchParams(data)) {\n      setContentTypeIfUnset(headers, 'application/x-www-form-urlencoded;charset=utf-8');\n      return data.toString();\n    }\n    if (utils.isObject(data) || (headers && headers['Content-Type'] === 'application/json')) {\n      setContentTypeIfUnset(headers, 'application/json');\n      return stringifySafely(data);\n    }\n    return data;\n  }],\n\n  transformResponse: [function transformResponse(data) {\n    var transitional = this.transitional;\n    var silentJSONParsing = transitional && transitional.silentJSONParsing;\n    var forcedJSONParsing = transitional && transitional.forcedJSONParsing;\n    var strictJSONParsing = !silentJSONParsing && this.responseType === 'json';\n\n    if (strictJSONParsing || (forcedJSONParsing && utils.isString(data) && data.length)) {\n      try {\n        return JSON.parse(data);\n      } catch (e) {\n        if (strictJSONParsing) {\n          if (e.name === 'SyntaxError') {\n            throw enhanceError(e, this, 'E_JSON_PARSE');\n          }\n          throw e;\n        }\n      }\n    }\n\n    return data;\n  }],\n\n  /**\n   * A timeout in milliseconds to abort a request. If set to 0 (default) a\n   * timeout is not created.\n   */\n  timeout: 0,\n\n  xsrfCookieName: 'XSRF-TOKEN',\n  xsrfHeaderName: 'X-XSRF-TOKEN',\n\n  maxContentLength: -1,\n  maxBodyLength: -1,\n\n  validateStatus: function validateStatus(status) {\n    return status >= 200 && status < 300;\n  }\n};\n\ndefaults.headers = {\n  common: {\n    'Accept': 'application/json, text/plain, */*'\n  }\n};\n\nutils.forEach(['delete', 'get', 'head'], function forEachMethodNoData(method) {\n  defaults.headers[method] = {};\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  defaults.headers[method] = utils.merge(DEFAULT_CONTENT_TYPE);\n});\n\nmodule.exports = defaults;\n","'use strict';\n\nmodule.exports = function bind(fn, thisArg) {\n  return function wrap() {\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n    return fn.apply(thisArg, args);\n  };\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction encode(val) {\n  return encodeURIComponent(val).\n    replace(/%3A/gi, ':').\n    replace(/%24/g, '$').\n    replace(/%2C/gi, ',').\n    replace(/%20/g, '+').\n    replace(/%5B/gi, '[').\n    replace(/%5D/gi, ']');\n}\n\n/**\n * Build a URL by appending params to the end\n *\n * @param {string} url The base of the url (e.g., http://www.google.com)\n * @param {object} [params] The params to be appended\n * @returns {string} The formatted url\n */\nmodule.exports = function buildURL(url, params, paramsSerializer) {\n  /*eslint no-param-reassign:0*/\n  if (!params) {\n    return url;\n  }\n\n  var serializedParams;\n  if (paramsSerializer) {\n    serializedParams = paramsSerializer(params);\n  } else if (utils.isURLSearchParams(params)) {\n    serializedParams = params.toString();\n  } else {\n    var parts = [];\n\n    utils.forEach(params, function serialize(val, key) {\n      if (val === null || typeof val === 'undefined') {\n        return;\n      }\n\n      if (utils.isArray(val)) {\n        key = key + '[]';\n      } else {\n        val = [val];\n      }\n\n      utils.forEach(val, function parseValue(v) {\n        if (utils.isDate(v)) {\n          v = v.toISOString();\n        } else if (utils.isObject(v)) {\n          v = JSON.stringify(v);\n        }\n        parts.push(encode(key) + '=' + encode(v));\n      });\n    });\n\n    serializedParams = parts.join('&');\n  }\n\n  if (serializedParams) {\n    var hashmarkIndex = url.indexOf('#');\n    if (hashmarkIndex !== -1) {\n      url = url.slice(0, hashmarkIndex);\n    }\n\n    url += (url.indexOf('?') === -1 ? '?' : '&') + serializedParams;\n  }\n\n  return url;\n};\n","'use strict';\n\n/**\n * Creates a new URL by combining the specified URLs\n *\n * @param {string} baseURL The base URL\n * @param {string} relativeURL The relative URL\n * @returns {string} The combined URL\n */\nmodule.exports = function combineURLs(baseURL, relativeURL) {\n  return relativeURL\n    ? baseURL.replace(/\\/+$/, '') + '/' + relativeURL.replace(/^\\/+/, '')\n    : baseURL;\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs support document.cookie\n    (function standardBrowserEnv() {\n      return {\n        write: function write(name, value, expires, path, domain, secure) {\n          var cookie = [];\n          cookie.push(name + '=' + encodeURIComponent(value));\n\n          if (utils.isNumber(expires)) {\n            cookie.push('expires=' + new Date(expires).toGMTString());\n          }\n\n          if (utils.isString(path)) {\n            cookie.push('path=' + path);\n          }\n\n          if (utils.isString(domain)) {\n            cookie.push('domain=' + domain);\n          }\n\n          if (secure === true) {\n            cookie.push('secure');\n          }\n\n          document.cookie = cookie.join('; ');\n        },\n\n        read: function read(name) {\n          var match = document.cookie.match(new RegExp('(^|;\\\\s*)(' + name + ')=([^;]*)'));\n          return (match ? decodeURIComponent(match[3]) : null);\n        },\n\n        remove: function remove(name) {\n          this.write(name, '', Date.now() - 86400000);\n        }\n      };\n    })() :\n\n  // Non standard browser env (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return {\n        write: function write() {},\n        read: function read() { return null; },\n        remove: function remove() {}\n      };\n    })()\n);\n","'use strict';\n\n/**\n * Determines whether the specified URL is absolute\n *\n * @param {string} url The URL to test\n * @returns {boolean} True if the specified URL is absolute, otherwise false\n */\nmodule.exports = function isAbsoluteURL(url) {\n  // A URL is considered absolute if it begins with \"<scheme>://\" or \"//\" (protocol-relative URL).\n  // RFC 3986 defines scheme name as a sequence of characters beginning with a letter and followed\n  // by any combination of letters, digits, plus, period, or hyphen.\n  return /^([a-z][a-z\\d\\+\\-\\.]*:)?\\/\\//i.test(url);\n};\n","'use strict';\n\n/**\n * Determines whether the payload is an error thrown by Axios\n *\n * @param {*} payload The value to test\n * @returns {boolean} True if the payload is an error thrown by Axios, otherwise false\n */\nmodule.exports = function isAxiosError(payload) {\n  return (typeof payload === 'object') && (payload.isAxiosError === true);\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs have full support of the APIs needed to test\n  // whether the request URL is of the same origin as current location.\n    (function standardBrowserEnv() {\n      var msie = /(msie|trident)/i.test(navigator.userAgent);\n      var urlParsingNode = document.createElement('a');\n      var originURL;\n\n      /**\n    * Parse a URL to discover it's components\n    *\n    * @param {String} url The URL to be parsed\n    * @returns {Object}\n    */\n      function resolveURL(url) {\n        var href = url;\n\n        if (msie) {\n        // IE needs attribute set twice to normalize properties\n          urlParsingNode.setAttribute('href', href);\n          href = urlParsingNode.href;\n        }\n\n        urlParsingNode.setAttribute('href', href);\n\n        // urlParsingNode provides the UrlUtils interface - http://url.spec.whatwg.org/#urlutils\n        return {\n          href: urlParsingNode.href,\n          protocol: urlParsingNode.protocol ? urlParsingNode.protocol.replace(/:$/, '') : '',\n          host: urlParsingNode.host,\n          search: urlParsingNode.search ? urlParsingNode.search.replace(/^\\?/, '') : '',\n          hash: urlParsingNode.hash ? urlParsingNode.hash.replace(/^#/, '') : '',\n          hostname: urlParsingNode.hostname,\n          port: urlParsingNode.port,\n          pathname: (urlParsingNode.pathname.charAt(0) === '/') ?\n            urlParsingNode.pathname :\n            '/' + urlParsingNode.pathname\n        };\n      }\n\n      originURL = resolveURL(window.location.href);\n\n      /**\n    * Determine if a URL shares the same origin as the current location\n    *\n    * @param {String} requestURL The URL to test\n    * @returns {boolean} True if URL shares the same origin, otherwise false\n    */\n      return function isURLSameOrigin(requestURL) {\n        var parsed = (utils.isString(requestURL)) ? resolveURL(requestURL) : requestURL;\n        return (parsed.protocol === originURL.protocol &&\n            parsed.host === originURL.host);\n      };\n    })() :\n\n  // Non standard browser envs (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return function isURLSameOrigin() {\n        return true;\n      };\n    })()\n);\n","'use strict';\n\nvar utils = require('../utils');\n\nmodule.exports = function normalizeHeaderName(headers, normalizedName) {\n  utils.forEach(headers, function processHeader(value, name) {\n    if (name !== normalizedName && name.toUpperCase() === normalizedName.toUpperCase()) {\n      headers[normalizedName] = value;\n      delete headers[name];\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n// Headers whose duplicates are ignored by node\n// c.f. https://nodejs.org/api/http.html#http_message_headers\nvar ignoreDuplicateOf = [\n  'age', 'authorization', 'content-length', 'content-type', 'etag',\n  'expires', 'from', 'host', 'if-modified-since', 'if-unmodified-since',\n  'last-modified', 'location', 'max-forwards', 'proxy-authorization',\n  'referer', 'retry-after', 'user-agent'\n];\n\n/**\n * Parse headers into an object\n *\n * ```\n * Date: Wed, 27 Aug 2014 08:58:49 GMT\n * Content-Type: application/json\n * Connection: keep-alive\n * Transfer-Encoding: chunked\n * ```\n *\n * @param {String} headers Headers needing to be parsed\n * @returns {Object} Headers parsed into an object\n */\nmodule.exports = function parseHeaders(headers) {\n  var parsed = {};\n  var key;\n  var val;\n  var i;\n\n  if (!headers) { return parsed; }\n\n  utils.forEach(headers.split('\\n'), function parser(line) {\n    i = line.indexOf(':');\n    key = utils.trim(line.substr(0, i)).toLowerCase();\n    val = utils.trim(line.substr(i + 1));\n\n    if (key) {\n      if (parsed[key] && ignoreDuplicateOf.indexOf(key) >= 0) {\n        return;\n      }\n      if (key === 'set-cookie') {\n        parsed[key] = (parsed[key] ? parsed[key] : []).concat([val]);\n      } else {\n        parsed[key] = parsed[key] ? parsed[key] + ', ' + val : val;\n      }\n    }\n  });\n\n  return parsed;\n};\n","'use strict';\n\n/**\n * Syntactic sugar for invoking a function and expanding an array for arguments.\n *\n * Common use case would be to use `Function.prototype.apply`.\n *\n *  ```js\n *  function f(x, y, z) {}\n *  var args = [1, 2, 3];\n *  f.apply(null, args);\n *  ```\n *\n * With `spread` this example can be re-written.\n *\n *  ```js\n *  spread(function(x, y, z) {})([1, 2, 3]);\n *  ```\n *\n * @param {Function} callback\n * @returns {Function}\n */\nmodule.exports = function spread(callback) {\n  return function wrap(arr) {\n    return callback.apply(null, arr);\n  };\n};\n","'use strict';\n\nvar pkg = require('./../../package.json');\n\nvar validators = {};\n\n// eslint-disable-next-line func-names\n['object', 'boolean', 'number', 'function', 'string', 'symbol'].forEach(function(type, i) {\n  validators[type] = function validator(thing) {\n    return typeof thing === type || 'a' + (i < 1 ? 'n ' : ' ') + type;\n  };\n});\n\nvar deprecatedWarnings = {};\nvar currentVerArr = pkg.version.split('.');\n\n/**\n * Compare package versions\n * @param {string} version\n * @param {string?} thanVersion\n * @returns {boolean}\n */\nfunction isOlderVersion(version, thanVersion) {\n  var pkgVersionArr = thanVersion ? thanVersion.split('.') : currentVerArr;\n  var destVer = version.split('.');\n  for (var i = 0; i < 3; i++) {\n    if (pkgVersionArr[i] > destVer[i]) {\n      return true;\n    } else if (pkgVersionArr[i] < destVer[i]) {\n      return false;\n    }\n  }\n  return false;\n}\n\n/**\n * Transitional option validator\n * @param {function|boolean?} validator\n * @param {string?} version\n * @param {string} message\n * @returns {function}\n */\nvalidators.transitional = function transitional(validator, version, message) {\n  var isDeprecated = version && isOlderVersion(version);\n\n  function formatMessage(opt, desc) {\n    return '[Axios v' + pkg.version + '] Transitional option \\'' + opt + '\\'' + desc + (message ? '. ' + message : '');\n  }\n\n  // eslint-disable-next-line func-names\n  return function(value, opt, opts) {\n    if (validator === false) {\n      throw new Error(formatMessage(opt, ' has been removed in ' + version));\n    }\n\n    if (isDeprecated && !deprecatedWarnings[opt]) {\n      deprecatedWarnings[opt] = true;\n      // eslint-disable-next-line no-console\n      console.warn(\n        formatMessage(\n          opt,\n          ' has been deprecated since v' + version + ' and will be removed in the near future'\n        )\n      );\n    }\n\n    return validator ? validator(value, opt, opts) : true;\n  };\n};\n\n/**\n * Assert object's properties type\n * @param {object} options\n * @param {object} schema\n * @param {boolean?} allowUnknown\n */\n\nfunction assertOptions(options, schema, allowUnknown) {\n  if (typeof options !== 'object') {\n    throw new TypeError('options must be an object');\n  }\n  var keys = Object.keys(options);\n  var i = keys.length;\n  while (i-- > 0) {\n    var opt = keys[i];\n    var validator = schema[opt];\n    if (validator) {\n      var value = options[opt];\n      var result = value === undefined || validator(value, opt, options);\n      if (result !== true) {\n        throw new TypeError('option ' + opt + ' must be ' + result);\n      }\n      continue;\n    }\n    if (allowUnknown !== true) {\n      throw Error('Unknown option ' + opt);\n    }\n  }\n}\n\nmodule.exports = {\n  isOlderVersion: isOlderVersion,\n  assertOptions: assertOptions,\n  validators: validators\n};\n","'use strict';\n\nvar bind = require('./helpers/bind');\n\n// utils is a library of generic helper functions non-specific to axios\n\nvar toString = Object.prototype.toString;\n\n/**\n * Determine if a value is an Array\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Array, otherwise false\n */\nfunction isArray(val) {\n  return toString.call(val) === '[object Array]';\n}\n\n/**\n * Determine if a value is undefined\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if the value is undefined, otherwise false\n */\nfunction isUndefined(val) {\n  return typeof val === 'undefined';\n}\n\n/**\n * Determine if a value is a Buffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Buffer, otherwise false\n */\nfunction isBuffer(val) {\n  return val !== null && !isUndefined(val) && val.constructor !== null && !isUndefined(val.constructor)\n    && typeof val.constructor.isBuffer === 'function' && val.constructor.isBuffer(val);\n}\n\n/**\n * Determine if a value is an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an ArrayBuffer, otherwise false\n */\nfunction isArrayBuffer(val) {\n  return toString.call(val) === '[object ArrayBuffer]';\n}\n\n/**\n * Determine if a value is a FormData\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an FormData, otherwise false\n */\nfunction isFormData(val) {\n  return (typeof FormData !== 'undefined') && (val instanceof FormData);\n}\n\n/**\n * Determine if a value is a view on an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a view on an ArrayBuffer, otherwise false\n */\nfunction isArrayBufferView(val) {\n  var result;\n  if ((typeof ArrayBuffer !== 'undefined') && (ArrayBuffer.isView)) {\n    result = ArrayBuffer.isView(val);\n  } else {\n    result = (val) && (val.buffer) && (val.buffer instanceof ArrayBuffer);\n  }\n  return result;\n}\n\n/**\n * Determine if a value is a String\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a String, otherwise false\n */\nfunction isString(val) {\n  return typeof val === 'string';\n}\n\n/**\n * Determine if a value is a Number\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Number, otherwise false\n */\nfunction isNumber(val) {\n  return typeof val === 'number';\n}\n\n/**\n * Determine if a value is an Object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Object, otherwise false\n */\nfunction isObject(val) {\n  return val !== null && typeof val === 'object';\n}\n\n/**\n * Determine if a value is a plain Object\n *\n * @param {Object} val The value to test\n * @return {boolean} True if value is a plain Object, otherwise false\n */\nfunction isPlainObject(val) {\n  if (toString.call(val) !== '[object Object]') {\n    return false;\n  }\n\n  var prototype = Object.getPrototypeOf(val);\n  return prototype === null || prototype === Object.prototype;\n}\n\n/**\n * Determine if a value is a Date\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Date, otherwise false\n */\nfunction isDate(val) {\n  return toString.call(val) === '[object Date]';\n}\n\n/**\n * Determine if a value is a File\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a File, otherwise false\n */\nfunction isFile(val) {\n  return toString.call(val) === '[object File]';\n}\n\n/**\n * Determine if a value is a Blob\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Blob, otherwise false\n */\nfunction isBlob(val) {\n  return toString.call(val) === '[object Blob]';\n}\n\n/**\n * Determine if a value is a Function\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Function, otherwise false\n */\nfunction isFunction(val) {\n  return toString.call(val) === '[object Function]';\n}\n\n/**\n * Determine if a value is a Stream\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Stream, otherwise false\n */\nfunction isStream(val) {\n  return isObject(val) && isFunction(val.pipe);\n}\n\n/**\n * Determine if a value is a URLSearchParams object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a URLSearchParams object, otherwise false\n */\nfunction isURLSearchParams(val) {\n  return typeof URLSearchParams !== 'undefined' && val instanceof URLSearchParams;\n}\n\n/**\n * Trim excess whitespace off the beginning and end of a string\n *\n * @param {String} str The String to trim\n * @returns {String} The String freed of excess whitespace\n */\nfunction trim(str) {\n  return str.trim ? str.trim() : str.replace(/^\\s+|\\s+$/g, '');\n}\n\n/**\n * Determine if we're running in a standard browser environment\n *\n * This allows axios to run in a web worker, and react-native.\n * Both environments support XMLHttpRequest, but not fully standard globals.\n *\n * web workers:\n *  typeof window -> undefined\n *  typeof document -> undefined\n *\n * react-native:\n *  navigator.product -> 'ReactNative'\n * nativescript\n *  navigator.product -> 'NativeScript' or 'NS'\n */\nfunction isStandardBrowserEnv() {\n  if (typeof navigator !== 'undefined' && (navigator.product === 'ReactNative' ||\n                                           navigator.product === 'NativeScript' ||\n                                           navigator.product === 'NS')) {\n    return false;\n  }\n  return (\n    typeof window !== 'undefined' &&\n    typeof document !== 'undefined'\n  );\n}\n\n/**\n * Iterate over an Array or an Object invoking a function for each item.\n *\n * If `obj` is an Array callback will be called passing\n * the value, index, and complete array for each item.\n *\n * If 'obj' is an Object callback will be called passing\n * the value, key, and complete object for each property.\n *\n * @param {Object|Array} obj The object to iterate\n * @param {Function} fn The callback to invoke for each item\n */\nfunction forEach(obj, fn) {\n  // Don't bother if no value provided\n  if (obj === null || typeof obj === 'undefined') {\n    return;\n  }\n\n  // Force an array if not already something iterable\n  if (typeof obj !== 'object') {\n    /*eslint no-param-reassign:0*/\n    obj = [obj];\n  }\n\n  if (isArray(obj)) {\n    // Iterate over array values\n    for (var i = 0, l = obj.length; i < l; i++) {\n      fn.call(null, obj[i], i, obj);\n    }\n  } else {\n    // Iterate over object keys\n    for (var key in obj) {\n      if (Object.prototype.hasOwnProperty.call(obj, key)) {\n        fn.call(null, obj[key], key, obj);\n      }\n    }\n  }\n}\n\n/**\n * Accepts varargs expecting each argument to be an object, then\n * immutably merges the properties of each object and returns result.\n *\n * When multiple objects contain the same key the later object in\n * the arguments list will take precedence.\n *\n * Example:\n *\n * ```js\n * var result = merge({foo: 123}, {foo: 456});\n * console.log(result.foo); // outputs 456\n * ```\n *\n * @param {Object} obj1 Object to merge\n * @returns {Object} Result of all merge properties\n */\nfunction merge(/* obj1, obj2, obj3, ... */) {\n  var result = {};\n  function assignValue(val, key) {\n    if (isPlainObject(result[key]) && isPlainObject(val)) {\n      result[key] = merge(result[key], val);\n    } else if (isPlainObject(val)) {\n      result[key] = merge({}, val);\n    } else if (isArray(val)) {\n      result[key] = val.slice();\n    } else {\n      result[key] = val;\n    }\n  }\n\n  for (var i = 0, l = arguments.length; i < l; i++) {\n    forEach(arguments[i], assignValue);\n  }\n  return result;\n}\n\n/**\n * Extends object a by mutably adding to it the properties of object b.\n *\n * @param {Object} a The object to be extended\n * @param {Object} b The object to copy properties from\n * @param {Object} thisArg The object to bind function to\n * @return {Object} The resulting value of object a\n */\nfunction extend(a, b, thisArg) {\n  forEach(b, function assignValue(val, key) {\n    if (thisArg && typeof val === 'function') {\n      a[key] = bind(val, thisArg);\n    } else {\n      a[key] = val;\n    }\n  });\n  return a;\n}\n\n/**\n * Remove byte order marker. This catches EF BB BF (the UTF-8 BOM)\n *\n * @param {string} content with BOM\n * @return {string} content value without BOM\n */\nfunction stripBOM(content) {\n  if (content.charCodeAt(0) === 0xFEFF) {\n    content = content.slice(1);\n  }\n  return content;\n}\n\nmodule.exports = {\n  isArray: isArray,\n  isArrayBuffer: isArrayBuffer,\n  isBuffer: isBuffer,\n  isFormData: isFormData,\n  isArrayBufferView: isArrayBufferView,\n  isString: isString,\n  isNumber: isNumber,\n  isObject: isObject,\n  isPlainObject: isPlainObject,\n  isUndefined: isUndefined,\n  isDate: isDate,\n  isFile: isFile,\n  isBlob: isBlob,\n  isFunction: isFunction,\n  isStream: isStream,\n  isURLSearchParams: isURLSearchParams,\n  isStandardBrowserEnv: isStandardBrowserEnv,\n  forEach: forEach,\n  merge: merge,\n  extend: extend,\n  trim: trim,\n  stripBOM: stripBOM\n};\n","/**\n * Helpers.\n */\n\nvar s = 1000;\nvar m = s * 60;\nvar h = m * 60;\nvar d = h * 24;\nvar y = d * 365.25;\n\n/**\n * Parse or format the given `val`.\n *\n * Options:\n *\n *  - `long` verbose formatting [false]\n *\n * @param {String|Number} val\n * @param {Object} [options]\n * @throws {Error} throw an error if val is not a non-empty string or a number\n * @return {String|Number}\n * @api public\n */\n\nmodule.exports = function(val, options) {\n  options = options || {};\n  var type = typeof val;\n  if (type === 'string' && val.length > 0) {\n    return parse(val);\n  } else if (type === 'number' && isNaN(val) === false) {\n    return options.long ? fmtLong(val) : fmtShort(val);\n  }\n  throw new Error(\n    'val is not a non-empty string or a valid number. val=' +\n      JSON.stringify(val)\n  );\n};\n\n/**\n * Parse the given `str` and return milliseconds.\n *\n * @param {String} str\n * @return {Number}\n * @api private\n */\n\nfunction parse(str) {\n  str = String(str);\n  if (str.length > 100) {\n    return;\n  }\n  var match = /^((?:\\d+)?\\.?\\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|years?|yrs?|y)?$/i.exec(\n    str\n  );\n  if (!match) {\n    return;\n  }\n  var n = parseFloat(match[1]);\n  var type = (match[2] || 'ms').toLowerCase();\n  switch (type) {\n    case 'years':\n    case 'year':\n    case 'yrs':\n    case 'yr':\n    case 'y':\n      return n * y;\n    case 'days':\n    case 'day':\n    case 'd':\n      return n * d;\n    case 'hours':\n    case 'hour':\n    case 'hrs':\n    case 'hr':\n    case 'h':\n      return n * h;\n    case 'minutes':\n    case 'minute':\n    case 'mins':\n    case 'min':\n    case 'm':\n      return n * m;\n    case 'seconds':\n    case 'second':\n    case 'secs':\n    case 'sec':\n    case 's':\n      return n * s;\n    case 'milliseconds':\n    case 'millisecond':\n    case 'msecs':\n    case 'msec':\n    case 'ms':\n      return n;\n    default:\n      return undefined;\n  }\n}\n\n/**\n * Short format for `ms`.\n *\n * @param {Number} ms\n * @return {String}\n * @api private\n */\n\nfunction fmtShort(ms) {\n  if (ms >= d) {\n    return Math.round(ms / d) + 'd';\n  }\n  if (ms >= h) {\n    return Math.round(ms / h) + 'h';\n  }\n  if (ms >= m) {\n    return Math.round(ms / m) + 'm';\n  }\n  if (ms >= s) {\n    return Math.round(ms / s) + 's';\n  }\n  return ms + 'ms';\n}\n\n/**\n * Long format for `ms`.\n *\n * @param {Number} ms\n * @return {String}\n * @api private\n */\n\nfunction fmtLong(ms) {\n  return plural(ms, d, 'day') ||\n    plural(ms, h, 'hour') ||\n    plural(ms, m, 'minute') ||\n    plural(ms, s, 'second') ||\n    ms + ' ms';\n}\n\n/**\n * Pluralization helper.\n */\n\nfunction plural(ms, n, name) {\n  if (ms < n) {\n    return;\n  }\n  if (ms < n * 1.5) {\n    return Math.floor(ms / n) + ' ' + name;\n  }\n  return Math.ceil(ms / n) + ' ' + name + 's';\n}\n","/**\n * This is the web browser implementation of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = require('./debug');\nexports.log = log;\nexports.formatArgs = formatArgs;\nexports.save = save;\nexports.load = load;\nexports.useColors = useColors;\nexports.storage = 'undefined' != typeof chrome\n               && 'undefined' != typeof chrome.storage\n                  ? chrome.storage.local\n                  : localstorage();\n\n/**\n * Colors.\n */\n\nexports.colors = [\n  'lightseagreen',\n  'forestgreen',\n  'goldenrod',\n  'dodgerblue',\n  'darkorchid',\n  'crimson'\n];\n\n/**\n * Currently only WebKit-based Web Inspectors, Firefox >= v31,\n * and the Firebug extension (any Firefox version) are known\n * to support \"%c\" CSS customizations.\n *\n * TODO: add a `localStorage` variable to explicitly enable/disable colors\n */\n\nfunction useColors() {\n  // NB: In an Electron preload script, document will be defined but not fully\n  // initialized. Since we know we're in Chrome, we'll just detect this case\n  // explicitly\n  if (typeof window !== 'undefined' && window.process && window.process.type === 'renderer') {\n    return true;\n  }\n\n  // is webkit? http://stackoverflow.com/a/16459606/376773\n  // document is undefined in react-native: https://github.com/facebook/react-native/pull/1632\n  return (typeof document !== 'undefined' && document.documentElement && document.documentElement.style && document.documentElement.style.WebkitAppearance) ||\n    // is firebug? http://stackoverflow.com/a/398120/376773\n    (typeof window !== 'undefined' && window.console && (window.console.firebug || (window.console.exception && window.console.table))) ||\n    // is firefox >= v31?\n    // https://developer.mozilla.org/en-US/docs/Tools/Web_Console#Styling_messages\n    (typeof navigator !== 'undefined' && navigator.userAgent && navigator.userAgent.toLowerCase().match(/firefox\\/(\\d+)/) && parseInt(RegExp.$1, 10) >= 31) ||\n    // double check webkit in userAgent just in case we are in a worker\n    (typeof navigator !== 'undefined' && navigator.userAgent && navigator.userAgent.toLowerCase().match(/applewebkit\\/(\\d+)/));\n}\n\n/**\n * Map %j to `JSON.stringify()`, since no Web Inspectors do that by default.\n */\n\nexports.formatters.j = function(v) {\n  try {\n    return JSON.stringify(v);\n  } catch (err) {\n    return '[UnexpectedJSONParseError]: ' + err.message;\n  }\n};\n\n\n/**\n * Colorize log arguments if enabled.\n *\n * @api public\n */\n\nfunction formatArgs(args) {\n  var useColors = this.useColors;\n\n  args[0] = (useColors ? '%c' : '')\n    + this.namespace\n    + (useColors ? ' %c' : ' ')\n    + args[0]\n    + (useColors ? '%c ' : ' ')\n    + '+' + exports.humanize(this.diff);\n\n  if (!useColors) return;\n\n  var c = 'color: ' + this.color;\n  args.splice(1, 0, c, 'color: inherit')\n\n  // the final \"%c\" is somewhat tricky, because there could be other\n  // arguments passed either before or after the %c, so we need to\n  // figure out the correct index to insert the CSS into\n  var index = 0;\n  var lastC = 0;\n  args[0].replace(/%[a-zA-Z%]/g, function(match) {\n    if ('%%' === match) return;\n    index++;\n    if ('%c' === match) {\n      // we only are interested in the *last* %c\n      // (the user may have provided their own)\n      lastC = index;\n    }\n  });\n\n  args.splice(lastC, 0, c);\n}\n\n/**\n * Invokes `console.log()` when available.\n * No-op when `console.log` is not a \"function\".\n *\n * @api public\n */\n\nfunction log() {\n  // this hackery is required for IE8/9, where\n  // the `console.log` function doesn't have 'apply'\n  return 'object' === typeof console\n    && console.log\n    && Function.prototype.apply.call(console.log, console, arguments);\n}\n\n/**\n * Save `namespaces`.\n *\n * @param {String} namespaces\n * @api private\n */\n\nfunction save(namespaces) {\n  try {\n    if (null == namespaces) {\n      exports.storage.removeItem('debug');\n    } else {\n      exports.storage.debug = namespaces;\n    }\n  } catch(e) {}\n}\n\n/**\n * Load `namespaces`.\n *\n * @return {String} returns the previously persisted debug modes\n * @api private\n */\n\nfunction load() {\n  var r;\n  try {\n    r = exports.storage.debug;\n  } catch(e) {}\n\n  // If debug isn't set in LS, and we're in Electron, try to load $DEBUG\n  if (!r && typeof process !== 'undefined' && 'env' in process) {\n    r = process.env.DEBUG;\n  }\n\n  return r;\n}\n\n/**\n * Enable namespaces listed in `localStorage.debug` initially.\n */\n\nexports.enable(load());\n\n/**\n * Localstorage attempts to return the localstorage.\n *\n * This is necessary because safari throws\n * when a user disables cookies/localstorage\n * and you attempt to access it.\n *\n * @return {LocalStorage}\n * @api private\n */\n\nfunction localstorage() {\n  try {\n    return window.localStorage;\n  } catch (e) {}\n}\n","\n/**\n * This is the common logic for both the Node.js and web browser\n * implementations of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = createDebug.debug = createDebug['default'] = createDebug;\nexports.coerce = coerce;\nexports.disable = disable;\nexports.enable = enable;\nexports.enabled = enabled;\nexports.humanize = require('ms');\n\n/**\n * The currently active debug mode names, and names to skip.\n */\n\nexports.names = [];\nexports.skips = [];\n\n/**\n * Map of special \"%n\" handling functions, for the debug \"format\" argument.\n *\n * Valid key names are a single, lower or upper-case letter, i.e. \"n\" and \"N\".\n */\n\nexports.formatters = {};\n\n/**\n * Previous log timestamp.\n */\n\nvar prevTime;\n\n/**\n * Select a color.\n * @param {String} namespace\n * @return {Number}\n * @api private\n */\n\nfunction selectColor(namespace) {\n  var hash = 0, i;\n\n  for (i in namespace) {\n    hash  = ((hash << 5) - hash) + namespace.charCodeAt(i);\n    hash |= 0; // Convert to 32bit integer\n  }\n\n  return exports.colors[Math.abs(hash) % exports.colors.length];\n}\n\n/**\n * Create a debugger with the given `namespace`.\n *\n * @param {String} namespace\n * @return {Function}\n * @api public\n */\n\nfunction createDebug(namespace) {\n\n  function debug() {\n    // disabled?\n    if (!debug.enabled) return;\n\n    var self = debug;\n\n    // set `diff` timestamp\n    var curr = +new Date();\n    var ms = curr - (prevTime || curr);\n    self.diff = ms;\n    self.prev = prevTime;\n    self.curr = curr;\n    prevTime = curr;\n\n    // turn the `arguments` into a proper Array\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n\n    args[0] = exports.coerce(args[0]);\n\n    if ('string' !== typeof args[0]) {\n      // anything else let's inspect with %O\n      args.unshift('%O');\n    }\n\n    // apply any `formatters` transformations\n    var index = 0;\n    args[0] = args[0].replace(/%([a-zA-Z%])/g, function(match, format) {\n      // if we encounter an escaped % then don't increase the array index\n      if (match === '%%') return match;\n      index++;\n      var formatter = exports.formatters[format];\n      if ('function' === typeof formatter) {\n        var val = args[index];\n        match = formatter.call(self, val);\n\n        // now we need to remove `args[index]` since it's inlined in the `format`\n        args.splice(index, 1);\n        index--;\n      }\n      return match;\n    });\n\n    // apply env-specific formatting (colors, etc.)\n    exports.formatArgs.call(self, args);\n\n    var logFn = debug.log || exports.log || console.log.bind(console);\n    logFn.apply(self, args);\n  }\n\n  debug.namespace = namespace;\n  debug.enabled = exports.enabled(namespace);\n  debug.useColors = exports.useColors();\n  debug.color = selectColor(namespace);\n\n  // env-specific initialization logic for debug instances\n  if ('function' === typeof exports.init) {\n    exports.init(debug);\n  }\n\n  return debug;\n}\n\n/**\n * Enables a debug mode by namespaces. This can include modes\n * separated by a colon and wildcards.\n *\n * @param {String} namespaces\n * @api public\n */\n\nfunction enable(namespaces) {\n  exports.save(namespaces);\n\n  exports.names = [];\n  exports.skips = [];\n\n  var split = (typeof namespaces === 'string' ? namespaces : '').split(/[\\s,]+/);\n  var len = split.length;\n\n  for (var i = 0; i < len; i++) {\n    if (!split[i]) continue; // ignore empty strings\n    namespaces = split[i].replace(/\\*/g, '.*?');\n    if (namespaces[0] === '-') {\n      exports.skips.push(new RegExp('^' + namespaces.substr(1) + '$'));\n    } else {\n      exports.names.push(new RegExp('^' + namespaces + '$'));\n    }\n  }\n}\n\n/**\n * Disable debug output.\n *\n * @api public\n */\n\nfunction disable() {\n  exports.enable('');\n}\n\n/**\n * Returns true if the given mode name is enabled, false otherwise.\n *\n * @param {String} name\n * @return {Boolean}\n * @api public\n */\n\nfunction enabled(name) {\n  var i, len;\n  for (i = 0, len = exports.skips.length; i < len; i++) {\n    if (exports.skips[i].test(name)) {\n      return false;\n    }\n  }\n  for (i = 0, len = exports.names.length; i < len; i++) {\n    if (exports.names[i].test(name)) {\n      return true;\n    }\n  }\n  return false;\n}\n\n/**\n * Coerce `val`.\n *\n * @param {Mixed} val\n * @return {Mixed}\n * @api private\n */\n\nfunction coerce(val) {\n  if (val instanceof Error) return val.stack || val.message;\n  return val;\n}\n","/**\n * Detect Electron renderer process, which is node, but we should\n * treat as a browser.\n */\n\nif (typeof process !== 'undefined' && process.type === 'renderer') {\n  module.exports = require('./browser.js');\n} else {\n  module.exports = require('./node.js');\n}\n","/**\n * Module dependencies.\n */\n\nvar tty = require('tty');\nvar util = require('util');\n\n/**\n * This is the Node.js implementation of `debug()`.\n *\n * Expose `debug()` as the module.\n */\n\nexports = module.exports = require('./debug');\nexports.init = init;\nexports.log = log;\nexports.formatArgs = formatArgs;\nexports.save = save;\nexports.load = load;\nexports.useColors = useColors;\n\n/**\n * Colors.\n */\n\nexports.colors = [6, 2, 3, 4, 5, 1];\n\n/**\n * Build up the default `inspectOpts` object from the environment variables.\n *\n *   $ DEBUG_COLORS=no DEBUG_DEPTH=10 DEBUG_SHOW_HIDDEN=enabled node script.js\n */\n\nexports.inspectOpts = Object.keys(process.env).filter(function (key) {\n  return /^debug_/i.test(key);\n}).reduce(function (obj, key) {\n  // camel-case\n  var prop = key\n    .substring(6)\n    .toLowerCase()\n    .replace(/_([a-z])/g, function (_, k) { return k.toUpperCase() });\n\n  // coerce string value into JS value\n  var val = process.env[key];\n  if (/^(yes|on|true|enabled)$/i.test(val)) val = true;\n  else if (/^(no|off|false|disabled)$/i.test(val)) val = false;\n  else if (val === 'null') val = null;\n  else val = Number(val);\n\n  obj[prop] = val;\n  return obj;\n}, {});\n\n/**\n * The file descriptor to write the `debug()` calls to.\n * Set the `DEBUG_FD` env variable to override with another value. i.e.:\n *\n *   $ DEBUG_FD=3 node script.js 3>debug.log\n */\n\nvar fd = parseInt(process.env.DEBUG_FD, 10) || 2;\n\nif (1 !== fd && 2 !== fd) {\n  util.deprecate(function(){}, 'except for stderr(2) and stdout(1), any other usage of DEBUG_FD is deprecated. Override debug.log if you want to use a different log function (https://git.io/debug_fd)')()\n}\n\nvar stream = 1 === fd ? process.stdout :\n             2 === fd ? process.stderr :\n             createWritableStdioStream(fd);\n\n/**\n * Is stdout a TTY? Colored output is enabled when `true`.\n */\n\nfunction useColors() {\n  return 'colors' in exports.inspectOpts\n    ? Boolean(exports.inspectOpts.colors)\n    : tty.isatty(fd);\n}\n\n/**\n * Map %o to `util.inspect()`, all on a single line.\n */\n\nexports.formatters.o = function(v) {\n  this.inspectOpts.colors = this.useColors;\n  return util.inspect(v, this.inspectOpts)\n    .split('\\n').map(function(str) {\n      return str.trim()\n    }).join(' ');\n};\n\n/**\n * Map %o to `util.inspect()`, allowing multiple lines if needed.\n */\n\nexports.formatters.O = function(v) {\n  this.inspectOpts.colors = this.useColors;\n  return util.inspect(v, this.inspectOpts);\n};\n\n/**\n * Adds ANSI color escape codes if enabled.\n *\n * @api public\n */\n\nfunction formatArgs(args) {\n  var name = this.namespace;\n  var useColors = this.useColors;\n\n  if (useColors) {\n    var c = this.color;\n    var prefix = '  \\u001b[3' + c + ';1m' + name + ' ' + '\\u001b[0m';\n\n    args[0] = prefix + args[0].split('\\n').join('\\n' + prefix);\n    args.push('\\u001b[3' + c + 'm+' + exports.humanize(this.diff) + '\\u001b[0m');\n  } else {\n    args[0] = new Date().toUTCString()\n      + ' ' + name + ' ' + args[0];\n  }\n}\n\n/**\n * Invokes `util.format()` with the specified arguments and writes to `stream`.\n */\n\nfunction log() {\n  return stream.write(util.format.apply(util, arguments) + '\\n');\n}\n\n/**\n * Save `namespaces`.\n *\n * @param {String} namespaces\n * @api private\n */\n\nfunction save(namespaces) {\n  if (null == namespaces) {\n    // If you set a process.env field to null or undefined, it gets cast to the\n    // string 'null' or 'undefined'. Just delete instead.\n    delete process.env.DEBUG;\n  } else {\n    process.env.DEBUG = namespaces;\n  }\n}\n\n/**\n * Load `namespaces`.\n *\n * @return {String} returns the previously persisted debug modes\n * @api private\n */\n\nfunction load() {\n  return process.env.DEBUG;\n}\n\n/**\n * Copied from `node/src/node.js`.\n *\n * XXX: It's lame that node doesn't expose this API out-of-the-box. It also\n * relies on the undocumented `tty_wrap.guessHandleType()` which is also lame.\n */\n\nfunction createWritableStdioStream (fd) {\n  var stream;\n  var tty_wrap = process.binding('tty_wrap');\n\n  // Note stream._type is used for test-module-load-list.js\n\n  switch (tty_wrap.guessHandleType(fd)) {\n    case 'TTY':\n      stream = new tty.WriteStream(fd);\n      stream._type = 'tty';\n\n      // Hack to have stream not keep the event loop alive.\n      // See https://github.com/joyent/node/issues/1726\n      if (stream._handle && stream._handle.unref) {\n        stream._handle.unref();\n      }\n      break;\n\n    case 'FILE':\n      var fs = require('fs');\n      stream = new fs.SyncWriteStream(fd, { autoClose: false });\n      stream._type = 'fs';\n      break;\n\n    case 'PIPE':\n    case 'TCP':\n      var net = require('net');\n      stream = new net.Socket({\n        fd: fd,\n        readable: false,\n        writable: true\n      });\n\n      // FIXME Should probably have an option in net.Socket to create a\n      // stream from an existing fd which is writable only. But for now\n      // we'll just add this hack and set the `readable` member to false.\n      // Test: ./node test/fixtures/echo.js < /etc/passwd\n      stream.readable = false;\n      stream.read = null;\n      stream._type = 'pipe';\n\n      // FIXME Hack to have stream not keep the event loop alive.\n      // See https://github.com/joyent/node/issues/1726\n      if (stream._handle && stream._handle.unref) {\n        stream._handle.unref();\n      }\n      break;\n\n    default:\n      // Probably an error on in uv_guess_handle()\n      throw new Error('Implement me. Unknown stream file type!');\n  }\n\n  // For supporting legacy API we put the FD here.\n  stream.fd = fd;\n\n  stream._isStdio = true;\n\n  return stream;\n}\n\n/**\n * Init logic for `debug` instances.\n *\n * Create a new `inspectOpts` object in case `useColors` is set\n * differently for a particular `debug` instance.\n */\n\nfunction init (debug) {\n  debug.inspectOpts = {};\n\n  var keys = Object.keys(exports.inspectOpts);\n  for (var i = 0; i < keys.length; i++) {\n    debug.inspectOpts[keys[i]] = exports.inspectOpts[keys[i]];\n  }\n}\n\n/**\n * Enable namespaces listed in `process.env.DEBUG` initially.\n */\n\nexports.enable(load());\n","'use strict'\n\nexports.toString = function (klass) {\n  switch (klass) {\n    case 1: return 'IN'\n    case 2: return 'CS'\n    case 3: return 'CH'\n    case 4: return 'HS'\n    case 255: return 'ANY'\n  }\n  return 'UNKNOWN_' + klass\n}\n\nexports.toClass = function (name) {\n  switch (name.toUpperCase()) {\n    case 'IN': return 1\n    case 'CS': return 2\n    case 'CH': return 3\n    case 'HS': return 4\n    case 'ANY': return 255\n  }\n  return 0\n}\n","'use strict'\n\nconst Buffer = require('buffer').Buffer\nconst types = require('./types')\nconst rcodes = require('./rcodes')\nconst opcodes = require('./opcodes')\nconst classes = require('./classes')\nconst optioncodes = require('./optioncodes')\nconst ip = require('@leichtgewicht/ip-codec')\n\nconst QUERY_FLAG = 0\nconst RESPONSE_FLAG = 1 << 15\nconst FLUSH_MASK = 1 << 15\nconst NOT_FLUSH_MASK = ~FLUSH_MASK\nconst QU_MASK = 1 << 15\nconst NOT_QU_MASK = ~QU_MASK\n\nconst name = exports.name = {}\n\nname.encode = function (str, buf, offset) {\n  if (!buf) buf = Buffer.alloc(name.encodingLength(str))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  // strip leading and trailing .\n  const n = str.replace(/^\\.|\\.$/gm, '')\n  if (n.length) {\n    const list = n.split('.')\n\n    for (let i = 0; i < list.length; i++) {\n      const len = buf.write(list[i], offset + 1)\n      buf[offset] = len\n      offset += len + 1\n    }\n  }\n\n  buf[offset++] = 0\n\n  name.encode.bytes = offset - oldOffset\n  return buf\n}\n\nname.encode.bytes = 0\n\nname.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const list = []\n  let oldOffset = offset\n  let totalLength = 0\n  let consumedBytes = 0\n  let jumped = false\n\n  while (true) {\n    if (offset >= buf.length) {\n      throw new Error('Cannot decode name (buffer overflow)')\n    }\n    const len = buf[offset++]\n    consumedBytes += jumped ? 0 : 1\n\n    if (len === 0) {\n      break\n    } else if ((len & 0xc0) === 0) {\n      if (offset + len > buf.length) {\n        throw new Error('Cannot decode name (buffer overflow)')\n      }\n      totalLength += len + 1\n      if (totalLength > 254) {\n        throw new Error('Cannot decode name (name too long)')\n      }\n      list.push(buf.toString('utf-8', offset, offset + len))\n      offset += len\n      consumedBytes += jumped ? 0 : len\n    } else if ((len & 0xc0) === 0xc0) {\n      if (offset + 1 > buf.length) {\n        throw new Error('Cannot decode name (buffer overflow)')\n      }\n      const jumpOffset = buf.readUInt16BE(offset - 1) - 0xc000\n      if (jumpOffset >= oldOffset) {\n        // Allow only pointers to prior data. RFC 1035, section 4.1.4 states:\n        // \"[...] an entire domain name or a list of labels at the end of a domain name\n        // is replaced with a pointer to a prior occurance (sic) of the same name.\"\n        throw new Error('Cannot decode name (bad pointer)')\n      }\n      offset = jumpOffset\n      oldOffset = jumpOffset\n      consumedBytes += jumped ? 0 : 1\n      jumped = true\n    } else {\n      throw new Error('Cannot decode name (bad label)')\n    }\n  }\n\n  name.decode.bytes = consumedBytes\n  return list.length === 0 ? '.' : list.join('.')\n}\n\nname.decode.bytes = 0\n\nname.encodingLength = function (n) {\n  if (n === '.' || n === '..') return 1\n  return Buffer.byteLength(n.replace(/^\\.|\\.$/gm, '')) + 2\n}\n\nconst string = {}\n\nstring.encode = function (s, buf, offset) {\n  if (!buf) buf = Buffer.alloc(string.encodingLength(s))\n  if (!offset) offset = 0\n\n  const len = buf.write(s, offset + 1)\n  buf[offset] = len\n  string.encode.bytes = len + 1\n  return buf\n}\n\nstring.encode.bytes = 0\n\nstring.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const len = buf[offset]\n  const s = buf.toString('utf-8', offset + 1, offset + 1 + len)\n  string.decode.bytes = len + 1\n  return s\n}\n\nstring.decode.bytes = 0\n\nstring.encodingLength = function (s) {\n  return Buffer.byteLength(s) + 1\n}\n\nconst header = {}\n\nheader.encode = function (h, buf, offset) {\n  if (!buf) buf = header.encodingLength(h)\n  if (!offset) offset = 0\n\n  const flags = (h.flags || 0) & 32767\n  const type = h.type === 'response' ? RESPONSE_FLAG : QUERY_FLAG\n\n  buf.writeUInt16BE(h.id || 0, offset)\n  buf.writeUInt16BE(flags | type, offset + 2)\n  buf.writeUInt16BE(h.questions.length, offset + 4)\n  buf.writeUInt16BE(h.answers.length, offset + 6)\n  buf.writeUInt16BE(h.authorities.length, offset + 8)\n  buf.writeUInt16BE(h.additionals.length, offset + 10)\n\n  return buf\n}\n\nheader.encode.bytes = 12\n\nheader.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  if (buf.length < 12) throw new Error('Header must be 12 bytes')\n  const flags = buf.readUInt16BE(offset + 2)\n\n  return {\n    id: buf.readUInt16BE(offset),\n    type: flags & RESPONSE_FLAG ? 'response' : 'query',\n    flags: flags & 32767,\n    flag_qr: ((flags >> 15) & 0x1) === 1,\n    opcode: opcodes.toString((flags >> 11) & 0xf),\n    flag_aa: ((flags >> 10) & 0x1) === 1,\n    flag_tc: ((flags >> 9) & 0x1) === 1,\n    flag_rd: ((flags >> 8) & 0x1) === 1,\n    flag_ra: ((flags >> 7) & 0x1) === 1,\n    flag_z: ((flags >> 6) & 0x1) === 1,\n    flag_ad: ((flags >> 5) & 0x1) === 1,\n    flag_cd: ((flags >> 4) & 0x1) === 1,\n    rcode: rcodes.toString(flags & 0xf),\n    questions: new Array(buf.readUInt16BE(offset + 4)),\n    answers: new Array(buf.readUInt16BE(offset + 6)),\n    authorities: new Array(buf.readUInt16BE(offset + 8)),\n    additionals: new Array(buf.readUInt16BE(offset + 10))\n  }\n}\n\nheader.decode.bytes = 12\n\nheader.encodingLength = function () {\n  return 12\n}\n\nconst runknown = exports.unknown = {}\n\nrunknown.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(runknown.encodingLength(data))\n  if (!offset) offset = 0\n\n  buf.writeUInt16BE(data.length, offset)\n  data.copy(buf, offset + 2)\n\n  runknown.encode.bytes = data.length + 2\n  return buf\n}\n\nrunknown.encode.bytes = 0\n\nrunknown.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const len = buf.readUInt16BE(offset)\n  const data = buf.slice(offset + 2, offset + 2 + len)\n  runknown.decode.bytes = len + 2\n  return data\n}\n\nrunknown.decode.bytes = 0\n\nrunknown.encodingLength = function (data) {\n  return data.length + 2\n}\n\nconst rns = exports.ns = {}\n\nrns.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rns.encodingLength(data))\n  if (!offset) offset = 0\n\n  name.encode(data, buf, offset + 2)\n  buf.writeUInt16BE(name.encode.bytes, offset)\n  rns.encode.bytes = name.encode.bytes + 2\n  return buf\n}\n\nrns.encode.bytes = 0\n\nrns.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const len = buf.readUInt16BE(offset)\n  const dd = name.decode(buf, offset + 2)\n\n  rns.decode.bytes = len + 2\n  return dd\n}\n\nrns.decode.bytes = 0\n\nrns.encodingLength = function (data) {\n  return name.encodingLength(data) + 2\n}\n\nconst rsoa = exports.soa = {}\n\nrsoa.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rsoa.encodingLength(data))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  offset += 2\n  name.encode(data.mname, buf, offset)\n  offset += name.encode.bytes\n  name.encode(data.rname, buf, offset)\n  offset += name.encode.bytes\n  buf.writeUInt32BE(data.serial || 0, offset)\n  offset += 4\n  buf.writeUInt32BE(data.refresh || 0, offset)\n  offset += 4\n  buf.writeUInt32BE(data.retry || 0, offset)\n  offset += 4\n  buf.writeUInt32BE(data.expire || 0, offset)\n  offset += 4\n  buf.writeUInt32BE(data.minimum || 0, offset)\n  offset += 4\n\n  buf.writeUInt16BE(offset - oldOffset - 2, oldOffset)\n  rsoa.encode.bytes = offset - oldOffset\n  return buf\n}\n\nrsoa.encode.bytes = 0\n\nrsoa.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  const data = {}\n  offset += 2\n  data.mname = name.decode(buf, offset)\n  offset += name.decode.bytes\n  data.rname = name.decode(buf, offset)\n  offset += name.decode.bytes\n  data.serial = buf.readUInt32BE(offset)\n  offset += 4\n  data.refresh = buf.readUInt32BE(offset)\n  offset += 4\n  data.retry = buf.readUInt32BE(offset)\n  offset += 4\n  data.expire = buf.readUInt32BE(offset)\n  offset += 4\n  data.minimum = buf.readUInt32BE(offset)\n  offset += 4\n\n  rsoa.decode.bytes = offset - oldOffset\n  return data\n}\n\nrsoa.decode.bytes = 0\n\nrsoa.encodingLength = function (data) {\n  return 22 + name.encodingLength(data.mname) + name.encodingLength(data.rname)\n}\n\nconst rtxt = exports.txt = {}\n\nrtxt.encode = function (data, buf, offset) {\n  if (!Array.isArray(data)) data = [data]\n  for (let i = 0; i < data.length; i++) {\n    if (typeof data[i] === 'string') {\n      data[i] = Buffer.from(data[i])\n    }\n    if (!Buffer.isBuffer(data[i])) {\n      throw new Error('Must be a Buffer')\n    }\n  }\n\n  if (!buf) buf = Buffer.alloc(rtxt.encodingLength(data))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  offset += 2\n\n  data.forEach(function (d) {\n    buf[offset++] = d.length\n    d.copy(buf, offset, 0, d.length)\n    offset += d.length\n  })\n\n  buf.writeUInt16BE(offset - oldOffset - 2, oldOffset)\n  rtxt.encode.bytes = offset - oldOffset\n  return buf\n}\n\nrtxt.encode.bytes = 0\n\nrtxt.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n  let remaining = buf.readUInt16BE(offset)\n  offset += 2\n\n  let data = []\n  while (remaining > 0) {\n    const len = buf[offset++]\n    --remaining\n    if (remaining < len) {\n      throw new Error('Buffer overflow')\n    }\n    data.push(buf.slice(offset, offset + len))\n    offset += len\n    remaining -= len\n  }\n\n  rtxt.decode.bytes = offset - oldOffset\n  return data\n}\n\nrtxt.decode.bytes = 0\n\nrtxt.encodingLength = function (data) {\n  if (!Array.isArray(data)) data = [data]\n  let length = 2\n  data.forEach(function (buf) {\n    if (typeof buf === 'string') {\n      length += Buffer.byteLength(buf) + 1\n    } else {\n      length += buf.length + 1\n    }\n  })\n  return length\n}\n\nconst rnull = exports.null = {}\n\nrnull.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rnull.encodingLength(data))\n  if (!offset) offset = 0\n\n  if (typeof data === 'string') data = Buffer.from(data)\n  if (!data) data = Buffer.alloc(0)\n\n  const oldOffset = offset\n  offset += 2\n\n  const len = data.length\n  data.copy(buf, offset, 0, len)\n  offset += len\n\n  buf.writeUInt16BE(offset - oldOffset - 2, oldOffset)\n  rnull.encode.bytes = offset - oldOffset\n  return buf\n}\n\nrnull.encode.bytes = 0\n\nrnull.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n  const len = buf.readUInt16BE(offset)\n\n  offset += 2\n\n  const data = buf.slice(offset, offset + len)\n  offset += len\n\n  rnull.decode.bytes = offset - oldOffset\n  return data\n}\n\nrnull.decode.bytes = 0\n\nrnull.encodingLength = function (data) {\n  if (!data) return 2\n  return (Buffer.isBuffer(data) ? data.length : Buffer.byteLength(data)) + 2\n}\n\nconst rhinfo = exports.hinfo = {}\n\nrhinfo.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rhinfo.encodingLength(data))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  offset += 2\n  string.encode(data.cpu, buf, offset)\n  offset += string.encode.bytes\n  string.encode(data.os, buf, offset)\n  offset += string.encode.bytes\n  buf.writeUInt16BE(offset - oldOffset - 2, oldOffset)\n  rhinfo.encode.bytes = offset - oldOffset\n  return buf\n}\n\nrhinfo.encode.bytes = 0\n\nrhinfo.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  const data = {}\n  offset += 2\n  data.cpu = string.decode(buf, offset)\n  offset += string.decode.bytes\n  data.os = string.decode(buf, offset)\n  offset += string.decode.bytes\n  rhinfo.decode.bytes = offset - oldOffset\n  return data\n}\n\nrhinfo.decode.bytes = 0\n\nrhinfo.encodingLength = function (data) {\n  return string.encodingLength(data.cpu) + string.encodingLength(data.os) + 2\n}\n\nconst rptr = exports.ptr = {}\nconst rcname = exports.cname = rptr\nconst rdname = exports.dname = rptr\n\nrptr.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rptr.encodingLength(data))\n  if (!offset) offset = 0\n\n  name.encode(data, buf, offset + 2)\n  buf.writeUInt16BE(name.encode.bytes, offset)\n  rptr.encode.bytes = name.encode.bytes + 2\n  return buf\n}\n\nrptr.encode.bytes = 0\n\nrptr.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const data = name.decode(buf, offset + 2)\n  rptr.decode.bytes = name.decode.bytes + 2\n  return data\n}\n\nrptr.decode.bytes = 0\n\nrptr.encodingLength = function (data) {\n  return name.encodingLength(data) + 2\n}\n\nconst rsrv = exports.srv = {}\n\nrsrv.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rsrv.encodingLength(data))\n  if (!offset) offset = 0\n\n  buf.writeUInt16BE(data.priority || 0, offset + 2)\n  buf.writeUInt16BE(data.weight || 0, offset + 4)\n  buf.writeUInt16BE(data.port || 0, offset + 6)\n  name.encode(data.target, buf, offset + 8)\n\n  const len = name.encode.bytes + 6\n  buf.writeUInt16BE(len, offset)\n\n  rsrv.encode.bytes = len + 2\n  return buf\n}\n\nrsrv.encode.bytes = 0\n\nrsrv.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const len = buf.readUInt16BE(offset)\n\n  const data = {}\n  data.priority = buf.readUInt16BE(offset + 2)\n  data.weight = buf.readUInt16BE(offset + 4)\n  data.port = buf.readUInt16BE(offset + 6)\n  data.target = name.decode(buf, offset + 8)\n\n  rsrv.decode.bytes = len + 2\n  return data\n}\n\nrsrv.decode.bytes = 0\n\nrsrv.encodingLength = function (data) {\n  return 8 + name.encodingLength(data.target)\n}\n\nconst rcaa = exports.caa = {}\n\nrcaa.ISSUER_CRITICAL = 1 << 7\n\nrcaa.encode = function (data, buf, offset) {\n  const len = rcaa.encodingLength(data)\n\n  if (!buf) buf = Buffer.alloc(rcaa.encodingLength(data))\n  if (!offset) offset = 0\n\n  if (data.issuerCritical) {\n    data.flags = rcaa.ISSUER_CRITICAL\n  }\n\n  buf.writeUInt16BE(len - 2, offset)\n  offset += 2\n  buf.writeUInt8(data.flags || 0, offset)\n  offset += 1\n  string.encode(data.tag, buf, offset)\n  offset += string.encode.bytes\n  buf.write(data.value, offset)\n  offset += Buffer.byteLength(data.value)\n\n  rcaa.encode.bytes = len\n  return buf\n}\n\nrcaa.encode.bytes = 0\n\nrcaa.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const len = buf.readUInt16BE(offset)\n  offset += 2\n\n  const oldOffset = offset\n  const data = {}\n  data.flags = buf.readUInt8(offset)\n  offset += 1\n  data.tag = string.decode(buf, offset)\n  offset += string.decode.bytes\n  data.value = buf.toString('utf-8', offset, oldOffset + len)\n\n  data.issuerCritical = !!(data.flags & rcaa.ISSUER_CRITICAL)\n\n  rcaa.decode.bytes = len + 2\n\n  return data\n}\n\nrcaa.decode.bytes = 0\n\nrcaa.encodingLength = function (data) {\n  return string.encodingLength(data.tag) + string.encodingLength(data.value) + 2\n}\n\nconst rmx = exports.mx = {}\n\nrmx.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rmx.encodingLength(data))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  offset += 2\n  buf.writeUInt16BE(data.preference || 0, offset)\n  offset += 2\n  name.encode(data.exchange, buf, offset)\n  offset += name.encode.bytes\n\n  buf.writeUInt16BE(offset - oldOffset - 2, oldOffset)\n  rmx.encode.bytes = offset - oldOffset\n  return buf\n}\n\nrmx.encode.bytes = 0\n\nrmx.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  const data = {}\n  offset += 2\n  data.preference = buf.readUInt16BE(offset)\n  offset += 2\n  data.exchange = name.decode(buf, offset)\n  offset += name.decode.bytes\n\n  rmx.decode.bytes = offset - oldOffset\n  return data\n}\n\nrmx.encodingLength = function (data) {\n  return 4 + name.encodingLength(data.exchange)\n}\n\nconst ra = exports.a = {}\n\nra.encode = function (host, buf, offset) {\n  if (!buf) buf = Buffer.alloc(ra.encodingLength(host))\n  if (!offset) offset = 0\n\n  buf.writeUInt16BE(4, offset)\n  offset += 2\n  ip.v4.encode(host, buf, offset)\n  ra.encode.bytes = 6\n  return buf\n}\n\nra.encode.bytes = 0\n\nra.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  offset += 2\n  const host = ip.v4.decode(buf, offset)\n  ra.decode.bytes = 6\n  return host\n}\n\nra.decode.bytes = 0\n\nra.encodingLength = function () {\n  return 6\n}\n\nconst raaaa = exports.aaaa = {}\n\nraaaa.encode = function (host, buf, offset) {\n  if (!buf) buf = Buffer.alloc(raaaa.encodingLength(host))\n  if (!offset) offset = 0\n\n  buf.writeUInt16BE(16, offset)\n  offset += 2\n  ip.v6.encode(host, buf, offset)\n  raaaa.encode.bytes = 18\n  return buf\n}\n\nraaaa.encode.bytes = 0\n\nraaaa.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  offset += 2\n  const host = ip.v6.decode(buf, offset)\n  raaaa.decode.bytes = 18\n  return host\n}\n\nraaaa.decode.bytes = 0\n\nraaaa.encodingLength = function () {\n  return 18\n}\n\nconst roption = exports.option = {}\n\nroption.encode = function (option, buf, offset) {\n  if (!buf) buf = Buffer.alloc(roption.encodingLength(option))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const code = optioncodes.toCode(option.code)\n  buf.writeUInt16BE(code, offset)\n  offset += 2\n  if (option.data) {\n    buf.writeUInt16BE(option.data.length, offset)\n    offset += 2\n    option.data.copy(buf, offset)\n    offset += option.data.length\n  } else {\n    switch (code) {\n      // case 3: NSID.  No encode makes sense.\n      // case 5,6,7: Not implementable\n      case 8: // ECS\n        // note: do IP math before calling\n        const spl = option.sourcePrefixLength || 0\n        const fam = option.family || ip.familyOf(option.ip)\n        const ipBuf = ip.encode(option.ip, Buffer.alloc)\n        const ipLen = Math.ceil(spl / 8)\n        buf.writeUInt16BE(ipLen + 4, offset)\n        offset += 2\n        buf.writeUInt16BE(fam, offset)\n        offset += 2\n        buf.writeUInt8(spl, offset++)\n        buf.writeUInt8(option.scopePrefixLength || 0, offset++)\n\n        ipBuf.copy(buf, offset, 0, ipLen)\n        offset += ipLen\n        break\n      // case 9: EXPIRE (experimental)\n      // case 10: COOKIE.  No encode makes sense.\n      case 11: // KEEP-ALIVE\n        if (option.timeout) {\n          buf.writeUInt16BE(2, offset)\n          offset += 2\n          buf.writeUInt16BE(option.timeout, offset)\n          offset += 2\n        } else {\n          buf.writeUInt16BE(0, offset)\n          offset += 2\n        }\n        break\n      case 12: // PADDING\n        const len = option.length || 0\n        buf.writeUInt16BE(len, offset)\n        offset += 2\n        buf.fill(0, offset, offset + len)\n        offset += len\n        break\n      // case 13:  CHAIN.  Experimental.\n      case 14: // KEY-TAG\n        const tagsLen = option.tags.length * 2\n        buf.writeUInt16BE(tagsLen, offset)\n        offset += 2\n        for (const tag of option.tags) {\n          buf.writeUInt16BE(tag, offset)\n          offset += 2\n        }\n        break\n      default:\n        throw new Error(`Unknown roption code: ${option.code}`)\n    }\n  }\n\n  roption.encode.bytes = offset - oldOffset\n  return buf\n}\n\nroption.encode.bytes = 0\n\nroption.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const option = {}\n  option.code = buf.readUInt16BE(offset)\n  option.type = optioncodes.toString(option.code)\n  offset += 2\n  const len = buf.readUInt16BE(offset)\n  offset += 2\n  option.data = buf.slice(offset, offset + len)\n  switch (option.code) {\n    // case 3: NSID.  No decode makes sense.\n    case 8: // ECS\n      option.family = buf.readUInt16BE(offset)\n      offset += 2\n      option.sourcePrefixLength = buf.readUInt8(offset++)\n      option.scopePrefixLength = buf.readUInt8(offset++)\n      const padded = Buffer.alloc((option.family === 1) ? 4 : 16)\n      buf.copy(padded, 0, offset, offset + len - 4)\n      option.ip = ip.decode(padded)\n      break\n    // case 12: Padding.  No decode makes sense.\n    case 11: // KEEP-ALIVE\n      if (len > 0) {\n        option.timeout = buf.readUInt16BE(offset)\n        offset += 2\n      }\n      break\n    case 14:\n      option.tags = []\n      for (let i = 0; i < len; i += 2) {\n        option.tags.push(buf.readUInt16BE(offset))\n        offset += 2\n      }\n    // don't worry about default.  caller will use data if desired\n  }\n\n  roption.decode.bytes = len + 4\n  return option\n}\n\nroption.decode.bytes = 0\n\nroption.encodingLength = function (option) {\n  if (option.data) {\n    return option.data.length + 4\n  }\n  const code = optioncodes.toCode(option.code)\n  switch (code) {\n    case 8: // ECS\n      const spl = option.sourcePrefixLength || 0\n      return Math.ceil(spl / 8) + 8\n    case 11: // KEEP-ALIVE\n      return (typeof option.timeout === 'number') ? 6 : 4\n    case 12: // PADDING\n      return option.length + 4\n    case 14: // KEY-TAG\n      return 4 + (option.tags.length * 2)\n  }\n  throw new Error(`Unknown roption code: ${option.code}`)\n}\n\nconst ropt = exports.opt = {}\n\nropt.encode = function (options, buf, offset) {\n  if (!buf) buf = Buffer.alloc(ropt.encodingLength(options))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const rdlen = encodingLengthList(options, roption)\n  buf.writeUInt16BE(rdlen, offset)\n  offset = encodeList(options, roption, buf, offset + 2)\n\n  ropt.encode.bytes = offset - oldOffset\n  return buf\n}\n\nropt.encode.bytes = 0\n\nropt.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const options = []\n  let rdlen = buf.readUInt16BE(offset)\n  offset += 2\n  let o = 0\n  while (rdlen > 0) {\n    options[o++] = roption.decode(buf, offset)\n    offset += roption.decode.bytes\n    rdlen -= roption.decode.bytes\n  }\n  ropt.decode.bytes = offset - oldOffset\n  return options\n}\n\nropt.decode.bytes = 0\n\nropt.encodingLength = function (options) {\n  return 2 + encodingLengthList(options || [], roption)\n}\n\nconst rdnskey = exports.dnskey = {}\n\nrdnskey.PROTOCOL_DNSSEC = 3\nrdnskey.ZONE_KEY = 0x80\nrdnskey.SECURE_ENTRYPOINT = 0x8000\n\nrdnskey.encode = function (key, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rdnskey.encodingLength(key))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const keydata = key.key\n  if (!Buffer.isBuffer(keydata)) {\n    throw new Error('Key must be a Buffer')\n  }\n\n  offset += 2 // Leave space for length\n  buf.writeUInt16BE(key.flags, offset)\n  offset += 2\n  buf.writeUInt8(rdnskey.PROTOCOL_DNSSEC, offset)\n  offset += 1\n  buf.writeUInt8(key.algorithm, offset)\n  offset += 1\n  keydata.copy(buf, offset, 0, keydata.length)\n  offset += keydata.length\n\n  rdnskey.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rdnskey.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrdnskey.encode.bytes = 0\n\nrdnskey.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var key = {}\n  var length = buf.readUInt16BE(offset)\n  offset += 2\n  key.flags = buf.readUInt16BE(offset)\n  offset += 2\n  if (buf.readUInt8(offset) !== rdnskey.PROTOCOL_DNSSEC) {\n    throw new Error('Protocol must be 3')\n  }\n  offset += 1\n  key.algorithm = buf.readUInt8(offset)\n  offset += 1\n  key.key = buf.slice(offset, oldOffset + length + 2)\n  offset += key.key.length\n  rdnskey.decode.bytes = offset - oldOffset\n  return key\n}\n\nrdnskey.decode.bytes = 0\n\nrdnskey.encodingLength = function (key) {\n  return 6 + Buffer.byteLength(key.key)\n}\n\nconst rrrsig = exports.rrsig = {}\n\nrrrsig.encode = function (sig, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rrrsig.encodingLength(sig))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const signature = sig.signature\n  if (!Buffer.isBuffer(signature)) {\n    throw new Error('Signature must be a Buffer')\n  }\n\n  offset += 2 // Leave space for length\n  buf.writeUInt16BE(types.toType(sig.typeCovered), offset)\n  offset += 2\n  buf.writeUInt8(sig.algorithm, offset)\n  offset += 1\n  buf.writeUInt8(sig.labels, offset)\n  offset += 1\n  buf.writeUInt32BE(sig.originalTTL, offset)\n  offset += 4\n  buf.writeUInt32BE(sig.expiration, offset)\n  offset += 4\n  buf.writeUInt32BE(sig.inception, offset)\n  offset += 4\n  buf.writeUInt16BE(sig.keyTag, offset)\n  offset += 2\n  name.encode(sig.signersName, buf, offset)\n  offset += name.encode.bytes\n  signature.copy(buf, offset, 0, signature.length)\n  offset += signature.length\n\n  rrrsig.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rrrsig.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrrrsig.encode.bytes = 0\n\nrrrsig.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var sig = {}\n  var length = buf.readUInt16BE(offset)\n  offset += 2\n  sig.typeCovered = types.toString(buf.readUInt16BE(offset))\n  offset += 2\n  sig.algorithm = buf.readUInt8(offset)\n  offset += 1\n  sig.labels = buf.readUInt8(offset)\n  offset += 1\n  sig.originalTTL = buf.readUInt32BE(offset)\n  offset += 4\n  sig.expiration = buf.readUInt32BE(offset)\n  offset += 4\n  sig.inception = buf.readUInt32BE(offset)\n  offset += 4\n  sig.keyTag = buf.readUInt16BE(offset)\n  offset += 2\n  sig.signersName = name.decode(buf, offset)\n  offset += name.decode.bytes\n  sig.signature = buf.slice(offset, oldOffset + length + 2)\n  offset += sig.signature.length\n  rrrsig.decode.bytes = offset - oldOffset\n  return sig\n}\n\nrrrsig.decode.bytes = 0\n\nrrrsig.encodingLength = function (sig) {\n  return 20 +\n    name.encodingLength(sig.signersName) +\n    Buffer.byteLength(sig.signature)\n}\n\nconst rrp = exports.rp = {}\n\nrrp.encode = function (data, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rrp.encodingLength(data))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  offset += 2 // Leave space for length\n  name.encode(data.mbox || '.', buf, offset)\n  offset += name.encode.bytes\n  name.encode(data.txt || '.', buf, offset)\n  offset += name.encode.bytes\n  rrp.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rrp.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrrp.encode.bytes = 0\n\nrrp.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const data = {}\n  offset += 2\n  data.mbox = name.decode(buf, offset) || '.'\n  offset += name.decode.bytes\n  data.txt = name.decode(buf, offset) || '.'\n  offset += name.decode.bytes\n  rrp.decode.bytes = offset - oldOffset\n  return data\n}\n\nrrp.decode.bytes = 0\n\nrrp.encodingLength = function (data) {\n  return 2 + name.encodingLength(data.mbox || '.') + name.encodingLength(data.txt || '.')\n}\n\nconst typebitmap = {}\n\ntypebitmap.encode = function (typelist, buf, offset) {\n  if (!buf) buf = Buffer.alloc(typebitmap.encodingLength(typelist))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var typesByWindow = []\n  for (var i = 0; i < typelist.length; i++) {\n    var typeid = types.toType(typelist[i])\n    if (typesByWindow[typeid >> 8] === undefined) {\n      typesByWindow[typeid >> 8] = []\n    }\n    typesByWindow[typeid >> 8][(typeid >> 3) & 0x1F] |= 1 << (7 - (typeid & 0x7))\n  }\n\n  for (i = 0; i < typesByWindow.length; i++) {\n    if (typesByWindow[i] !== undefined) {\n      var windowBuf = Buffer.from(typesByWindow[i])\n      buf.writeUInt8(i, offset)\n      offset += 1\n      buf.writeUInt8(windowBuf.length, offset)\n      offset += 1\n      windowBuf.copy(buf, offset)\n      offset += windowBuf.length\n    }\n  }\n\n  typebitmap.encode.bytes = offset - oldOffset\n  return buf\n}\n\ntypebitmap.encode.bytes = 0\n\ntypebitmap.decode = function (buf, offset, length) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var typelist = []\n  while (offset - oldOffset < length) {\n    var window = buf.readUInt8(offset)\n    offset += 1\n    var windowLength = buf.readUInt8(offset)\n    offset += 1\n    for (var i = 0; i < windowLength; i++) {\n      var b = buf.readUInt8(offset + i)\n      for (var j = 0; j < 8; j++) {\n        if (b & (1 << (7 - j))) {\n          var typeid = types.toString((window << 8) | (i << 3) | j)\n          typelist.push(typeid)\n        }\n      }\n    }\n    offset += windowLength\n  }\n\n  typebitmap.decode.bytes = offset - oldOffset\n  return typelist\n}\n\ntypebitmap.decode.bytes = 0\n\ntypebitmap.encodingLength = function (typelist) {\n  var extents = []\n  for (var i = 0; i < typelist.length; i++) {\n    var typeid = types.toType(typelist[i])\n    extents[typeid >> 8] = Math.max(extents[typeid >> 8] || 0, typeid & 0xFF)\n  }\n\n  var len = 0\n  for (i = 0; i < extents.length; i++) {\n    if (extents[i] !== undefined) {\n      len += 2 + Math.ceil((extents[i] + 1) / 8)\n    }\n  }\n\n  return len\n}\n\nconst rnsec = exports.nsec = {}\n\nrnsec.encode = function (record, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rnsec.encodingLength(record))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  offset += 2 // Leave space for length\n  name.encode(record.nextDomain, buf, offset)\n  offset += name.encode.bytes\n  typebitmap.encode(record.rrtypes, buf, offset)\n  offset += typebitmap.encode.bytes\n\n  rnsec.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rnsec.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrnsec.encode.bytes = 0\n\nrnsec.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var record = {}\n  var length = buf.readUInt16BE(offset)\n  offset += 2\n  record.nextDomain = name.decode(buf, offset)\n  offset += name.decode.bytes\n  record.rrtypes = typebitmap.decode(buf, offset, length - (offset - oldOffset))\n  offset += typebitmap.decode.bytes\n\n  rnsec.decode.bytes = offset - oldOffset\n  return record\n}\n\nrnsec.decode.bytes = 0\n\nrnsec.encodingLength = function (record) {\n  return 2 +\n    name.encodingLength(record.nextDomain) +\n    typebitmap.encodingLength(record.rrtypes)\n}\n\nconst rnsec3 = exports.nsec3 = {}\n\nrnsec3.encode = function (record, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rnsec3.encodingLength(record))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const salt = record.salt\n  if (!Buffer.isBuffer(salt)) {\n    throw new Error('salt must be a Buffer')\n  }\n\n  const nextDomain = record.nextDomain\n  if (!Buffer.isBuffer(nextDomain)) {\n    throw new Error('nextDomain must be a Buffer')\n  }\n\n  offset += 2 // Leave space for length\n  buf.writeUInt8(record.algorithm, offset)\n  offset += 1\n  buf.writeUInt8(record.flags, offset)\n  offset += 1\n  buf.writeUInt16BE(record.iterations, offset)\n  offset += 2\n  buf.writeUInt8(salt.length, offset)\n  offset += 1\n  salt.copy(buf, offset, 0, salt.length)\n  offset += salt.length\n  buf.writeUInt8(nextDomain.length, offset)\n  offset += 1\n  nextDomain.copy(buf, offset, 0, nextDomain.length)\n  offset += nextDomain.length\n  typebitmap.encode(record.rrtypes, buf, offset)\n  offset += typebitmap.encode.bytes\n\n  rnsec3.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rnsec3.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrnsec3.encode.bytes = 0\n\nrnsec3.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var record = {}\n  var length = buf.readUInt16BE(offset)\n  offset += 2\n  record.algorithm = buf.readUInt8(offset)\n  offset += 1\n  record.flags = buf.readUInt8(offset)\n  offset += 1\n  record.iterations = buf.readUInt16BE(offset)\n  offset += 2\n  const saltLength = buf.readUInt8(offset)\n  offset += 1\n  record.salt = buf.slice(offset, offset + saltLength)\n  offset += saltLength\n  const hashLength = buf.readUInt8(offset)\n  offset += 1\n  record.nextDomain = buf.slice(offset, offset + hashLength)\n  offset += hashLength\n  record.rrtypes = typebitmap.decode(buf, offset, length - (offset - oldOffset))\n  offset += typebitmap.decode.bytes\n\n  rnsec3.decode.bytes = offset - oldOffset\n  return record\n}\n\nrnsec3.decode.bytes = 0\n\nrnsec3.encodingLength = function (record) {\n  return 8 +\n    record.salt.length +\n    record.nextDomain.length +\n    typebitmap.encodingLength(record.rrtypes)\n}\n\nconst rds = exports.ds = {}\n\nrds.encode = function (digest, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rds.encodingLength(digest))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const digestdata = digest.digest\n  if (!Buffer.isBuffer(digestdata)) {\n    throw new Error('Digest must be a Buffer')\n  }\n\n  offset += 2 // Leave space for length\n  buf.writeUInt16BE(digest.keyTag, offset)\n  offset += 2\n  buf.writeUInt8(digest.algorithm, offset)\n  offset += 1\n  buf.writeUInt8(digest.digestType, offset)\n  offset += 1\n  digestdata.copy(buf, offset, 0, digestdata.length)\n  offset += digestdata.length\n\n  rds.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rds.encode.bytes - 2, oldOffset)\n  return buf\n}\n\nrds.encode.bytes = 0\n\nrds.decode = function (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  var digest = {}\n  var length = buf.readUInt16BE(offset)\n  offset += 2\n  digest.keyTag = buf.readUInt16BE(offset)\n  offset += 2\n  digest.algorithm = buf.readUInt8(offset)\n  offset += 1\n  digest.digestType = buf.readUInt8(offset)\n  offset += 1\n  digest.digest = buf.slice(offset, oldOffset + length + 2)\n  offset += digest.digest.length\n  rds.decode.bytes = offset - oldOffset\n  return digest\n}\n\nrds.decode.bytes = 0\n\nrds.encodingLength = function (digest) {\n  return 6 + Buffer.byteLength(digest.digest)\n}\n\nconst rsshfp = exports.sshfp = {}\n\nrsshfp.getFingerprintLengthForHashType = function getFingerprintLengthForHashType (hashType) {\n  switch (hashType) {\n    case 1: return 20\n    case 2: return 32\n  }\n}\n\nrsshfp.encode = function encode (record, buf, offset) {\n  if (!buf) buf = Buffer.alloc(rsshfp.encodingLength(record))\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  offset += 2 // The function call starts with the offset pointer at the RDLENGTH field, not the RDATA one\n  buf[offset] = record.algorithm\n  offset += 1\n  buf[offset] = record.hash\n  offset += 1\n\n  const fingerprintBuf = Buffer.from(record.fingerprint.toUpperCase(), 'hex')\n  if (fingerprintBuf.length !== rsshfp.getFingerprintLengthForHashType(record.hash)) {\n    throw new Error('Invalid fingerprint length')\n  }\n  fingerprintBuf.copy(buf, offset)\n  offset += fingerprintBuf.byteLength\n\n  rsshfp.encode.bytes = offset - oldOffset\n  buf.writeUInt16BE(rsshfp.encode.bytes - 2, oldOffset)\n\n  return buf\n}\n\nrsshfp.encode.bytes = 0\n\nrsshfp.decode = function decode (buf, offset) {\n  if (!offset) offset = 0\n  const oldOffset = offset\n\n  const record = {}\n  offset += 2 // Account for the RDLENGTH field\n  record.algorithm = buf[offset]\n  offset += 1\n  record.hash = buf[offset]\n  offset += 1\n\n  const fingerprintLength = rsshfp.getFingerprintLengthForHashType(record.hash)\n  record.fingerprint = buf.slice(offset, offset + fingerprintLength).toString('hex').toUpperCase()\n  offset += fingerprintLength\n  rsshfp.decode.bytes = offset - oldOffset\n  return record\n}\n\nrsshfp.decode.bytes = 0\n\nrsshfp.encodingLength = function (record) {\n  return 4 + Buffer.from(record.fingerprint, 'hex').byteLength\n}\n\nconst renc = exports.record = function (type) {\n  switch (type.toUpperCase()) {\n    case 'A': return ra\n    case 'PTR': return rptr\n    case 'CNAME': return rcname\n    case 'DNAME': return rdname\n    case 'TXT': return rtxt\n    case 'NULL': return rnull\n    case 'AAAA': return raaaa\n    case 'SRV': return rsrv\n    case 'HINFO': return rhinfo\n    case 'CAA': return rcaa\n    case 'NS': return rns\n    case 'SOA': return rsoa\n    case 'MX': return rmx\n    case 'OPT': return ropt\n    case 'DNSKEY': return rdnskey\n    case 'RRSIG': return rrrsig\n    case 'RP': return rrp\n    case 'NSEC': return rnsec\n    case 'NSEC3': return rnsec3\n    case 'SSHFP': return rsshfp\n    case 'DS': return rds\n  }\n  return runknown\n}\n\nconst answer = exports.answer = {}\n\nanswer.encode = function (a, buf, offset) {\n  if (!buf) buf = Buffer.alloc(answer.encodingLength(a))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  name.encode(a.name, buf, offset)\n  offset += name.encode.bytes\n\n  buf.writeUInt16BE(types.toType(a.type), offset)\n\n  if (a.type.toUpperCase() === 'OPT') {\n    if (a.name !== '.') {\n      throw new Error('OPT name must be root.')\n    }\n    buf.writeUInt16BE(a.udpPayloadSize || 4096, offset + 2)\n    buf.writeUInt8(a.extendedRcode || 0, offset + 4)\n    buf.writeUInt8(a.ednsVersion || 0, offset + 5)\n    buf.writeUInt16BE(a.flags || 0, offset + 6)\n\n    offset += 8\n    ropt.encode(a.options || [], buf, offset)\n    offset += ropt.encode.bytes\n  } else {\n    let klass = classes.toClass(a.class === undefined ? 'IN' : a.class)\n    if (a.flush) klass |= FLUSH_MASK // the 1st bit of the class is the flush bit\n    buf.writeUInt16BE(klass, offset + 2)\n    buf.writeUInt32BE(a.ttl || 0, offset + 4)\n\n    offset += 8\n    const enc = renc(a.type)\n    enc.encode(a.data, buf, offset)\n    offset += enc.encode.bytes\n  }\n\n  answer.encode.bytes = offset - oldOffset\n  return buf\n}\n\nanswer.encode.bytes = 0\n\nanswer.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const a = {}\n  const oldOffset = offset\n\n  a.name = name.decode(buf, offset)\n  offset += name.decode.bytes\n  a.type = types.toString(buf.readUInt16BE(offset))\n  if (a.type === 'OPT') {\n    a.udpPayloadSize = buf.readUInt16BE(offset + 2)\n    a.extendedRcode = buf.readUInt8(offset + 4)\n    a.ednsVersion = buf.readUInt8(offset + 5)\n    a.flags = buf.readUInt16BE(offset + 6)\n    a.flag_do = ((a.flags >> 15) & 0x1) === 1\n    a.options = ropt.decode(buf, offset + 8)\n    offset += 8 + ropt.decode.bytes\n  } else {\n    const klass = buf.readUInt16BE(offset + 2)\n    a.ttl = buf.readUInt32BE(offset + 4)\n\n    a.class = classes.toString(klass & NOT_FLUSH_MASK)\n    a.flush = !!(klass & FLUSH_MASK)\n\n    const enc = renc(a.type)\n    a.data = enc.decode(buf, offset + 8)\n    offset += 8 + enc.decode.bytes\n  }\n\n  answer.decode.bytes = offset - oldOffset\n  return a\n}\n\nanswer.decode.bytes = 0\n\nanswer.encodingLength = function (a) {\n  const data = (a.data !== null && a.data !== undefined) ? a.data : a.options\n  return name.encodingLength(a.name) + 8 + renc(a.type).encodingLength(data)\n}\n\nconst question = exports.question = {}\n\nquestion.encode = function (q, buf, offset) {\n  if (!buf) buf = Buffer.alloc(question.encodingLength(q))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  name.encode(q.name, buf, offset)\n  offset += name.encode.bytes\n\n  buf.writeUInt16BE(types.toType(q.type), offset)\n  offset += 2\n\n  buf.writeUInt16BE(classes.toClass(q.class === undefined ? 'IN' : q.class), offset)\n  offset += 2\n\n  question.encode.bytes = offset - oldOffset\n  return q\n}\n\nquestion.encode.bytes = 0\n\nquestion.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  const q = {}\n\n  q.name = name.decode(buf, offset)\n  offset += name.decode.bytes\n\n  q.type = types.toString(buf.readUInt16BE(offset))\n  offset += 2\n\n  q.class = classes.toString(buf.readUInt16BE(offset))\n  offset += 2\n\n  const qu = !!(q.class & QU_MASK)\n  if (qu) q.class &= NOT_QU_MASK\n\n  question.decode.bytes = offset - oldOffset\n  return q\n}\n\nquestion.decode.bytes = 0\n\nquestion.encodingLength = function (q) {\n  return name.encodingLength(q.name) + 4\n}\n\nexports.AUTHORITATIVE_ANSWER = 1 << 10\nexports.TRUNCATED_RESPONSE = 1 << 9\nexports.RECURSION_DESIRED = 1 << 8\nexports.RECURSION_AVAILABLE = 1 << 7\nexports.AUTHENTIC_DATA = 1 << 5\nexports.CHECKING_DISABLED = 1 << 4\nexports.DNSSEC_OK = 1 << 15\n\nexports.encode = function (result, buf, offset) {\n  const allocing = !buf\n\n  if (allocing) buf = Buffer.alloc(exports.encodingLength(result))\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n\n  if (!result.questions) result.questions = []\n  if (!result.answers) result.answers = []\n  if (!result.authorities) result.authorities = []\n  if (!result.additionals) result.additionals = []\n\n  header.encode(result, buf, offset)\n  offset += header.encode.bytes\n\n  offset = encodeList(result.questions, question, buf, offset)\n  offset = encodeList(result.answers, answer, buf, offset)\n  offset = encodeList(result.authorities, answer, buf, offset)\n  offset = encodeList(result.additionals, answer, buf, offset)\n\n  exports.encode.bytes = offset - oldOffset\n\n  // just a quick sanity check\n  if (allocing && exports.encode.bytes !== buf.length) {\n    return buf.slice(0, exports.encode.bytes)\n  }\n\n  return buf\n}\n\nexports.encode.bytes = 0\n\nexports.decode = function (buf, offset) {\n  if (!offset) offset = 0\n\n  const oldOffset = offset\n  const result = header.decode(buf, offset)\n  offset += header.decode.bytes\n\n  offset = decodeList(result.questions, question, buf, offset)\n  offset = decodeList(result.answers, answer, buf, offset)\n  offset = decodeList(result.authorities, answer, buf, offset)\n  offset = decodeList(result.additionals, answer, buf, offset)\n\n  exports.decode.bytes = offset - oldOffset\n\n  return result\n}\n\nexports.decode.bytes = 0\n\nexports.encodingLength = function (result) {\n  return header.encodingLength(result) +\n    encodingLengthList(result.questions || [], question) +\n    encodingLengthList(result.answers || [], answer) +\n    encodingLengthList(result.authorities || [], answer) +\n    encodingLengthList(result.additionals || [], answer)\n}\n\nexports.streamEncode = function (result) {\n  const buf = exports.encode(result)\n  const sbuf = Buffer.alloc(2)\n  sbuf.writeUInt16BE(buf.byteLength)\n  const combine = Buffer.concat([sbuf, buf])\n  exports.streamEncode.bytes = combine.byteLength\n  return combine\n}\n\nexports.streamEncode.bytes = 0\n\nexports.streamDecode = function (sbuf) {\n  const len = sbuf.readUInt16BE(0)\n  if (sbuf.byteLength < len + 2) {\n    // not enough data\n    return null\n  }\n  const result = exports.decode(sbuf.slice(2))\n  exports.streamDecode.bytes = exports.decode.bytes\n  return result\n}\n\nexports.streamDecode.bytes = 0\n\nfunction encodingLengthList (list, enc) {\n  let len = 0\n  for (let i = 0; i < list.length; i++) len += enc.encodingLength(list[i])\n  return len\n}\n\nfunction encodeList (list, enc, buf, offset) {\n  for (let i = 0; i < list.length; i++) {\n    enc.encode(list[i], buf, offset)\n    offset += enc.encode.bytes\n  }\n  return offset\n}\n\nfunction decodeList (list, enc, buf, offset) {\n  for (let i = 0; i < list.length; i++) {\n    list[i] = enc.decode(buf, offset)\n    offset += enc.decode.bytes\n  }\n  return offset\n}\n","'use strict'\n\n/*\n * Traditional DNS header OPCODEs (4-bits) defined by IANA in\n * https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml#dns-parameters-5\n */\n\nexports.toString = function (opcode) {\n  switch (opcode) {\n    case 0: return 'QUERY'\n    case 1: return 'IQUERY'\n    case 2: return 'STATUS'\n    case 3: return 'OPCODE_3'\n    case 4: return 'NOTIFY'\n    case 5: return 'UPDATE'\n    case 6: return 'OPCODE_6'\n    case 7: return 'OPCODE_7'\n    case 8: return 'OPCODE_8'\n    case 9: return 'OPCODE_9'\n    case 10: return 'OPCODE_10'\n    case 11: return 'OPCODE_11'\n    case 12: return 'OPCODE_12'\n    case 13: return 'OPCODE_13'\n    case 14: return 'OPCODE_14'\n    case 15: return 'OPCODE_15'\n  }\n  return 'OPCODE_' + opcode\n}\n\nexports.toOpcode = function (code) {\n  switch (code.toUpperCase()) {\n    case 'QUERY': return 0\n    case 'IQUERY': return 1\n    case 'STATUS': return 2\n    case 'OPCODE_3': return 3\n    case 'NOTIFY': return 4\n    case 'UPDATE': return 5\n    case 'OPCODE_6': return 6\n    case 'OPCODE_7': return 7\n    case 'OPCODE_8': return 8\n    case 'OPCODE_9': return 9\n    case 'OPCODE_10': return 10\n    case 'OPCODE_11': return 11\n    case 'OPCODE_12': return 12\n    case 'OPCODE_13': return 13\n    case 'OPCODE_14': return 14\n    case 'OPCODE_15': return 15\n  }\n  return 0\n}\n","'use strict'\n\nexports.toString = function (type) {\n  switch (type) {\n    // list at\n    // https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml#dns-parameters-11\n    case 1: return 'LLQ'\n    case 2: return 'UL'\n    case 3: return 'NSID'\n    case 5: return 'DAU'\n    case 6: return 'DHU'\n    case 7: return 'N3U'\n    case 8: return 'CLIENT_SUBNET'\n    case 9: return 'EXPIRE'\n    case 10: return 'COOKIE'\n    case 11: return 'TCP_KEEPALIVE'\n    case 12: return 'PADDING'\n    case 13: return 'CHAIN'\n    case 14: return 'KEY_TAG'\n    case 26946: return 'DEVICEID'\n  }\n  if (type < 0) {\n    return null\n  }\n  return `OPTION_${type}`\n}\n\nexports.toCode = function (name) {\n  if (typeof name === 'number') {\n    return name\n  }\n  if (!name) {\n    return -1\n  }\n  switch (name.toUpperCase()) {\n    case 'OPTION_0': return 0\n    case 'LLQ': return 1\n    case 'UL': return 2\n    case 'NSID': return 3\n    case 'OPTION_4': return 4\n    case 'DAU': return 5\n    case 'DHU': return 6\n    case 'N3U': return 7\n    case 'CLIENT_SUBNET': return 8\n    case 'EXPIRE': return 9\n    case 'COOKIE': return 10\n    case 'TCP_KEEPALIVE': return 11\n    case 'PADDING': return 12\n    case 'CHAIN': return 13\n    case 'KEY_TAG': return 14\n    case 'DEVICEID': return 26946\n    case 'OPTION_65535': return 65535\n  }\n  const m = name.match(/_(\\d+)$/)\n  if (m) {\n    return parseInt(m[1], 10)\n  }\n  return -1\n}\n","'use strict'\n\n/*\n * Traditional DNS header RCODEs (4-bits) defined by IANA in\n * https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml\n */\n\nexports.toString = function (rcode) {\n  switch (rcode) {\n    case 0: return 'NOERROR'\n    case 1: return 'FORMERR'\n    case 2: return 'SERVFAIL'\n    case 3: return 'NXDOMAIN'\n    case 4: return 'NOTIMP'\n    case 5: return 'REFUSED'\n    case 6: return 'YXDOMAIN'\n    case 7: return 'YXRRSET'\n    case 8: return 'NXRRSET'\n    case 9: return 'NOTAUTH'\n    case 10: return 'NOTZONE'\n    case 11: return 'RCODE_11'\n    case 12: return 'RCODE_12'\n    case 13: return 'RCODE_13'\n    case 14: return 'RCODE_14'\n    case 15: return 'RCODE_15'\n  }\n  return 'RCODE_' + rcode\n}\n\nexports.toRcode = function (code) {\n  switch (code.toUpperCase()) {\n    case 'NOERROR': return 0\n    case 'FORMERR': return 1\n    case 'SERVFAIL': return 2\n    case 'NXDOMAIN': return 3\n    case 'NOTIMP': return 4\n    case 'REFUSED': return 5\n    case 'YXDOMAIN': return 6\n    case 'YXRRSET': return 7\n    case 'NXRRSET': return 8\n    case 'NOTAUTH': return 9\n    case 'NOTZONE': return 10\n    case 'RCODE_11': return 11\n    case 'RCODE_12': return 12\n    case 'RCODE_13': return 13\n    case 'RCODE_14': return 14\n    case 'RCODE_15': return 15\n  }\n  return 0\n}\n","'use strict'\n\nexports.toString = function (type) {\n  switch (type) {\n    case 1: return 'A'\n    case 10: return 'NULL'\n    case 28: return 'AAAA'\n    case 18: return 'AFSDB'\n    case 42: return 'APL'\n    case 257: return 'CAA'\n    case 60: return 'CDNSKEY'\n    case 59: return 'CDS'\n    case 37: return 'CERT'\n    case 5: return 'CNAME'\n    case 49: return 'DHCID'\n    case 32769: return 'DLV'\n    case 39: return 'DNAME'\n    case 48: return 'DNSKEY'\n    case 43: return 'DS'\n    case 55: return 'HIP'\n    case 13: return 'HINFO'\n    case 45: return 'IPSECKEY'\n    case 25: return 'KEY'\n    case 36: return 'KX'\n    case 29: return 'LOC'\n    case 15: return 'MX'\n    case 35: return 'NAPTR'\n    case 2: return 'NS'\n    case 47: return 'NSEC'\n    case 50: return 'NSEC3'\n    case 51: return 'NSEC3PARAM'\n    case 12: return 'PTR'\n    case 46: return 'RRSIG'\n    case 17: return 'RP'\n    case 24: return 'SIG'\n    case 6: return 'SOA'\n    case 99: return 'SPF'\n    case 33: return 'SRV'\n    case 44: return 'SSHFP'\n    case 32768: return 'TA'\n    case 249: return 'TKEY'\n    case 52: return 'TLSA'\n    case 250: return 'TSIG'\n    case 16: return 'TXT'\n    case 252: return 'AXFR'\n    case 251: return 'IXFR'\n    case 41: return 'OPT'\n    case 255: return 'ANY'\n  }\n  return 'UNKNOWN_' + type\n}\n\nexports.toType = function (name) {\n  switch (name.toUpperCase()) {\n    case 'A': return 1\n    case 'NULL': return 10\n    case 'AAAA': return 28\n    case 'AFSDB': return 18\n    case 'APL': return 42\n    case 'CAA': return 257\n    case 'CDNSKEY': return 60\n    case 'CDS': return 59\n    case 'CERT': return 37\n    case 'CNAME': return 5\n    case 'DHCID': return 49\n    case 'DLV': return 32769\n    case 'DNAME': return 39\n    case 'DNSKEY': return 48\n    case 'DS': return 43\n    case 'HIP': return 55\n    case 'HINFO': return 13\n    case 'IPSECKEY': return 45\n    case 'KEY': return 25\n    case 'KX': return 36\n    case 'LOC': return 29\n    case 'MX': return 15\n    case 'NAPTR': return 35\n    case 'NS': return 2\n    case 'NSEC': return 47\n    case 'NSEC3': return 50\n    case 'NSEC3PARAM': return 51\n    case 'PTR': return 12\n    case 'RRSIG': return 46\n    case 'RP': return 17\n    case 'SIG': return 24\n    case 'SOA': return 6\n    case 'SPF': return 99\n    case 'SRV': return 33\n    case 'SSHFP': return 44\n    case 'TA': return 32768\n    case 'TKEY': return 249\n    case 'TLSA': return 52\n    case 'TSIG': return 250\n    case 'TXT': return 16\n    case 'AXFR': return 252\n    case 'IXFR': return 251\n    case 'OPT': return 41\n    case 'ANY': return 255\n    case '*': return 255\n  }\n  if (name.toUpperCase().startsWith('UNKNOWN_')) return parseInt(name.slice(8))\n  return 0\n}\n","var debug;\n\nmodule.exports = function () {\n  if (!debug) {\n    try {\n      /* eslint global-require: off */\n      debug = require(\"debug\")(\"follow-redirects\");\n    }\n    catch (error) { /* */ }\n    if (typeof debug !== \"function\") {\n      debug = function () { /* */ };\n    }\n  }\n  debug.apply(null, arguments);\n};\n","var url = require(\"url\");\nvar URL = url.URL;\nvar http = require(\"http\");\nvar https = require(\"https\");\nvar Writable = require(\"stream\").Writable;\nvar assert = require(\"assert\");\nvar debug = require(\"./debug\");\n\n// Create handlers that pass events from native requests\nvar events = [\"abort\", \"aborted\", \"connect\", \"error\", \"socket\", \"timeout\"];\nvar eventHandlers = Object.create(null);\nevents.forEach(function (event) {\n  eventHandlers[event] = function (arg1, arg2, arg3) {\n    this._redirectable.emit(event, arg1, arg2, arg3);\n  };\n});\n\n// Error types with codes\nvar RedirectionError = createErrorType(\n  \"ERR_FR_REDIRECTION_FAILURE\",\n  \"Redirected request failed\"\n);\nvar TooManyRedirectsError = createErrorType(\n  \"ERR_FR_TOO_MANY_REDIRECTS\",\n  \"Maximum number of redirects exceeded\"\n);\nvar MaxBodyLengthExceededError = createErrorType(\n  \"ERR_FR_MAX_BODY_LENGTH_EXCEEDED\",\n  \"Request body larger than maxBodyLength limit\"\n);\nvar WriteAfterEndError = createErrorType(\n  \"ERR_STREAM_WRITE_AFTER_END\",\n  \"write after end\"\n);\n\n// An HTTP(S) request that can be redirected\nfunction RedirectableRequest(options, responseCallback) {\n  // Initialize the request\n  Writable.call(this);\n  this._sanitizeOptions(options);\n  this._options = options;\n  this._ended = false;\n  this._ending = false;\n  this._redirectCount = 0;\n  this._redirects = [];\n  this._requestBodyLength = 0;\n  this._requestBodyBuffers = [];\n\n  // Attach a callback if passed\n  if (responseCallback) {\n    this.on(\"response\", responseCallback);\n  }\n\n  // React to responses of native requests\n  var self = this;\n  this._onNativeResponse = function (response) {\n    self._processResponse(response);\n  };\n\n  // Perform the first request\n  this._performRequest();\n}\nRedirectableRequest.prototype = Object.create(Writable.prototype);\n\nRedirectableRequest.prototype.abort = function () {\n  abortRequest(this._currentRequest);\n  this.emit(\"abort\");\n};\n\n// Writes buffered data to the current native request\nRedirectableRequest.prototype.write = function (data, encoding, callback) {\n  // Writing is not allowed if end has been called\n  if (this._ending) {\n    throw new WriteAfterEndError();\n  }\n\n  // Validate input and shift parameters if necessary\n  if (!(typeof data === \"string\" || typeof data === \"object\" && (\"length\" in data))) {\n    throw new TypeError(\"data should be a string, Buffer or Uint8Array\");\n  }\n  if (typeof encoding === \"function\") {\n    callback = encoding;\n    encoding = null;\n  }\n\n  // Ignore empty buffers, since writing them doesn't invoke the callback\n  // https://github.com/nodejs/node/issues/22066\n  if (data.length === 0) {\n    if (callback) {\n      callback();\n    }\n    return;\n  }\n  // Only write when we don't exceed the maximum body length\n  if (this._requestBodyLength + data.length <= this._options.maxBodyLength) {\n    this._requestBodyLength += data.length;\n    this._requestBodyBuffers.push({ data: data, encoding: encoding });\n    this._currentRequest.write(data, encoding, callback);\n  }\n  // Error when we exceed the maximum body length\n  else {\n    this.emit(\"error\", new MaxBodyLengthExceededError());\n    this.abort();\n  }\n};\n\n// Ends the current native request\nRedirectableRequest.prototype.end = function (data, encoding, callback) {\n  // Shift parameters if necessary\n  if (typeof data === \"function\") {\n    callback = data;\n    data = encoding = null;\n  }\n  else if (typeof encoding === \"function\") {\n    callback = encoding;\n    encoding = null;\n  }\n\n  // Write data if needed and end\n  if (!data) {\n    this._ended = this._ending = true;\n    this._currentRequest.end(null, null, callback);\n  }\n  else {\n    var self = this;\n    var currentRequest = this._currentRequest;\n    this.write(data, encoding, function () {\n      self._ended = true;\n      currentRequest.end(null, null, callback);\n    });\n    this._ending = true;\n  }\n};\n\n// Sets a header value on the current native request\nRedirectableRequest.prototype.setHeader = function (name, value) {\n  this._options.headers[name] = value;\n  this._currentRequest.setHeader(name, value);\n};\n\n// Clears a header value on the current native request\nRedirectableRequest.prototype.removeHeader = function (name) {\n  delete this._options.headers[name];\n  this._currentRequest.removeHeader(name);\n};\n\n// Global timeout for all underlying requests\nRedirectableRequest.prototype.setTimeout = function (msecs, callback) {\n  var self = this;\n\n  // Destroys the socket on timeout\n  function destroyOnTimeout(socket) {\n    socket.setTimeout(msecs);\n    socket.removeListener(\"timeout\", socket.destroy);\n    socket.addListener(\"timeout\", socket.destroy);\n  }\n\n  // Sets up a timer to trigger a timeout event\n  function startTimer(socket) {\n    if (self._timeout) {\n      clearTimeout(self._timeout);\n    }\n    self._timeout = setTimeout(function () {\n      self.emit(\"timeout\");\n      clearTimer();\n    }, msecs);\n    destroyOnTimeout(socket);\n  }\n\n  // Stops a timeout from triggering\n  function clearTimer() {\n    // Clear the timeout\n    if (self._timeout) {\n      clearTimeout(self._timeout);\n      self._timeout = null;\n    }\n\n    // Clean up all attached listeners\n    self.removeListener(\"abort\", clearTimer);\n    self.removeListener(\"error\", clearTimer);\n    self.removeListener(\"response\", clearTimer);\n    if (callback) {\n      self.removeListener(\"timeout\", callback);\n    }\n    if (!self.socket) {\n      self._currentRequest.removeListener(\"socket\", startTimer);\n    }\n  }\n\n  // Attach callback if passed\n  if (callback) {\n    this.on(\"timeout\", callback);\n  }\n\n  // Start the timer if or when the socket is opened\n  if (this.socket) {\n    startTimer(this.socket);\n  }\n  else {\n    this._currentRequest.once(\"socket\", startTimer);\n  }\n\n  // Clean up on events\n  this.on(\"socket\", destroyOnTimeout);\n  this.on(\"abort\", clearTimer);\n  this.on(\"error\", clearTimer);\n  this.on(\"response\", clearTimer);\n\n  return this;\n};\n\n// Proxy all other public ClientRequest methods\n[\n  \"flushHeaders\", \"getHeader\",\n  \"setNoDelay\", \"setSocketKeepAlive\",\n].forEach(function (method) {\n  RedirectableRequest.prototype[method] = function (a, b) {\n    return this._currentRequest[method](a, b);\n  };\n});\n\n// Proxy all public ClientRequest properties\n[\"aborted\", \"connection\", \"socket\"].forEach(function (property) {\n  Object.defineProperty(RedirectableRequest.prototype, property, {\n    get: function () { return this._currentRequest[property]; },\n  });\n});\n\nRedirectableRequest.prototype._sanitizeOptions = function (options) {\n  // Ensure headers are always present\n  if (!options.headers) {\n    options.headers = {};\n  }\n\n  // Since http.request treats host as an alias of hostname,\n  // but the url module interprets host as hostname plus port,\n  // eliminate the host property to avoid confusion.\n  if (options.host) {\n    // Use hostname if set, because it has precedence\n    if (!options.hostname) {\n      options.hostname = options.host;\n    }\n    delete options.host;\n  }\n\n  // Complete the URL object when necessary\n  if (!options.pathname && options.path) {\n    var searchPos = options.path.indexOf(\"?\");\n    if (searchPos < 0) {\n      options.pathname = options.path;\n    }\n    else {\n      options.pathname = options.path.substring(0, searchPos);\n      options.search = options.path.substring(searchPos);\n    }\n  }\n};\n\n\n// Executes the next native request (initial or redirect)\nRedirectableRequest.prototype._performRequest = function () {\n  // Load the native protocol\n  var protocol = this._options.protocol;\n  var nativeProtocol = this._options.nativeProtocols[protocol];\n  if (!nativeProtocol) {\n    this.emit(\"error\", new TypeError(\"Unsupported protocol \" + protocol));\n    return;\n  }\n\n  // If specified, use the agent corresponding to the protocol\n  // (HTTP and HTTPS use different types of agents)\n  if (this._options.agents) {\n    var scheme = protocol.slice(0, -1);\n    this._options.agent = this._options.agents[scheme];\n  }\n\n  // Create the native request and set up its event handlers\n  var request = this._currentRequest =\n        nativeProtocol.request(this._options, this._onNativeResponse);\n  request._redirectable = this;\n  for (var event of events) {\n    request.on(event, eventHandlers[event]);\n  }\n\n  // RFC72305.3.1: When making a request directly to an origin server, []\n  // a client MUST send only the absolute path [] as the request-target.\n  this._currentUrl = /^\\//.test(this._options.path) ?\n    url.format(this._options) :\n    // When making a request to a proxy, []\n    // a client MUST send the target URI in absolute-form [].\n    this._currentUrl = this._options.path;\n\n  // End a redirected request\n  // (The first request must be ended explicitly with RedirectableRequest#end)\n  if (this._isRedirect) {\n    // Write the request entity and end\n    var i = 0;\n    var self = this;\n    var buffers = this._requestBodyBuffers;\n    (function writeNext(error) {\n      // Only write if this request has not been redirected yet\n      /* istanbul ignore else */\n      if (request === self._currentRequest) {\n        // Report any write errors\n        /* istanbul ignore if */\n        if (error) {\n          self.emit(\"error\", error);\n        }\n        // Write the next buffer if there are still left\n        else if (i < buffers.length) {\n          var buffer = buffers[i++];\n          /* istanbul ignore else */\n          if (!request.finished) {\n            request.write(buffer.data, buffer.encoding, writeNext);\n          }\n        }\n        // End the request if `end` has been called on us\n        else if (self._ended) {\n          request.end();\n        }\n      }\n    }());\n  }\n};\n\n// Processes a response from the current native request\nRedirectableRequest.prototype._processResponse = function (response) {\n  // Store the redirected response\n  var statusCode = response.statusCode;\n  if (this._options.trackRedirects) {\n    this._redirects.push({\n      url: this._currentUrl,\n      headers: response.headers,\n      statusCode: statusCode,\n    });\n  }\n\n  // RFC72316.4: The 3xx (Redirection) class of status code indicates\n  // that further action needs to be taken by the user agent in order to\n  // fulfill the request. If a Location header field is provided,\n  // the user agent MAY automatically redirect its request to the URI\n  // referenced by the Location field value,\n  // even if the specific status code is not understood.\n\n  // If the response is not a redirect; return it as-is\n  var location = response.headers.location;\n  if (!location || this._options.followRedirects === false ||\n      statusCode < 300 || statusCode >= 400) {\n    response.responseUrl = this._currentUrl;\n    response.redirects = this._redirects;\n    this.emit(\"response\", response);\n\n    // Clean up\n    this._requestBodyBuffers = [];\n    return;\n  }\n\n  // The response is a redirect, so abort the current request\n  abortRequest(this._currentRequest);\n  // Discard the remainder of the response to avoid waiting for data\n  response.destroy();\n\n  // RFC72316.4: A client SHOULD detect and intervene\n  // in cyclical redirections (i.e., \"infinite\" redirection loops).\n  if (++this._redirectCount > this._options.maxRedirects) {\n    this.emit(\"error\", new TooManyRedirectsError());\n    return;\n  }\n\n  // Store the request headers if applicable\n  var requestHeaders;\n  var beforeRedirect = this._options.beforeRedirect;\n  if (beforeRedirect) {\n    requestHeaders = Object.assign({\n      // The Host header was set by nativeProtocol.request\n      Host: response.req.getHeader(\"host\"),\n    }, this._options.headers);\n  }\n\n  // RFC72316.4: Automatic redirection needs to done with\n  // care for methods not known to be safe, []\n  // RFC72316.4.23: For historical reasons, a user agent MAY change\n  // the request method from POST to GET for the subsequent request.\n  var method = this._options.method;\n  if ((statusCode === 301 || statusCode === 302) && this._options.method === \"POST\" ||\n      // RFC72316.4.4: The 303 (See Other) status code indicates that\n      // the server is redirecting the user agent to a different resource []\n      // A user agent can perform a retrieval request targeting that URI\n      // (a GET or HEAD request if using HTTP) []\n      (statusCode === 303) && !/^(?:GET|HEAD)$/.test(this._options.method)) {\n    this._options.method = \"GET\";\n    // Drop a possible entity and headers related to it\n    this._requestBodyBuffers = [];\n    removeMatchingHeaders(/^content-/i, this._options.headers);\n  }\n\n  // Drop the Host header, as the redirect might lead to a different host\n  var currentHostHeader = removeMatchingHeaders(/^host$/i, this._options.headers);\n\n  // If the redirect is relative, carry over the host of the last request\n  var currentUrlParts = url.parse(this._currentUrl);\n  var currentHost = currentHostHeader || currentUrlParts.host;\n  var currentUrl = /^\\w+:/.test(location) ? this._currentUrl :\n    url.format(Object.assign(currentUrlParts, { host: currentHost }));\n\n  // Determine the URL of the redirection\n  var redirectUrl;\n  try {\n    redirectUrl = url.resolve(currentUrl, location);\n  }\n  catch (cause) {\n    this.emit(\"error\", new RedirectionError(cause));\n    return;\n  }\n\n  // Create the redirected request\n  debug(\"redirecting to\", redirectUrl);\n  this._isRedirect = true;\n  var redirectUrlParts = url.parse(redirectUrl);\n  Object.assign(this._options, redirectUrlParts);\n\n  // Drop confidential headers when redirecting to a less secure protocol\n  // or to a different domain that is not a superdomain\n  if (redirectUrlParts.protocol !== currentUrlParts.protocol &&\n     redirectUrlParts.protocol !== \"https:\" ||\n     redirectUrlParts.host !== currentHost &&\n     !isSubdomain(redirectUrlParts.host, currentHost)) {\n    removeMatchingHeaders(/^(?:authorization|cookie)$/i, this._options.headers);\n  }\n\n  // Evaluate the beforeRedirect callback\n  if (typeof beforeRedirect === \"function\") {\n    var responseDetails = {\n      headers: response.headers,\n      statusCode: statusCode,\n    };\n    var requestDetails = {\n      url: currentUrl,\n      method: method,\n      headers: requestHeaders,\n    };\n    try {\n      beforeRedirect(this._options, responseDetails, requestDetails);\n    }\n    catch (err) {\n      this.emit(\"error\", err);\n      return;\n    }\n    this._sanitizeOptions(this._options);\n  }\n\n  // Perform the redirected request\n  try {\n    this._performRequest();\n  }\n  catch (cause) {\n    this.emit(\"error\", new RedirectionError(cause));\n  }\n};\n\n// Wraps the key/value object of protocols with redirect functionality\nfunction wrap(protocols) {\n  // Default settings\n  var exports = {\n    maxRedirects: 21,\n    maxBodyLength: 10 * 1024 * 1024,\n  };\n\n  // Wrap each protocol\n  var nativeProtocols = {};\n  Object.keys(protocols).forEach(function (scheme) {\n    var protocol = scheme + \":\";\n    var nativeProtocol = nativeProtocols[protocol] = protocols[scheme];\n    var wrappedProtocol = exports[scheme] = Object.create(nativeProtocol);\n\n    // Executes a request, following redirects\n    function request(input, options, callback) {\n      // Parse parameters\n      if (typeof input === \"string\") {\n        var urlStr = input;\n        try {\n          input = urlToOptions(new URL(urlStr));\n        }\n        catch (err) {\n          /* istanbul ignore next */\n          input = url.parse(urlStr);\n        }\n      }\n      else if (URL && (input instanceof URL)) {\n        input = urlToOptions(input);\n      }\n      else {\n        callback = options;\n        options = input;\n        input = { protocol: protocol };\n      }\n      if (typeof options === \"function\") {\n        callback = options;\n        options = null;\n      }\n\n      // Set defaults\n      options = Object.assign({\n        maxRedirects: exports.maxRedirects,\n        maxBodyLength: exports.maxBodyLength,\n      }, input, options);\n      options.nativeProtocols = nativeProtocols;\n\n      assert.equal(options.protocol, protocol, \"protocol mismatch\");\n      debug(\"options\", options);\n      return new RedirectableRequest(options, callback);\n    }\n\n    // Executes a GET request, following redirects\n    function get(input, options, callback) {\n      var wrappedRequest = wrappedProtocol.request(input, options, callback);\n      wrappedRequest.end();\n      return wrappedRequest;\n    }\n\n    // Expose the properties on the wrapped protocol\n    Object.defineProperties(wrappedProtocol, {\n      request: { value: request, configurable: true, enumerable: true, writable: true },\n      get: { value: get, configurable: true, enumerable: true, writable: true },\n    });\n  });\n  return exports;\n}\n\n/* istanbul ignore next */\nfunction noop() { /* empty */ }\n\n// from https://github.com/nodejs/node/blob/master/lib/internal/url.js\nfunction urlToOptions(urlObject) {\n  var options = {\n    protocol: urlObject.protocol,\n    hostname: urlObject.hostname.startsWith(\"[\") ?\n      /* istanbul ignore next */\n      urlObject.hostname.slice(1, -1) :\n      urlObject.hostname,\n    hash: urlObject.hash,\n    search: urlObject.search,\n    pathname: urlObject.pathname,\n    path: urlObject.pathname + urlObject.search,\n    href: urlObject.href,\n  };\n  if (urlObject.port !== \"\") {\n    options.port = Number(urlObject.port);\n  }\n  return options;\n}\n\nfunction removeMatchingHeaders(regex, headers) {\n  var lastValue;\n  for (var header in headers) {\n    if (regex.test(header)) {\n      lastValue = headers[header];\n      delete headers[header];\n    }\n  }\n  return (lastValue === null || typeof lastValue === \"undefined\") ?\n    undefined : String(lastValue).trim();\n}\n\nfunction createErrorType(code, defaultMessage) {\n  function CustomError(cause) {\n    Error.captureStackTrace(this, this.constructor);\n    if (!cause) {\n      this.message = defaultMessage;\n    }\n    else {\n      this.message = defaultMessage + \": \" + cause.message;\n      this.cause = cause;\n    }\n  }\n  CustomError.prototype = new Error();\n  CustomError.prototype.constructor = CustomError;\n  CustomError.prototype.name = \"Error [\" + code + \"]\";\n  CustomError.prototype.code = code;\n  return CustomError;\n}\n\nfunction abortRequest(request) {\n  for (var event of events) {\n    request.removeListener(event, eventHandlers[event]);\n  }\n  request.on(\"error\", noop);\n  request.abort();\n}\n\nfunction isSubdomain(subdomain, domain) {\n  const dot = subdomain.length - domain.length - 1;\n  return dot > 0 && subdomain[dot] === \".\" && subdomain.endsWith(domain);\n}\n\n// Exports\nmodule.exports = wrap({ http: http, https: https });\nmodule.exports.wrap = wrap;\n","'use strict';\n\nvar WHITELIST = [\n\t'ETIMEDOUT',\n\t'ECONNRESET',\n\t'EADDRINUSE',\n\t'ESOCKETTIMEDOUT',\n\t'ECONNREFUSED',\n\t'EPIPE',\n\t'EHOSTUNREACH',\n\t'EAI_AGAIN'\n];\n\nvar BLACKLIST = [\n\t'ENOTFOUND',\n\t'ENETUNREACH',\n\n\t// SSL errors from https://github.com/nodejs/node/blob/ed3d8b13ee9a705d89f9e0397d9e96519e7e47ac/src/node_crypto.cc#L1950\n\t'UNABLE_TO_GET_ISSUER_CERT',\n\t'UNABLE_TO_GET_CRL',\n\t'UNABLE_TO_DECRYPT_CERT_SIGNATURE',\n\t'UNABLE_TO_DECRYPT_CRL_SIGNATURE',\n\t'UNABLE_TO_DECODE_ISSUER_PUBLIC_KEY',\n\t'CERT_SIGNATURE_FAILURE',\n\t'CRL_SIGNATURE_FAILURE',\n\t'CERT_NOT_YET_VALID',\n\t'CERT_HAS_EXPIRED',\n\t'CRL_NOT_YET_VALID',\n\t'CRL_HAS_EXPIRED',\n\t'ERROR_IN_CERT_NOT_BEFORE_FIELD',\n\t'ERROR_IN_CERT_NOT_AFTER_FIELD',\n\t'ERROR_IN_CRL_LAST_UPDATE_FIELD',\n\t'ERROR_IN_CRL_NEXT_UPDATE_FIELD',\n\t'OUT_OF_MEM',\n\t'DEPTH_ZERO_SELF_SIGNED_CERT',\n\t'SELF_SIGNED_CERT_IN_CHAIN',\n\t'UNABLE_TO_GET_ISSUER_CERT_LOCALLY',\n\t'UNABLE_TO_VERIFY_LEAF_SIGNATURE',\n\t'CERT_CHAIN_TOO_LONG',\n\t'CERT_REVOKED',\n\t'INVALID_CA',\n\t'PATH_LENGTH_EXCEEDED',\n\t'INVALID_PURPOSE',\n\t'CERT_UNTRUSTED',\n\t'CERT_REJECTED'\n];\n\nmodule.exports = function (err) {\n\tif (!err || !err.code) {\n\t\treturn true;\n\t}\n\n\tif (WHITELIST.indexOf(err.code) !== -1) {\n\t\treturn true;\n\t}\n\n\tif (BLACKLIST.indexOf(err.code) !== -1) {\n\t\treturn false;\n\t}\n\n\treturn true;\n};\n","const Kafka = require('./src')\nconst PartitionAssigners = require('./src/consumer/assigners')\nconst AssignerProtocol = require('./src/consumer/assignerProtocol')\nconst Partitioners = require('./src/producer/partitioners')\nconst Compression = require('./src/protocol/message/compression')\nconst ResourceTypes = require('./src/protocol/resourceTypes')\nconst ConfigResourceTypes = require('./src/protocol/configResourceTypes')\nconst ConfigSource = require('./src/protocol/configSource')\nconst AclResourceTypes = require('./src/protocol/aclResourceTypes')\nconst AclOperationTypes = require('./src/protocol/aclOperationTypes')\nconst AclPermissionTypes = require('./src/protocol/aclPermissionTypes')\nconst ResourcePatternTypes = require('./src/protocol/resourcePatternTypes')\nconst Errors = require('./src/errors')\nconst { LEVELS } = require('./src/loggers')\n\nmodule.exports = {\n  Kafka,\n  PartitionAssigners,\n  AssignerProtocol,\n  Partitioners,\n  logLevel: LEVELS,\n  CompressionTypes: Compression.Types,\n  CompressionCodecs: Compression.Codecs,\n  /**\n   * @deprecated\n   * @see https://github.com/tulios/kafkajs/issues/649\n   *\n   * Use ConfigResourceTypes or AclResourceTypes instead.\n   */\n  ResourceTypes,\n  ConfigResourceTypes,\n  AclResourceTypes,\n  AclOperationTypes,\n  AclPermissionTypes,\n  ResourcePatternTypes,\n  ConfigSource,\n  ...Errors,\n}\n","const createRetry = require('../retry')\nconst flatten = require('../utils/flatten')\nconst waitFor = require('../utils/waitFor')\nconst groupBy = require('../utils/groupBy')\nconst createConsumer = require('../consumer')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { LEVELS } = require('../loggers')\nconst {\n  KafkaJSNonRetriableError,\n  KafkaJSDeleteGroupsError,\n  KafkaJSBrokerNotFound,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSAggregateError,\n} = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst CONFIG_RESOURCE_TYPES = require('../protocol/configResourceTypes')\nconst ACL_RESOURCE_TYPES = require('../protocol/aclResourceTypes')\nconst ACL_OPERATION_TYPES = require('../protocol/aclOperationTypes')\nconst ACL_PERMISSION_TYPES = require('../protocol/aclPermissionTypes')\nconst RESOURCE_PATTERN_TYPES = require('../protocol/resourcePatternTypes')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\n\nconst { CONNECT, DISCONNECT } = events\n\nconst NO_CONTROLLER_ID = -1\n\nconst { values, keys, entries } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `admin.events.${key}`)\n  .join(', ')\n\nconst retryOnLeaderNotAvailable = (fn, opts = {}) => {\n  const callback = async () => {\n    try {\n      return await fn()\n    } catch (e) {\n      if (e.type !== 'LEADER_NOT_AVAILABLE') {\n        throw e\n      }\n      return false\n    }\n  }\n\n  return waitFor(callback, opts)\n}\n\nconst isConsumerGroupRunning = description => ['Empty', 'Dead'].includes(description.state)\nconst findTopicPartitions = async (cluster, topic) => {\n  await cluster.addTargetTopic(topic)\n  await cluster.refreshMetadataIfNecessary()\n\n  return cluster\n    .findTopicPartitionMetadata(topic)\n    .map(({ partitionId }) => partitionId)\n    .sort()\n}\nconst indexByPartition = array =>\n  array.reduce(\n    (obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }),\n    {}\n  )\n\n/**\n *\n * @param {Object} params\n * @param {import(\"../../types\").Logger} params.logger\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {import('../../types').RetryOptions} params.retry\n * @param {import(\"../../types\").Cluster} params.cluster\n *\n * @returns {import(\"../../types\").Admin}\n */\nmodule.exports = ({\n  logger: rootLogger,\n  instrumentationEmitter: rootInstrumentationEmitter,\n  retry,\n  cluster,\n}) => {\n  const logger = rootLogger.namespace('Admin')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n\n  /**\n   * @returns {Promise}\n   */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const disconnect = async () => {\n    await cluster.disconnect()\n    instrumentationEmitter.emit(DISCONNECT)\n  }\n\n  /**\n   * @return {Promise}\n   */\n  const listTopics = async () => {\n    const { topicMetadata } = await cluster.metadata()\n    const topics = topicMetadata.map(t => t.topic)\n    return topics\n  }\n\n  /**\n   * @param {Object} request\n   * @param {array} request.topics\n   * @param {boolean} [request.validateOnly=false]\n   * @param {number} [request.timeout=5000]\n   * @param {boolean} [request.waitForLeaders=true]\n   * @return {Promise}\n   */\n  const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topics.map(({ topic }) => topic))\n    if (topicNames.size < topics.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topics array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createTopics({ topics, validateOnly, timeout })\n\n        if (waitForLeaders) {\n          const topicNamesArray = Array.from(topicNames.values())\n          await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {\n            delay: 100,\n            maxWait: timeout,\n            timeoutMessage: 'Timed out while waiting for topic leaders',\n          })\n        }\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e instanceof KafkaJSAggregateError) {\n          if (e.errors.every(error => error.type === 'TOPIC_ALREADY_EXISTS')) {\n            return false\n          }\n        }\n\n        bail(e)\n      }\n    })\n  }\n  /**\n   * @param {array} topicPartitions\n   * @param {boolean} [validateOnly=false]\n   * @param {number} [timeout=5000]\n   * @return {Promise<void>}\n   */\n  const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {\n    if (!topicPartitions || !Array.isArray(topicPartitions)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`)\n    }\n    if (topicPartitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Empty topic partitions array`)\n    }\n\n    if (topicPartitions.filter(({ topic }) => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, the topic names have to be a valid string'\n      )\n    }\n\n    const topicNames = new Set(topicPartitions.map(({ topic }) => topic))\n    if (topicNames.size < topicPartitions.length) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid topic partitions array, it cannot have multiple entries for the same topic'\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createPartitions({ topicPartitions, validateOnly, timeout })\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string[]} topics\n   * @param {number} [timeout=5000]\n   * @return {Promise}\n   */\n  const deleteTopics = async ({ topics, timeout }) => {\n    if (!topics || !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`)\n    }\n\n    if (topics.filter(topic => typeof topic !== 'string').length > 0) {\n      throw new KafkaJSNonRetriableError('Invalid topics array, the names must be a valid string')\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.deleteTopics({ topics, timeout })\n\n        // Remove deleted topics\n        for (const topic of topics) {\n          cluster.targetTopics.delete(topic)\n        }\n\n        await cluster.refreshMetadata()\n      } catch (e) {\n        if (['NOT_CONTROLLER', 'UNKNOWN_TOPIC_OR_PARTITION'].includes(e.type)) {\n          logger.warn('Could not delete topics', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        if (e.type === 'REQUEST_TIMED_OUT') {\n          logger.error(\n            'Could not delete topics, check if \"delete.topic.enable\" is set to \"true\" (the default value is \"false\") or increase the timeout',\n            {\n              error: e.message,\n              retryCount,\n              retryTime,\n            }\n          )\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   */\n\n  const fetchTopicOffsets = async topic => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const low = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: true,\n            partitions: metadata.map(p => ({ partition: p.partitionId })),\n          },\n        ])\n\n        const { partitions: highPartitions } = high.pop()\n        const { partitions: lowPartitions } = low.pop()\n        return highPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset,\n          high: offset,\n          low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition)\n            .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} [timestamp]\n   */\n\n  const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.addTargetTopic(topic)\n        await cluster.refreshMetadataIfNecessary()\n\n        const metadata = cluster.findTopicPartitionMetadata(topic)\n        const partitions = metadata.map(p => ({ partition: p.partitionId }))\n\n        const high = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromBeginning: false,\n            partitions,\n          },\n        ])\n        const { partitions: highPartitions } = high.pop()\n\n        const offsets = await cluster.fetchTopicsOffset([\n          {\n            topic,\n            fromTimestamp: timestamp,\n            partitions,\n          },\n        ])\n        const { partitions: lowPartitions } = offsets.pop()\n\n        return lowPartitions.map(({ partition, offset }) => ({\n          partition,\n          offset:\n            parseInt(offset, 10) >= 0\n              ? offset\n              : highPartitions.find(({ partition: highPartition }) => highPartition === partition)\n                  .offset,\n        }))\n      } catch (e) {\n        if (e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Fetch offsets for a topic or multiple topics\n   *\n   * Note: set either topic or topics but not both.\n   *\n   * @param {string} groupId\n   * @param {string} topic - deprecated, use the `topics` parameter. Topic to fetch offsets for.\n   * @param {string[]} topics - list of topics to fetch offsets for, defaults to `[]` which fetches all topics for `groupId`.\n   * @param {boolean} [resolveOffsets=false]\n   * @return {Promise}\n   */\n  const fetchOffsets = async ({ groupId, topic, topics, resolveOffsets = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic && !topics) {\n      topics = []\n    }\n\n    if (!topic && !Array.isArray(topics)) {\n      throw new KafkaJSNonRetriableError(`Expected topic or topics array to be set`)\n    }\n\n    if (topic && topics) {\n      throw new KafkaJSNonRetriableError(`Either topic or topics must be set, not both`)\n    }\n\n    if (topic) {\n      topics = [topic]\n    }\n\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const topicsToFetch = await Promise.all(\n      topics.map(async topic => {\n        const partitions = await findTopicPartitions(cluster, topic)\n        const partitionsToFetch = partitions.map(partition => ({ partition }))\n        return { topic, partitions: partitionsToFetch }\n      })\n    )\n    let { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: topicsToFetch,\n    })\n\n    if (resolveOffsets) {\n      consumerOffsets = await Promise.all(\n        consumerOffsets.map(async ({ topic, partitions }) => {\n          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic))\n          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {\n            let resolvedOffset = offset\n            if (Number(offset) === EARLIEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].low\n            }\n            if (Number(offset) === LATEST_OFFSET) {\n              resolvedOffset = indexedOffsets[partition].high\n            }\n            return {\n              partition,\n              offset: resolvedOffset,\n              ...props,\n            }\n          })\n\n          await setOffsets({ groupId, topic, partitions: recalculatedPartitions })\n\n          return {\n            topic,\n            partitions: recalculatedPartitions,\n          }\n        })\n      )\n    }\n\n    const result = consumerOffsets.map(({ topic, partitions }) => {\n      const completePartitions = partitions.map(({ partition, offset, metadata }) => ({\n        partition,\n        offset,\n        metadata: metadata || null,\n      }))\n\n      return { topic, partitions: completePartitions }\n    })\n\n    if (topic) {\n      return result.pop().partitions\n    } else {\n      return result\n    }\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {boolean} [earliest=false]\n   * @return {Promise}\n   */\n  const resetOffsets = async ({ groupId, topic, earliest = false }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const partitions = await findTopicPartitions(cluster, topic)\n    const partitionsToSeek = partitions.map(partition => ({\n      partition,\n      offset: cluster.defaultOffset({ fromBeginning: earliest }),\n    }))\n\n    return setOffsets({ groupId, topic, partitions: partitionsToSeek })\n  }\n\n  /**\n   * @param {string} groupId\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const setOffsets = async ({ groupId, topic, partitions }) => {\n    if (!groupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`)\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const consumer = createConsumer({\n      logger: rootLogger.namespace('Admin', LEVELS.NOTHING),\n      cluster,\n      groupId,\n    })\n\n    await consumer.subscribe({ topic, fromBeginning: true })\n    const description = await consumer.describeGroup()\n\n    if (!isConsumerGroupRunning(description)) {\n      throw new KafkaJSNonRetriableError(\n        `The consumer group must have no running instances, current state: ${description.state}`\n      )\n    }\n\n    return new Promise((resolve, reject) => {\n      consumer.on(consumer.events.FETCH, async () =>\n        consumer\n          .stop()\n          .then(resolve)\n          .catch(reject)\n      )\n\n      consumer\n        .run({\n          eachBatchAutoResolve: false,\n          eachBatch: async () => true,\n        })\n        .catch(reject)\n\n      // This consumer doesn't need to consume any data\n      consumer.pause([{ topic }])\n\n      for (const seekData of partitions) {\n        consumer.seek({ topic, ...seekData })\n      }\n    })\n  }\n\n  const isBrokerConfig = type =>\n    [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type)\n\n  /**\n   * Broker configs can only be returned by the target broker\n   *\n   * @see\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L3783\n   * https://github.com/apache/kafka/blob/821c1ac6641845aeca96a43bc2b946ecec5cba4f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L2027\n   *\n   * @param {Broker} defaultBroker. Broker used in case the configuration is not a broker config\n   */\n  const groupResourcesByBroker = ({ resources, defaultBroker }) =>\n    groupBy(resources, async ({ type, name: nodeId }) => {\n      return isBrokerConfig(type)\n        ? await cluster.findBroker({ nodeId: String(nodeId) })\n        : defaultBroker\n    })\n\n  /**\n   * @param {Array<ResourceConfigQuery>} resources\n   * @param {boolean} [includeSynonyms=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfigQuery\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<String>} [configNames=[]]\n   */\n  const describeConfigs = async ({ resources, includeSynonyms }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(\n      r => !Array.isArray(r.configNames) && r.configNames != null\n    )\n\n    if (invalidConfigs) {\n      const { configNames } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const describeConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.describeConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            includeSynonyms,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(describeConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ResourceConfig>} resources\n   * @param {boolean} [validateOnly=false]\n   * @return {Promise}\n   *\n   * @typedef {Object} ResourceConfig\n   * @property {ConfigResourceType} type\n   * @property {string} name\n   * @property {Array<ResourceConfigEntry>} configEntries\n   *\n   * @typedef {Object} ResourceConfigEntry\n   * @property {string} name\n   * @property {string} value\n   */\n  const alterConfigs = async ({ resources, validateOnly }) => {\n    if (!resources || !Array.isArray(resources)) {\n      throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`)\n    }\n\n    if (resources.length === 0) {\n      throw new KafkaJSNonRetriableError('Resources array cannot be empty')\n    }\n\n    const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES)\n    const invalidType = resources.find(r => !validResourceTypes.includes(r.type))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const invalidName = resources.find(r => !r.name || typeof r.name !== 'string')\n\n    if (invalidName) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`\n      )\n    }\n\n    const invalidConfigs = resources.find(r => !Array.isArray(r.configEntries))\n\n    if (invalidConfigs) {\n      const { configEntries } = invalidConfigs\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`\n      )\n    }\n\n    const invalidConfigValue = resources.find(r =>\n      r.configEntries.some(e => typeof e.name !== 'string' || typeof e.value !== 'string')\n    )\n\n    if (invalidConfigValue) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const controller = await cluster.findControllerBroker()\n        const resourcerByBroker = await groupResourcesByBroker({\n          resources,\n          defaultBroker: controller,\n        })\n\n        const alterConfigsAction = async broker => {\n          const targetBroker = broker || controller\n          return targetBroker.alterConfigs({\n            resources: resourcerByBroker.get(targetBroker),\n            validateOnly: !!validateOnly,\n          })\n        }\n\n        const brokers = Array.from(resourcerByBroker.keys())\n        const responses = await Promise.all(brokers.map(alterConfigsAction))\n        const responseResources = responses.reduce(\n          (result, { resources }) => [...result, ...resources],\n          []\n        )\n\n        return { resources: responseResources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not alter configs', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @deprecated - This method was replaced by `fetchTopicMetadata`. This implementation\n   * is limited by the topics in the target group, so it can't fetch all topics when\n   * necessary.\n   *\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics of which we are aware.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const getTopicMetadata = async options => {\n    const { topics } = options || {}\n\n    if (topics) {\n      await Promise.all(\n        topics.map(async topic => {\n          if (!topic) {\n            throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n          }\n\n          try {\n            await cluster.addTargetTopic(topic)\n          } catch (e) {\n            e.message = `Failed to add target topic ${topic}: ${e.message}`\n            throw e\n          }\n        })\n      )\n    }\n\n    await cluster.refreshMetadataIfNecessary()\n    const targetTopics = topics || [...cluster.targetTopics]\n\n    return {\n      topics: await Promise.all(\n        targetTopics.map(async topic => ({\n          name: topic,\n          partitions: cluster.findTopicPartitionMetadata(topic),\n        }))\n      ),\n    }\n  }\n\n  /**\n   * Fetch metadata for provided topics.\n   *\n   * If no topics are provided fetch metadata for all topics.\n   * @see https://kafka.apache.org/protocol#The_Messages_Metadata\n   *\n   * @param {Object} [options]\n   * @param {string[]} [options.topics]\n   * @return {Promise<TopicsMetadata>}\n   *\n   * @typedef {Object} TopicsMetadata\n   * @property {Array<TopicMetadata>} topics\n   *\n   * @typedef {Object} TopicMetadata\n   * @property {String} name\n   * @property {Array<PartitionMetadata>} partitions\n   *\n   * @typedef {Object} PartitionMetadata\n   * @property {number} partitionErrorCode Response error code\n   * @property {number} partitionId Topic partition id\n   * @property {number} leader  The id of the broker acting as leader for this partition.\n   * @property {Array<number>} replicas The set of all nodes that host this partition.\n   * @property {Array<number>} isr The set of nodes that are in sync with the leader for this partition.\n   */\n  const fetchTopicMetadata = async ({ topics = [] } = {}) => {\n    if (topics) {\n      topics.forEach(topic => {\n        if (!topic || typeof topic !== 'string') {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n      })\n    }\n\n    const metadata = await cluster.metadata({ topics })\n\n    return {\n      topics: metadata.topicMetadata.map(topicMetadata => ({\n        name: topicMetadata.topic,\n        partitions: topicMetadata.partitionMetadata,\n      })),\n    }\n  }\n\n  /**\n   * Describe cluster\n   *\n   * @return {Promise<ClusterMetadata>}\n   *\n   * @typedef {Object} ClusterMetadata\n   * @property {Array<Broker>} brokers\n   * @property {Number} controller Current controller id. Returns null if unknown.\n   * @property {String} clusterId\n   *\n   * @typedef {Object} Broker\n   * @property {Number} nodeId\n   * @property {String} host\n   * @property {Number} port\n   */\n  const describeCluster = async () => {\n    const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] })\n    const brokers = nodes.map(({ nodeId, host, port }) => ({\n      nodeId,\n      host,\n      port,\n    }))\n    const controller =\n      controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId\n\n    return {\n      brokers,\n      controller,\n      clusterId,\n    }\n  }\n\n  /**\n   * List groups in a broker\n   *\n   * @return {Promise<ListGroups>}\n   *\n   * @typedef {Object} ListGroups\n   * @property {Array<ListGroup>} groups\n   *\n   * @typedef {Object} ListGroup\n   * @property {string} groupId\n   * @property {string} protocolType\n   */\n  const listGroups = async () => {\n    await cluster.refreshMetadata()\n    let groups = []\n    for (var nodeId in cluster.brokerPool.brokers) {\n      const broker = await cluster.findBroker({ nodeId })\n      const response = await broker.listGroups()\n      groups = groups.concat(response.groups)\n    }\n\n    return { groups }\n  }\n\n  /**\n   * Describe groups by group ids\n   * @param {Array<string>} groupIds\n   *\n   * @typedef {Object} GroupDescriptions\n   * @property {Array<GroupDescription>} groups\n   *\n   * @return {Promise<GroupDescriptions>}\n   */\n  const describeGroups = async groupIds => {\n    const coordinatorsForGroup = await Promise.all(\n      groupIds.map(async groupId => {\n        const coordinator = await cluster.findGroupCoordinator({ groupId })\n        return {\n          coordinator,\n          groupId,\n        }\n      })\n    )\n\n    const groupsByCoordinator = Object.values(\n      coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {\n        const group = coordinators[coordinator.nodeId]\n\n        if (group) {\n          coordinators[coordinator.nodeId] = {\n            ...group,\n            groupIds: [...group.groupIds, groupId],\n          }\n        } else {\n          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] }\n        }\n        return coordinators\n      }, {})\n    )\n\n    const responses = await Promise.all(\n      groupsByCoordinator.map(async ({ coordinator, groupIds }) => {\n        const retrier = createRetry(retry)\n        const { groups } = await retrier(() => coordinator.describeGroups({ groupIds }))\n        return groups\n      })\n    )\n\n    const groups = [].concat.apply([], responses)\n\n    return { groups }\n  }\n\n  /**\n   * Delete groups in a broker\n   *\n   * @param {string[]} [groupIds]\n   * @return {Promise<DeleteGroups>}\n   *\n   * @typedef {Array} DeleteGroups\n   * @property {string} groupId\n   * @property {number} errorCode\n   */\n  const deleteGroups = async groupIds => {\n    if (!groupIds || !Array.isArray(groupIds)) {\n      throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`)\n    }\n\n    const invalidGroupId = groupIds.some(g => typeof g !== 'string')\n\n    if (invalidGroupId) {\n      throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    let results = []\n\n    let clonedGroupIds = groupIds.slice()\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        if (clonedGroupIds.length === 0) return []\n\n        await cluster.refreshMetadata()\n\n        const brokersPerGroups = {}\n        const brokersPerNode = {}\n        for (const groupId of clonedGroupIds) {\n          const broker = await cluster.findGroupCoordinator({ groupId })\n          if (brokersPerGroups[broker.nodeId] === undefined) brokersPerGroups[broker.nodeId] = []\n          brokersPerGroups[broker.nodeId].push(groupId)\n          brokersPerNode[broker.nodeId] = broker\n        }\n\n        const res = await Promise.all(\n          Object.keys(brokersPerNode).map(\n            async nodeId => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])\n          )\n        )\n\n        const errors = flatten(\n          res.map(({ results }) =>\n            results.map(({ groupId, errorCode, error }) => {\n              return { groupId, errorCode, error }\n            })\n          )\n        ).filter(({ errorCode }) => errorCode !== 0)\n\n        clonedGroupIds = errors.map(({ groupId }) => groupId)\n\n        if (errors.length > 0) throw new KafkaJSDeleteGroupsError('Error in DeleteGroups', errors)\n\n        results = flatten(res.map(({ results }) => results))\n\n        return results\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER' || e.type === 'COORDINATOR_NOT_AVAILABLE') {\n          logger.warn('Could not delete groups', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Delete topic records up to the selected partition offsets\n   *\n   * @param {string} topic\n   * @param {Array<SeekEntry>} partitions\n   * @return {Promise}\n   *\n   * @typedef {Object} SeekEntry\n   * @property {number} partition\n   * @property {string} offset\n   */\n  const deleteTopicRecords = async ({ topic, partitions }) => {\n    if (!topic || typeof topic !== 'string') {\n      throw new KafkaJSNonRetriableError(`Invalid topic \"${topic}\"`)\n    }\n\n    if (!partitions || partitions.length === 0) {\n      throw new KafkaJSNonRetriableError(`Invalid partitions`)\n    }\n\n    const partitionsByBroker = cluster.findLeaderForPartitions(\n      topic,\n      partitions.map(p => p.partition)\n    )\n\n    const partitionsFound = flatten(values(partitionsByBroker))\n    const topicOffsets = await fetchTopicOffsets(topic)\n\n    const leaderNotFoundErrors = []\n    partitions.forEach(({ partition, offset }) => {\n      // throw if no leader found for partition\n      if (!partitionsFound.includes(partition)) {\n        leaderNotFoundErrors.push({\n          partition,\n          offset,\n          error: new KafkaJSBrokerNotFound('Could not find the leader for the partition', {\n            retriable: false,\n          }),\n        })\n        return\n      }\n      const { low } = topicOffsets.find(p => p.partition === partition) || {\n        high: undefined,\n        low: undefined,\n      }\n      // warn in case of offset below low watermark\n      if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {\n        logger.warn(\n          'The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition',\n          {\n            topic,\n            partition,\n            offset,\n          }\n        )\n      }\n    })\n\n    if (leaderNotFoundErrors.length > 0) {\n      throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors })\n    }\n\n    const seekEntriesByBroker = entries(partitionsByBroker).reduce(\n      (obj, [nodeId, nodePartitions]) => {\n        obj[nodeId] = {\n          topic,\n          partitions: partitions.filter(p => nodePartitions.includes(p.partition)),\n        }\n        return obj\n      },\n      {}\n    )\n\n    const retrier = createRetry(retry)\n    return retrier(async bail => {\n      try {\n        const partitionErrors = []\n\n        const brokerRequests = entries(seekEntriesByBroker).map(\n          ([nodeId, { topic, partitions }]) => async () => {\n            const broker = await cluster.findBroker({ nodeId })\n            await broker.deleteRecords({ topics: [{ topic, partitions }] })\n            // remove successful entry so it's ignored on retry\n            delete seekEntriesByBroker[nodeId]\n          }\n        )\n\n        await Promise.all(\n          brokerRequests.map(request =>\n            request().catch(e => {\n              if (e.name === 'KafkaJSDeleteTopicRecordsError') {\n                e.partitions.forEach(({ partition, offset, error }) => {\n                  partitionErrors.push({\n                    partition,\n                    offset,\n                    error,\n                  })\n                })\n              } else {\n                // then it's an unknown error, not from the broker response\n                throw e\n              }\n            })\n          )\n        )\n\n        if (partitionErrors.length > 0) {\n          throw new KafkaJSDeleteTopicRecordsError({\n            topic,\n            partitions: partitionErrors,\n          })\n        }\n      } catch (e) {\n        if (\n          e.retriable &&\n          e.partitions.some(\n            ({ error }) => staleMetadata(error) || error.name === 'KafkaJSMetadataNotLoaded'\n          )\n        ) {\n          await cluster.refreshMetadata()\n        }\n        throw e\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLEntry>} acl\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLEntry\n   */\n  const createAcls = async ({ acl }) => {\n    if (!acl || !Array.isArray(acl)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`)\n    }\n    if (acl.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL array')\n    }\n\n    // Validate principal\n    if (acl.some(({ principal }) => typeof principal !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (acl.some(({ host }) => typeof host !== 'string')) {\n      throw new KafkaJSNonRetriableError('Invalid ACL array, the hosts have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (acl.some(({ resourceName }) => typeof resourceName !== 'string')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = acl.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = acl.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = acl.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = acl.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        await broker.createAcls({ acl })\n\n        return true\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not create ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {ACLResourceTypes} resourceType The type of resource\n   * @param {string} resourceName The name of the resource\n   * @param {ACLResourcePatternTypes} resourcePatternType The resource pattern type filter\n   * @param {string} principal The principal name\n   * @param {string} host The hostname\n   * @param {ACLOperationTypes} operation The type of operation\n   * @param {ACLPermissionTypes} permissionType The type of permission\n   * @return {Promise<void>}\n   *\n   * @typedef {number} ACLResourceTypes\n   * @typedef {number} ACLResourcePatternTypes\n   * @typedef {number} ACLOperationTypes\n   * @typedef {number} ACLPermissionTypes\n   */\n  const describeAcls = async ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    // Validate principal\n    if (typeof principal !== 'string' && typeof principal !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid principal, the principal have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (typeof host !== 'string' && typeof host !== 'undefined') {\n      throw new KafkaJSNonRetriableError('Invalid host, the host have to be a valid string')\n    }\n\n    // Validate resourceName\n    if (typeof resourceName !== 'string' && typeof resourceName !== 'undefined') {\n      throw new KafkaJSNonRetriableError(\n        'Invalid resourceName, the resourceName have to be a valid string'\n      )\n    }\n\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    if (!validOperationTypes.includes(operation)) {\n      throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`)\n    }\n\n    // Validate resourcePatternType\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    if (!validResourcePatternTypes.includes(resourcePatternType)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern filter type ${resourcePatternType}`\n      )\n    }\n\n    // Validate permissionType\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    if (!validPermissionTypes.includes(permissionType)) {\n      throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`)\n    }\n\n    // Validate resourceType\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    if (!validResourceTypes.includes(resourceType)) {\n      throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`)\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { resources } = await broker.describeAcls({\n          resourceType,\n          resourceName,\n          resourcePatternType,\n          principal,\n          host,\n          operation,\n          permissionType,\n        })\n        return { resources }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not describe ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {Array<ACLFilter>} filters\n   * @return {Promise<void>}\n   *\n   * @typedef {Object} ACLFilter\n   */\n  const deleteAcls = async ({ filters }) => {\n    if (!filters || !Array.isArray(filters)) {\n      throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`)\n    }\n\n    if (filters.length === 0) {\n      throw new KafkaJSNonRetriableError('Empty ACL Filter array')\n    }\n\n    // Validate principal\n    if (\n      filters.some(\n        ({ principal }) => typeof principal !== 'string' && typeof principal !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the principals have to be a valid string'\n      )\n    }\n\n    // Validate host\n    if (filters.some(({ host }) => typeof host !== 'string' && typeof host !== 'undefined')) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the hosts have to be a valid string'\n      )\n    }\n\n    // Validate resourceName\n    if (\n      filters.some(\n        ({ resourceName }) =>\n          typeof resourceName !== 'string' && typeof resourceName !== 'undefined'\n      )\n    ) {\n      throw new KafkaJSNonRetriableError(\n        'Invalid ACL Filter array, the resourceNames have to be a valid string'\n      )\n    }\n\n    let invalidType\n    // Validate operation\n    const validOperationTypes = Object.values(ACL_OPERATION_TYPES)\n    invalidType = filters.find(i => !validOperationTypes.includes(i.operation))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourcePatternTypes\n    const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES)\n    invalidType = filters.find(i => !validResourcePatternTypes.includes(i.resourcePatternType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(\n          invalidType\n        )}`\n      )\n    }\n\n    // Validate permissionTypes\n    const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES)\n    invalidType = filters.find(i => !validPermissionTypes.includes(i.permissionType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    // Validate resourceTypes\n    const validResourceTypes = Object.values(ACL_RESOURCE_TYPES)\n    invalidType = filters.find(i => !validResourceTypes.includes(i.resourceType))\n\n    if (invalidType) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`\n      )\n    }\n\n    const retrier = createRetry(retry)\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await cluster.refreshMetadata()\n        const broker = await cluster.findControllerBroker()\n        const { filterResponses } = await broker.deleteAcls({ filters })\n        return { filterResponses }\n      } catch (e) {\n        if (e.type === 'NOT_CONTROLLER') {\n          logger.warn('Could not delete ACL', { error: e.message, retryCount, retryTime })\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /** @type {import(\"../../types\").Admin[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    listTopics,\n    createTopics,\n    deleteTopics,\n    createPartitions,\n    getTopicMetadata,\n    fetchTopicMetadata,\n    describeCluster,\n    events,\n    fetchOffsets,\n    fetchTopicOffsets,\n    fetchTopicOffsetsByTimestamp,\n    setOffsets,\n    resetOffsets,\n    describeConfigs,\n    alterConfigs,\n    on,\n    logger: getLogger,\n    listGroups,\n    describeGroups,\n    deleteGroups,\n    describeAcls,\n    deleteAcls,\n    createAcls,\n    deleteTopicRecords,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst adminType = InstrumentationEventType('admin')\n\nconst events = {\n  CONNECT: adminType('connect'),\n  DISCONNECT: adminType('disconnect'),\n  REQUEST: adminType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../utils/long')\nconst Lock = require('../utils/lock')\nconst { Types: Compression } = require('../protocol/message/compression')\nconst { requests, lookup } = require('../protocol/requests')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst apiKeys = require('../protocol/requests/apiKeys')\nconst SASLAuthenticator = require('./saslAuthenticator')\nconst shuffle = require('../utils/shuffle')\nconst { ApiVersions: apiVersionsApiKey } = require('../protocol/requests/apiKeys')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst PRIVATE = {\n  SHOULD_REAUTHENTICATE: Symbol('private:Broker:shouldReauthenticate'),\n  SEND_REQUEST: Symbol('private:Broker:sendRequest'),\n  AUTHENTICATE: Symbol('private:Broker:authenticate'),\n}\n\n/** @type {import(\"../protocol/requests\").Lookup} */\nconst notInitializedLookup = () => {\n  throw new Error('Broker not connected')\n}\n\n/**\n * @param request - request from protocol\n * @returns {boolean}\n */\nconst isAuthenticatedRequest = request => {\n  return request.apiKey !== apiVersionsApiKey\n}\n\n/**\n * Each node in a Kafka cluster is called broker. This class contains\n * the high-level operations a node can perform.\n *\n * @type {import(\"../../types\").Broker}\n */\nmodule.exports = class Broker {\n  /**\n   * @param {Object} options\n   * @param {import(\"../network/connection\")} options.connection\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {number} [options.nodeId]\n   * @param {import(\"../../types\").ApiVersions} [options.versions=null] The object with all available versions and APIs\n   *                                 supported by this cluster. The output of broker#apiVersions\n   * @param {number} [options.authenticationTimeout=1000]\n   * @param {number} [options.reauthenticationThreshold=10000]\n   * @param {boolean} [options.allowAutoTopicCreation=true] If this and the broker config 'auto.create.topics.enable'\n   *                                                are true, topics that don't exist will be created when\n   *                                                fetching metadata.\n   * @param {boolean} [options.supportAuthenticationProtocol=null] If the server supports the SASLAuthenticate protocol\n   */\n  constructor({\n    connection,\n    logger,\n    nodeId = null,\n    versions = null,\n    authenticationTimeout = 1000,\n    reauthenticationThreshold = 10000,\n    allowAutoTopicCreation = true,\n    supportAuthenticationProtocol = null,\n  }) {\n    this.connection = connection\n    this.nodeId = nodeId\n    this.rootLogger = logger\n    this.logger = logger.namespace('Broker')\n    this.versions = versions\n    this.authenticationTimeout = authenticationTimeout\n    this.reauthenticationThreshold = reauthenticationThreshold\n    this.allowAutoTopicCreation = allowAutoTopicCreation\n    this.supportAuthenticationProtocol = supportAuthenticationProtocol\n\n    this.authenticatedAt = null\n    this.sessionLifetime = Long.ZERO\n\n    // The lock timeout has twice the connectionTimeout because the same timeout is used\n    // for the first apiVersions call\n    const lockTimeout = 2 * this.connection.connectionTimeout + this.authenticationTimeout\n    this.brokerAddress = `${this.connection.host}:${this.connection.port}`\n\n    this.lock = new Lock({\n      timeout: lockTimeout,\n      description: `connect to broker ${this.brokerAddress}`,\n    })\n\n    this.lookupRequest = notInitializedLookup\n\n    /**\n     * @private\n     * @returns {Promise}\n     */\n    this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {\n      if (this.connection.sasl && !this.isAuthenticated()) {\n        const authenticator = new SASLAuthenticator(\n          this.connection,\n          this.rootLogger,\n          this.versions,\n          this.supportAuthenticationProtocol\n        )\n\n        await authenticator.authenticate()\n        this.authenticatedAt = process.hrtime()\n        this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isAuthenticated() {\n    return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]()\n  }\n\n  /**\n   * @public\n   * @returns {boolean}\n   */\n  isConnected() {\n    const { connected, sasl } = this.connection\n    return sasl ? connected && this.isAuthenticated() : connected\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async connect() {\n    try {\n      await this.lock.acquire()\n      if (this.isConnected()) {\n        return\n      }\n\n      this.authenticatedAt = null\n      await this.connection.connect()\n\n      if (!this.versions) {\n        this.versions = await this.apiVersions()\n      }\n\n      this.lookupRequest = lookup(this.versions)\n\n      if (this.supportAuthenticationProtocol === null) {\n        try {\n          this.lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n          this.supportAuthenticationProtocol = true\n        } catch (_) {\n          this.supportAuthenticationProtocol = false\n        }\n\n        this.logger.debug(`Verified support for SaslAuthenticate`, {\n          broker: this.brokerAddress,\n          supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n        })\n      }\n\n      await this[PRIVATE.AUTHENTICATE]()\n    } finally {\n      await this.lock.release()\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.authenticatedAt = null\n    await this.connection.disconnect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").ApiVersions>}\n   */\n  async apiVersions() {\n    let response\n    const availableVersions = requests.ApiVersions.versions\n      .map(Number)\n      .sort()\n      .reverse()\n\n    // Find the best version implemented by the server\n    for (const candidateVersion of availableVersions) {\n      try {\n        const apiVersions = requests.ApiVersions.protocol({ version: candidateVersion })\n        response = await this[PRIVATE.SEND_REQUEST]({\n          ...apiVersions(),\n          requestTimeout: this.connection.connectionTimeout,\n        })\n        break\n      } catch (e) {\n        if (e.type !== 'UNSUPPORTED_VERSION') {\n          throw e\n        }\n      }\n    }\n\n    if (!response) {\n      throw new KafkaJSNonRetriableError('API Versions not supported')\n    }\n\n    return response.apiVersions.reduce(\n      (obj, version) =>\n        Object.assign(obj, {\n          [version.apiKey]: {\n            minVersion: version.minVersion,\n            maxVersion: version.maxVersion,\n          },\n        }),\n      {}\n    )\n  }\n\n  /**\n   * @public\n   * @type {import(\"../../types\").Broker['metadata']}\n   * @param {string[]} [topics=[]] An array of topics to fetch metadata for.\n   *                            If no topics are specified fetch metadata for all topics\n   */\n  async metadata(topics = []) {\n    const metadata = this.lookupRequest(apiKeys.Metadata, requests.Metadata)\n    const shuffledTopics = shuffle(topics)\n    return await this[PRIVATE.SEND_REQUEST](\n      metadata({ topics: shuffledTopics, allowAutoTopicCreation: this.allowAutoTopicCreation })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {Array} request.topicData An array of messages per topic and per partition, example:\n   *                          [\n   *                            {\n   *                              topic: 'test-topic-1',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 0,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '1', value: 'A' },\n   *                                    { key: '2', value: 'B' },\n   *                                  ]\n   *                                },\n   *                                {\n   *                                  partition: 1,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '3', value: 'C' },\n   *                                  ]\n   *                                }\n   *                              ]\n   *                            },\n   *                            {\n   *                              topic: 'test-topic-2',\n   *                              partitions: [\n   *                                {\n   *                                  partition: 4,\n   *                                  firstSequence: 0,\n   *                                  messages: [\n   *                                    { key: '32', value: 'E' },\n   *                                  ]\n   *                                },\n   *                              ]\n   *                            },\n   *                          ]\n   * @param {number} [request.acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @param {number} [request.timeout=30000] The time to await a response in ms\n   * @param {string} [request.transactionalId=null]\n   * @param {number} [request.producerId=-1] Broker assigned producerId\n   * @param {number} [request.producerEpoch=0] Broker assigned producerEpoch\n   * @param {import(\"../../types\").CompressionTypes} [request.compression=CompressionTypes.None] Compression codec\n   * @returns {Promise}\n   */\n  async produce({\n    topicData,\n    transactionalId,\n    producerId,\n    producerEpoch,\n    acks = -1,\n    timeout = 30000,\n    compression = Compression.None,\n  }) {\n    const produce = this.lookupRequest(apiKeys.Produce, requests.Produce)\n    return await this[PRIVATE.SEND_REQUEST](\n      produce({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {number} [request.replicaId=-1] Broker id of the follower. For normal consumers, use -1\n   * @param {number} [request.isolationLevel=1] This setting controls the visibility of transactional records. Default READ_COMMITTED.\n   * @param {number} [request.maxWaitTime=5000] Maximum time in ms to wait for the response\n   * @param {number} [request.minBytes=1] Minimum bytes to accumulate in the response\n   * @param {number} [request.maxBytes=10485760] Maximum bytes to accumulate in the response. Note that this is\n   *                                   not an absolute maximum, if the first message in the first non-empty\n   *                                   partition of the fetch is larger than this value, the message will still\n   *                                   be returned to ensure that progress can be made. Default 10MB.\n   * @param {Array} request.topics Topics to fetch\n   *                        [\n   *                          {\n   *                            topic: 'topic-name',\n   *                            partitions: [\n   *                              {\n   *                                partition: 0,\n   *                                fetchOffset: '4124',\n   *                                maxBytes: 2048\n   *                              }\n   *                            ]\n   *                          }\n   *                        ]\n   * @param {string} [request.rackId=''] A rack identifier for this client. This can be any string value which indicates where this\n   *                           client is physically located. It corresponds with the broker config `broker.rack`.\n   * @returns {Promise}\n   */\n  async fetch({\n    replicaId,\n    isolationLevel,\n    maxWaitTime = 5000,\n    minBytes = 1,\n    maxBytes = 10485760,\n    topics,\n    rackId = '',\n  }) {\n    // TODO: validate topics not null/empty\n    const fetch = this.lookupRequest(apiKeys.Fetch, requests.Fetch)\n\n    // Shuffle topic-partitions to ensure fair response allocation across partitions (KIP-74)\n    const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {\n      partitions.forEach(partition => {\n        topicPartitions.push({ topic, partition })\n      })\n      return topicPartitions\n    }, [])\n\n    const shuffledTopicPartitions = shuffle(flattenedTopicPartitions)\n\n    // Consecutive partitions for the same topic can be combined into a single `topic` entry\n    const consolidatedTopicPartitions = shuffledTopicPartitions.reduce(\n      (topicPartitions, { topic, partition }) => {\n        const last = topicPartitions[topicPartitions.length - 1]\n\n        if (last != null && last.topic === topic) {\n          topicPartitions[topicPartitions.length - 1].partitions.push(partition)\n        } else {\n          topicPartitions.push({ topic, partitions: [partition] })\n        }\n\n        return topicPartitions\n      },\n      []\n    )\n\n    return await this[PRIVATE.SEND_REQUEST](\n      fetch({\n        replicaId,\n        isolationLevel,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics: consolidatedTopicPartitions,\n        rackId,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The group id\n   * @param {number} request.groupGenerationId The generation of the group\n   * @param {string} request.memberId The member id assigned by the group coordinator\n   * @returns {Promise}\n   */\n  async heartbeat({ groupId, groupGenerationId, memberId }) {\n    const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat)\n    return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} request.coordinatorType The type of coordinator to find\n   * @returns {Promise}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType }) {\n    // TODO: validate groupId, mandatory\n    const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator)\n    return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId The unique group id\n   * @param {number} request.sessionTimeout The coordinator considers the consumer dead if it receives\n   *                                no heartbeat after this timeout in ms\n   * @param {number} request.rebalanceTimeout The maximum time that the coordinator will wait for each member\n   *                                  to rejoin when rebalancing the group\n   * @param {string} [request.memberId=\"\"] The assigned consumer id or an empty string for a new consumer\n   * @param {string} [request.protocolType=\"consumer\"] Unique name for class of protocols implemented by group\n   * @param {Array} request.groupProtocols List of protocols that the member supports (assignment strategy)\n   *                                [{ name: 'AssignerName', metadata: '{\"version\": 1, \"topics\": []}' }]\n   * @returns {Promise}\n   */\n  async joinGroup({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId = '',\n    protocolType = 'consumer',\n    groupProtocols,\n  }) {\n    const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup)\n    const makeRequest = (assignedMemberId = memberId) =>\n      this[PRIVATE.SEND_REQUEST](\n        joinGroup({\n          groupId,\n          sessionTimeout,\n          rebalanceTimeout,\n          memberId: assignedMemberId,\n          protocolType,\n          groupProtocols,\n        })\n      )\n\n    try {\n      return await makeRequest()\n    } catch (error) {\n      if (error.name === 'KafkaJSMemberIdRequired') {\n        return makeRequest(error.memberId)\n      }\n\n      throw error\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {string} request.memberId\n   * @returns {Promise}\n   */\n  async leaveGroup({ groupId, memberId }) {\n    const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup)\n    return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.generationId\n   * @param {string} request.memberId\n   * @param {object} request.groupAssignment\n   * @returns {Promise}\n   */\n  async syncGroup({ groupId, generationId, memberId, groupAssignment }) {\n    const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup)\n    return await this[PRIVATE.SEND_REQUEST](\n      syncGroup({\n        groupId,\n        generationId,\n        memberId,\n        groupAssignment,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {number} request.replicaId=-1 Broker id of the follower. For normal consumers, use -1\n   * @param {number} request.isolationLevel=1 This setting controls the visibility of transactional records (default READ_COMMITTED, Kafka >0.11 only)\n   * @param {TopicPartitionOffset[]} request.topics e.g:\n   *\n   * @typedef {Object} TopicPartitionOffset\n   * @property {string} topic\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {number} partition\n   * @property {number} [timestamp=-1]\n   *\n   *\n   * @returns {Promise}\n   */\n  async listOffsets({ replicaId, isolationLevel, topics }) {\n    const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets)\n    const result = await this[PRIVATE.SEND_REQUEST](\n      listOffsets({ replicaId, isolationLevel, topics })\n    )\n\n    // ListOffsets >= v1 will return a single `offset` rather than an array of `offsets` (ListOffsets V0).\n    // Normalize to just return `offset`.\n    for (const response of result.responses) {\n      response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {\n        return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {number} request.groupGenerationId\n   * @param {string} request.memberId\n   * @param {number} [request.retentionTime=-1] -1 signals to the broker that its default configuration\n   *                                    should be used.\n   * @param {object} request.topics Topics to commit offsets, e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0, offset: '11' }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {\n    const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      offsetCommit({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string} request.groupId\n   * @param {object} request.topics - If the topic array is null fetch offsets for all topics. e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [\n   *                        { partition: 0 }\n   *                      ]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async offsetFetch({ groupId, topics }) {\n    const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch)\n    return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.groupIds\n   * @returns {Promise}\n   */\n  async describeGroups({ groupIds }) {\n    const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups)\n    return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topics e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     numPartitions: 1,\n   *                     replicationFactor: 1\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise}\n   */\n  async createTopics({ topics, validateOnly = false, timeout = 5000 }) {\n    const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics)\n    return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {Array} request.topicPartitions e.g:\n   *                 [\n   *                   {\n   *                     topic: 'topic-name',\n   *                     count: 3,\n   *                     assignments: []\n   *                   }\n   *                 ]\n   * @param {boolean} [request.validateOnly=false] If this is true, the request will be validated, but the topic\n   *                                       won't be created\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely created\n   *                                on the controller node\n   * @returns {Promise<void>}\n   */\n  async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {\n    const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions)\n    return await this[PRIVATE.SEND_REQUEST](\n      createPartitions({ topicPartitions, validateOnly, timeout })\n    )\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {string[]} request.topics An array of topics to be deleted\n   * @param {number} [request.timeout=5000] The time in ms to wait for a topic to be completely deleted on the\n   *                                controller node. Values <= 0 will trigger topic deletion and return\n   *                                immediately\n   * @returns {Promise}\n   */\n  async deleteTopics({ topics, timeout = 5000 }) {\n    const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics)\n    return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").ResourceConfigQuery[]} request.resources\n   *                                 [{\n   *                                   type: RESOURCE_TYPES.TOPIC,\n   *                                   name: 'topic-name',\n   *                                   configNames: ['compression.type', 'retention.ms']\n   *                                 }]\n   * @param {boolean} [request.includeSynonyms=false]\n   * @returns {Promise}\n   */\n  async describeConfigs({ resources, includeSynonyms = false }) {\n    const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs)\n    return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").IResourceConfig[]} request.resources\n   *                                 [{\n   *                                  type: RESOURCE_TYPES.TOPIC,\n   *                                  name: 'topic-name',\n   *                                  configEntries: [\n   *                                    {\n   *                                      name: 'cleanup.policy',\n   *                                      value: 'compact'\n   *                                    }\n   *                                  ]\n   *                                 }]\n   * @param {boolean} [request.validateOnly=false]\n   * @returns {Promise}\n   */\n  async alterConfigs({ resources, validateOnly = false }) {\n    const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs)\n    return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }))\n  }\n\n  /**\n   * Send an `InitProducerId` request to fetch a PID and bump the producer epoch.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {number} request.transactionTimeout The time in ms to wait for before aborting idle transactions\n   * @param {number} [request.transactionalId] The transactional id or null if the producer is not transactional\n   * @returns {Promise}\n   */\n  async initProducerId({ transactionalId, transactionTimeout }) {\n    const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId)\n    return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }))\n  }\n\n  /**\n   * Send an `AddPartitionsToTxn` request to mark a TopicPartition as participating in the transaction.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {object[]} request.topics e.g:\n   *                  [\n   *                    {\n   *                      topic: 'topic-name',\n   *                      partitions: [ 0, 1]\n   *                    }\n   *                  ]\n   * @returns {Promise}\n   */\n  async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {\n    const addPartitionsToTxn = this.lookupRequest(\n      apiKeys.AddPartitionsToTxn,\n      requests.AddPartitionsToTxn\n    )\n    return await this[PRIVATE.SEND_REQUEST](\n      addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `AddOffsetsToTxn` request.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @returns {Promise}\n   */\n  async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {\n    const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId })\n    )\n  }\n\n  /**\n   * Send a `TxnOffsetCommit` request to persist the offsets in the `__consumer_offsets` topics.\n   *\n   * Request should be made to the consumer coordinator.\n   * @public\n   * @param {object} request\n   * @param {OffsetCommitTopic[]} request.topics\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {string} request.groupId The unique group identifier (for the consumer group)\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {OffsetCommitTopic[]} request.topics\n   *\n   * @typedef {Object} OffsetCommitTopic\n   * @property {string} topic\n   * @property {OffsetCommitTopicPartition[]} partitions\n   *\n   * @typedef {Object} OffsetCommitTopicPartition\n   * @property {number} partition\n   * @property {number} offset\n   * @property {string} [metadata]\n   *\n   * @returns {Promise}\n   */\n  async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {\n    const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit)\n    return await this[PRIVATE.SEND_REQUEST](\n      txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics })\n    )\n  }\n\n  /**\n   * Send an `EndTxn` request to indicate transaction should be committed or aborted.\n   *\n   * Request should be made to the transaction coordinator.\n   * @public\n   * @param {object} request\n   * @param {string} request.transactionalId The transactional id corresponding to the transaction.\n   * @param {number} request.producerId Current producer id in use by the transactional id.\n   * @param {number} request.producerEpoch Current epoch associated with the producer id.\n   * @param {boolean} request.transactionResult The result of the transaction (false = ABORT, true = COMMIT)\n   * @returns {Promise}\n   */\n  async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {\n    const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn)\n    return await this[PRIVATE.SEND_REQUEST](\n      endTxn({ transactionalId, producerId, producerEpoch, transactionResult })\n    )\n  }\n\n  /**\n   * Send request for list of groups\n   * @public\n   * @returns {Promise}\n   */\n  async listGroups() {\n    const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups)\n    return await this[PRIVATE.SEND_REQUEST](listGroups())\n  }\n\n  /**\n   * Send request to delete groups\n   * @param {string[]} groupIds\n   * @public\n   * @returns {Promise}\n   */\n  async deleteGroups(groupIds) {\n    const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups)\n    return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds))\n  }\n\n  /**\n   * Send request to delete records\n   * @public\n   * @param {object} request\n   * @param {TopicPartitionRecords[]} request.topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset 2 },\n   *                                { partition: 1, offset 4 },\n   *                              ],\n   *                            }\n   *                          ]\n   * @returns {Promise<Array>} example:\n   *                          {\n   *                            throttleTime: 0\n   *                           [\n   *                              {\n   *                                topic: 'my-topic-name',\n   *                                partitions: [\n   *                                 { partition: 0, lowWatermark: '2n', errorCode: 0 },\n   *                                 { partition: 1, lowWatermark: '4n', errorCode: 0 },\n   *                               ],\n   *                             },\n   *                           ]\n   *                          }\n   *\n   * @typedef {object} TopicPartitionRecords\n   * @property {string} topic\n   * @property {PartitionRecord[]} partitions\n   *\n   * @typedef {object} PartitionRecord\n   * @property {number} partition\n   * @property {number} offset\n   */\n  async deleteRecords({ topics }) {\n    const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords)\n    return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }))\n  }\n\n  /**\n   * @public\n   * @param {object} request\n   * @param {import(\"../../types\").AclEntry[]} request.acl e.g:\n   *                 [\n   *                   {\n   *                     resourceType: AclResourceTypes.TOPIC,\n   *                     resourceName: 'topic-name',\n   *                     resourcePatternType: ResourcePatternTypes.LITERAL,\n   *                     principal: 'User:bob',\n   *                     host: '*',\n   *                     operation: AclOperationTypes.ALL,\n   *                     permissionType: AclPermissionTypes.DENY,\n   *                   }\n   *                 ]\n   * @returns {Promise<void>}\n   */\n  async createAcls({ acl }) {\n    const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls)\n    return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }))\n  }\n\n  /**\n   * @public\n   * @param {import(\"../../types\").AclEntry} aclEntry\n   * @returns {Promise<void>}\n   */\n  async describeAcls({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) {\n    const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls)\n    return await this[PRIVATE.SEND_REQUEST](\n      describeAcls({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      })\n    )\n  }\n\n  /**\n   * @public\n   * @param {Object} request\n   * @param {import(\"../../types\").AclEntry[]} request.filters\n   * @returns {Promise<void>}\n   */\n  async deleteAcls({ filters }) {\n    const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls)\n    return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }))\n  }\n\n  /***\n   * @private\n   */\n  [PRIVATE.SHOULD_REAUTHENTICATE]() {\n    if (this.sessionLifetime.equals(Long.ZERO)) {\n      return false\n    }\n\n    if (this.authenticatedAt == null) {\n      return true\n    }\n\n    const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt)\n    const millisSince = Long.fromValue(secondsSince)\n      .multiply(1000)\n      .add(Long.fromValue(remainingNanosSince).divide(1000000))\n\n    const reauthenticateAt = millisSince.add(this.reauthenticationThreshold)\n    return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime)\n  }\n\n  /**\n   * @private\n   */\n  async [PRIVATE.SEND_REQUEST](protocolRequest) {\n    if (!this.isAuthenticated() && isAuthenticatedRequest(protocolRequest.request)) {\n      await this[PRIVATE.AUTHENTICATE]()\n    }\n    try {\n      return await this.connection.send(protocolRequest)\n    } catch (e) {\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        await this.disconnect()\n      }\n\n      throw e\n    }\n  }\n}\n","const awsIam = require('../../protocol/sasl/awsIam')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class AWSIAMAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLAWSIAMAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (!sasl.authorizationIdentity) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing authorizationIdentity')\n    }\n    if (!sasl.accessKeyId) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing accessKeyId')\n    }\n    if (!sasl.secretAccessKey) {\n      throw new KafkaJSSASLAuthenticationError('SASL AWS-IAM: Missing secretAccessKey')\n    }\n    if (!sasl.sessionToken) {\n      sasl.sessionToken = ''\n    }\n\n    const request = awsIam.request(sasl)\n    const response = awsIam.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL AWS-IAM', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL AWS-IAM authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL AWS-IAM authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const { requests, lookup } = require('../../protocol/requests')\nconst apiKeys = require('../../protocol/requests/apiKeys')\nconst PlainAuthenticator = require('./plain')\nconst SCRAM256Authenticator = require('./scram256')\nconst SCRAM512Authenticator = require('./scram512')\nconst AWSIAMAuthenticator = require('./awsIam')\nconst OAuthBearerAuthenticator = require('./oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nconst AUTHENTICATORS = {\n  PLAIN: PlainAuthenticator,\n  'SCRAM-SHA-256': SCRAM256Authenticator,\n  'SCRAM-SHA-512': SCRAM512Authenticator,\n  AWS: AWSIAMAuthenticator,\n  OAUTHBEARER: OAuthBearerAuthenticator,\n}\n\nconst SUPPORTED_MECHANISMS = Object.keys(AUTHENTICATORS)\nconst UNLIMITED_SESSION_LIFETIME = '0'\n\nmodule.exports = class SASLAuthenticator {\n  constructor(connection, logger, versions, supportAuthenticationProtocol) {\n    this.connection = connection\n    this.logger = logger\n    this.sessionLifetime = UNLIMITED_SESSION_LIFETIME\n\n    const lookupRequest = lookup(versions)\n    this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake)\n    this.protocolAuthentication = supportAuthenticationProtocol\n      ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate)\n      : null\n  }\n\n  async authenticate() {\n    const mechanism = this.connection.sasl.mechanism.toUpperCase()\n    if (!SUPPORTED_MECHANISMS.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the client`\n      )\n    }\n\n    const handshake = await this.connection.send(this.saslHandshake({ mechanism }))\n    if (!handshake.enabledMechanisms.includes(mechanism)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `SASL ${mechanism} mechanism is not supported by the server`\n      )\n    }\n\n    const saslAuthenticate = async ({ request, response, authExpectResponse }) => {\n      if (this.protocolAuthentication) {\n        const { buffer: requestAuthBytes } = await request.encode()\n        const authResponse = await this.connection.send(\n          this.protocolAuthentication({ authBytes: requestAuthBytes })\n        )\n\n        // `0` is a string because `sessionLifetimeMs` is an int64 encoded as string.\n        // This is not present in SaslAuthenticateV0, so we default to `\"0\"`\n        this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME\n\n        if (!authExpectResponse) {\n          return\n        }\n\n        const { authBytes: responseAuthBytes } = authResponse\n        const payloadDecoded = await response.decode(responseAuthBytes)\n        return response.parse(payloadDecoded)\n      }\n\n      return this.connection.authenticate({ request, response, authExpectResponse })\n    }\n\n    const Authenticator = AUTHENTICATORS[mechanism]\n    await new Authenticator(this.connection, this.logger, saslAuthenticate).authenticate()\n  }\n}\n","/**\n * The sasl object must include a property named oauthBearerProvider, an\n * async function that is used to return the OAuth bearer token.\n *\n * The OAuth bearer token must be an object with properties value and\n * (optionally) extensions, that will be sent during the SASL/OAUTHBEARER\n * request.\n *\n * The implementation of the oauthBearerProvider must take care that tokens are\n * reused and refreshed when appropriate.\n */\n\nconst oauthBearer = require('../../protocol/sasl/oauthBearer')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class OAuthBearerAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLOAuthBearerAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.oauthBearerProvider == null) {\n      throw new KafkaJSSASLAuthenticationError(\n        'SASL OAUTHBEARER: Missing OAuth bearer token provider'\n      )\n    }\n\n    const { oauthBearerProvider } = sasl\n\n    const oauthBearerToken = await oauthBearerProvider()\n\n    if (oauthBearerToken.value == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL OAUTHBEARER: Invalid OAuth bearer token')\n    }\n\n    const request = await oauthBearer.request(sasl, oauthBearerToken)\n    const response = oauthBearer.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL OAUTHBEARER', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL OAUTHBEARER authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL OAUTHBEARER authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const plain = require('../../protocol/sasl/plain')\nconst { KafkaJSSASLAuthenticationError } = require('../../errors')\n\nmodule.exports = class PlainAuthenticator {\n  constructor(connection, logger, saslAuthenticate) {\n    this.connection = connection\n    this.logger = logger.namespace('SASLPlainAuthenticator')\n    this.saslAuthenticate = saslAuthenticate\n  }\n\n  async authenticate() {\n    const { sasl } = this.connection\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError('SASL Plain: Invalid username or password')\n    }\n\n    const request = plain.request(sasl)\n    const response = plain.response\n    const { host, port } = this.connection\n    const broker = `${host}:${port}`\n\n    try {\n      this.logger.debug('Authenticate with SASL PLAIN', { broker })\n      await this.saslAuthenticate({ request, response })\n      this.logger.debug('SASL PLAIN authentication successful', { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(\n        `SASL PLAIN authentication failed: ${e.message}`\n      )\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n}\n","const crypto = require('crypto')\nconst scram = require('../../protocol/sasl/scram')\nconst { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require('../../errors')\n\nconst GS2_HEADER = 'n,,'\n\nconst EQUAL_SIGN_REGEX = /=/g\nconst COMMA_SIGN_REGEX = /,/g\n\nconst URLSAFE_BASE64_PLUS_REGEX = /\\+/g\nconst URLSAFE_BASE64_SLASH_REGEX = /\\//g\nconst URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/\n\nconst HMAC_CLIENT_KEY = 'Client Key'\nconst HMAC_SERVER_KEY = 'Server Key'\n\nconst DIGESTS = {\n  SHA256: {\n    length: 32,\n    type: 'sha256',\n    minIterations: 4096,\n  },\n  SHA512: {\n    length: 64,\n    type: 'sha512',\n    minIterations: 4096,\n  },\n}\n\nconst encode64 = str => Buffer.from(str).toString('base64')\n\nclass SCRAM {\n  /**\n   * From https://tools.ietf.org/html/rfc5802#section-5.1\n   *\n   * The characters ',' or '=' in usernames are sent as '=2C' and\n   * '=3D' respectively.  If the server receives a username that\n   * contains '=' not followed by either '2C' or '3D', then the\n   * server MUST fail the authentication.\n   *\n   * @returns {String}\n   */\n  static sanitizeString(str) {\n    return str.replace(EQUAL_SIGN_REGEX, '=3D').replace(COMMA_SIGN_REGEX, '=2C')\n  }\n\n  /**\n   * In cryptography, a nonce is an arbitrary number that can be used just once.\n   * It is similar in spirit to a nonce * word, hence the name. It is often a random or pseudo-random\n   * number issued in an authentication protocol to * ensure that old communications cannot be reused\n   * in replay attacks.\n   *\n   * @returns {String}\n   */\n  static nonce() {\n    return crypto\n      .randomBytes(16)\n      .toString('base64')\n      .replace(URLSAFE_BASE64_PLUS_REGEX, '-') // make it url safe\n      .replace(URLSAFE_BASE64_SLASH_REGEX, '_')\n      .replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, '')\n      .toString('ascii')\n  }\n\n  /**\n   * Hi() is, essentially, PBKDF2 [RFC2898] with HMAC() as the\n   * pseudorandom function (PRF) and with dkLen == output length of\n   * HMAC() == output length of H()\n   *\n   * @returns {Promise<Buffer>}\n   */\n  static hi(password, salt, iterations, digestDefinition) {\n    return new Promise((resolve, reject) => {\n      crypto.pbkdf2(\n        password,\n        salt,\n        iterations,\n        digestDefinition.length,\n        digestDefinition.type,\n        (err, derivedKey) => (err ? reject(err) : resolve(derivedKey))\n      )\n    })\n  }\n\n  /**\n   * Apply the exclusive-or operation to combine the octet string\n   * on the left of this operator with the octet string on the right of\n   * this operator.  The length of the output and each of the two\n   * inputs will be the same for this use\n   *\n   * @returns {Buffer}\n   */\n  static xor(left, right) {\n    const bufferA = Buffer.from(left)\n    const bufferB = Buffer.from(right)\n    const length = Buffer.byteLength(bufferA)\n\n    if (length !== Buffer.byteLength(bufferB)) {\n      throw new KafkaJSNonRetriableError('Buffers must be of the same length')\n    }\n\n    const result = []\n    for (let i = 0; i < length; i++) {\n      result.push(bufferA[i] ^ bufferB[i])\n    }\n\n    return Buffer.from(result)\n  }\n\n  /**\n   * @param {Connection} connection\n   * @param {Logger} logger\n   * @param {Function} saslAuthenticate\n   * @param {DigestDefinition} digestDefinition\n   */\n  constructor(connection, logger, saslAuthenticate, digestDefinition) {\n    this.connection = connection\n    this.logger = logger\n    this.saslAuthenticate = saslAuthenticate\n    this.digestDefinition = digestDefinition\n\n    const digestType = digestDefinition.type.toUpperCase()\n    this.PREFIX = `SASL SCRAM ${digestType} authentication`\n\n    this.currentNonce = SCRAM.nonce()\n  }\n\n  async authenticate() {\n    const { PREFIX } = this\n    const { host, port, sasl } = this.connection\n    const broker = `${host}:${port}`\n\n    if (sasl.username == null || sasl.password == null) {\n      throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`)\n    }\n\n    try {\n      this.logger.debug('Exchanging first client message', { broker })\n      const clientMessageResponse = await this.sendClientFirstMessage()\n\n      this.logger.debug('Sending final message', { broker })\n      const finalResponse = await this.sendClientFinalMessage(clientMessageResponse)\n\n      if (finalResponse.e) {\n        throw new Error(finalResponse.e)\n      }\n\n      const serverKey = await this.serverKey(clientMessageResponse)\n      const serverSignature = this.serverSignature(serverKey, clientMessageResponse)\n\n      if (finalResponse.v !== serverSignature) {\n        throw new Error('Invalid server signature in server final message')\n      }\n\n      this.logger.debug(`${PREFIX} successful`, { broker })\n    } catch (e) {\n      const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`)\n      this.logger.error(error.message, { broker })\n      throw error\n    }\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFirstMessage() {\n    const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`\n    const request = scram.firstMessage.request({ clientFirstMessage })\n    const response = scram.firstMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async sendClientFinalMessage(clientMessageResponse) {\n    const { PREFIX } = this\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    const { minIterations } = this.digestDefinition\n\n    if (!clientMessageResponse.r.startsWith(this.currentNonce)) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`\n      )\n    }\n\n    if (iterations < minIterations) {\n      throw new KafkaJSSASLAuthenticationError(\n        `${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`\n      )\n    }\n\n    const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse)\n    const clientProof = await this.clientProof(clientMessageResponse)\n    const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`\n    const request = scram.finalMessage.request({ finalMessage })\n    const response = scram.finalMessage.response\n\n    return this.saslAuthenticate({\n      authExpectResponse: true,\n      request,\n      response,\n    })\n  }\n\n  /**\n   * @private\n   */\n  async clientProof(clientMessageResponse) {\n    const clientKey = await this.clientKey(clientMessageResponse)\n    const storedKey = this.H(clientKey)\n    const clientSignature = this.clientSignature(storedKey, clientMessageResponse)\n    return encode64(SCRAM.xor(clientKey, clientSignature))\n  }\n\n  /**\n   * @private\n   */\n  async clientKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_CLIENT_KEY)\n  }\n\n  /**\n   * @private\n   */\n  async serverKey(clientMessageResponse) {\n    const saltedPassword = await this.saltPassword(clientMessageResponse)\n    return this.HMAC(saltedPassword, HMAC_SERVER_KEY)\n  }\n\n  /**\n   * @private\n   */\n  clientSignature(storedKey, clientMessageResponse) {\n    return this.HMAC(storedKey, this.authMessage(clientMessageResponse))\n  }\n\n  /**\n   * @private\n   */\n  serverSignature(serverKey, clientMessageResponse) {\n    return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)))\n  }\n\n  /**\n   * @private\n   */\n  authMessage(clientMessageResponse) {\n    return [\n      this.firstMessageBare(),\n      clientMessageResponse.original,\n      this.finalMessageWithoutProof(clientMessageResponse),\n    ].join(',')\n  }\n\n  /**\n   * @private\n   */\n  async saltPassword(clientMessageResponse) {\n    const salt = Buffer.from(clientMessageResponse.s, 'base64')\n    const iterations = parseInt(clientMessageResponse.i, 10)\n    return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition)\n  }\n\n  /**\n   * @private\n   */\n  firstMessageBare() {\n    return `n=${this.encodedUsername()},r=${this.currentNonce}`\n  }\n\n  /**\n   * @private\n   */\n  finalMessageWithoutProof(clientMessageResponse) {\n    const rnonce = clientMessageResponse.r\n    return `c=${encode64(GS2_HEADER)},r=${rnonce}`\n  }\n\n  /**\n   * @private\n   */\n  encodedUsername() {\n    const { username } = this.connection.sasl\n    return SCRAM.sanitizeString(username).toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  encodedPassword() {\n    const { password } = this.connection.sasl\n    return password.toString('utf-8')\n  }\n\n  /**\n   * @private\n   */\n  H(data) {\n    return crypto\n      .createHash(this.digestDefinition.type)\n      .update(data)\n      .digest()\n  }\n\n  /**\n   * @private\n   */\n  HMAC(key, data) {\n    return crypto\n      .createHmac(this.digestDefinition.type, key)\n      .update(data)\n      .digest()\n  }\n}\n\nmodule.exports = {\n  DIGESTS,\n  SCRAM,\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM256Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM256Authenticator'), saslAuthenticate, DIGESTS.SHA256)\n  }\n}\n","const { SCRAM, DIGESTS } = require('./scram')\n\nmodule.exports = class SCRAM512Authenticator extends SCRAM {\n  constructor(connection, logger, saslAuthenticate) {\n    super(connection, logger.namespace('SCRAM512Authenticator'), saslAuthenticate, DIGESTS.SHA512)\n  }\n}\n","const Broker = require('../broker')\nconst createRetry = require('../retry')\nconst shuffle = require('../utils/shuffle')\nconst arrayDiff = require('../utils/arrayDiff')\nconst { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require('../errors')\n\nconst { keys, assign, values } = Object\nconst hasBrokerBeenReplaced = (broker, { host, port, rack }) =>\n  broker.connection.host !== host ||\n  broker.connection.port !== port ||\n  broker.connection.rack !== rack\n\nmodule.exports = class BrokerPool {\n  /**\n   * @param {object} options\n   * @param {import(\"./connectionBuilder\").ConnectionBuilder} options.connectionBuilder\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.allowAutoTopicCreation]\n   * @param {number} [options.authenticationTimeout]\n   * @param {number} [options.reauthenticationThreshold]\n   * @param {number} [options.metadataMaxAge]\n   */\n  constructor({\n    connectionBuilder,\n    logger,\n    retry,\n    allowAutoTopicCreation,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    metadataMaxAge,\n  }) {\n    this.rootLogger = logger\n    this.connectionBuilder = connectionBuilder\n    this.metadataMaxAge = metadataMaxAge || 0\n    this.logger = logger.namespace('BrokerPool')\n    this.retrier = createRetry(assign({}, retry))\n\n    this.createBroker = options =>\n      new Broker({\n        allowAutoTopicCreation,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        ...options,\n      })\n\n    this.brokers = {}\n    /** @type {Broker | undefined} */\n    this.seedBroker = undefined\n    /** @type {import(\"../../types\").BrokerMetadata | null} */\n    this.metadata = null\n    this.metadataExpireAt = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @returns {Boolean}\n   */\n  hasConnectedBrokers() {\n    const brokers = values(this.brokers)\n    return (\n      !!brokers.find(broker => broker.isConnected()) ||\n      (this.seedBroker ? this.seedBroker.isConnected() : false)\n    )\n  }\n\n  async createSeedBroker() {\n    if (this.seedBroker) {\n      await this.seedBroker.disconnect()\n    }\n\n    this.seedBroker = this.createBroker({\n      connection: await this.connectionBuilder.build(),\n      logger: this.rootLogger,\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    if (this.hasConnectedBrokers()) {\n      return\n    }\n\n    if (!this.seedBroker) {\n      await this.createSeedBroker()\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.seedBroker.connect()\n        this.versions = this.seedBroker.versions\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          // Connection builder will always rotate the seed broker\n          await this.createSeedBroker()\n          this.logger.error(\n            `Failed to connect to seed broker, trying another broker from the list: ${e.message}`,\n            { retryCount, retryTime }\n          )\n        } else {\n          this.logger.error(e.message, { retryCount, retryTime })\n        }\n\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.seedBroker && (await this.seedBroker.disconnect())\n    await Promise.all(values(this.brokers).map(broker => broker.disconnect()))\n\n    this.brokers = {}\n    this.metadata = null\n    this.versions = null\n    this.supportAuthenticationProtocol = null\n  }\n\n  /**\n   * @public\n   * @param {Object} destination\n   * @param {string} destination.host\n   * @param {number} destination.port\n   */\n  removeBroker({ host, port }) {\n    const removedBroker = values(this.brokers).find(\n      broker => broker.connection.host === host && broker.connection.port === port\n    )\n\n    if (removedBroker) {\n      delete this.brokers[removedBroker.nodeId]\n      this.metadataExpireAt = null\n\n      if (this.seedBroker.nodeId === removedBroker.nodeId) {\n        this.seedBroker = shuffle(values(this.brokers))[0]\n      }\n    }\n  }\n\n  /**\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadata(topics) {\n    const broker = await this.findConnectedBroker()\n    const { host: seedHost, port: seedPort } = this.seedBroker.connection\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.metadata = await broker.metadata(topics)\n        this.metadataExpireAt = Date.now() + this.metadataMaxAge\n\n        const replacedBrokers = []\n\n        this.brokers = await this.metadata.brokers.reduce(\n          async (resultPromise, { nodeId, host, port, rack }) => {\n            const result = await resultPromise\n\n            if (result[nodeId]) {\n              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {\n                return result\n              }\n\n              replacedBrokers.push(result[nodeId])\n            }\n\n            if (host === seedHost && port === seedPort) {\n              this.seedBroker.nodeId = nodeId\n              this.seedBroker.connection.rack = rack\n              return assign(result, {\n                [nodeId]: this.seedBroker,\n              })\n            }\n\n            return assign(result, {\n              [nodeId]: this.createBroker({\n                logger: this.rootLogger,\n                versions: this.versions,\n                supportAuthenticationProtocol: this.supportAuthenticationProtocol,\n                connection: await this.connectionBuilder.build({ host, port, rack }),\n                nodeId,\n              }),\n            })\n          },\n          this.brokers\n        )\n\n        const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort()\n        const currentBrokerIds = keys(this.brokers).sort()\n        const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds)\n\n        const brokerDisconnects = unusedBrokerIds.map(nodeId => {\n          const broker = this.brokers[nodeId]\n          return broker.disconnect().then(() => {\n            delete this.brokers[nodeId]\n          })\n        })\n\n        const replacedBrokersDisconnects = replacedBrokers.map(broker => broker.disconnect())\n        await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects])\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * Only refreshes metadata if the data is stale according to the `metadataMaxAge` param or does not contain information about the provided topics\n   *\n   * @public\n   * @param {Array<String>} topics\n   * @returns {Promise<null>}\n   */\n  async refreshMetadataIfNecessary(topics) {\n    const shouldRefresh =\n      this.metadata == null ||\n      this.metadataExpireAt == null ||\n      Date.now() > this.metadataExpireAt ||\n      !topics.every(topic =>\n        this.metadata.topicMetadata.some(topicMetadata => topicMetadata.topic === topic)\n      )\n\n    if (shouldRefresh) {\n      return this.refreshMetadata(topics)\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<Broker>}\n   */\n  async findBroker({ nodeId }) {\n    const broker = this.brokers[nodeId]\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`)\n    }\n\n    await this.connectBroker(broker)\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {(params: { nodeId: string, broker: Broker }) => Promise<T>} callback\n   * @returns {Promise<T>}\n   * @template T\n   */\n  async withBroker(callback) {\n    const brokers = shuffle(keys(this.brokers))\n    if (brokers.length === 0) {\n      throw new KafkaJSBrokerNotFound('No brokers in the broker pool')\n    }\n\n    for (const nodeId of brokers) {\n      const broker = await this.findBroker({ nodeId })\n      try {\n        return await callback({ nodeId, broker })\n      } catch (e) {}\n    }\n\n    return null\n  }\n\n  /**\n   * @public\n   * @returns {Promise<Broker>}\n   */\n  async findConnectedBroker() {\n    const nodeIds = shuffle(keys(this.brokers))\n    const connectedBrokerId = nodeIds.find(nodeId => this.brokers[nodeId].isConnected())\n\n    if (connectedBrokerId) {\n      return await this.findBroker({ nodeId: connectedBrokerId })\n    }\n\n    // Cycle through the nodes until one connects\n    for (const nodeId of nodeIds) {\n      try {\n        return await this.findBroker({ nodeId })\n      } catch (e) {}\n    }\n\n    // Failed to connect to all known brokers, metadata might be old\n    await this.connect()\n    return this.seedBroker\n  }\n\n  /**\n   * @private\n   * @param {Broker} broker\n   * @returns {Promise<null>}\n   */\n  async connectBroker(broker) {\n    if (broker.isConnected()) {\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await broker.connect()\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionError' || e.type === 'ILLEGAL_SASL_STATE') {\n          await broker.disconnect()\n        }\n\n        // To avoid reconnecting to an unavailable host, we bail on connection errors\n        // and refresh metadata on a higher level before reconnecting\n        if (e.name === 'KafkaJSConnectionError') {\n          return bail(e)\n        }\n\n        if (e.type === 'ILLEGAL_SASL_STATE') {\n          // Rebuild the connection since it can't recover from illegal SASL state\n          broker.connection = await this.connectionBuilder.build({\n            host: broker.connection.host,\n            port: broker.connection.port,\n            rack: broker.connection.rack,\n          })\n\n          this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime })\n          throw new KafkaJSProtocolError(e, { retriable: true })\n        }\n\n        if (e.retriable) throw e\n        this.logger.error(e, { retryCount, retryTime, stack: e.stack })\n        bail(e)\n      }\n    })\n  }\n}\n","const Connection = require('../network/connection')\nconst { KafkaJSConnectionError, KafkaJSNonRetriableError } = require('../errors')\n\n/**\n * @typedef {Object} ConnectionBuilder\n * @property {(destination?: { host?: string, port?: number, rack?: string }) => Promise<Connection>} build\n */\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} [options.socketFactory]\n * @param {string[]|(() => string[])} options.brokers\n * @param {Object} [options.ssl]\n * @param {Object} [options.sasl]\n * @param {string} options.clientId\n * @param {number} options.requestTimeout\n * @param {boolean} [options.enforceRequestTimeout]\n * @param {number} [options.connectionTimeout]\n * @param {number} [options.maxInFlightRequests]\n * @param {import(\"../../types\").RetryOptions} [options.retry]\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter]\n * @returns {ConnectionBuilder}\n */\nmodule.exports = ({\n  socketFactory,\n  brokers,\n  ssl,\n  sasl,\n  clientId,\n  requestTimeout,\n  enforceRequestTimeout,\n  connectionTimeout,\n  maxInFlightRequests,\n  logger,\n  instrumentationEmitter = null,\n}) => {\n  let index = 0\n\n  const isValidBroker = broker => {\n    return broker && typeof broker === 'string' && broker.length > 0\n  }\n\n  const validateBrokers = brokers => {\n    if (!brokers) {\n      throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`)\n    }\n\n    if (Array.isArray(brokers)) {\n      if (!brokers.length) {\n        throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`)\n      }\n\n      brokers.forEach((broker, index) => {\n        if (!isValidBroker(broker)) {\n          throw new KafkaJSNonRetriableError(\n            `Failed to connect: broker at index ${index} is invalid \"${typeof broker}\"`\n          )\n        }\n      })\n    }\n  }\n\n  const getBrokers = async () => {\n    let list\n\n    if (typeof brokers === 'function') {\n      try {\n        list = await brokers()\n      } catch (e) {\n        const wrappedError = new KafkaJSConnectionError(\n          `Failed to connect: \"config.brokers\" threw: ${e.message}`\n        )\n        wrappedError.stack = `${wrappedError.name}\\n  Caused by: ${e.stack}`\n        throw wrappedError\n      }\n    } else {\n      list = brokers\n    }\n\n    validateBrokers(list)\n\n    return list\n  }\n\n  return {\n    build: async ({ host, port, rack } = {}) => {\n      if (!host) {\n        const list = await getBrokers()\n\n        const randomBroker = list[index++ % list.length]\n\n        host = randomBroker.split(':')[0]\n        port = Number(randomBroker.split(':')[1])\n      }\n\n      return new Connection({\n        host,\n        port,\n        rack,\n        sasl,\n        ssl,\n        clientId,\n        socketFactory,\n        connectionTimeout,\n        requestTimeout,\n        enforceRequestTimeout,\n        maxInFlightRequests,\n        instrumentationEmitter,\n        logger,\n      })\n    },\n  }\n}\n","const BrokerPool = require('./brokerPool')\nconst Lock = require('../utils/lock')\nconst createRetry = require('../retry')\nconst connectionBuilder = require('./connectionBuilder')\nconst flatten = require('../utils/flatten')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst {\n  KafkaJSError,\n  KafkaJSBrokerNotFound,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSGroupCoordinatorNotFound,\n} = require('../errors')\nconst COORDINATOR_TYPES = require('../protocol/coordinatorTypes')\n\nconst { keys } = Object\n\nconst mergeTopics = (obj, { topic, partitions }) => ({\n  ...obj,\n  [topic]: [...(obj[topic] || []), ...partitions],\n})\n\nmodule.exports = class Cluster {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} options.metadataMaxAge - in milliseconds\n   * @param {boolean} options.allowAutoTopicCreation\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.isolationLevel\n   * @param {import(\"../../types\").RetryOptions} options.retry\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {Map} [options.offsets]\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    logger: rootLogger,\n    socketFactory,\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout = 30000,\n    enforceRequestTimeout,\n    metadataMaxAge,\n    retry,\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    isolationLevel,\n    instrumentationEmitter = null,\n    offsets = new Map(),\n  }) {\n    this.rootLogger = rootLogger\n    this.logger = rootLogger.namespace('Cluster')\n    this.retrier = createRetry(retry)\n    this.connectionBuilder = connectionBuilder({\n      logger: rootLogger,\n      instrumentationEmitter,\n      socketFactory,\n      brokers,\n      ssl,\n      sasl,\n      clientId,\n      connectionTimeout,\n      requestTimeout,\n      enforceRequestTimeout,\n      maxInFlightRequests,\n    })\n\n    this.targetTopics = new Set()\n    this.mutatingTargetTopics = new Lock({\n      description: `updating target topics`,\n      timeout: requestTimeout,\n    })\n    this.isolationLevel = isolationLevel\n    this.brokerPool = new BrokerPool({\n      connectionBuilder: this.connectionBuilder,\n      logger: this.rootLogger,\n      retry,\n      allowAutoTopicCreation,\n      authenticationTimeout,\n      reauthenticationThreshold,\n      metadataMaxAge,\n    })\n    this.committedOffsetsByGroup = offsets\n  }\n\n  isConnected() {\n    return this.brokerPool.hasConnectedBrokers()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async connect() {\n    await this.brokerPool.connect()\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async disconnect() {\n    await this.brokerPool.disconnect()\n  }\n\n  /**\n   * @public\n   * @param {object} destination\n   * @param {String} destination.host\n   * @param {Number} destination.port\n   */\n  removeBroker({ host, port }) {\n    this.brokerPool.removeBroker({ host, port })\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadata() {\n    await this.brokerPool.refreshMetadata(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<void>}\n   */\n  async refreshMetadataIfNecessary() {\n    await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics))\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").BrokerMetadata>}\n   */\n  async metadata({ topics = [] } = {}) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.brokerPool.refreshMetadataIfNecessary(topics)\n        return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics))\n      } catch (e) {\n        if (e.type === 'LEADER_NOT_AVAILABLE') {\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @return {Promise}\n   */\n  async addTargetTopic(topic) {\n    return this.addMultipleTargetTopics([topic])\n  }\n\n  /**\n   * @public\n   * @param {string[]} topics\n   * @return {Promise}\n   */\n  async addMultipleTargetTopics(topics) {\n    await this.mutatingTargetTopics.acquire()\n\n    try {\n      const previousSize = this.targetTopics.size\n      const previousTopics = new Set(this.targetTopics)\n      for (const topic of topics) {\n        this.targetTopics.add(topic)\n      }\n\n      const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata\n\n      if (hasChanged) {\n        try {\n          await this.refreshMetadata()\n        } catch (e) {\n          if (e.type === 'INVALID_TOPIC_EXCEPTION' || e.type === 'UNKNOWN_TOPIC_OR_PARTITION') {\n            this.targetTopics = previousTopics\n          }\n\n          throw e\n        }\n      }\n    } finally {\n      await this.mutatingTargetTopics.release()\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} options\n   * @param {string} options.nodeId\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findBroker({ nodeId }) {\n    try {\n      return await this.brokerPool.findBroker({ nodeId })\n    } catch (e) {\n      // The client probably has stale metadata\n      if (\n        e.name === 'KafkaJSBrokerNotFound' ||\n        e.name === 'KafkaJSLockTimeout' ||\n        e.name === 'KafkaJSConnectionError'\n      ) {\n        await this.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  /**\n   * @public\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findControllerBroker() {\n    const { metadata } = this.brokerPool\n\n    if (!metadata || metadata.controllerId == null) {\n      throw new KafkaJSMetadataNotLoaded('Topic metadata not loaded')\n    }\n\n    const broker = await this.findBroker({ nodeId: metadata.controllerId })\n\n    if (!broker) {\n      throw new KafkaJSBrokerNotFound(\n        `Controller broker with id ${metadata.controllerId} not found in the cached metadata`\n      )\n    }\n\n    return broker\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @returns {import(\"../../types\").PartitionMetadata[]} Example:\n   *                   [{\n   *                     isr: [2],\n   *                     leader: 2,\n   *                     partitionErrorCode: 0,\n   *                     partitionId: 0,\n   *                     replicas: [2],\n   *                   }]\n   */\n  findTopicPartitionMetadata(topic) {\n    const { metadata } = this.brokerPool\n    if (!metadata || !metadata.topicMetadata) {\n      throw new KafkaJSTopicMetadataNotLoaded('Topic metadata not loaded', { topic })\n    }\n\n    const topicMetadata = metadata.topicMetadata.find(t => t.topic === topic)\n    return topicMetadata ? topicMetadata.partitionMetadata : []\n  }\n\n  /**\n   * @public\n   * @param {string} topic\n   * @param {(number|string)[]} partitions\n   * @returns {Object} Object with leader and partitions. For partitions 0 and 5\n   *                   the result could be:\n   *                     { '0': [0], '2': [5] }\n   *\n   *                   where the key is the nodeId.\n   */\n  findLeaderForPartitions(topic, partitions) {\n    const partitionMetadata = this.findTopicPartitionMetadata(topic)\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader === null || metadata.leader === undefined) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      const { leader } = metadata\n      const current = result[leader] || []\n      return { ...result, [leader]: [...current, partitionId] }\n    }, {})\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<import(\"../../types\").Broker>}\n   */\n  async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        const { coordinator } = await this.findGroupCoordinatorMetadata({\n          groupId,\n          coordinatorType,\n        })\n        return await this.findBroker({ nodeId: coordinator.nodeId })\n      } catch (e) {\n        // A new broker can join the cluster before we have the chance\n        // to refresh metadata\n        if (e.name === 'KafkaJSBrokerNotFound' || e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n          this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {\n            groupId,\n            retryCount,\n            retryTime,\n          })\n\n          await this.refreshMetadata()\n          throw e\n        }\n\n        if (e.code === 'ECONNREFUSED') {\n          // During maintenance the current coordinator can go down; findBroker will\n          // refresh metadata and re-throw the error. findGroupCoordinator has to re-throw\n          // the error to go through the retry cycle.\n          throw e\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} params\n   * @param {string} params.groupId\n   * @param {import(\"../protocol/coordinatorTypes\").CoordinatorType} [params.coordinatorType=0]\n   * @returns {Promise<Object>}\n   */\n  async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {\n    const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {\n      return await this.retrier(async (bail, retryCount, retryTime) => {\n        try {\n          const brokerMetadata = await broker.findGroupCoordinator({ groupId, coordinatorType })\n          this.logger.debug('Found group coordinator', {\n            broker: brokerMetadata.host,\n            nodeId: brokerMetadata.coordinator.nodeId,\n          })\n          return brokerMetadata\n        } catch (e) {\n          this.logger.debug('Tried to find group coordinator', {\n            nodeId,\n            error: e,\n          })\n\n          if (e.type === 'GROUP_COORDINATOR_NOT_AVAILABLE') {\n            this.logger.debug('Group coordinator not available, retrying...', {\n              nodeId,\n              retryCount,\n              retryTime,\n            })\n\n            throw e\n          }\n\n          bail(e)\n        }\n      })\n    })\n\n    if (brokerMetadata) {\n      return brokerMetadata\n    }\n\n    throw new KafkaJSGroupCoordinatorNotFound('Failed to find group coordinator')\n  }\n\n  /**\n   * @param {object} topicConfiguration\n   * @returns {number}\n   */\n  defaultOffset({ fromBeginning }) {\n    return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET\n  }\n\n  /**\n   * @public\n   * @param {Array<Object>} topics\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [{ partition: 0 }],\n   *                              fromBeginning: false\n   *                            }\n   *                          ]\n   * @returns {Promise<import(\"../../types\").TopicOffsets[]>} example:\n   *                          [\n   *                            {\n   *                              topic: 'my-topic-name',\n   *                              partitions: [\n   *                                { partition: 0, offset: '1' },\n   *                                { partition: 1, offset: '2' },\n   *                                { partition: 2, offset: '1' },\n   *                              ],\n   *                            },\n   *                          ]\n   */\n  async fetchTopicsOffset(topics) {\n    const partitionsPerBroker = {}\n    const topicConfigurations = {}\n\n    const addDefaultOffset = topic => partition => {\n      const { timestamp } = topicConfigurations[topic]\n      return { ...partition, timestamp }\n    }\n\n    // Index all topics and partitions per leader (nodeId)\n    for (const topicData of topics) {\n      const { topic, partitions, fromBeginning, fromTimestamp } = topicData\n      const partitionsPerLeader = this.findLeaderForPartitions(\n        topic,\n        partitions.map(p => p.partition)\n      )\n      const timestamp =\n        fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning })\n\n      topicConfigurations[topic] = { timestamp }\n\n      keys(partitionsPerLeader).forEach(nodeId => {\n        partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {}\n        partitionsPerBroker[nodeId][topic] = partitions.filter(p =>\n          partitionsPerLeader[nodeId].includes(p.partition)\n        )\n      })\n    }\n\n    // Create a list of requests to fetch the offset of all partitions\n    const requests = keys(partitionsPerBroker).map(async nodeId => {\n      const broker = await this.findBroker({ nodeId })\n      const partitions = partitionsPerBroker[nodeId]\n\n      const { responses: topicOffsets } = await broker.listOffsets({\n        isolationLevel: this.isolationLevel,\n        topics: keys(partitions).map(topic => ({\n          topic,\n          partitions: partitions[topic].map(addDefaultOffset(topic)),\n        })),\n      })\n\n      return topicOffsets\n    })\n\n    // Execute all requests, merge and normalize the responses\n    const responses = await Promise.all(requests)\n    const partitionsPerTopic = flatten(responses).reduce(mergeTopics, {})\n\n    return keys(partitionsPerTopic).map(topic => ({\n      topic,\n      partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({\n        partition,\n        offset,\n      })),\n    }))\n  }\n\n  /**\n   * Retrieve the object mapping for committed offsets for a single consumer group\n   * @param {object} options\n   * @param {string} options.groupId\n   * @returns {Object}\n   */\n  committedOffsets({ groupId }) {\n    if (!this.committedOffsetsByGroup.has(groupId)) {\n      this.committedOffsetsByGroup.set(groupId, {})\n    }\n\n    return this.committedOffsetsByGroup.get(groupId)\n  }\n\n  /**\n   * Mark offset as committed for a single consumer group's topic-partition\n   * @param {object} options\n   * @param {string} options.groupId\n   * @param {string} options.topic\n   * @param {string|number} options.partition\n   * @param {string} options.offset\n   */\n  markOffsetAsCommitted({ groupId, topic, partition, offset }) {\n    const committedOffsets = this.committedOffsets({ groupId })\n\n    committedOffsets[topic] = committedOffsets[topic] || {}\n    committedOffsets[topic][partition] = offset\n  }\n}\n","const EARLIEST_OFFSET = -2\nconst LATEST_OFFSET = -1\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\n\nmodule.exports = {\n  EARLIEST_OFFSET,\n  LATEST_OFFSET,\n  INT_32_MAX_VALUE,\n}\n","const Encoder = require('../protocol/encoder')\nconst Decoder = require('../protocol/decoder')\n\nconst MemberMetadata = {\n  /**\n   * @param {Object} metadata\n   * @param {number} metadata.version\n   * @param {Array<string>} metadata.topics\n   * @param {Buffer} [metadata.userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, topics, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(topics)\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    return {\n      version: decoder.readInt16(),\n      topics: decoder.readArray(d => d.readString()),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nconst MemberAssignment = {\n  /**\n   * @param {number} version\n   * @param {Object<String,Array>} assignment, example:\n   *                               {\n   *                                 'topic-A': [0, 2, 4, 6],\n   *                                 'topic-B': [0, 2],\n   *                               }\n   * @param {Buffer} [userData=Buffer.alloc(0)]\n   *\n   * @returns Buffer\n   */\n  encode({ version, assignment, userData = Buffer.alloc(0) }) {\n    return new Encoder()\n      .writeInt16(version)\n      .writeArray(\n        Object.keys(assignment).map(topic =>\n          new Encoder().writeString(topic).writeArray(assignment[topic])\n        )\n      )\n      .writeBytes(userData).buffer\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Object|null}\n   */\n  decode(buffer) {\n    const decoder = new Decoder(buffer)\n    const decodePartitions = d => d.readInt32()\n    const decodeAssignment = d => ({\n      topic: d.readString(),\n      partitions: d.readArray(decodePartitions),\n    })\n    const indexAssignment = (obj, { topic, partitions }) =>\n      Object.assign(obj, { [topic]: partitions })\n\n    if (!decoder.canReadInt16()) {\n      return null\n    }\n\n    return {\n      version: decoder.readInt16(),\n      assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),\n      userData: decoder.readBytes(),\n    }\n  },\n}\n\nmodule.exports = {\n  MemberMetadata,\n  MemberAssignment,\n}\n","const roundRobin = require('./roundRobinAssigner')\n\nmodule.exports = {\n  roundRobin,\n}\n","const { MemberMetadata, MemberAssignment } = require('../../assignerProtocol')\nconst flatten = require('../../../utils/flatten')\n\n/**\n * RoundRobinAssigner\n * @param {Cluster} cluster\n * @returns {function}\n */\nmodule.exports = ({ cluster }) => ({\n  name: 'RoundRobinAssigner',\n  version: 1,\n\n  /**\n   * Assign the topics to the provided members.\n   *\n   * The members array contains information about each member, `memberMetadata` is the result of the\n   * `protocol` operation.\n   *\n   * @param {array} members array of members, e.g:\n                              [{ memberId: 'test-5f93f5a3', memberMetadata: Buffer }]\n   * @param {array} topics\n   * @returns {array} object partitions per topic per member, e.g:\n   *                   [\n   *                     {\n   *                       memberId: 'test-5f93f5a3',\n   *                       memberAssignment: {\n   *                         'topic-A': [0, 2, 4, 6],\n   *                         'topic-B': [1],\n   *                       },\n   *                     },\n   *                     {\n   *                       memberId: 'test-3d3d5341',\n   *                       memberAssignment: {\n   *                         'topic-A': [1, 3, 5],\n   *                         'topic-B': [0, 2],\n   *                       },\n   *                     }\n   *                   ]\n   */\n  async assign({ members, topics }) {\n    const membersCount = members.length\n    const sortedMembers = members.map(({ memberId }) => memberId).sort()\n    const assignment = {}\n\n    const topicsPartionArrays = topics.map(topic => {\n      const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n      return partitionMetadata.map(m => ({ topic: topic, partitionId: m.partitionId }))\n    })\n    const topicsPartitions = flatten(topicsPartionArrays)\n\n    topicsPartitions.forEach((topicPartition, i) => {\n      const assignee = sortedMembers[i % membersCount]\n\n      if (!assignment[assignee]) {\n        assignment[assignee] = Object.create(null)\n      }\n\n      if (!assignment[assignee][topicPartition.topic]) {\n        assignment[assignee][topicPartition.topic] = []\n      }\n\n      assignment[assignee][topicPartition.topic].push(topicPartition.partitionId)\n    })\n\n    return Object.keys(assignment).map(memberId => ({\n      memberId,\n      memberAssignment: MemberAssignment.encode({\n        version: this.version,\n        assignment: assignment[memberId],\n      }),\n    }))\n  },\n\n  protocol({ topics }) {\n    return {\n      name: this.name,\n      metadata: MemberMetadata.encode({\n        version: this.version,\n        topics,\n      }),\n    }\n  },\n})\n","/**\n * @template T\n * @return {{lock: Promise<T>, unlock: (v?: T) => void, unlockWithError: (e: Error) => void}}\n */\nmodule.exports = () => {\n  let unlock\n  let unlockWithError\n  const lock = new Promise(resolve => {\n    unlock = resolve\n    unlockWithError = resolve\n  })\n\n  return { lock, unlock, unlockWithError }\n}\n","const Long = require('../utils/long')\nconst filterAbortedMessages = require('./filterAbortedMessages')\n\n/**\n * A batch collects messages returned from a single fetch call.\n *\n * A batch could contain _multiple_ Kafka RecordBatches.\n */\nmodule.exports = class Batch {\n  constructor(topic, fetchedOffset, partitionData) {\n    this.fetchedOffset = fetchedOffset\n    const longFetchedOffset = Long.fromValue(this.fetchedOffset)\n    const { abortedTransactions, messages } = partitionData\n\n    this.topic = topic\n    this.partition = partitionData.partition\n    this.highWatermark = partitionData.highWatermark\n\n    this.rawMessages = messages\n    // Apparently fetch can return different offsets than the target offset provided to the fetch API.\n    // Discard messages that are not in the requested offset\n    // https://github.com/apache/kafka/blob/bf237fa7c576bd141d78fdea9f17f65ea269c290/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L912\n    this.messagesWithinOffset = this.rawMessages.filter(message =>\n      Long.fromValue(message.offset).gte(longFetchedOffset)\n    )\n\n    // 1. Don't expose aborted messages\n    // 2. Don't expose control records\n    // @see https://kafka.apache.org/documentation/#controlbatch\n    this.messages = filterAbortedMessages({\n      messages: this.messagesWithinOffset,\n      abortedTransactions,\n    }).filter(message => !message.isControlRecord)\n  }\n\n  isEmpty() {\n    return this.messages.length === 0\n  }\n\n  isEmptyIncludingFiltered() {\n    return this.messagesWithinOffset.length === 0\n  }\n\n  /**\n   * If the batch contained raw messages (i.e was not truely empty) but all messages were filtered out due to\n   * log compaction, control records or other reasons\n   */\n  isEmptyDueToFiltering() {\n    return this.isEmpty() && this.rawMessages.length > 0\n  }\n\n  isEmptyControlRecord() {\n    return (\n      this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord)\n    )\n  }\n\n  /**\n   * With compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset.\n   * These messages will be filtered out (i.e. they are not even included in this.messagesWithinOffset)\n   * If these are the only messages, the batch will appear as an empty batch.\n   *\n   * isEmpty() and isEmptyIncludingFiltered() will always return true if the batch is empty,\n   * but this method will only return true if the batch is empty due to log compacted messages.\n   *\n   * @returns boolean True if the batch is empty, because of log compacted messages in the partition.\n   */\n  isEmptyDueToLogCompactedMessages() {\n    const hasMessages = this.rawMessages.length > 0\n    return hasMessages && this.isEmptyIncludingFiltered()\n  }\n\n  firstOffset() {\n    return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset\n  }\n\n  lastOffset() {\n    if (this.isEmptyDueToLogCompactedMessages()) {\n      return this.fetchedOffset\n    }\n\n    if (this.isEmptyIncludingFiltered()) {\n      return Long.fromValue(this.highWatermark)\n        .add(-1)\n        .toString()\n    }\n\n    return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset\n  }\n\n  /**\n   * Returns the lag based on the last offset in the batch (also known as \"high\")\n   */\n  offsetLag() {\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const lastConsumedOffset = Long.fromValue(this.lastOffset())\n    return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString()\n  }\n\n  /**\n   * Returns the lag based on the first offset in the batch\n   */\n  offsetLagLow() {\n    if (this.isEmptyIncludingFiltered()) {\n      return '0'\n    }\n\n    const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1)\n    const firstConsumedOffset = Long.fromValue(this.firstOffset())\n    return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString()\n  }\n}\n","const flatten = require('../utils/flatten')\nconst sleep = require('../utils/sleep')\nconst BufferedAsyncIterator = require('../utils/bufferedAsyncIterator')\nconst websiteUrl = require('../utils/websiteUrl')\nconst arrayDiff = require('../utils/arrayDiff')\nconst createRetry = require('../retry')\nconst sharedPromiseTo = require('../utils/sharedPromiseTo')\n\nconst OffsetManager = require('./offsetManager')\nconst Batch = require('./batch')\nconst SeekOffsets = require('./seekOffsets')\nconst SubscriptionState = require('./subscriptionState')\nconst {\n  events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS },\n} = require('./instrumentationEvents')\nconst { MemberAssignment } = require('./assignerProtocol')\nconst {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSStaleTopicMetadataAssignment,\n} = require('../errors')\n\nconst { keys } = Object\n\nconst STALE_METADATA_ERRORS = [\n  'LEADER_NOT_AVAILABLE',\n  // Fetch before v9 uses NOT_LEADER_FOR_PARTITION\n  'NOT_LEADER_FOR_PARTITION',\n  // Fetch after v9 uses {FENCED,UNKNOWN}_LEADER_EPOCH\n  'FENCED_LEADER_EPOCH',\n  'UNKNOWN_LEADER_EPOCH',\n  'UNKNOWN_TOPIC_OR_PARTITION',\n]\n\nconst isRebalancing = e =>\n  e.type === 'REBALANCE_IN_PROGRESS' || e.type === 'NOT_COORDINATOR_FOR_GROUP'\n\nconst PRIVATE = {\n  JOIN: Symbol('private:ConsumerGroup:join'),\n  SYNC: Symbol('private:ConsumerGroup:sync'),\n  HEARTBEAT: Symbol('private:ConsumerGroup:heartbeat'),\n  SHAREDHEARTBEAT: Symbol('private:ConsumerGroup:sharedHeartbeat'),\n}\n\nmodule.exports = class ConsumerGroup {\n  constructor({\n    retry,\n    cluster,\n    groupId,\n    topics,\n    topicConfigurations,\n    logger,\n    instrumentationEmitter,\n    assigners,\n    sessionTimeout,\n    rebalanceTimeout,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    isolationLevel,\n    rackId,\n    metadataMaxAge,\n  }) {\n    /** @type {import(\"../../types\").Cluster} */\n    this.cluster = cluster\n    this.groupId = groupId\n    this.topics = topics\n    this.topicsSubscribed = topics\n    this.topicConfigurations = topicConfigurations\n    this.logger = logger.namespace('ConsumerGroup')\n    this.instrumentationEmitter = instrumentationEmitter\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.assigners = assigners\n    this.sessionTimeout = sessionTimeout\n    this.rebalanceTimeout = rebalanceTimeout\n    this.maxBytesPerPartition = maxBytesPerPartition\n    this.minBytes = minBytes\n    this.maxBytes = maxBytes\n    this.maxWaitTime = maxWaitTimeInMs\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.isolationLevel = isolationLevel\n    this.rackId = rackId\n    this.metadataMaxAge = metadataMaxAge\n\n    this.seekOffset = new SeekOffsets()\n    this.coordinator = null\n    this.generationId = null\n    this.leaderId = null\n    this.memberId = null\n    this.members = null\n    this.groupProtocol = null\n\n    this.partitionsPerSubscribedTopic = null\n    /**\n     * Preferred read replica per topic and partition\n     *\n     * Each of the partitions tracks the preferred read replica (`nodeId`) and a timestamp\n     * until when that preference is valid.\n     *\n     * @type {{[topicName: string]: {[partition: number]: {nodeId: number, expireAt: number}}}}\n     */\n    this.preferredReadReplicasPerTopicPartition = {}\n    this.offsetManager = null\n    this.subscriptionState = new SubscriptionState()\n\n    this.lastRequest = Date.now()\n\n    this[PRIVATE.SHAREDHEARTBEAT] = sharedPromiseTo(async ({ interval }) => {\n      const { groupId, generationId, memberId } = this\n      const now = Date.now()\n\n      if (memberId && now >= this.lastRequest + interval) {\n        const payload = {\n          groupId,\n          memberId,\n          groupGenerationId: generationId,\n        }\n\n        await this.coordinator.heartbeat(payload)\n        this.instrumentationEmitter.emit(HEARTBEAT, payload)\n        this.lastRequest = Date.now()\n      }\n    })\n  }\n\n  isLeader() {\n    return this.leaderId && this.memberId === this.leaderId\n  }\n\n  async connect() {\n    await this.cluster.connect()\n    this.instrumentationEmitter.emit(CONNECT)\n    await this.cluster.refreshMetadataIfNecessary()\n  }\n\n  async [PRIVATE.JOIN]() {\n    const { groupId, sessionTimeout, rebalanceTimeout } = this\n\n    this.coordinator = await this.cluster.findGroupCoordinator({ groupId })\n\n    const groupData = await this.coordinator.joinGroup({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId: this.memberId || '',\n      groupProtocols: this.assigners.map(assigner =>\n        assigner.protocol({\n          topics: this.topicsSubscribed,\n        })\n      ),\n    })\n\n    this.generationId = groupData.generationId\n    this.leaderId = groupData.leaderId\n    this.memberId = groupData.memberId\n    this.members = groupData.members\n    this.groupProtocol = groupData.groupProtocol\n  }\n\n  async leave() {\n    const { groupId, memberId } = this\n    if (memberId) {\n      await this.coordinator.leaveGroup({ groupId, memberId })\n      this.memberId = null\n    }\n  }\n\n  async [PRIVATE.SYNC]() {\n    let assignment = []\n    const {\n      groupId,\n      generationId,\n      memberId,\n      members,\n      groupProtocol,\n      topics,\n      topicsSubscribed,\n      coordinator,\n    } = this\n\n    if (this.isLeader()) {\n      this.logger.debug('Chosen as group leader', { groupId, generationId, memberId, topics })\n      const assigner = this.assigners.find(({ name }) => name === groupProtocol)\n\n      if (!assigner) {\n        throw new KafkaJSNonRetriableError(\n          `Unsupported partition assigner \"${groupProtocol}\", the assigner wasn't found in the assigners list`\n        )\n      }\n\n      await this.cluster.refreshMetadata()\n      assignment = await assigner.assign({ members, topics: topicsSubscribed })\n\n      this.logger.debug('Group assignment', {\n        groupId,\n        generationId,\n        groupProtocol,\n        assignment,\n        topics: topicsSubscribed,\n      })\n    }\n\n    // Keep track of the partitions for the subscribed topics\n    this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n    const { memberAssignment } = await this.coordinator.syncGroup({\n      groupId,\n      generationId,\n      memberId,\n      groupAssignment: assignment,\n    })\n\n    const decodedMemberAssignment = MemberAssignment.decode(memberAssignment)\n    const decodedAssignment =\n      decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {}\n\n    this.logger.debug('Received assignment', {\n      groupId,\n      generationId,\n      memberId,\n      memberAssignment: decodedAssignment,\n    })\n\n    const assignedTopics = keys(decodedAssignment)\n    const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed)\n\n    if (topicsNotSubscribed.length > 0) {\n      const payload = {\n        groupId,\n        generationId,\n        memberId,\n        assignedTopics,\n        topicsSubscribed,\n        topicsNotSubscribed,\n      }\n\n      this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload)\n      this.logger.warn('Consumer group received unsubscribed topics', {\n        ...payload,\n        helpUrl: websiteUrl(\n          'docs/faq',\n          'why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to'\n        ),\n      })\n    }\n\n    // Remove unsubscribed topics from the list\n    const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed)\n    const currentMemberAssignment = safeAssignment.map(topic => ({\n      topic,\n      partitions: decodedAssignment[topic],\n    }))\n\n    // Check if the consumer is aware of all assigned partitions\n    for (const assignment of currentMemberAssignment) {\n      const { topic, partitions: assignedPartitions } = assignment\n      const knownPartitions = this.partitionsPerSubscribedTopic.get(topic)\n      const isAwareOfAllAssignedPartitions = assignedPartitions.every(partition =>\n        knownPartitions.includes(partition)\n      )\n\n      if (!isAwareOfAllAssignedPartitions) {\n        this.logger.warn('Consumer is not aware of all assigned partitions, refreshing metadata', {\n          groupId,\n          generationId,\n          memberId,\n          topic,\n          knownPartitions,\n          assignedPartitions,\n        })\n\n        // If the consumer is not aware of all assigned partitions, refresh metadata\n        // and update the list of partitions per subscribed topic. It's enough to perform\n        // this operation once since refresh metadata will update metadata for all topics\n        await this.cluster.refreshMetadata()\n        this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n        break\n      }\n    }\n\n    this.topics = currentMemberAssignment.map(({ topic }) => topic)\n    this.subscriptionState.assign(currentMemberAssignment)\n    this.offsetManager = new OffsetManager({\n      cluster: this.cluster,\n      topicConfigurations: this.topicConfigurations,\n      instrumentationEmitter: this.instrumentationEmitter,\n      memberAssignment: currentMemberAssignment.reduce(\n        (partitionsByTopic, { topic, partitions }) => ({\n          ...partitionsByTopic,\n          [topic]: partitions,\n        }),\n        {}\n      ),\n      autoCommit: this.autoCommit,\n      autoCommitInterval: this.autoCommitInterval,\n      autoCommitThreshold: this.autoCommitThreshold,\n      coordinator,\n      groupId,\n      generationId,\n      memberId,\n    })\n  }\n\n  joinAndSync() {\n    const startJoin = Date.now()\n    return this.retrier(async bail => {\n      try {\n        await this[PRIVATE.JOIN]()\n        await this[PRIVATE.SYNC]()\n\n        const memberAssignment = this.assigned().reduce(\n          (result, { topic, partitions }) => ({ ...result, [topic]: partitions }),\n          {}\n        )\n\n        const payload = {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          leaderId: this.leaderId,\n          isLeader: this.isLeader(),\n          memberAssignment,\n          groupProtocol: this.groupProtocol,\n          duration: Date.now() - startJoin,\n        }\n\n        this.instrumentationEmitter.emit(GROUP_JOIN, payload)\n        this.logger.info('Consumer has joined the group', payload)\n      } catch (e) {\n        if (isRebalancing(e)) {\n          // Rebalance in progress isn't a retriable protocol error since the consumer\n          // has to go through find coordinator and join again before it can\n          // actually retry the operation. We wrap the original error in a retriable error\n          // here instead in order to restart the join + sync sequence using the retrier.\n          throw new KafkaJSError(e)\n        }\n\n        bail(e)\n      }\n    })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.offsetManager.resetOffset({ topic, partition })\n  }\n\n  /**\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.offsetManager.resolveOffset({ topic, partition, offset })\n  }\n\n  /**\n   * Update the consumer offset for the given topic/partition. This will be used\n   * on the next fetch. If this API is invoked for the same topic/partition more\n   * than once, the latest offset will be used on the next fetch.\n   *\n   * @param {import(\"../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  seek({ topic, partition, offset }) {\n    this.seekOffset.set(topic, partition, offset)\n  }\n\n  pause(topicPartitions) {\n    this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.pause(topicPartitions)\n  }\n\n  resume(topicPartitions) {\n    this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {\n      topicPartitions,\n    })\n    this.subscriptionState.resume(topicPartitions)\n  }\n\n  assigned() {\n    return this.subscriptionState.assigned()\n  }\n\n  paused() {\n    return this.subscriptionState.paused()\n  }\n\n  async commitOffsetsIfNecessary() {\n    await this.offsetManager.commitOffsetsIfNecessary()\n  }\n\n  async commitOffsets(offsets) {\n    await this.offsetManager.commitOffsets(offsets)\n  }\n\n  uncommittedOffsets() {\n    return this.offsetManager.uncommittedOffsets()\n  }\n\n  async heartbeat({ interval }) {\n    return this[PRIVATE.SHAREDHEARTBEAT]({ interval })\n  }\n\n  async fetch() {\n    try {\n      const { topics, maxBytesPerPartition, maxWaitTime, minBytes, maxBytes } = this\n      /** @type {{[nodeId: string]: {topic: string, partitions: { partition: number; fetchOffset: string; maxBytes: number }[]}[]}} */\n      const requestsPerNode = {}\n\n      await this.cluster.refreshMetadataIfNecessary()\n      this.checkForStaleAssignment()\n\n      while (this.seekOffset.size > 0) {\n        const seekEntry = this.seekOffset.pop()\n        this.logger.debug('Seek offset', {\n          groupId: this.groupId,\n          memberId: this.memberId,\n          seek: seekEntry,\n        })\n        await this.offsetManager.seek(seekEntry)\n      }\n\n      const pausedTopicPartitions = this.subscriptionState.paused()\n      const activeTopicPartitions = this.subscriptionState.active()\n\n      const activePartitions = flatten(activeTopicPartitions.map(({ partitions }) => partitions))\n      const activeTopics = activeTopicPartitions\n        .filter(({ partitions }) => partitions.length > 0)\n        .map(({ topic }) => topic)\n\n      if (activePartitions.length === 0) {\n        this.logger.debug(`No active topic partitions, sleeping for ${this.maxWaitTime}ms`, {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        })\n\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      await this.offsetManager.resolveOffsets()\n\n      this.logger.debug(\n        `Fetching from ${activePartitions.length} partitions for ${activeTopics.length} out of ${topics.length} topics`,\n        {\n          topics,\n          activeTopicPartitions,\n          pausedTopicPartitions,\n        }\n      )\n\n      for (const topicPartition of activeTopicPartitions) {\n        const partitionsPerNode = this.findReadReplicaForPartitions(\n          topicPartition.topic,\n          topicPartition.partitions\n        )\n\n        const nodeIds = keys(partitionsPerNode)\n        const committedOffsets = this.offsetManager.committedOffsets()\n\n        for (const nodeId of nodeIds) {\n          const partitions = partitionsPerNode[nodeId]\n            .filter(partition => {\n              /**\n               * When recovering from OffsetOutOfRange, each partition can recover\n               * concurrently, which invalidates resolved and committed offsets as part\n               * of the recovery mechanism (see OffsetManager.clearOffsets). In concurrent\n               * scenarios this can initiate a new fetch with invalid offsets.\n               *\n               * This was further highlighted by https://github.com/tulios/kafkajs/pull/570,\n               * which increased concurrency, making this more likely to happen.\n               *\n               * This is solved by only making requests for partitions with initialized offsets.\n               *\n               * See the following pull request which explains the context of the problem:\n               * @issue https://github.com/tulios/kafkajs/pull/578\n               */\n              return committedOffsets[topicPartition.topic][partition] != null\n            })\n            .map(partition => ({\n              partition,\n              fetchOffset: this.offsetManager\n                .nextOffset(topicPartition.topic, partition)\n                .toString(),\n              maxBytes: maxBytesPerPartition,\n            }))\n\n          requestsPerNode[nodeId] = requestsPerNode[nodeId] || []\n          requestsPerNode[nodeId].push({ topic: topicPartition.topic, partitions })\n        }\n      }\n\n      const requests = keys(requestsPerNode).map(async nodeId => {\n        const broker = await this.cluster.findBroker({ nodeId })\n        const { responses } = await broker.fetch({\n          maxWaitTime,\n          minBytes,\n          maxBytes,\n          isolationLevel: this.isolationLevel,\n          topics: requestsPerNode[nodeId],\n          rackId: this.rackId,\n        })\n\n        const batchesPerPartition = responses.map(({ topicName, partitions }) => {\n          const topicRequestData = requestsPerNode[nodeId].find(({ topic }) => topic === topicName)\n          let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName]\n          if (!preferredReadReplicas) {\n            this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {}\n          }\n\n          return partitions\n            .filter(\n              partitionData =>\n                !this.seekOffset.has(topicName, partitionData.partition) &&\n                !this.subscriptionState.isPaused(topicName, partitionData.partition)\n            )\n            .map(partitionData => {\n              const { partition, preferredReadReplica } = partitionData\n              if (preferredReadReplica != null && preferredReadReplica !== -1) {\n                const { nodeId: currentPreferredReadReplica } =\n                  preferredReadReplicas[partition] || {}\n                if (currentPreferredReadReplica !== preferredReadReplica) {\n                  this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {\n                    groupId: this.groupId,\n                    memberId: this.memberId,\n                    topic: topicName,\n                    partition,\n                  })\n                }\n                preferredReadReplicas[partition] = {\n                  nodeId: preferredReadReplica,\n                  expireAt: Date.now() + this.metadataMaxAge,\n                }\n              }\n\n              const partitionRequestData = topicRequestData.partitions.find(\n                ({ partition }) => partition === partitionData.partition\n              )\n\n              const fetchedOffset = partitionRequestData.fetchOffset\n              return new Batch(topicName, fetchedOffset, partitionData)\n            })\n        })\n\n        return flatten(batchesPerPartition)\n      })\n\n      // fetch can generate empty requests when the consumer group receives an assignment\n      // with more topics than the subscribed, so to prevent a busy loop we wait the\n      // configured max wait time\n      if (requests.length === 0) {\n        await sleep(this.maxWaitTime)\n        return BufferedAsyncIterator([])\n      }\n\n      return BufferedAsyncIterator(requests, e => this.recoverFromFetch(e))\n    } catch (e) {\n      await this.recoverFromFetch(e)\n    }\n  }\n\n  async recoverFromFetch(e) {\n    if (STALE_METADATA_ERRORS.includes(e.type) || e.name === 'KafkaJSTopicMetadataNotLoaded') {\n      this.logger.debug('Stale cluster metadata, refreshing...', {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        error: e.message,\n      })\n\n      await this.cluster.refreshMetadata()\n      await this.joinAndSync()\n      throw new KafkaJSError(e.message)\n    }\n\n    if (e.name === 'KafkaJSStaleTopicMetadataAssignment') {\n      this.logger.warn(`${e.message}, resync group`, {\n        groupId: this.groupId,\n        memberId: this.memberId,\n        topic: e.topic,\n        unknownPartitions: e.unknownPartitions,\n      })\n\n      await this.joinAndSync()\n    }\n\n    if (e.name === 'KafkaJSOffsetOutOfRange') {\n      await this.recoverFromOffsetOutOfRange(e)\n    }\n\n    if (e.name === 'KafkaJSConnectionClosedError') {\n      this.cluster.removeBroker({ host: e.host, port: e.port })\n    }\n\n    if (e.name === 'KafkaJSBrokerNotFound' || e.name === 'KafkaJSConnectionClosedError') {\n      this.logger.debug(`${e.message}, refreshing metadata and retrying...`)\n      await this.cluster.refreshMetadata()\n    }\n\n    throw e\n  }\n\n  async recoverFromOffsetOutOfRange(e) {\n    // If we are fetching from a follower try with the leader before resetting offsets\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic]\n    if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === 'number') {\n      this.logger.info('Offset out of range while fetching from follower, retrying with leader', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n      delete preferredReadReplicas[e.partition]\n    } else {\n      this.logger.error('Offset out of range, resetting to default offset', {\n        topic: e.topic,\n        partition: e.partition,\n        groupId: this.groupId,\n        memberId: this.memberId,\n      })\n\n      await this.offsetManager.setDefaultOffset({\n        topic: e.topic,\n        partition: e.partition,\n      })\n    }\n  }\n\n  generatePartitionsPerSubscribedTopic() {\n    const map = new Map()\n\n    for (const topic of this.topicsSubscribed) {\n      const partitions = this.cluster\n        .findTopicPartitionMetadata(topic)\n        .map(m => m.partitionId)\n        .sort()\n\n      map.set(topic, partitions)\n    }\n\n    return map\n  }\n\n  checkForStaleAssignment() {\n    if (!this.partitionsPerSubscribedTopic) {\n      return\n    }\n\n    const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic()\n\n    for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {\n      const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic))\n\n      if (diff.length > 0) {\n        throw new KafkaJSStaleTopicMetadataAssignment('Topic has been updated', {\n          topic,\n          unknownPartitions: diff,\n        })\n      }\n    }\n  }\n\n  hasSeekOffset({ topic, partition }) {\n    return this.seekOffset.has(topic, partition)\n  }\n\n  /**\n   * For each of the partitions find the best nodeId to read it from\n   *\n   * @param {string} topic\n   * @param {number[]} partitions\n   * @returns {{[nodeId: number]: number[]}} per-node assignment of partitions\n   * @see Cluster~findLeaderForPartitions\n   */\n  // Invariant: The resulting object has each partition referenced exactly once\n  findReadReplicaForPartitions(topic, partitions) {\n    const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic)\n    const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic]\n    return partitions.reduce((result, id) => {\n      const partitionId = parseInt(id, 10)\n      const metadata = partitionMetadata.find(p => p.partitionId === partitionId)\n      if (!metadata) {\n        return result\n      }\n\n      if (metadata.leader == null) {\n        throw new KafkaJSError('Invalid partition metadata', { topic, partitionId, metadata })\n      }\n\n      // Pick the preferred replica if there is one, and it isn't known to be offline, otherwise the leader.\n      let nodeId = metadata.leader\n      if (preferredReadReplicas) {\n        const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {}\n        if (Date.now() >= expireAt) {\n          this.logger.debug('Preferred read replica information has expired, using leader', {\n            topic,\n            partitionId,\n            groupId: this.groupId,\n            memberId: this.memberId,\n            preferredReadReplica,\n            leader: metadata.leader,\n          })\n          // Drop the entry\n          delete preferredReadReplicas[partitionId]\n        } else if (preferredReadReplica != null) {\n          // Valid entry, check whether it is not offline\n          // Note that we don't delete the preference here, and rather hope that eventually that replica comes online again\n          const offlineReplicas = metadata.offlineReplicas\n          if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {\n            this.logger.debug('Preferred read replica is offline, using leader', {\n              topic,\n              partitionId,\n              groupId: this.groupId,\n              memberId: this.memberId,\n              preferredReadReplica,\n              leader: metadata.leader,\n            })\n          } else {\n            nodeId = preferredReadReplica\n          }\n        }\n      }\n      const current = result[nodeId] || []\n      return { ...result, [nodeId]: [...current, partitionId] }\n    }, {})\n  }\n}\n","const Long = require('../utils/long')\nconst ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0])\n\nconst isAbortMarker = ({ key }) => {\n  // Handle null/undefined keys.\n  if (!key) return false\n  // Cast key to buffer defensively\n  return Buffer.from(key).equals(ABORTED_MESSAGE_KEY)\n}\n\n/**\n * Remove messages marked as aborted according to the aborted transactions list.\n *\n * Start of an aborted transaction is determined by message offset.\n * End of an aborted transaction is determined by control messages.\n * @param {Message[]} messages\n * @param {Transaction[]} [abortedTransactions]\n * @returns {Message[]} Messages which did not participate in an aborted transaction\n *\n * @typedef {object} Message\n * @param {Buffer} key\n * @param {lastOffset} key  Int64\n * @param {RecordBatch}  batchContext\n *\n * @typedef {object} Transaction\n * @param {string} firstOffset  Int64\n * @param {string} producerId  Int64\n *\n * @typedef {object} RecordBatch\n * @param {string}  producerId  Int64\n * @param {boolean}  inTransaction\n */\nmodule.exports = ({ messages, abortedTransactions }) => {\n  const currentAbortedTransactions = new Map()\n\n  if (!abortedTransactions || !abortedTransactions.length) {\n    return messages\n  }\n\n  const remainingAbortedTransactions = [...abortedTransactions]\n\n  return messages.filter(message => {\n    // If the message offset is GTE the first offset of the next aborted transaction\n    // then we have stepped into an aborted transaction.\n    if (\n      remainingAbortedTransactions.length &&\n      Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)\n    ) {\n      const { producerId } = remainingAbortedTransactions.shift()\n      currentAbortedTransactions.set(producerId, true)\n    }\n\n    const { producerId, inTransaction } = message.batchContext\n\n    if (isAbortMarker(message)) {\n      // Transaction is over, we no longer need to ignore messages from this producer\n      currentAbortedTransactions.delete(producerId)\n    } else if (currentAbortedTransactions.has(producerId) && inTransaction) {\n      return false\n    }\n\n    return true\n  })\n}\n","const Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst { initialRetryTime } = require('../retry/defaults')\nconst ConsumerGroup = require('./consumerGroup')\nconst Runner = require('./runner')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst { KafkaJSNonRetriableError } = require('../errors')\nconst { roundRobin } = require('./assigners')\nconst { EARLIEST_OFFSET, LATEST_OFFSET } = require('../constants')\nconst ISOLATION_LEVEL = require('../protocol/isolationLevel')\n\nconst { keys, values } = Object\nconst { CONNECT, DISCONNECT, STOP, CRASH } = events\n\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `consumer.events.${key}`)\n  .join(', ')\n\nconst specialOffsets = [\n  Long.fromValue(EARLIEST_OFFSET).toString(),\n  Long.fromValue(LATEST_OFFSET).toString(),\n]\n\n/**\n * @param {Object} params\n * @param {import(\"../../types\").Cluster} params.cluster\n * @param {String} params.groupId\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').PartitionAssigner[]} [params.partitionAssigners]\n * @param {number} [params.sessionTimeout]\n * @param {number} [params.rebalanceTimeout]\n * @param {number} [params.heartbeatInterval]\n * @param {number} [params.maxBytesPerPartition]\n * @param {number} [params.minBytes]\n * @param {number} [params.maxBytes]\n * @param {number} [params.maxWaitTimeInMs]\n * @param {number} [params.isolationLevel]\n * @param {string} [params.rackId]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n * @param {number} params.metadataMaxAge\n *\n * @returns {import(\"../../types\").Consumer}\n */\nmodule.exports = ({\n  cluster,\n  groupId,\n  retry,\n  logger: rootLogger,\n  partitionAssigners = [roundRobin],\n  sessionTimeout = 30000,\n  rebalanceTimeout = 60000,\n  heartbeatInterval = 3000,\n  maxBytesPerPartition = 1048576, // 1MB\n  minBytes = 1,\n  maxBytes = 10485760, // 10MB\n  maxWaitTimeInMs = 5000,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  rackId = '',\n  instrumentationEmitter: rootInstrumentationEmitter,\n  metadataMaxAge,\n}) => {\n  if (!groupId) {\n    throw new KafkaJSNonRetriableError('Consumer groupId must be a non-empty string.')\n  }\n\n  const logger = rootLogger.namespace('Consumer')\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const assigners = partitionAssigners.map(createAssigner =>\n    createAssigner({ groupId, logger, cluster })\n  )\n\n  const topics = {}\n  let runner = null\n  let consumerGroup = null\n\n  if (heartbeatInterval >= sessionTimeout) {\n    throw new KafkaJSNonRetriableError(\n      `Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`\n    )\n  }\n\n  const createConsumerGroup = ({ autoCommit, autoCommitInterval, autoCommitThreshold }) => {\n    return new ConsumerGroup({\n      logger: rootLogger,\n      topics: keys(topics),\n      topicConfigurations: topics,\n      retry,\n      cluster,\n      groupId,\n      assigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      instrumentationEmitter,\n      autoCommit,\n      autoCommitInterval,\n      autoCommitThreshold,\n      isolationLevel,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  const createRunner = ({\n    eachBatchAutoResolve,\n    eachBatch,\n    eachMessage,\n    onCrash,\n    autoCommit,\n    partitionsConsumedConcurrently,\n  }) => {\n    return new Runner({\n      autoCommit,\n      logger: rootLogger,\n      consumerGroup,\n      instrumentationEmitter,\n      eachBatchAutoResolve,\n      eachBatch,\n      eachMessage,\n      heartbeatInterval,\n      retry,\n      onCrash,\n      partitionsConsumedConcurrently,\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"connect\"]} */\n  const connect = async () => {\n    await cluster.connect()\n    instrumentationEmitter.emit(CONNECT)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"disconnect\"]} */\n  const disconnect = async () => {\n    try {\n      await stop()\n      logger.debug('consumer has stopped, disconnecting', { groupId })\n      await cluster.disconnect()\n      instrumentationEmitter.emit(DISCONNECT)\n    } catch (e) {\n      logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"stop\"]} */\n  const stop = async () => {\n    try {\n      if (runner) {\n        await runner.stop()\n        runner = null\n        consumerGroup = null\n        instrumentationEmitter.emit(STOP)\n      }\n\n      logger.info('Stopped', { groupId })\n    } catch (e) {\n      logger.error(`Caught error when stopping the consumer: ${e.message}`, {\n        stack: e.stack,\n        groupId,\n      })\n\n      throw e\n    }\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"subscribe\"]} */\n  const subscribe = async ({ topic, fromBeginning = false }) => {\n    if (consumerGroup) {\n      throw new KafkaJSNonRetriableError('Cannot subscribe to topic while consumer is running')\n    }\n\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    const isRegExp = topic instanceof RegExp\n    if (typeof topic !== 'string' && !isRegExp) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid topic ${topic} (${typeof topic}), the topic name has to be a String or a RegExp`\n      )\n    }\n\n    const topicsToSubscribe = []\n    if (isRegExp) {\n      const topicRegExp = topic\n      const metadata = await cluster.metadata()\n      const matchedTopics = metadata.topicMetadata\n        .map(({ topic: topicName }) => topicName)\n        .filter(topicName => topicRegExp.test(topicName))\n\n      logger.debug('Subscription based on RegExp', {\n        groupId,\n        topicRegExp: topicRegExp.toString(),\n        matchedTopics,\n      })\n\n      topicsToSubscribe.push(...matchedTopics)\n    } else {\n      topicsToSubscribe.push(topic)\n    }\n\n    for (const t of topicsToSubscribe) {\n      topics[t] = { fromBeginning }\n    }\n\n    await cluster.addMultipleTargetTopics(topicsToSubscribe)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"run\"]} */\n  const run = async ({\n    autoCommit = true,\n    autoCommitInterval = null,\n    autoCommitThreshold = null,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently = 1,\n    eachBatch = null,\n    eachMessage = null,\n  } = {}) => {\n    if (consumerGroup) {\n      logger.warn('consumer#run was called, but the consumer is already running', { groupId })\n      return\n    }\n\n    consumerGroup = createConsumerGroup({\n      autoCommit,\n      autoCommitInterval,\n      autoCommitThreshold,\n    })\n\n    const start = async onCrash => {\n      logger.info('Starting', { groupId })\n      runner = createRunner({\n        autoCommit,\n        eachBatchAutoResolve,\n        eachBatch,\n        eachMessage,\n        onCrash,\n        partitionsConsumedConcurrently,\n      })\n\n      await runner.start()\n    }\n\n    const restart = onCrash => {\n      consumerGroup = createConsumerGroup({\n        autoCommitInterval,\n        autoCommitThreshold,\n      })\n\n      start(onCrash)\n    }\n\n    const onCrash = async e => {\n      logger.error(`Crash: ${e.name}: ${e.message}`, {\n        groupId,\n        retryCount: e.retryCount,\n        stack: e.stack,\n      })\n\n      if (e.name === 'KafkaJSConnectionClosedError') {\n        cluster.removeBroker({ host: e.host, port: e.port })\n      }\n\n      await disconnect()\n\n      const isErrorRetriable = e.name === 'KafkaJSNumberOfRetriesExceeded' || e.retriable === true\n      const shouldRestart =\n        isErrorRetriable &&\n        (!retry ||\n          !retry.restartOnFailure ||\n          (await retry.restartOnFailure(e).catch(error => {\n            logger.error(\n              'Caught error when invoking user-provided \"restartOnFailure\" callback. Defaulting to restarting.',\n              {\n                error: error.message || error,\n                originalError: e.message || e,\n                groupId,\n              }\n            )\n\n            return true\n          })))\n\n      instrumentationEmitter.emit(CRASH, {\n        error: e,\n        groupId,\n        restart: shouldRestart,\n      })\n\n      if (shouldRestart) {\n        const retryTime = e.retryTime || (retry && retry.initialRetryTime) || initialRetryTime\n        logger.error(`Restarting the consumer in ${retryTime}ms`, {\n          retryCount: e.retryCount,\n          retryTime,\n          groupId,\n        })\n\n        setTimeout(() => restart(onCrash), retryTime)\n      }\n    }\n\n    await start(onCrash)\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"commitOffsets\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partition: 0, offset: '1', metadata: 'event-id-3' }]\n   */\n  const commitOffsets = async (topicPartitions = []) => {\n    const commitsByTopic = topicPartitions.reduce(\n      (payload, { topic, partition, offset, metadata = null }) => {\n        if (!topic) {\n          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n        }\n\n        if (isNaN(partition)) {\n          throw new KafkaJSNonRetriableError(\n            `Invalid partition, expected a number received ${partition}`\n          )\n        }\n\n        let commitOffset\n        try {\n          commitOffset = Long.fromValue(offset)\n        } catch (_) {\n          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n        }\n\n        if (commitOffset.lessThan(0)) {\n          throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n        }\n\n        if (metadata !== null && typeof metadata !== 'string') {\n          throw new KafkaJSNonRetriableError(\n            `Invalid offset metadata, expected string or null, received ${metadata}`\n          )\n        }\n\n        const topicCommits = payload[topic] || []\n\n        topicCommits.push({ partition, offset: commitOffset, metadata })\n\n        return { ...payload, [topic]: topicCommits }\n      },\n      {}\n    )\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    const topics = Object.keys(commitsByTopic)\n\n    return runner.commitOffsets({\n      topics: topics.map(topic => {\n        return {\n          topic,\n          partitions: commitsByTopic[topic],\n        }\n      }),\n    })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"seek\"]} */\n  const seek = ({ topic, partition, offset }) => {\n    if (!topic) {\n      throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`)\n    }\n\n    if (isNaN(partition)) {\n      throw new KafkaJSNonRetriableError(\n        `Invalid partition, expected a number received ${partition}`\n      )\n    }\n\n    let seekOffset\n    try {\n      seekOffset = Long.fromValue(offset)\n    } catch (_) {\n      throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`)\n    }\n\n    if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {\n      throw new KafkaJSNonRetriableError('Offset must not be a negative number')\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.seek({ topic, partition, offset: seekOffset.toString() })\n  }\n\n  /** @type {import(\"../../types\").Consumer[\"describeGroup\"]} */\n  const describeGroup = async () => {\n    const coordinator = await cluster.findGroupCoordinator({ groupId })\n    const retrier = createRetry(retry)\n    return retrier(async () => {\n      const { groups } = await coordinator.describeGroups({ groupIds: [groupId] })\n      return groups.find(group => group.groupId === groupId)\n    })\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"pause\"]}\n   * @param topicPartitions\n   *   Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const pause = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.pause(topicPartitions)\n  }\n\n  /**\n   * Returns the list of topic partitions paused on this consumer\n   *\n   * @type {import(\"../../types\").Consumer[\"paused\"]}\n   */\n  const paused = () => {\n    if (!consumerGroup) {\n      return []\n    }\n\n    return consumerGroup.paused()\n  }\n\n  /**\n   * @type {import(\"../../types\").Consumer[\"resume\"]}\n   * @param topicPartitions\n   *  Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  const resume = (topicPartitions = []) => {\n    for (const topicPartition of topicPartitions) {\n      if (!topicPartition || !topicPartition.topic) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid topic ${(topicPartition && topicPartition.topic) || topicPartition}`\n        )\n      } else if (\n        typeof topicPartition.partitions !== 'undefined' &&\n        (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))\n      ) {\n        throw new KafkaJSNonRetriableError(\n          `Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`\n        )\n      }\n    }\n\n    if (!consumerGroup) {\n      throw new KafkaJSNonRetriableError(\n        'Consumer group was not initialized, consumer#run must be called first'\n      )\n    }\n\n    consumerGroup.resume(topicPartitions)\n  }\n\n  /**\n   * @return {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    connect,\n    disconnect,\n    subscribe,\n    stop,\n    run,\n    commitOffsets,\n    seek,\n    describeGroup,\n    pause,\n    paused,\n    resume,\n    on,\n    events,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst networkEvents = require('../network/instrumentationEvents')\nconst consumerType = InstrumentationEventType('consumer')\n\nconst events = {\n  HEARTBEAT: consumerType('heartbeat'),\n  COMMIT_OFFSETS: consumerType('commit_offsets'),\n  GROUP_JOIN: consumerType('group_join'),\n  FETCH: consumerType('fetch'),\n  FETCH_START: consumerType('fetch_start'),\n  START_BATCH_PROCESS: consumerType('start_batch_process'),\n  END_BATCH_PROCESS: consumerType('end_batch_process'),\n  CONNECT: consumerType('connect'),\n  DISCONNECT: consumerType('disconnect'),\n  STOP: consumerType('stop'),\n  CRASH: consumerType('crash'),\n  REBALANCING: consumerType('rebalancing'),\n  RECEIVED_UNSUBSCRIBED_TOPICS: consumerType('received_unsubscribed_topics'),\n  REQUEST: consumerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const Long = require('../../utils/long')\nconst flatten = require('../../utils/flatten')\nconst isInvalidOffset = require('./isInvalidOffset')\nconst initializeConsumerOffsets = require('./initializeConsumerOffsets')\nconst {\n  events: { COMMIT_OFFSETS },\n} = require('../instrumentationEvents')\n\nconst { keys, assign } = Object\nconst indexTopics = topics => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {})\n\nconst PRIVATE = {\n  COMMITTED_OFFSETS: Symbol('private:OffsetManager:committedOffsets'),\n}\nmodule.exports = class OffsetManager {\n  /**\n   * @param {Object} options\n   * @param {import(\"../../../types\").Cluster} options.cluster\n   * @param {import(\"../../../types\").Broker} options.coordinator\n   * @param {import(\"../../../types\").IMemberAssignment} options.memberAssignment\n   * @param {boolean} options.autoCommit\n   * @param {number | null} options.autoCommitInterval\n   * @param {number | null} options.autoCommitThreshold\n   * @param {{[topic: string]: { fromBeginning: boolean }}} options.topicConfigurations\n   * @param {import(\"../../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {string} options.groupId\n   * @param {number} options.generationId\n   * @param {string} options.memberId\n   */\n  constructor({\n    cluster,\n    coordinator,\n    memberAssignment,\n    autoCommit,\n    autoCommitInterval,\n    autoCommitThreshold,\n    topicConfigurations,\n    instrumentationEmitter,\n    groupId,\n    generationId,\n    memberId,\n  }) {\n    this.cluster = cluster\n    this.coordinator = coordinator\n\n    // memberAssignment format:\n    // {\n    //   'topic1': [0, 1, 2, 3],\n    //   'topic2': [0, 1, 2, 3, 4, 5],\n    // }\n    this.memberAssignment = memberAssignment\n\n    this.topicConfigurations = topicConfigurations\n    this.instrumentationEmitter = instrumentationEmitter\n    this.groupId = groupId\n    this.generationId = generationId\n    this.memberId = memberId\n\n    this.autoCommit = autoCommit\n    this.autoCommitInterval = autoCommitInterval\n    this.autoCommitThreshold = autoCommitThreshold\n    this.lastCommit = Date.now()\n\n    this.topics = keys(memberAssignment)\n    this.clearAllOffsets()\n  }\n\n  /**\n   * @param {string} topic\n   * @param {number} partition\n   * @returns {Long}\n   */\n  nextOffset(topic, partition) {\n    if (!this.resolvedOffsets[topic][partition]) {\n      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n    }\n\n    let offset = this.resolvedOffsets[topic][partition]\n    if (isInvalidOffset(offset)) {\n      offset = '0'\n    }\n\n    return Long.fromValue(offset)\n  }\n\n  /**\n   * @returns {Promise<import(\"../../../types\").Broker>}\n   */\n  async getCoordinator() {\n    if (!this.coordinator.isConnected()) {\n      this.coordinator = await this.cluster.findBroker(this.coordinator)\n    }\n\n    return this.coordinator\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  resetOffset({ topic, partition }) {\n    this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition]\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  resolveOffset({ topic, partition, offset }) {\n    this.resolvedOffsets[topic][partition] = Long.fromValue(offset)\n      .add(1)\n      .toString()\n  }\n\n  /**\n   * @returns {Long}\n   */\n  countResolvedOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    const subtractOffsets = (resolvedOffset, committedOffset) => {\n      const resolvedOffsetLong = Long.fromValue(resolvedOffset)\n      return isInvalidOffset(committedOffset)\n        ? resolvedOffsetLong\n        : resolvedOffsetLong.subtract(Long.fromValue(committedOffset))\n    }\n\n    const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) =>\n      keys(resolvedTopicOffsets).map(partition =>\n        subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition])\n      )\n\n    const subtractTopicOffsets = topic =>\n      subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic])\n\n    const offsetsDiff = this.topics.map(subtractTopicOffsets)\n    return flatten(offsetsDiff).reduce((sum, offset) => sum.add(offset), Long.fromValue(0))\n  }\n\n  /**\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  async setDefaultOffset({ topic, partition }) {\n    const { groupId, generationId, memberId } = this\n    const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic])\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset: defaultOffset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  /**\n   * Commit the given offset to the topic/partition. If the consumer isn't assigned to the given\n   * topic/partition this method will be a NO-OP.\n   *\n   * @param {import(\"../../../types\").TopicPartitionOffset} topicPartitionOffset\n   */\n  async seek({ topic, partition, offset }) {\n    if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {\n      return\n    }\n\n    if (!this.autoCommit) {\n      this.resolveOffset({\n        topic,\n        partition,\n        offset: Long.fromValue(offset)\n          .subtract(1)\n          .toString(),\n      })\n      return\n    }\n\n    const { groupId, generationId, memberId } = this\n    const coordinator = await this.getCoordinator()\n\n    await coordinator.offsetCommit({\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics: [\n        {\n          topic,\n          partitions: [{ partition, offset }],\n        },\n      ],\n    })\n\n    this.clearOffsets({ topic, partition })\n  }\n\n  async commitOffsetsIfNecessary() {\n    const now = Date.now()\n\n    const timeoutReached =\n      this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval\n\n    const thresholdReached =\n      this.autoCommitThreshold != null &&\n      this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold))\n\n    if (timeoutReached || thresholdReached) {\n      return this.commitOffsets()\n    }\n  }\n\n  /**\n   * Return all locally resolved offsets which are not marked as committed, by topic-partition.\n   * @returns {OffsetsByTopicPartition}\n   *\n   * @typedef {Object} OffsetsByTopicPartition\n   * @property {TopicOffsets[]} topics\n   *\n   * @typedef {Object} TopicOffsets\n   * @property {PartitionOffset[]} partitions\n   *\n   * @typedef {Object} PartitionOffset\n   * @property {string} partition\n   * @property {string} offset\n   */\n  uncommittedOffsets() {\n    const offsets = topic => keys(this.resolvedOffsets[topic])\n    const emptyPartitions = ({ partitions }) => partitions.length > 0\n    const toPartitions = topic => partition => ({\n      partition,\n      offset: this.resolvedOffsets[topic][partition],\n    })\n    const changedOffsets = topic => ({ partition, offset }) => {\n      return (\n        offset !== this.committedOffsets()[topic][partition] &&\n        Long.fromValue(offset).greaterThanOrEqual(0)\n      )\n    }\n\n    // Select and format updated partitions\n    const topicsWithPartitionsToCommit = this.topics\n      .map(topic => ({\n        topic,\n        partitions: offsets(topic)\n          .map(toPartitions(topic))\n          .filter(changedOffsets(topic)),\n      }))\n      .filter(emptyPartitions)\n\n    return { topics: topicsWithPartitionsToCommit }\n  }\n\n  async commitOffsets(offsets = {}) {\n    const { groupId, generationId, memberId } = this\n    const { topics = this.uncommittedOffsets().topics } = offsets\n\n    if (topics.length === 0) {\n      this.lastCommit = Date.now()\n      return\n    }\n\n    const payload = {\n      groupId,\n      memberId,\n      groupGenerationId: generationId,\n      topics,\n    }\n\n    try {\n      const coordinator = await this.getCoordinator()\n      await coordinator.offsetCommit(payload)\n      this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload)\n\n      // Update local reference of committed offsets\n      topics.forEach(({ topic, partitions }) => {\n        const updatedOffsets = partitions.reduce(\n          (obj, { partition, offset }) => assign(obj, { [partition]: offset }),\n          {}\n        )\n\n        this[PRIVATE.COMMITTED_OFFSETS][topic] = assign(\n          {},\n          this.committedOffsets()[topic],\n          updatedOffsets\n        )\n      })\n\n      this.lastCommit = Date.now()\n    } catch (e) {\n      // metadata is stale, the coordinator has changed due to a restart or\n      // broker reassignment\n      if (e.type === 'NOT_COORDINATOR_FOR_GROUP') {\n        await this.cluster.refreshMetadata()\n      }\n\n      throw e\n    }\n  }\n\n  async resolveOffsets() {\n    const { groupId } = this\n    const invalidOffset = topic => partition => {\n      return isInvalidOffset(this.committedOffsets()[topic][partition])\n    }\n\n    const pendingPartitions = this.topics\n      .map(topic => ({\n        topic,\n        partitions: this.memberAssignment[topic]\n          .filter(invalidOffset(topic))\n          .map(partition => ({ partition })),\n      }))\n      .filter(t => t.partitions.length > 0)\n\n    if (pendingPartitions.length === 0) {\n      return\n    }\n\n    const coordinator = await this.getCoordinator()\n    const { responses: consumerOffsets } = await coordinator.offsetFetch({\n      groupId,\n      topics: pendingPartitions,\n    })\n\n    const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) =>\n      assign(\n        {\n          topic,\n          partitions: partitions\n            .filter(({ offset }) => isInvalidOffset(offset))\n            .map(({ partition }) => assign({ partition })),\n        },\n        this.topicConfigurations[topic]\n      )\n    )\n\n    const indexPartitions = (obj, { partition, offset }) => {\n      return assign(obj, { [partition]: offset })\n    }\n\n    const hasUnresolvedPartitions = () => unresolvedPartitions.some(t => t.partitions.length > 0)\n\n    let offsets = consumerOffsets\n    if (hasUnresolvedPartitions()) {\n      const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions)\n      offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets)\n    }\n\n    offsets.forEach(({ topic, partitions }) => {\n      this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {\n        ...this.committedOffsets()[topic],\n      })\n    })\n  }\n\n  /**\n   * @private\n   * @param {import(\"../../../types\").TopicPartition} topicPartition\n   */\n  clearOffsets({ topic, partition }) {\n    delete this.committedOffsets()[topic][partition]\n    delete this.resolvedOffsets[topic][partition]\n  }\n\n  /**\n   * @private\n   */\n  clearAllOffsets() {\n    const committedOffsets = this.committedOffsets()\n\n    for (const topic in committedOffsets) {\n      delete committedOffsets[topic]\n    }\n\n    for (const topic of this.topics) {\n      committedOffsets[topic] = {}\n    }\n\n    this.resolvedOffsets = indexTopics(this.topics)\n  }\n\n  committedOffsets() {\n    if (!this[PRIVATE.COMMITTED_OFFSETS]) {\n      this[PRIVATE.COMMITTED_OFFSETS] = this.groupId\n        ? this.cluster.committedOffsets({ groupId: this.groupId })\n        : {}\n    }\n\n    return this[PRIVATE.COMMITTED_OFFSETS]\n  }\n}\n","const isInvalidOffset = require('./isInvalidOffset')\nconst { keys, assign } = Object\n\nconst indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset })\nconst indexTopics = (obj, { topic, partitions }) =>\n  assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) })\n\nmodule.exports = (consumerOffsets, topicOffsets) => {\n  const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {})\n  const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {})\n\n  return keys(indexedConsumerOffsets).map(topic => {\n    const partitions = indexedConsumerOffsets[topic]\n    return {\n      topic,\n      partitions: keys(partitions).map(partition => {\n        const offset = partitions[partition]\n        const resolvedOffset = isInvalidOffset(offset)\n          ? indexedTopicOffsets[topic][partition]\n          : offset\n\n        return { partition: Number(partition), offset: resolvedOffset }\n      }),\n    }\n  })\n}\n","const Long = require('../../utils/long')\n\nmodule.exports = offset => (!offset && offset !== 0) || Long.fromValue(offset).isNegative()\n","const { EventEmitter } = require('events')\nconst Long = require('../utils/long')\nconst createRetry = require('../retry')\nconst limitConcurrency = require('../utils/concurrency')\nconst { KafkaJSError } = require('../errors')\nconst barrier = require('./barrier')\n\nconst {\n  events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING },\n} = require('./instrumentationEvents')\n\nconst isRebalancing = e =>\n  e.type === 'REBALANCE_IN_PROGRESS' || e.type === 'NOT_COORDINATOR_FOR_GROUP'\n\nconst isKafkaJSError = e => e instanceof KafkaJSError\nconst isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB))\nconst CONSUMING_START = 'consuming-start'\nconst CONSUMING_STOP = 'consuming-stop'\n\nmodule.exports = class Runner extends EventEmitter {\n  /**\n   * @param {object} options\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"./consumerGroup\")} options.consumerGroup\n   * @param {import(\"../instrumentation/emitter\")} options.instrumentationEmitter\n   * @param {boolean} [options.eachBatchAutoResolve=true]\n   * @param {number} [options.partitionsConsumedConcurrently]\n   * @param {(payload: import(\"../../types\").EachBatchPayload) => Promise<void>} options.eachBatch\n   * @param {(payload: import(\"../../types\").EachMessagePayload) => Promise<void>} options.eachMessage\n   * @param {number} [options.heartbeatInterval]\n   * @param {(reason: Error) => void} options.onCrash\n   * @param {import(\"../../types\").RetryOptions} [options.retry]\n   * @param {boolean} [options.autoCommit=true]\n   */\n  constructor({\n    logger,\n    consumerGroup,\n    instrumentationEmitter,\n    eachBatchAutoResolve = true,\n    partitionsConsumedConcurrently,\n    eachBatch,\n    eachMessage,\n    heartbeatInterval,\n    onCrash,\n    retry,\n    autoCommit = true,\n  }) {\n    super()\n    this.logger = logger.namespace('Runner')\n    this.consumerGroup = consumerGroup\n    this.instrumentationEmitter = instrumentationEmitter\n    this.eachBatchAutoResolve = eachBatchAutoResolve\n    this.eachBatch = eachBatch\n    this.eachMessage = eachMessage\n    this.heartbeatInterval = heartbeatInterval\n    this.retrier = createRetry(Object.assign({}, retry))\n    this.onCrash = onCrash\n    this.autoCommit = autoCommit\n    this.partitionsConsumedConcurrently = partitionsConsumedConcurrently\n\n    this.running = false\n    this.consuming = false\n  }\n\n  get consuming() {\n    return this._consuming\n  }\n\n  set consuming(value) {\n    if (this._consuming !== value) {\n      this._consuming = value\n      this.emit(value ? CONSUMING_START : CONSUMING_STOP)\n    }\n  }\n\n  async join() {\n    await this.consumerGroup.joinAndSync()\n    this.running = true\n  }\n\n  async scheduleJoin() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n      return\n    }\n\n    return this.join().catch(this.onCrash)\n  }\n\n  async start() {\n    if (this.running) {\n      return\n    }\n\n    try {\n      await this.consumerGroup.connect()\n      await this.join()\n\n      this.running = true\n      this.scheduleFetch()\n    } catch (e) {\n      this.onCrash(e)\n    }\n  }\n\n  async stop() {\n    if (!this.running) {\n      return\n    }\n\n    this.logger.debug('stop consumer group', {\n      groupId: this.consumerGroup.groupId,\n      memberId: this.consumerGroup.memberId,\n    })\n\n    this.running = false\n\n    try {\n      await this.waitForConsumer()\n      await this.consumerGroup.leave()\n    } catch (e) {}\n  }\n\n  waitForConsumer() {\n    return new Promise(resolve => {\n      if (!this.consuming) {\n        return resolve()\n      }\n\n      this.logger.debug('waiting for consumer to finish...', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      this.once(CONSUMING_STOP, () => resolve())\n    })\n  }\n\n  async processEachMessage(batch) {\n    const { topic, partition } = batch\n\n    for (const message of batch.messages) {\n      if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {\n        break\n      }\n\n      try {\n        await this.eachMessage({\n          topic,\n          partition,\n          message,\n          heartbeat: async () => {\n            await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n          },\n        })\n      } catch (e) {\n        if (!isKafkaJSError(e)) {\n          this.logger.error(`Error when calling eachMessage`, {\n            topic,\n            partition,\n            offset: message.offset,\n            stack: e.stack,\n            error: e,\n          })\n        }\n\n        // In case of errors, commit the previously consumed offsets unless autoCommit is disabled\n        await this.autoCommitOffsets()\n        throw e\n      }\n\n      this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset })\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n      await this.autoCommitOffsetsIfNecessary()\n    }\n  }\n\n  async processEachBatch(batch) {\n    const { topic, partition } = batch\n    const lastFilteredMessage = batch.messages[batch.messages.length - 1]\n\n    try {\n      await this.eachBatch({\n        batch,\n        resolveOffset: offset => {\n          /**\n           * The transactional producer generates a control record after committing the transaction.\n           * The control record is the last record on the RecordBatch, and it is filtered before it\n           * reaches the eachBatch callback. When disabling auto-resolve, the user-land code won't\n           * be able to resolve the control record offset, since it never reaches the callback,\n           * causing stuck consumers as the consumer will never move the offset marker.\n           *\n           * When the last offset of the batch is resolved, we should automatically resolve\n           * the control record offset as this entry doesn't have any meaning to the user-land code,\n           * and won't interfere with the stream processing.\n           *\n           * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n           */\n          const offsetToResolve =\n            lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset)\n              ? batch.lastOffset()\n              : offset\n\n          this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve })\n        },\n        heartbeat: async () => {\n          await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n        },\n        /**\n         * Commit offsets if provided. Otherwise commit most recent resolved offsets\n         * if the autoCommit conditions are met.\n         *\n         * @param {OffsetsByTopicPartition} [offsets] Optional.\n         */\n        commitOffsetsIfNecessary: async offsets => {\n          return offsets\n            ? this.consumerGroup.commitOffsets(offsets)\n            : this.consumerGroup.commitOffsetsIfNecessary()\n        },\n        uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),\n        isRunning: () => this.running,\n        isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition }),\n      })\n    } catch (e) {\n      if (!isKafkaJSError(e)) {\n        this.logger.error(`Error when calling eachBatch`, {\n          topic,\n          partition,\n          offset: batch.firstOffset(),\n          stack: e.stack,\n          error: e,\n        })\n      }\n\n      // eachBatch has a special resolveOffset which can be used\n      // to keep track of the messages\n      await this.autoCommitOffsets()\n      throw e\n    }\n\n    // resolveOffset for the last offset can be disabled to allow the users of eachBatch to\n    // stop their consumers without resolving unprocessed offsets (issues/18)\n    if (this.eachBatchAutoResolve) {\n      this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() })\n    }\n  }\n\n  async fetch() {\n    const startFetch = Date.now()\n\n    this.instrumentationEmitter.emit(FETCH_START, {})\n\n    const iterator = await this.consumerGroup.fetch()\n\n    this.instrumentationEmitter.emit(FETCH, {\n      /**\n       * PR #570 removed support for the number of batches in this instrumentation event;\n       * The new implementation uses an async generation to deliver the batches, which makes\n       * this number impossible to get. The number is set to 0 to keep the event backward\n       * compatible until we bump KafkaJS to version 2, following the end of node 8 LTS.\n       *\n       * @since 2019-11-29\n       */\n      numberOfBatches: 0,\n      duration: Date.now() - startFetch,\n    })\n\n    const onBatch = async batch => {\n      const startBatchProcess = Date.now()\n      const payload = {\n        topic: batch.topic,\n        partition: batch.partition,\n        highWatermark: batch.highWatermark,\n        offsetLag: batch.offsetLag(),\n        /**\n         * @since 2019-06-24 (>= 1.8.0)\n         *\n         * offsetLag returns the lag based on the latest offset in the batch, to\n         * keep the event backward compatible we just introduced \"offsetLagLow\"\n         * which calculates the lag based on the first offset in the batch\n         */\n        offsetLagLow: batch.offsetLagLow(),\n        batchSize: batch.messages.length,\n        firstOffset: batch.firstOffset(),\n        lastOffset: batch.lastOffset(),\n      }\n\n      /**\n       * If the batch contained only control records or only aborted messages then we still\n       * need to resolve and auto-commit to ensure the consumer can move forward.\n       *\n       * We also need to emit batch instrumentation events to allow any listeners keeping\n       * track of offsets to know about the latest point of consumption.\n       *\n       * Added in #1256\n       *\n       * @see https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L1499-L1505\n       */\n      if (batch.isEmptyDueToFiltering()) {\n        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n        this.consumerGroup.resolveOffset({\n          topic: batch.topic,\n          partition: batch.partition,\n          offset: batch.lastOffset(),\n        })\n        await this.autoCommitOffsetsIfNecessary()\n\n        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n          ...payload,\n          duration: Date.now() - startBatchProcess,\n        })\n        return\n      }\n\n      if (batch.isEmpty()) {\n        return\n      }\n\n      this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload)\n\n      if (this.eachMessage) {\n        await this.processEachMessage(batch)\n      } else if (this.eachBatch) {\n        await this.processEachBatch(batch)\n      }\n\n      this.instrumentationEmitter.emit(END_BATCH_PROCESS, {\n        ...payload,\n        duration: Date.now() - startBatchProcess,\n      })\n\n      await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n    }\n\n    const { lock, unlock, unlockWithError } = barrier()\n    const concurrently = limitConcurrency({ limit: this.partitionsConsumedConcurrently })\n\n    let requestsCompleted = false\n    let numberOfExecutions = 0\n    let expectedNumberOfExecutions = 0\n    const enqueuedTasks = []\n\n    while (true) {\n      const result = iterator.next()\n\n      if (result.done) {\n        break\n      }\n\n      if (!this.running) {\n        result.value.catch(error => {\n          this.logger.debug('Ignoring error in fetch request while stopping runner', {\n            error: error.message || error,\n            stack: error.stack,\n          })\n        })\n\n        continue\n      }\n\n      enqueuedTasks.push(async () => {\n        const batches = await result.value\n        expectedNumberOfExecutions += batches.length\n\n        batches.map(batch =>\n          concurrently(async () => {\n            try {\n              if (!this.running) {\n                return\n              }\n\n              await onBatch(batch)\n            } catch (e) {\n              unlockWithError(e)\n            } finally {\n              numberOfExecutions++\n              if (requestsCompleted && numberOfExecutions === expectedNumberOfExecutions) {\n                unlock()\n              }\n            }\n          }).catch(unlockWithError)\n        )\n      })\n    }\n\n    await Promise.all(enqueuedTasks.map(fn => fn()))\n    requestsCompleted = true\n\n    if (expectedNumberOfExecutions === numberOfExecutions) {\n      unlock()\n    }\n\n    const error = await lock\n    if (error) {\n      throw error\n    }\n\n    await this.autoCommitOffsets()\n    await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval })\n  }\n\n  async scheduleFetch() {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n      })\n\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        this.consuming = true\n        await this.fetch()\n        this.consuming = false\n\n        if (this.running) {\n          setImmediate(() => this.scheduleFetch())\n        }\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          await this.join()\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSOffsetOutOfRange') {\n          setImmediate(() => this.scheduleFetch())\n          return\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while fetching data, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n        })\n\n        throw e\n      } finally {\n        this.consuming = false\n      }\n    }).catch(this.onCrash)\n  }\n\n  autoCommitOffsets() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsets()\n    }\n  }\n\n  autoCommitOffsetsIfNecessary() {\n    if (this.autoCommit) {\n      return this.consumerGroup.commitOffsetsIfNecessary()\n    }\n  }\n\n  commitOffsets(offsets) {\n    if (!this.running) {\n      this.logger.debug('consumer not running, exiting', {\n        groupId: this.consumerGroup.groupId,\n        memberId: this.consumerGroup.memberId,\n        offsets,\n      })\n      return\n    }\n\n    return this.retrier(async (bail, retryCount, retryTime) => {\n      try {\n        await this.consumerGroup.commitOffsets(offsets)\n      } catch (e) {\n        if (!this.running) {\n          this.logger.debug('consumer not running, exiting', {\n            error: e.message,\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            offsets,\n          })\n          return\n        }\n\n        if (isRebalancing(e)) {\n          this.logger.warn('The group is rebalancing, re-joining', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.instrumentationEmitter.emit(REBALANCING, {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n          })\n\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError(e))\n        }\n\n        if (e.type === 'UNKNOWN_MEMBER_ID') {\n          this.logger.error('The coordinator is not aware of this member, re-joining the group', {\n            groupId: this.consumerGroup.groupId,\n            memberId: this.consumerGroup.memberId,\n            error: e.message,\n            retryCount,\n            retryTime,\n          })\n\n          this.consumerGroup.memberId = null\n          setImmediate(() => this.scheduleJoin())\n\n          bail(new KafkaJSError(e))\n        }\n\n        if (e.name === 'KafkaJSNotImplemented') {\n          return bail(e)\n        }\n\n        this.logger.debug('Error while committing offsets, trying again...', {\n          groupId: this.consumerGroup.groupId,\n          memberId: this.consumerGroup.memberId,\n          error: e.message,\n          stack: e.stack,\n          retryCount,\n          retryTime,\n          offsets,\n        })\n\n        throw e\n      }\n    })\n  }\n}\n","module.exports = class SeekOffsets extends Map {\n  set(topic, partition, offset) {\n    super.set([topic, partition], offset)\n  }\n\n  has(topic, partition) {\n    return Array.from(this.keys()).some(([t, p]) => t === topic && p === partition)\n  }\n\n  pop() {\n    if (this.size === 0) {\n      return\n    }\n\n    const [key, offset] = this.entries().next().value\n    this.delete(key)\n    const [topic, partition] = key\n    return { topic, partition, offset }\n  }\n}\n","const createState = topic => ({\n  topic,\n  paused: new Set(),\n  pauseAll: false,\n  resumed: new Set(),\n})\n\nmodule.exports = class SubscriptionState {\n  constructor() {\n    this.assignedPartitionsByTopic = {}\n    this.subscriptionStatesByTopic = {}\n  }\n\n  /**\n   * Replace the current assignment with a new set of assignments\n   *\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assign(topicPartitions = []) {\n    this.assignedPartitionsByTopic = topicPartitions.reduce(\n      (assigned, { topic, partitions = [] }) => {\n        return { ...assigned, [topic]: { topic, partitions } }\n      },\n      {}\n    )\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  pause(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = true\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.add(partition)\n          state.resumed.delete(partition)\n        })\n        state.pauseAll = false\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @param {Array<TopicPartitions>} topicPartitions Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  resume(topicPartitions = []) {\n    topicPartitions.forEach(({ topic, partitions }) => {\n      const state = this.subscriptionStatesByTopic[topic] || createState(topic)\n\n      if (typeof partitions === 'undefined') {\n        state.paused.clear()\n        state.resumed.clear()\n        state.pauseAll = false\n      } else if (Array.isArray(partitions)) {\n        partitions.forEach(partition => {\n          state.paused.delete(partition)\n\n          if (state.pauseAll) {\n            state.resumed.add(partition)\n          }\n        })\n      }\n\n      this.subscriptionStatesByTopic[topic] = state\n    })\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  assigned() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  active() {\n    return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({\n      topic,\n      partitions: partitions.filter(partition => !this.isPaused(topic, partition)).sort(),\n    }))\n  }\n\n  /**\n   * @returns {Array<import(\"../../types\").TopicPartitions>} topicPartitions\n   * Example: [{ topic: 'topic-name', partitions: [1, 2] }]\n   */\n  paused() {\n    return Object.values(this.assignedPartitionsByTopic)\n      .map(({ topic, partitions }) => ({\n        topic,\n        partitions: partitions.filter(partition => this.isPaused(topic, partition)).sort(),\n      }))\n      .filter(({ partitions }) => partitions.length !== 0)\n  }\n\n  isPaused(topic, partition) {\n    const state = this.subscriptionStatesByTopic[topic]\n\n    if (!state) {\n      return false\n    }\n\n    const partitionResumed = state.resumed.has(partition)\n    const partitionPaused = state.paused.has(partition)\n\n    return (state.pauseAll && !partitionResumed) || partitionPaused\n  }\n}\n","module.exports = () => ({\n  KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,\n  KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS,\n})\n","const pkgJson = require('../package.json')\nconst { bugs } = pkgJson\n\nclass KafkaJSError extends Error {\n  constructor(e, { retriable = true } = {}) {\n    super(e)\n    Error.captureStackTrace(this, this.constructor)\n    this.message = e.message || e\n    this.name = 'KafkaJSError'\n    this.retriable = retriable\n    this.helpUrl = e.helpUrl\n  }\n}\n\nclass KafkaJSNonRetriableError extends KafkaJSError {\n  constructor(e) {\n    super(e, { retriable: false })\n    this.name = 'KafkaJSNonRetriableError'\n    this.originalError = e\n  }\n}\n\nclass KafkaJSProtocolError extends KafkaJSError {\n  constructor(e, { retriable = e.retriable } = {}) {\n    super(e, { retriable })\n    this.type = e.type\n    this.code = e.code\n    this.name = 'KafkaJSProtocolError'\n  }\n}\n\nclass KafkaJSOffsetOutOfRange extends KafkaJSProtocolError {\n  constructor(e, { topic, partition }) {\n    super(e)\n    this.topic = topic\n    this.partition = partition\n    this.name = 'KafkaJSOffsetOutOfRange'\n  }\n}\n\nclass KafkaJSMemberIdRequired extends KafkaJSProtocolError {\n  constructor(e, { memberId }) {\n    super(e)\n    this.memberId = memberId\n    this.name = 'KafkaJSMemberIdRequired'\n  }\n}\n\nclass KafkaJSNumberOfRetriesExceeded extends KafkaJSNonRetriableError {\n  constructor(e, { retryCount, retryTime }) {\n    super(e)\n    this.stack = `${this.name}\\n  Caused by: ${e.stack}`\n    this.originalError = e\n    this.retryCount = retryCount\n    this.retryTime = retryTime\n    this.name = 'KafkaJSNumberOfRetriesExceeded'\n  }\n}\n\nclass KafkaJSConnectionError extends KafkaJSError {\n  constructor(e, { broker, code } = {}) {\n    super(e)\n    this.broker = broker\n    this.code = code\n    this.name = 'KafkaJSConnectionError'\n  }\n}\n\nclass KafkaJSConnectionClosedError extends KafkaJSConnectionError {\n  constructor(e, { host, port } = {}) {\n    super(e, { broker: `${host}:${port}` })\n    this.host = host\n    this.port = port\n    this.name = 'KafkaJSConnectionClosedError'\n  }\n}\n\nclass KafkaJSRequestTimeoutError extends KafkaJSError {\n  constructor(e, { broker, correlationId, createdAt, sentAt, pendingDuration } = {}) {\n    super(e)\n    this.broker = broker\n    this.correlationId = correlationId\n    this.createdAt = createdAt\n    this.sentAt = sentAt\n    this.pendingDuration = pendingDuration\n    this.name = 'KafkaJSRequestTimeoutError'\n  }\n}\n\nclass KafkaJSMetadataNotLoaded extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSMetadataNotLoaded'\n  }\n}\nclass KafkaJSTopicMetadataNotLoaded extends KafkaJSMetadataNotLoaded {\n  constructor(e, { topic } = {}) {\n    super(e)\n    this.topic = topic\n    this.name = 'KafkaJSTopicMetadataNotLoaded'\n  }\n}\nclass KafkaJSStaleTopicMetadataAssignment extends KafkaJSError {\n  constructor(e, { topic, unknownPartitions } = {}) {\n    super(e)\n    this.topic = topic\n    this.unknownPartitions = unknownPartitions\n    this.name = 'KafkaJSStaleTopicMetadataAssignment'\n  }\n}\n\nclass KafkaJSDeleteGroupsError extends KafkaJSError {\n  constructor(e, groups = []) {\n    super(e)\n    this.groups = groups\n    this.name = 'KafkaJSDeleteGroupsError'\n  }\n}\n\nclass KafkaJSServerDoesNotSupportApiKey extends KafkaJSNonRetriableError {\n  constructor(e, { apiKey, apiName } = {}) {\n    super(e)\n    this.apiKey = apiKey\n    this.apiName = apiName\n    this.name = 'KafkaJSServerDoesNotSupportApiKey'\n  }\n}\n\nclass KafkaJSBrokerNotFound extends KafkaJSError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSBrokerNotFound'\n  }\n}\n\nclass KafkaJSPartialMessageError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSPartialMessageError'\n  }\n}\n\nclass KafkaJSSASLAuthenticationError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSSASLAuthenticationError'\n  }\n}\n\nclass KafkaJSGroupCoordinatorNotFound extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSGroupCoordinatorNotFound'\n  }\n}\n\nclass KafkaJSNotImplemented extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNotImplemented'\n  }\n}\n\nclass KafkaJSTimeout extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSTimeout'\n  }\n}\n\nclass KafkaJSLockTimeout extends KafkaJSTimeout {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSLockTimeout'\n  }\n}\n\nclass KafkaJSUnsupportedMagicByteInMessageSet extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSUnsupportedMagicByteInMessageSet'\n  }\n}\n\nclass KafkaJSDeleteTopicRecordsError extends KafkaJSError {\n  constructor({ partitions }) {\n    /*\n     * This error is retriable if all the errors were retriable\n     */\n    const retriable = partitions\n      .filter(({ error }) => error != null)\n      .every(({ error }) => error.retriable === true)\n\n    super('Error while deleting records', { retriable })\n    this.name = 'KafkaJSDeleteTopicRecordsError'\n    this.partitions = partitions\n  }\n}\n\nconst issueUrl = bugs ? bugs.url : null\n\nclass KafkaJSInvariantViolation extends KafkaJSNonRetriableError {\n  constructor(e) {\n    const message = e.message || e\n    super(`Invariant violated: ${message}. This is likely a bug and should be reported.`)\n    this.name = 'KafkaJSInvariantViolation'\n\n    if (issueUrl !== null) {\n      const issueTitle = encodeURIComponent(`Invariant violation: ${message}`)\n      this.helpUrl = `${issueUrl}/new?assignees=&labels=bug&template=bug_report.md&title=${issueTitle}`\n    }\n  }\n}\n\nclass KafkaJSInvalidVarIntError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNonRetriableError'\n  }\n}\n\nclass KafkaJSInvalidLongError extends KafkaJSNonRetriableError {\n  constructor() {\n    super(...arguments)\n    this.name = 'KafkaJSNonRetriableError'\n  }\n}\n\nclass KafkaJSCreateTopicError extends KafkaJSProtocolError {\n  constructor(e, topicName) {\n    super(e)\n    this.topic = topicName\n    this.name = 'KafkaJSCreateTopicError'\n  }\n}\nclass KafkaJSAggregateError extends Error {\n  constructor(message, errors) {\n    super(message)\n    this.errors = errors\n    this.name = 'KafkaJSAggregateError'\n  }\n}\n\nmodule.exports = {\n  KafkaJSError,\n  KafkaJSNonRetriableError,\n  KafkaJSPartialMessageError,\n  KafkaJSBrokerNotFound,\n  KafkaJSProtocolError,\n  KafkaJSConnectionError,\n  KafkaJSConnectionClosedError,\n  KafkaJSRequestTimeoutError,\n  KafkaJSSASLAuthenticationError,\n  KafkaJSNumberOfRetriesExceeded,\n  KafkaJSOffsetOutOfRange,\n  KafkaJSMemberIdRequired,\n  KafkaJSGroupCoordinatorNotFound,\n  KafkaJSNotImplemented,\n  KafkaJSMetadataNotLoaded,\n  KafkaJSTopicMetadataNotLoaded,\n  KafkaJSStaleTopicMetadataAssignment,\n  KafkaJSDeleteGroupsError,\n  KafkaJSTimeout,\n  KafkaJSLockTimeout,\n  KafkaJSServerDoesNotSupportApiKey,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n  KafkaJSDeleteTopicRecordsError,\n  KafkaJSInvariantViolation,\n  KafkaJSInvalidVarIntError,\n  KafkaJSInvalidLongError,\n  KafkaJSCreateTopicError,\n  KafkaJSAggregateError,\n}\n","const {\n  createLogger,\n  LEVELS: { INFO },\n} = require('./loggers')\n\nconst InstrumentationEventEmitter = require('./instrumentation/emitter')\nconst LoggerConsole = require('./loggers/console')\nconst Cluster = require('./cluster')\nconst createProducer = require('./producer')\nconst createConsumer = require('./consumer')\nconst createAdmin = require('./admin')\nconst ISOLATION_LEVEL = require('./protocol/isolationLevel')\nconst defaultSocketFactory = require('./network/socketFactory')\n\nconst PRIVATE = {\n  CREATE_CLUSTER: Symbol('private:Kafka:createCluster'),\n  CLUSTER_RETRY: Symbol('private:Kafka:clusterRetry'),\n  LOGGER: Symbol('private:Kafka:logger'),\n  OFFSETS: Symbol('private:Kafka:offsets'),\n}\n\nconst DEFAULT_METADATA_MAX_AGE = 300000\n\nmodule.exports = class Client {\n  /**\n   * @param {Object} options\n   * @param {Array<string>} options.brokers example: ['127.0.0.1:9092', '127.0.0.1:9094']\n   * @param {Object} options.ssl\n   * @param {Object} options.sasl\n   * @param {string} options.clientId\n   * @param {number} options.connectionTimeout - in milliseconds\n   * @param {number} options.authenticationTimeout - in milliseconds\n   * @param {number} options.reauthenticationThreshold - in milliseconds\n   * @param {number} [options.requestTimeout=30000] - in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {import(\"../types\").RetryOptions} [options.retry]\n   * @param {import(\"../types\").ISocketFactory} [options.socketFactory]\n   */\n  constructor({\n    brokers,\n    ssl,\n    sasl,\n    clientId,\n    connectionTimeout,\n    authenticationTimeout,\n    reauthenticationThreshold,\n    requestTimeout,\n    enforceRequestTimeout = false,\n    retry,\n    socketFactory = defaultSocketFactory(),\n    logLevel = INFO,\n    logCreator = LoggerConsole,\n  }) {\n    this[PRIVATE.OFFSETS] = new Map()\n    this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator })\n    this[PRIVATE.CLUSTER_RETRY] = retry\n    this[PRIVATE.CREATE_CLUSTER] = ({\n      metadataMaxAge,\n      allowAutoTopicCreation = true,\n      maxInFlightRequests = null,\n      instrumentationEmitter = null,\n      isolationLevel,\n    }) =>\n      new Cluster({\n        logger: this[PRIVATE.LOGGER],\n        retry: this[PRIVATE.CLUSTER_RETRY],\n        offsets: this[PRIVATE.OFFSETS],\n        socketFactory,\n        brokers,\n        ssl,\n        sasl,\n        clientId,\n        connectionTimeout,\n        authenticationTimeout,\n        reauthenticationThreshold,\n        requestTimeout,\n        enforceRequestTimeout,\n        metadataMaxAge,\n        instrumentationEmitter,\n        allowAutoTopicCreation,\n        maxInFlightRequests,\n        isolationLevel,\n      })\n  }\n\n  /**\n   * @public\n   */\n  producer({\n    createPartitioner,\n    retry,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    allowAutoTopicCreation,\n    idempotent,\n    transactionalId,\n    transactionTimeout,\n    maxInFlightRequests,\n  } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      instrumentationEmitter,\n    })\n\n    return createProducer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      createPartitioner,\n      idempotent,\n      transactionalId,\n      transactionTimeout,\n      instrumentationEmitter,\n    })\n  }\n\n  /**\n   * @public\n   */\n  consumer({\n    groupId,\n    partitionAssigners,\n    metadataMaxAge = DEFAULT_METADATA_MAX_AGE,\n    sessionTimeout,\n    rebalanceTimeout,\n    heartbeatInterval,\n    maxBytesPerPartition,\n    minBytes,\n    maxBytes,\n    maxWaitTimeInMs,\n    retry = { retries: 5 },\n    allowAutoTopicCreation,\n    maxInFlightRequests,\n    readUncommitted = false,\n    rackId = '',\n  } = {}) {\n    const isolationLevel = readUncommitted\n      ? ISOLATION_LEVEL.READ_UNCOMMITTED\n      : ISOLATION_LEVEL.READ_COMMITTED\n\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      metadataMaxAge,\n      allowAutoTopicCreation,\n      maxInFlightRequests,\n      isolationLevel,\n      instrumentationEmitter,\n    })\n\n    return createConsumer({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      cluster,\n      groupId,\n      partitionAssigners,\n      sessionTimeout,\n      rebalanceTimeout,\n      heartbeatInterval,\n      maxBytesPerPartition,\n      minBytes,\n      maxBytes,\n      maxWaitTimeInMs,\n      isolationLevel,\n      instrumentationEmitter,\n      rackId,\n      metadataMaxAge,\n    })\n  }\n\n  /**\n   * @public\n   */\n  admin({ retry } = {}) {\n    const instrumentationEmitter = new InstrumentationEventEmitter()\n    const cluster = this[PRIVATE.CREATE_CLUSTER]({\n      allowAutoTopicCreation: false,\n      instrumentationEmitter,\n    })\n\n    return createAdmin({\n      retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },\n      logger: this[PRIVATE.LOGGER],\n      instrumentationEmitter,\n      cluster,\n    })\n  }\n\n  /**\n   * @public\n   */\n  logger() {\n    return this[PRIVATE.LOGGER]\n  }\n}\n","const { EventEmitter } = require('events')\nconst InstrumentationEvent = require('./event')\nconst { KafkaJSError } = require('../errors')\n\nmodule.exports = class InstrumentationEventEmitter {\n  constructor() {\n    this.emitter = new EventEmitter()\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {Object} payload\n   */\n  emit(eventName, payload) {\n    if (!eventName) {\n      throw new KafkaJSError('Invalid event name', { retriable: false })\n    }\n\n    if (this.emitter.listenerCount(eventName) > 0) {\n      const event = new InstrumentationEvent(eventName, payload)\n      this.emitter.emit(eventName, event)\n    }\n  }\n\n  /**\n   * @param {string} eventName\n   * @param {(...args: any[]) => void} listener\n   * @returns {import(\"../../types\").RemoveInstrumentationEventListener<string>} removeListener\n   */\n  addListener(eventName, listener) {\n    this.emitter.addListener(eventName, listener)\n    return () => this.emitter.removeListener(eventName, listener)\n  }\n}\n","let id = 0\nconst nextId = () => {\n  if (id === Number.MAX_VALUE) {\n    id = 0\n  }\n\n  return id++\n}\n\nclass InstrumentationEvent {\n  /**\n   * @param {String} type\n   * @param {Object} payload\n   */\n  constructor(type, payload) {\n    this.id = nextId()\n    this.type = type\n    this.timestamp = Date.now()\n    this.payload = payload\n  }\n}\n\nmodule.exports = InstrumentationEvent\n","module.exports = namespace => type => `${namespace}.${type}`\n","const { LEVELS: logLevel } = require('./index')\n\nmodule.exports = () => ({ namespace, level, label, log }) => {\n  const prefix = namespace ? `[${namespace}] ` : ''\n  const message = JSON.stringify(\n    Object.assign({ level: label }, log, {\n      message: `${prefix}${log.message}`,\n    })\n  )\n\n  switch (level) {\n    case logLevel.INFO:\n      return console.info(message)\n    case logLevel.ERROR:\n      return console.error(message)\n    case logLevel.WARN:\n      return console.warn(message)\n    case logLevel.DEBUG:\n      return console.log(message)\n  }\n}\n","const { assign } = Object\n\nconst LEVELS = {\n  NOTHING: 0,\n  ERROR: 1,\n  WARN: 2,\n  INFO: 4,\n  DEBUG: 5,\n}\n\nconst createLevel = (label, level, currentLevel, namespace, logFunction) => (\n  message,\n  extra = {}\n) => {\n  if (level > currentLevel()) return\n  logFunction({\n    namespace,\n    level,\n    label,\n    log: assign(\n      {\n        timestamp: new Date().toISOString(),\n        logger: 'kafkajs',\n        message,\n      },\n      extra\n    ),\n  })\n}\n\nconst evaluateLogLevel = logLevel => {\n  const envLogLevel = (process.env.KAFKAJS_LOG_LEVEL || '').toUpperCase()\n  return LEVELS[envLogLevel] == null ? logLevel : LEVELS[envLogLevel]\n}\n\nconst createLogger = ({ level = LEVELS.INFO, logCreator } = {}) => {\n  let logLevel = evaluateLogLevel(level)\n  const logFunction = logCreator(logLevel)\n\n  const createNamespace = (namespace, logLevel = null) => {\n    const namespaceLogLevel = evaluateLogLevel(logLevel)\n    return createLogFunctions(namespace, namespaceLogLevel)\n  }\n\n  const createLogFunctions = (namespace, namespaceLogLevel = null) => {\n    const currentLogLevel = () => (namespaceLogLevel == null ? logLevel : namespaceLogLevel)\n    const logger = {\n      info: createLevel('INFO', LEVELS.INFO, currentLogLevel, namespace, logFunction),\n      error: createLevel('ERROR', LEVELS.ERROR, currentLogLevel, namespace, logFunction),\n      warn: createLevel('WARN', LEVELS.WARN, currentLogLevel, namespace, logFunction),\n      debug: createLevel('DEBUG', LEVELS.DEBUG, currentLogLevel, namespace, logFunction),\n    }\n\n    return assign(logger, {\n      namespace: createNamespace,\n      setLogLevel: newLevel => {\n        logLevel = newLevel\n      },\n    })\n  }\n\n  return createLogFunctions()\n}\n\nmodule.exports = {\n  LEVELS,\n  createLogger,\n}\n","const createSocket = require('./socket')\nconst createRequest = require('../protocol/request')\nconst Decoder = require('../protocol/decoder')\nconst { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require('../errors')\nconst { INT_32_MAX_VALUE } = require('../constants')\nconst getEnv = require('../env')\nconst RequestQueue = require('./requestQueue')\nconst { CONNECTION_STATUS, CONNECTED_STATUS } = require('./connectionStatus')\n\nconst requestInfo = ({ apiName, apiKey, apiVersion }) =>\n  `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n\nmodule.exports = class Connection {\n  /**\n   * @param {Object} options\n   * @param {string} options.host\n   * @param {number} options.port\n   * @param {import(\"../../types\").Logger} options.logger\n   * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n   * @param {string} [options.clientId='kafkajs']\n   * @param {number} options.requestTimeout The maximum amount of time the client will wait for the response of a request,\n   *                                in milliseconds\n   * @param {string} [options.rack=null]\n   * @param {Object} [options.ssl=null] Options for the TLS Secure Context. It accepts all options,\n   *                            usually \"cert\", \"key\" and \"ca\". More information at\n   *                            https://nodejs.org/api/tls.html#tls_tls_createsecurecontext_options\n   * @param {Object} [options.sasl=null] Attributes used for SASL authentication. Options based on the\n   *                             key \"mechanism\". Connection is not actively using the SASL attributes\n   *                             but acting as a data object for this information\n   * @param {number} [options.connectionTimeout=1000] The connection timeout, in milliseconds\n   * @param {boolean} [options.enforceRequestTimeout]\n   * @param {number} [options.maxInFlightRequests=null] The maximum number of unacknowledged requests on a connection before\n   *                                            enqueuing\n   * @param {import(\"../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    host,\n    port,\n    logger,\n    socketFactory,\n    requestTimeout,\n    rack = null,\n    ssl = null,\n    sasl = null,\n    clientId = 'kafkajs',\n    connectionTimeout = 1000,\n    enforceRequestTimeout = false,\n    maxInFlightRequests = null,\n    instrumentationEmitter = null,\n  }) {\n    this.host = host\n    this.port = port\n    this.rack = rack\n    this.clientId = clientId\n    this.broker = `${this.host}:${this.port}`\n    this.logger = logger.namespace('Connection')\n\n    this.socketFactory = socketFactory\n    this.ssl = ssl\n    this.sasl = sasl\n\n    this.requestTimeout = requestTimeout\n    this.connectionTimeout = connectionTimeout\n\n    this.bytesBuffered = 0\n    this.bytesNeeded = Decoder.int32Size()\n    this.chunks = []\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.correlationId = 0\n    this.requestQueue = new RequestQueue({\n      instrumentationEmitter,\n      maxInFlightRequests,\n      requestTimeout,\n      enforceRequestTimeout,\n      clientId,\n      broker: this.broker,\n      logger: logger.namespace('RequestQueue'),\n      isConnected: () => this.connected,\n    })\n\n    this.authHandlers = null\n    this.authExpectResponse = false\n\n    const log = level => (message, extra = {}) => {\n      const logFn = this.logger[level]\n      logFn(message, { broker: this.broker, clientId, ...extra })\n    }\n\n    this.logDebug = log('debug')\n    this.logError = log('error')\n\n    const env = getEnv()\n    this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === '1'\n    this.shouldLogFetchBuffer =\n      this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === '1'\n  }\n\n  get connected() {\n    return CONNECTED_STATUS.includes(this.connectionStatus)\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  connect() {\n    return new Promise((resolve, reject) => {\n      if (this.connected) {\n        return resolve(true)\n      }\n\n      let timeoutId\n\n      const onConnect = () => {\n        clearTimeout(timeoutId)\n        this.connectionStatus = CONNECTION_STATUS.CONNECTED\n        this.requestQueue.scheduleRequestTimeoutCheck()\n        resolve(true)\n      }\n\n      const onData = data => {\n        this.processData(data)\n      }\n\n      const onEnd = async () => {\n        clearTimeout(timeoutId)\n\n        const wasConnected = this.connected\n\n        if (this.authHandlers) {\n          this.authHandlers.onError()\n        } else if (wasConnected) {\n          this.logDebug('Kafka server has closed connection')\n          this.rejectRequests(\n            new KafkaJSConnectionClosedError('Closed connection', {\n              host: this.host,\n              port: this.port,\n            })\n          )\n        }\n\n        await this.disconnect()\n      }\n\n      const onError = async e => {\n        clearTimeout(timeoutId)\n\n        const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {\n          broker: `${this.host}:${this.port}`,\n          code: e.code,\n        })\n\n        this.logError(error.message, { stack: e.stack })\n        this.rejectRequests(error)\n        await this.disconnect()\n\n        reject(error)\n      }\n\n      const onTimeout = async () => {\n        const error = new KafkaJSConnectionError('Connection timeout', {\n          broker: `${this.host}:${this.port}`,\n        })\n\n        this.logError(error.message)\n        this.rejectRequests(error)\n        await this.disconnect()\n        reject(error)\n      }\n\n      this.logDebug(`Connecting`, {\n        ssl: !!this.ssl,\n        sasl: !!this.sasl,\n      })\n\n      try {\n        timeoutId = setTimeout(onTimeout, this.connectionTimeout)\n        this.socket = createSocket({\n          socketFactory: this.socketFactory,\n          host: this.host,\n          port: this.port,\n          ssl: this.ssl,\n          onConnect,\n          onData,\n          onEnd,\n          onError,\n          onTimeout,\n        })\n      } catch (e) {\n        clearTimeout(timeoutId)\n        reject(\n          new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {\n            broker: `${this.host}:${this.port}`,\n          })\n        )\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  async disconnect() {\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTING\n    this.logDebug('disconnecting...')\n\n    await this.requestQueue.waitForPendingRequests()\n    this.requestQueue.destroy()\n\n    if (this.socket) {\n      this.socket.end()\n      this.socket.unref()\n    }\n\n    this.connectionStatus = CONNECTION_STATUS.DISCONNECTED\n    this.logDebug('disconnected')\n    return true\n  }\n\n  /**\n   * @public\n   * @returns {Promise}\n   */\n  authenticate({ authExpectResponse = false, request, response }) {\n    this.authExpectResponse = authExpectResponse\n\n    /**\n     * TODO: rewrite removing the async promise executor\n     */\n\n    /* eslint-disable no-async-promise-executor */\n    return new Promise(async (resolve, reject) => {\n      this.authHandlers = {\n        onSuccess: rawData => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          response\n            .decode(rawData)\n            .then(data => response.parse(data))\n            .then(resolve)\n            .catch(reject)\n        },\n        onError: () => {\n          this.authHandlers = null\n          this.authExpectResponse = false\n\n          reject(\n            new KafkaJSConnectionError('Connection closed by the server', {\n              broker: `${this.host}:${this.port}`,\n            })\n          )\n        },\n      }\n\n      try {\n        const requestPayload = await request.encode()\n\n        this.failIfNotConnected()\n        this.socket.write(requestPayload.buffer, 'binary')\n      } catch (e) {\n        reject(e)\n      }\n    })\n  }\n\n  /**\n   * @public\n   * @param {object} protocol\n   * @param {object} protocol.request It is defined by the protocol and consists of an object with \"apiKey\",\n   *                         \"apiVersion\", \"apiName\" and an \"encode\" function. The encode function\n   *                         must return an instance of Encoder\n   *\n   * @param {object} protocol.response It is defined by the protocol and consists of an object with two functions:\n   *                          \"decode\" and \"parse\"\n   *\n   * @param {number} [protocol.requestTimeout=null] Override for the default requestTimeout\n   * @param {boolean} [protocol.logResponseError=true] Whether to log errors\n   * @returns {Promise<data>} where data is the return of \"response#parse\"\n   */\n  async send({ request, response, requestTimeout = null, logResponseError = true }) {\n    this.failIfNotConnected()\n\n    const expectResponse = !request.expectResponse || request.expectResponse()\n    const sendRequest = async () => {\n      const { clientId } = this\n      const correlationId = this.nextCorrelationId()\n\n      const requestPayload = await createRequest({ request, correlationId, clientId })\n      const { apiKey, apiName, apiVersion } = request\n      this.logDebug(`Request ${requestInfo(request)}`, {\n        correlationId,\n        expectResponse,\n        size: Buffer.byteLength(requestPayload.buffer),\n      })\n\n      return new Promise((resolve, reject) => {\n        try {\n          this.failIfNotConnected()\n          const entry = { apiKey, apiName, apiVersion, correlationId, resolve, reject }\n\n          this.requestQueue.push({\n            entry,\n            expectResponse,\n            requestTimeout,\n            sendRequest: () => {\n              this.socket.write(requestPayload.buffer, 'binary')\n            },\n          })\n        } catch (e) {\n          reject(e)\n        }\n      })\n    }\n\n    const { correlationId, size, entry, payload } = await sendRequest()\n\n    if (!expectResponse) {\n      return\n    }\n\n    try {\n      const payloadDecoded = await response.decode(payload)\n\n      /**\n       * @see KIP-219\n       * If the response indicates that the client-side needs to throttle, do that.\n       */\n      this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime)\n\n      const data = await response.parse(payloadDecoded)\n      const isFetchApi = entry.apiName === 'Fetch'\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        correlationId,\n        size,\n        data: isFetchApi && !this.shouldLogFetchBuffer ? '[filtered]' : data,\n      })\n\n      return data\n    } catch (e) {\n      if (logResponseError) {\n        this.logError(`Response ${requestInfo(entry)}`, {\n          error: e.message,\n          correlationId,\n          size,\n        })\n      }\n\n      const isBuffer = Buffer.isBuffer(payload)\n      this.logDebug(`Response ${requestInfo(entry)}`, {\n        error: e.message,\n        correlationId,\n        payload:\n          isBuffer && !this.shouldLogBuffers ? { type: 'Buffer', data: '[filtered]' } : payload,\n      })\n\n      throw e\n    }\n  }\n\n  /**\n   * @private\n   */\n  failIfNotConnected() {\n    if (!this.connected) {\n      throw new KafkaJSConnectionError('Not connected', {\n        broker: `${this.host}:${this.port}`,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  nextCorrelationId() {\n    if (this.correlationId >= INT_32_MAX_VALUE) {\n      this.correlationId = 0\n    }\n\n    return this.correlationId++\n  }\n\n  /**\n   * @private\n   */\n  processData(rawData) {\n    if (this.authHandlers && !this.authExpectResponse) {\n      return this.authHandlers.onSuccess(rawData)\n    }\n\n    // Accumulate the new chunk\n    this.chunks.push(rawData)\n    this.bytesBuffered += Buffer.byteLength(rawData)\n\n    // Process data if there are enough bytes to read the expected response size,\n    // otherwise keep buffering\n    while (this.bytesNeeded <= this.bytesBuffered) {\n      const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0]\n      const decoder = new Decoder(buffer)\n      const expectedResponseSize = decoder.readInt32()\n\n      // Return early if not enough bytes to read the full response\n      if (!decoder.canReadBytes(expectedResponseSize)) {\n        this.chunks = [buffer]\n        this.bytesBuffered = Buffer.byteLength(buffer)\n        this.bytesNeeded = Decoder.int32Size() + expectedResponseSize\n        return\n      }\n\n      const response = new Decoder(decoder.readBytes(expectedResponseSize))\n\n      // Reset the buffered chunks as the rest of the bytes\n      const remainderBuffer = decoder.readAll()\n      this.chunks = [remainderBuffer]\n      this.bytesBuffered = Buffer.byteLength(remainderBuffer)\n      this.bytesNeeded = Decoder.int32Size()\n\n      if (this.authHandlers) {\n        const rawResponseSize = Decoder.int32Size() + expectedResponseSize\n        const rawResponseBuffer = buffer.slice(0, rawResponseSize)\n        return this.authHandlers.onSuccess(rawResponseBuffer)\n      }\n\n      const correlationId = response.readInt32()\n      const payload = response.readAll()\n\n      this.requestQueue.fulfillRequest({\n        size: expectedResponseSize,\n        correlationId,\n        payload,\n      })\n    }\n  }\n\n  /**\n   * @private\n   */\n  rejectRequests(error) {\n    this.requestQueue.rejectAll(error)\n  }\n}\n","const CONNECTION_STATUS = {\n  CONNECTED: 'connected',\n  DISCONNECTING: 'disconnecting',\n  DISCONNECTED: 'disconnected',\n}\n\nconst CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING]\n\nmodule.exports = {\n  CONNECTION_STATUS,\n  CONNECTED_STATUS,\n}\n","const InstrumentationEventType = require('../instrumentation/eventType')\nconst eventType = InstrumentationEventType('network')\n\nmodule.exports = {\n  NETWORK_REQUEST: eventType('request'),\n  NETWORK_REQUEST_TIMEOUT: eventType('request_timeout'),\n  NETWORK_REQUEST_QUEUE_SIZE: eventType('request_queue_size'),\n}\n","const { EventEmitter } = require('events')\nconst SocketRequest = require('./socketRequest')\nconst events = require('../instrumentationEvents')\nconst { KafkaJSInvariantViolation } = require('../../errors')\n\nconst PRIVATE = {\n  EMIT_QUEUE_SIZE_EVENT: Symbol('private:RequestQueue:emitQueueSizeEvent'),\n  EMIT_REQUEST_QUEUE_EMPTY: Symbol('private:RequestQueue:emitQueueEmpty'),\n}\n\nconst REQUEST_QUEUE_EMPTY = 'requestQueueEmpty'\n\nmodule.exports = class RequestQueue extends EventEmitter {\n  /**\n   * @param {Object} options\n   * @param {number} options.maxInFlightRequests\n   * @param {number} options.requestTimeout\n   * @param {boolean} options.enforceRequestTimeout\n   * @param {string} options.clientId\n   * @param {string} options.broker\n   * @param {import(\"../../../types\").Logger} options.logger\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   * @param {() => boolean} [options.isConnected]\n   */\n  constructor({\n    instrumentationEmitter = null,\n    maxInFlightRequests,\n    requestTimeout,\n    enforceRequestTimeout,\n    clientId,\n    broker,\n    logger,\n    isConnected = () => true,\n  }) {\n    super()\n    this.instrumentationEmitter = instrumentationEmitter\n    this.maxInFlightRequests = maxInFlightRequests\n    this.requestTimeout = requestTimeout\n    this.enforceRequestTimeout = enforceRequestTimeout\n    this.clientId = clientId\n    this.broker = broker\n    this.logger = logger\n    this.isConnected = isConnected\n\n    this.inflight = new Map()\n    this.pending = []\n\n    /**\n     * Until when this request queue is throttled and shouldn't send requests\n     *\n     * The value represents the timestamp of the end of the throttling in ms-since-epoch. If the value\n     * is smaller than the current timestamp no throttling is active.\n     *\n     * @type {number}\n     */\n    this.throttledUntil = -1\n\n    /**\n     * Timeout id if we have scheduled a check for pending requests due to client-side throttling\n     *\n     * @type {null|NodeJS.Timeout}\n     */\n    this.throttleCheckTimeoutId = null\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        this.emit(REQUEST_QUEUE_EMPTY)\n      }\n    }\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {\n      instrumentationEmitter &&\n        instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {\n          broker: this.broker,\n          clientId: this.clientId,\n          queueSize: this.pending.length,\n        })\n\n      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n    }\n  }\n\n  /**\n   * @public\n   */\n  scheduleRequestTimeoutCheck() {\n    if (this.enforceRequestTimeout) {\n      this.destroy()\n\n      this.requestTimeoutIntervalId = setInterval(() => {\n        this.inflight.forEach(request => {\n          if (Date.now() - request.sentAt > request.requestTimeout) {\n            request.timeoutRequest()\n          }\n        })\n\n        if (!this.isConnected()) {\n          this.destroy()\n        }\n      }, Math.min(this.requestTimeout, 100))\n    }\n  }\n\n  maybeThrottle(clientSideThrottleTime) {\n    if (clientSideThrottleTime) {\n      const minimumThrottledUntil = Date.now() + clientSideThrottleTime\n      this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil)\n    }\n  }\n\n  /**\n   * @typedef {Object} PushedRequest\n   * @property {import(\"./socketRequest\").RequestEntry} entry\n   * @property {boolean} expectResponse\n   * @property {Function} sendRequest\n   * @property {number} [requestTimeout]\n   *\n   * @public\n   * @param {PushedRequest} pushedRequest\n   */\n  push(pushedRequest) {\n    const { correlationId } = pushedRequest.entry\n    const defaultRequestTimeout = this.requestTimeout\n    const customRequestTimeout = pushedRequest.requestTimeout\n\n    // Some protocol requests have custom request timeouts (e.g JoinGroup, Fetch, etc). The custom\n    // timeouts are influenced by user configurations, which can be lower than the default requestTimeout\n    const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0)\n\n    const socketRequest = new SocketRequest({\n      entry: pushedRequest.entry,\n      expectResponse: pushedRequest.expectResponse,\n      broker: this.broker,\n      clientId: this.clientId,\n      instrumentationEmitter: this.instrumentationEmitter,\n      requestTimeout,\n      send: () => {\n        if (this.inflight.has(correlationId)) {\n          throw new KafkaJSInvariantViolation('Correlation id already exists')\n        }\n        this.inflight.set(correlationId, socketRequest)\n        pushedRequest.sendRequest()\n      },\n      timeout: () => {\n        this.inflight.delete(correlationId)\n        this.checkPendingRequests()\n        // Try to emit REQUEST_QUEUE_EMPTY. Otherwise, waitForPendingRequests may stuck forever\n        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n      },\n    })\n\n    if (this.canSendSocketRequestImmediately()) {\n      this.sendSocketRequest(socketRequest)\n      return\n    }\n\n    this.pending.push(socketRequest)\n    this.scheduleCheckPendingRequests()\n\n    this.logger.debug(`Request enqueued`, {\n      clientId: this.clientId,\n      broker: this.broker,\n      correlationId,\n    })\n\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @param {SocketRequest} socketRequest\n   */\n  sendSocketRequest(socketRequest) {\n    socketRequest.send()\n\n    if (!socketRequest.expectResponse) {\n      this.logger.debug(`Request does not expect a response, resolving immediately`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: socketRequest.correlationId,\n      })\n\n      this.inflight.delete(socketRequest.correlationId)\n      socketRequest.completed({ size: 0, payload: null })\n    }\n  }\n\n  /**\n   * @public\n   * @param {object} response\n   * @param {number} response.correlationId\n   * @param {Buffer} response.payload\n   * @param {number} response.size\n   */\n  fulfillRequest({ correlationId, payload, size }) {\n    const socketRequest = this.inflight.get(correlationId)\n    this.inflight.delete(correlationId)\n    this.checkPendingRequests()\n\n    if (socketRequest) {\n      socketRequest.completed({ size, payload })\n    } else {\n      this.logger.warn(`Response without match`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId,\n      })\n    }\n\n    this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]()\n  }\n\n  /**\n   * @public\n   * @param {Error} error\n   */\n  rejectAll(error) {\n    const requests = [...this.inflight.values(), ...this.pending]\n\n    for (const socketRequest of requests) {\n      socketRequest.rejected(error)\n      this.inflight.delete(socketRequest.correlationId)\n    }\n\n    this.pending = []\n    this.inflight.clear()\n    this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n  }\n\n  /**\n   * @public\n   */\n  waitForPendingRequests() {\n    return new Promise(resolve => {\n      if (this.pending.length === 0 && this.inflight.size === 0) {\n        return resolve()\n      }\n\n      this.logger.debug('Waiting for pending requests', {\n        clientId: this.clientId,\n        broker: this.broker,\n        currentInflightRequests: this.inflight.size,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this.once(REQUEST_QUEUE_EMPTY, () => resolve())\n    })\n  }\n\n  /**\n   * @public\n   */\n  destroy() {\n    clearInterval(this.requestTimeoutIntervalId)\n    clearTimeout(this.throttleCheckTimeoutId)\n    this.throttleCheckTimeoutId = null\n  }\n\n  canSendSocketRequestImmediately() {\n    const shouldEnqueue =\n      (this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests) ||\n      this.throttledUntil > Date.now()\n\n    return !shouldEnqueue\n  }\n\n  /**\n   * Check and process pending requests either now or in the future\n   *\n   * This function will send out as many pending requests as possible taking throttling and\n   * in-flight limits into account.\n   */\n  checkPendingRequests() {\n    while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {\n      const pendingRequest = this.pending.shift() // first in first out\n      this.sendSocketRequest(pendingRequest)\n\n      this.logger.debug(`Consumed pending request`, {\n        clientId: this.clientId,\n        broker: this.broker,\n        correlationId: pendingRequest.correlationId,\n        pendingDuration: pendingRequest.pendingDuration,\n        currentPendingQueueSize: this.pending.length,\n      })\n\n      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]()\n    }\n\n    this.scheduleCheckPendingRequests()\n  }\n\n  /**\n   * Ensure that pending requests will be checked in the future\n   *\n   * If there is a client-side throttling in place this will ensure that we will check\n   * the pending request queue eventually.\n   */\n  scheduleCheckPendingRequests() {\n    // If we're throttled: Schedule checkPendingRequests when the throttle\n    // should be resolved. If there is already something scheduled we assume that that\n    // will be fine, and potentially fix up a new timeout if needed at that time.\n    // Note that if we're merely \"overloaded\" by having too many inflight requests\n    // we will anyways check the queue when one of them gets fulfilled.\n    const timeUntilUnthrottled = this.throttledUntil - Date.now()\n    if (timeUntilUnthrottled > 0 && !this.throttleCheckTimeoutId) {\n      this.throttleCheckTimeoutId = setTimeout(() => {\n        this.throttleCheckTimeoutId = null\n        this.checkPendingRequests()\n      }, timeUntilUnthrottled)\n    }\n  }\n}\n","const { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require('../../errors')\nconst events = require('../instrumentationEvents')\n\nconst PRIVATE = {\n  STATE: Symbol('private:SocketRequest:state'),\n  EMIT_EVENT: Symbol('private:SocketRequest:emitEvent'),\n}\n\nconst REQUEST_STATE = {\n  PENDING: Symbol('PENDING'),\n  SENT: Symbol('SENT'),\n  COMPLETED: Symbol('COMPLETED'),\n  REJECTED: Symbol('REJECTED'),\n}\n\n/**\n * SocketRequest abstracts the life cycle of a socket request, making it easier to track\n * request durations and to have individual timeouts per request.\n *\n * @typedef {Object} SocketRequest\n * @property {number} createdAt\n * @property {number} sentAt\n * @property {number} pendingDuration\n * @property {number} duration\n * @property {number} requestTimeout\n * @property {string} broker\n * @property {string} clientId\n * @property {RequestEntry} entry\n * @property {boolean} expectResponse\n * @property {Function} send\n * @property {Function} timeout\n *\n * @typedef {Object} RequestEntry\n * @property {string} apiKey\n * @property {string} apiName\n * @property {number} apiVersion\n * @property {number} correlationId\n * @property {Function} resolve\n * @property {Function} reject\n */\nmodule.exports = class SocketRequest {\n  /**\n   * @param {Object} options\n   * @param {number} options.requestTimeout\n   * @param {string} options.broker - e.g: 127.0.0.1:9092\n   * @param {string} options.clientId\n   * @param {RequestEntry} options.entry\n   * @param {boolean} options.expectResponse\n   * @param {Function} options.send\n   * @param {() => void} options.timeout\n   * @param {import(\"../../instrumentation/emitter\")} [options.instrumentationEmitter=null]\n   */\n  constructor({\n    requestTimeout,\n    broker,\n    clientId,\n    entry,\n    expectResponse,\n    send,\n    timeout,\n    instrumentationEmitter = null,\n  }) {\n    this.createdAt = Date.now()\n    this.requestTimeout = requestTimeout\n    this.broker = broker\n    this.clientId = clientId\n    this.entry = entry\n    this.correlationId = entry.correlationId\n    this.expectResponse = expectResponse\n    this.sendRequest = send\n    this.timeoutHandler = timeout\n\n    this.sentAt = null\n    this.duration = null\n    this.pendingDuration = null\n\n    this[PRIVATE.STATE] = REQUEST_STATE.PENDING\n    this[PRIVATE.EMIT_EVENT] = (eventName, payload) =>\n      instrumentationEmitter && instrumentationEmitter.emit(eventName, payload)\n  }\n\n  send() {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING],\n      next: REQUEST_STATE.SENT,\n    })\n\n    this.sendRequest()\n    this.sentAt = Date.now()\n    this.pendingDuration = this.sentAt - this.createdAt\n    this[PRIVATE.STATE] = REQUEST_STATE.SENT\n  }\n\n  timeoutRequest() {\n    const { apiName, apiKey, apiVersion } = this.entry\n    const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`\n    const eventData = {\n      broker: this.broker,\n      clientId: this.clientId,\n      correlationId: this.correlationId,\n      createdAt: this.createdAt,\n      sentAt: this.sentAt,\n      pendingDuration: this.pendingDuration,\n    }\n\n    this.timeoutHandler()\n    this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData))\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {\n      ...eventData,\n      apiName,\n      apiKey,\n      apiVersion,\n    })\n  }\n\n  completed({ size, payload }) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.SENT],\n      next: REQUEST_STATE.COMPLETED,\n    })\n\n    const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this\n\n    this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED\n    this.duration = Date.now() - this.sentAt\n    entry.resolve({ correlationId, entry, size, payload })\n\n    this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {\n      broker,\n      clientId,\n      correlationId,\n      size,\n      createdAt,\n      sentAt,\n      pendingDuration,\n      duration: this.duration,\n      apiName: entry.apiName,\n      apiKey: entry.apiKey,\n      apiVersion: entry.apiVersion,\n    })\n  }\n\n  rejected(error) {\n    this.throwIfInvalidState({\n      accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],\n      next: REQUEST_STATE.REJECTED,\n    })\n\n    this[PRIVATE.STATE] = REQUEST_STATE.REJECTED\n    this.duration = Date.now() - this.sentAt\n    this.entry.reject(error)\n  }\n\n  /**\n   * @private\n   */\n  throwIfInvalidState({ accepted, next }) {\n    if (accepted.includes(this[PRIVATE.STATE])) {\n      return\n    }\n\n    const current = this[PRIVATE.STATE].toString()\n\n    throw new KafkaJSNonRetriableError(\n      `Invalid state, can't transition from ${current} to ${next.toString()}`\n    )\n  }\n}\n","/**\n * @param {Object} options\n * @param {import(\"../../types\").ISocketFactory} options.socketFactory\n * @param {string} options.host\n * @param {number} options.port\n * @param {Object} options.ssl\n * @param {() => void} options.onConnect\n * @param {(data: Buffer) => void} options.onData\n * @param {() => void} options.onEnd\n * @param {(err: Error) => void} options.onError\n * @param {() => void} options.onTimeout\n */\nmodule.exports = ({\n  socketFactory,\n  host,\n  port,\n  ssl,\n  onConnect,\n  onData,\n  onEnd,\n  onError,\n  onTimeout,\n}) => {\n  const socket = socketFactory({ host, port, ssl, onConnect })\n\n  socket.on('data', onData)\n  socket.on('end', onEnd)\n  socket.on('error', onError)\n  socket.on('timeout', onTimeout)\n\n  return socket\n}\n","const KEEP_ALIVE_DELAY = 60000 // in ms\n\n/**\n * @returns {import(\"../../types\").ISocketFactory}\n */\nmodule.exports = () => {\n  const net = require('net')\n  const tls = require('tls')\n\n  return ({ host, port, ssl, onConnect }) => {\n    const socket = ssl\n      ? tls.connect(Object.assign({ host, port, servername: host }, ssl), onConnect)\n      : net.connect({ host, port }, onConnect)\n\n    socket.setKeepAlive(true, KEEP_ALIVE_DELAY)\n\n    return socket\n  }\n}\n","module.exports = topicDataForBroker => {\n  return topicDataForBroker.map(\n    ({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({\n      topic,\n      partitions: partitions.map(partition => ({\n        partition,\n        firstSequence: sequencePerPartition[partition],\n        messages: messagesPerPartition[partition],\n      })),\n    })\n  )\n}\n","const createRetry = require('../../retry')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst COORDINATOR_TYPES = require('../../protocol/coordinatorTypes')\nconst createStateMachine = require('./transactionStateMachine')\nconst assert = require('assert')\n\nconst STATES = require('./transactionStates')\nconst NO_PRODUCER_ID = -1\nconst SEQUENCE_START = 0\nconst INT_32_MAX_VALUE = Math.pow(2, 32)\nconst INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [\n  'NOT_COORDINATOR_FOR_GROUP',\n  'GROUP_COORDINATOR_NOT_AVAILABLE',\n  'GROUP_LOAD_IN_PROGRESS',\n  /**\n   * The producer might have crashed and never committed the transaction; retry the\n   * request so Kafka can abort the current transaction\n   * @see https://github.com/apache/kafka/blob/201da0542726472d954080d54bc585b111aaf86f/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L1001-L1002\n   */\n  'CONCURRENT_TRANSACTIONS',\n]\nconst COMMIT_RETRIABLE_PROTOCOL_ERRORS = [\n  'UNKNOWN_TOPIC_OR_PARTITION',\n  'COORDINATOR_LOAD_IN_PROGRESS',\n]\nconst COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ['COORDINATOR_NOT_AVAILABLE', 'NOT_COORDINATOR']\n\n/**\n * @typedef {Object} EosManager\n */\n\n/**\n * Manage behavior for an idempotent producer and transactions.\n *\n * @returns {EosManager}\n */\nmodule.exports = ({\n  logger,\n  cluster,\n  transactionTimeout = 60000,\n  transactional,\n  transactionalId,\n}) => {\n  if (transactional && !transactionalId) {\n    throw new KafkaJSNonRetriableError('Cannot manage transactions without a transactionalId')\n  }\n\n  const retrier = createRetry(cluster.retry)\n\n  /**\n   * Current producer ID\n   */\n  let producerId = NO_PRODUCER_ID\n\n  /**\n   * Current producer epoch\n   */\n  let producerEpoch = 0\n\n  /**\n   * Idempotent production requires that the producer track the sequence number of messages.\n   *\n   * Sequences are sent with every Record Batch and tracked per Topic-Partition\n   */\n  let producerSequence = {}\n\n  /**\n   * Topic partitions already participating in the transaction\n   */\n  let transactionTopicPartitions = {}\n\n  const stateMachine = createStateMachine({ logger })\n  stateMachine.on('transition', ({ to }) => {\n    if (to === STATES.READY) {\n      transactionTopicPartitions = {}\n    }\n  })\n\n  const findTransactionCoordinator = () => {\n    return cluster.findGroupCoordinator({\n      groupId: transactionalId,\n      coordinatorType: COORDINATOR_TYPES.TRANSACTION,\n    })\n  }\n\n  const transactionalGuard = () => {\n    if (!transactional) {\n      throw new KafkaJSNonRetriableError('Method unavailable if non-transactional')\n    }\n  }\n\n  const eosManager = stateMachine.createGuarded(\n    {\n      /**\n       * Get the current producer id\n       * @returns {number}\n       */\n      getProducerId() {\n        return producerId\n      },\n\n      /**\n       * Get the current producer epoch\n       * @returns {number}\n       */\n      getProducerEpoch() {\n        return producerEpoch\n      },\n\n      getTransactionalId() {\n        return transactionalId\n      },\n\n      /**\n       * Initialize the idempotent producer by making an `InitProducerId` request.\n       * Overwrites any existing state in this transaction manager\n       */\n      async initProducerId() {\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await cluster.refreshMetadataIfNecessary()\n\n            // If non-transactional we can request the PID from any broker\n            const broker = await (transactional\n              ? findTransactionCoordinator()\n              : cluster.findControllerBroker())\n\n            const result = await broker.initProducerId({\n              transactionalId: transactional ? transactionalId : undefined,\n              transactionTimeout,\n            })\n\n            stateMachine.transitionTo(STATES.READY)\n            producerId = result.producerId\n            producerEpoch = result.producerEpoch\n            producerSequence = {}\n\n            logger.debug('Initialized producer id & epoch', { producerId, producerEpoch })\n          } catch (e) {\n            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              if (e.type === 'CONCURRENT_TRANSACTIONS') {\n                logger.debug('There is an ongoing transaction on this transactionId, retrying', {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                })\n              }\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n\n      /**\n       * Get the current sequence for a given Topic-Partition. Defaults to 0.\n       *\n       * @param {string} topic\n       * @param {string} partition\n       * @returns {number}\n       */\n      getSequence(topic, partition) {\n        if (!eosManager.isInitialized()) {\n          return SEQUENCE_START\n        }\n\n        producerSequence[topic] = producerSequence[topic] || {}\n        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START\n\n        return producerSequence[topic][partition]\n      },\n\n      /**\n       * Update the sequence for a given Topic-Partition.\n       *\n       * Do nothing if not yet initialized (not idempotent)\n       * @param {string} topic\n       * @param {string} partition\n       * @param {number} increment\n       */\n      updateSequence(topic, partition, increment) {\n        if (!eosManager.isInitialized()) {\n          return\n        }\n\n        const previous = eosManager.getSequence(topic, partition)\n        let sequence = previous + increment\n\n        // Sequence is defined as Int32 in the Record Batch,\n        // so theoretically should need to rotate here\n        if (sequence >= INT_32_MAX_VALUE) {\n          logger.debug(\n            `Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`\n          )\n          sequence = 0\n        }\n\n        producerSequence[topic][partition] = sequence\n      },\n\n      /**\n       * Begin a transaction\n       */\n      beginTransaction() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.TRANSACTING)\n      },\n\n      /**\n       * Add partitions to a transaction if they are not already marked as participating.\n       *\n       * Should be called prior to sending any messages during a transaction\n       * @param {TopicData[]} topicData\n       *\n       * @typedef {Object} TopicData\n       * @property {string} topic\n       * @property {object[]} partitions\n       * @property {number} partitions[].partition\n       */\n      async addPartitionsToTransaction(topicData) {\n        transactionalGuard()\n        const newTopicPartitions = {}\n\n        topicData.forEach(({ topic, partitions }) => {\n          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {}\n\n          partitions.forEach(({ partition }) => {\n            if (!transactionTopicPartitions[topic][partition]) {\n              newTopicPartitions[topic] = newTopicPartitions[topic] || []\n              newTopicPartitions[topic].push(partition)\n            }\n          })\n        })\n\n        const topics = Object.keys(newTopicPartitions).map(topic => ({\n          topic,\n          partitions: newTopicPartitions[topic],\n        }))\n\n        if (topics.length) {\n          const broker = await findTransactionCoordinator()\n          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics })\n        }\n\n        topics.forEach(({ topic, partitions }) => {\n          partitions.forEach(partition => {\n            transactionTopicPartitions[topic][partition] = true\n          })\n        })\n      },\n\n      /**\n       * Commit the ongoing transaction\n       */\n      async commit() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.COMMITTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: true,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Abort the ongoing transaction\n       */\n      async abort() {\n        transactionalGuard()\n        stateMachine.transitionTo(STATES.ABORTING)\n\n        const broker = await findTransactionCoordinator()\n        await broker.endTxn({\n          producerId,\n          producerEpoch,\n          transactionalId,\n          transactionResult: false,\n        })\n\n        stateMachine.transitionTo(STATES.READY)\n      },\n\n      /**\n       * Whether the producer id has already been initialized\n       */\n      isInitialized() {\n        return producerId !== NO_PRODUCER_ID\n      },\n\n      isTransactional() {\n        return transactional\n      },\n\n      isInTransaction() {\n        return stateMachine.state() === STATES.TRANSACTING\n      },\n\n      /**\n       * Mark the provided offsets as participating in the transaction for the given consumer group.\n       *\n       * This allows us to commit an offset as consumed only if the transaction passes.\n       * @param {string} consumerGroupId The unique group identifier\n       * @param {OffsetCommitTopic[]} topics The unique group identifier\n       * @returns {Promise}\n       *\n       * @typedef {Object} OffsetCommitTopic\n       * @property {string} topic\n       * @property {OffsetCommitTopicPartition[]} partitions\n       *\n       * @typedef {Object} OffsetCommitTopicPartition\n       * @property {number} partition\n       * @property {number} offset\n       */\n      async sendOffsets({ consumerGroupId, topics }) {\n        assert(consumerGroupId, 'Missing consumerGroupId')\n        assert(topics, 'Missing offset topics')\n\n        const transactionCoordinator = await findTransactionCoordinator()\n\n        // Do we need to add offsets if we've already done so for this consumer group?\n        await transactionCoordinator.addOffsetsToTxn({\n          transactionalId,\n          producerId,\n          producerEpoch,\n          groupId: consumerGroupId,\n        })\n\n        let groupCoordinator = await cluster.findGroupCoordinator({\n          groupId: consumerGroupId,\n          coordinatorType: COORDINATOR_TYPES.GROUP,\n        })\n\n        return retrier(async (bail, retryCount, retryTime) => {\n          try {\n            await groupCoordinator.txnOffsetCommit({\n              transactionalId,\n              producerId,\n              producerEpoch,\n              groupId: consumerGroupId,\n              topics,\n            })\n          } catch (e) {\n            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {\n              logger.debug('Group coordinator is not ready yet, retrying', {\n                error: e.message,\n                stack: e.stack,\n                transactionalId,\n                retryCount,\n                retryTime,\n              })\n\n              throw e\n            }\n\n            if (\n              COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) ||\n              e.code === 'ECONNREFUSED'\n            ) {\n              logger.debug(\n                'Invalid group coordinator, finding new group coordinator and retrying',\n                {\n                  error: e.message,\n                  stack: e.stack,\n                  transactionalId,\n                  retryCount,\n                  retryTime,\n                }\n              )\n\n              groupCoordinator = await cluster.findGroupCoordinator({\n                groupId: consumerGroupId,\n                coordinatorType: COORDINATOR_TYPES.GROUP,\n              })\n\n              throw e\n            }\n\n            bail(e)\n          }\n        })\n      },\n    },\n\n    /**\n     * Transaction state guards\n     */\n    {\n      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },\n      beginTransaction: { legalStates: [STATES.READY], async: false },\n      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },\n      sendOffsets: { legalStates: [STATES.TRANSACTING] },\n      commit: { legalStates: [STATES.TRANSACTING] },\n      abort: { legalStates: [STATES.TRANSACTING] },\n    }\n  )\n\n  return eosManager\n}\n","const { EventEmitter } = require('events')\nconst { KafkaJSNonRetriableError } = require('../../errors')\nconst STATES = require('./transactionStates')\n\nconst VALID_STATE_TRANSITIONS = {\n  [STATES.UNINITIALIZED]: [STATES.READY],\n  [STATES.READY]: [STATES.READY, STATES.TRANSACTING],\n  [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],\n  [STATES.COMMITTING]: [STATES.READY],\n  [STATES.ABORTING]: [STATES.READY],\n}\n\nmodule.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {\n  let currentState = initialState\n\n  const guard = (object, method, { legalStates, async: isAsync = true }) => {\n    if (!object[method]) {\n      throw new KafkaJSNonRetriableError(`Cannot add guard on missing method \"${method}\"`)\n    }\n\n    return (...args) => {\n      const fn = object[method]\n\n      if (!legalStates.includes(currentState)) {\n        const error = new KafkaJSNonRetriableError(\n          `Transaction state exception: Cannot call \"${method}\" in state \"${currentState}\"`\n        )\n\n        if (isAsync) {\n          return Promise.reject(error)\n        } else {\n          throw error\n        }\n      }\n\n      return fn.apply(object, args)\n    }\n  }\n\n  const stateMachine = Object.assign(new EventEmitter(), {\n    /**\n     * Create a clone of \"object\" where we ensure state machine is in correct state\n     * prior to calling any of the configured methods\n     * @param {Object} object The object whose methods we will guard\n     * @param {Object} methodStateMapping Keys are method names on \"object\"\n     * @param {string[]} methodStateMapping.legalStates Legal states for this method\n     * @param {boolean=true} methodStateMapping.async Whether this method is async (throw vs reject)\n     */\n    createGuarded(object, methodStateMapping) {\n      const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {\n        guards[method] = guard(object, method, methodStateMapping[method])\n        return guards\n      }, {})\n\n      return { ...object, ...guardedMethods }\n    },\n    /**\n     * Transition safely to a new state\n     */\n    transitionTo(state) {\n      logger.debug(`Transaction state transition ${currentState} --> ${state}`)\n\n      if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {\n        throw new KafkaJSNonRetriableError(\n          `Transaction state exception: Invalid transition ${currentState} --> ${state}`\n        )\n      }\n\n      stateMachine.emit('transition', { to: state, from: currentState })\n      currentState = state\n    },\n\n    state() {\n      return currentState\n    },\n  })\n\n  return stateMachine\n}\n","module.exports = {\n  UNINITIALIZED: 'UNINITIALIZED',\n  READY: 'READY',\n  TRANSACTING: 'TRANSACTING',\n  COMMITTING: 'COMMITTING',\n  ABORTING: 'ABORTING',\n}\n","module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {\n  if (partitionMetadata.length === 0) {\n    return {}\n  }\n\n  return messages.reduce((result, message) => {\n    const partition = partitioner({ topic, partitionMetadata, message })\n    const current = result[partition] || []\n    return Object.assign(result, { [partition]: [...current, message] })\n  }, {})\n}\n","const createRetry = require('../retry')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\nconst { DefaultPartitioner } = require('./partitioners/')\nconst InstrumentationEventEmitter = require('../instrumentation/emitter')\nconst createEosManager = require('./eosManager')\nconst createMessageProducer = require('./messageProducer')\nconst { events, wrap: wrapEvent, unwrap: unwrapEvent } = require('./instrumentationEvents')\nconst { KafkaJSNonRetriableError } = require('../errors')\n\nconst { values, keys } = Object\nconst eventNames = values(events)\nconst eventKeys = keys(events)\n  .map(key => `producer.events.${key}`)\n  .join(', ')\n\nconst { CONNECT, DISCONNECT } = events\n\n/**\n *\n * @param {Object} params\n * @param {import('../../types').Cluster} params.cluster\n * @param {import('../../types').Logger} params.logger\n * @param {import('../../types').ICustomPartitioner} [params.createPartitioner]\n * @param {import('../../types').RetryOptions} [params.retry]\n * @param {boolean} [params.idempotent]\n * @param {string} [params.transactionalId]\n * @param {number} [params.transactionTimeout]\n * @param {InstrumentationEventEmitter} [params.instrumentationEmitter]\n *\n * @returns {import('../../types').Producer}\n */\nmodule.exports = ({\n  cluster,\n  logger: rootLogger,\n  createPartitioner = DefaultPartitioner,\n  retry,\n  idempotent = false,\n  transactionalId,\n  transactionTimeout,\n  instrumentationEmitter: rootInstrumentationEmitter,\n}) => {\n  let connectionStatus = CONNECTION_STATUS.DISCONNECTED\n  retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 }\n\n  if (idempotent && retry.retries < 1) {\n    throw new KafkaJSNonRetriableError(\n      'Idempotent producer must allow retries to protect against transient errors'\n    )\n  }\n\n  const logger = rootLogger.namespace('Producer')\n\n  if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {\n    logger.warn('Limiting retries for the idempotent producer may invalidate EoS guarantees')\n  }\n\n  const partitioner = createPartitioner()\n  const retrier = createRetry(Object.assign({}, cluster.retry, retry))\n  const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter()\n  const idempotentEosManager = createEosManager({\n    logger,\n    cluster,\n    transactionTimeout,\n    transactional: false,\n    transactionalId,\n  })\n\n  const { send, sendBatch } = createMessageProducer({\n    logger,\n    cluster,\n    partitioner,\n    eosManager: idempotentEosManager,\n    idempotent,\n    retrier,\n    getConnectionStatus: () => connectionStatus,\n  })\n\n  let transactionalEosManager\n\n  /** @type {import(\"../../types\").Producer[\"on\"]} */\n  const on = (eventName, listener) => {\n    if (!eventNames.includes(eventName)) {\n      throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`)\n    }\n\n    return instrumentationEmitter.addListener(unwrapEvent(eventName), event => {\n      event.type = wrapEvent(event.type)\n      Promise.resolve(listener(event)).catch(e => {\n        logger.error(`Failed to execute listener: ${e.message}`, {\n          eventName,\n          stack: e.stack,\n        })\n      })\n    })\n  }\n\n  /**\n   * Begin a transaction. The returned object contains methods to send messages\n   * to the transaction and end the transaction by committing or aborting.\n   *\n   * Only messages sent on the transaction object will participate in the transaction.\n   *\n   * Calling any of the transactional methods after the transaction has ended\n   * will raise an exception (use `isActive` to ascertain if ended).\n   * @returns {Promise<Transaction>}\n   *\n   * @typedef {Object} Transaction\n   * @property {Function} send  Identical to the producer \"send\" method\n   * @property {Function} sendBatch Identical to the producer \"sendBatch\" method\n   * @property {Function} abort Abort the transaction\n   * @property {Function} commit  Commit the transaction\n   * @property {Function} isActive  Whether the transaction is active\n   */\n  const transaction = async () => {\n    if (!transactionalId) {\n      throw new KafkaJSNonRetriableError('Must provide transactional id for transactional producer')\n    }\n\n    let transactionDidEnd = false\n    transactionalEosManager =\n      transactionalEosManager ||\n      createEosManager({\n        logger,\n        cluster,\n        transactionTimeout,\n        transactional: true,\n        transactionalId,\n      })\n\n    if (transactionalEosManager.isInTransaction()) {\n      throw new KafkaJSNonRetriableError(\n        'There is already an ongoing transaction for this producer. Please end the transaction before beginning another.'\n      )\n    }\n\n    // We only initialize the producer id once\n    if (!transactionalEosManager.isInitialized()) {\n      await transactionalEosManager.initProducerId()\n    }\n    transactionalEosManager.beginTransaction()\n\n    const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({\n      logger,\n      cluster,\n      partitioner,\n      retrier,\n      eosManager: transactionalEosManager,\n      idempotent: true,\n      getConnectionStatus: () => connectionStatus,\n    })\n\n    const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd\n\n    const transactionGuard = fn => (...args) => {\n      if (!isActive()) {\n        return Promise.reject(\n          new KafkaJSNonRetriableError('Cannot continue to use transaction once ended')\n        )\n      }\n\n      return fn(...args)\n    }\n\n    return {\n      sendBatch: transactionGuard(sendBatchTxn),\n      send: transactionGuard(sendTxn),\n      /**\n       * Abort the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      abort: transactionGuard(async () => {\n        await transactionalEosManager.abort()\n        transactionDidEnd = true\n      }),\n      /**\n       * Commit the ongoing transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      commit: transactionGuard(async () => {\n        await transactionalEosManager.commit()\n        transactionDidEnd = true\n      }),\n      /**\n       * Sends a list of specified offsets to the consumer group coordinator, and also marks those offsets as part of the current transaction.\n       *\n       * @throws {KafkaJSNonRetriableError} If transaction has ended\n       */\n      sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {\n        await transactionalEosManager.sendOffsets({ consumerGroupId, topics })\n\n        for (const topicOffsets of topics) {\n          const { topic, partitions } = topicOffsets\n          for (const { partition, offset } of partitions) {\n            cluster.markOffsetAsCommitted({\n              groupId: consumerGroupId,\n              topic,\n              partition,\n              offset,\n            })\n          }\n        }\n      }),\n      isActive,\n    }\n  }\n\n  /**\n   * @returns {Object} logger\n   */\n  const getLogger = () => logger\n\n  return {\n    /**\n     * @returns {Promise}\n     */\n    connect: async () => {\n      await cluster.connect()\n      connectionStatus = CONNECTION_STATUS.CONNECTED\n      instrumentationEmitter.emit(CONNECT)\n\n      if (idempotent && !idempotentEosManager.isInitialized()) {\n        await idempotentEosManager.initProducerId()\n      }\n    },\n    /**\n     * @return {Promise}\n     */\n    disconnect: async () => {\n      connectionStatus = CONNECTION_STATUS.DISCONNECTING\n      await cluster.disconnect()\n      connectionStatus = CONNECTION_STATUS.DISCONNECTED\n      instrumentationEmitter.emit(DISCONNECT)\n    },\n    isIdempotent: () => {\n      return idempotent\n    },\n    events,\n    on,\n    send,\n    sendBatch,\n    transaction,\n    logger: getLogger,\n  }\n}\n","const swapObject = require('../utils/swapObject')\nconst networkEvents = require('../network/instrumentationEvents')\nconst InstrumentationEventType = require('../instrumentation/eventType')\nconst producerType = InstrumentationEventType('producer')\n\nconst events = {\n  CONNECT: producerType('connect'),\n  DISCONNECT: producerType('disconnect'),\n  REQUEST: producerType(networkEvents.NETWORK_REQUEST),\n  REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),\n  REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE),\n}\n\nconst wrappedEvents = {\n  [events.REQUEST]: networkEvents.NETWORK_REQUEST,\n  [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,\n  [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE,\n}\n\nconst reversedWrappedEvents = swapObject(wrappedEvents)\nconst unwrap = eventName => wrappedEvents[eventName] || eventName\nconst wrap = eventName => reversedWrappedEvents[eventName] || eventName\n\nmodule.exports = {\n  events,\n  wrap,\n  unwrap,\n}\n","const createSendMessages = require('./sendMessages')\nconst { KafkaJSError, KafkaJSNonRetriableError } = require('../errors')\nconst { CONNECTION_STATUS } = require('../network/connectionStatus')\n\nmodule.exports = ({\n  logger,\n  cluster,\n  partitioner,\n  eosManager,\n  idempotent,\n  retrier,\n  getConnectionStatus,\n}) => {\n  const sendMessages = createSendMessages({\n    logger,\n    cluster,\n    retrier,\n    partitioner,\n    eosManager,\n  })\n\n  const validateConnectionStatus = () => {\n    const connectionStatus = getConnectionStatus()\n\n    switch (connectionStatus) {\n      case CONNECTION_STATUS.DISCONNECTING:\n        throw new KafkaJSNonRetriableError(\n          `The producer is disconnecting; therefore, it can't safely accept messages anymore`\n        )\n      case CONNECTION_STATUS.DISCONNECTED:\n        throw new KafkaJSError('The producer is disconnected')\n    }\n  }\n\n  /**\n   * @typedef {Object} TopicMessages\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   *\n   * @typedef {Object} SendBatchRequest\n   * @property {Array<TopicMessages>} topicMessages\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   *\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   *\n   * @param {SendBatchRequest}\n   * @returns {Promise}\n   */\n  const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {\n    if (topicMessages.some(({ topic }) => !topic)) {\n      throw new KafkaJSNonRetriableError(`Invalid topic`)\n    }\n\n    if (idempotent && acks !== -1) {\n      throw new KafkaJSNonRetriableError(\n        `Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`\n      )\n    }\n\n    for (const { topic, messages } of topicMessages) {\n      if (!messages) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid messages array [${messages}] for topic \"${topic}\"`\n        )\n      }\n\n      const messageWithoutValue = messages.find(message => message.value === undefined)\n      if (messageWithoutValue) {\n        throw new KafkaJSNonRetriableError(\n          `Invalid message without value for topic \"${topic}\": ${JSON.stringify(\n            messageWithoutValue\n          )}`\n        )\n      }\n    }\n\n    validateConnectionStatus()\n    const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {\n      const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic)\n\n      if (index === -1) {\n        merged.push({ topic, messages })\n      } else {\n        merged[index].messages = [...merged[index].messages, ...messages]\n      }\n\n      return merged\n    }, [])\n\n    return await sendMessages({\n      acks,\n      timeout,\n      compression,\n      topicMessages: mergedTopicMessages,\n    })\n  }\n\n  /**\n   * @param {ProduceRequest} ProduceRequest\n   * @returns {Promise}\n   *\n   * @typedef {Object} ProduceRequest\n   * @property {string} topic\n   * @property {Array} messages An array of objects with \"key\" and \"value\", example:\n   *                         [{ key: 'my-key', value: 'my-value'}]\n   * @property {number} [acks=-1] Control the number of required acks.\n   *                           -1 = all replicas must acknowledge\n   *                            0 = no acknowledgments\n   *                            1 = only waits for the leader to acknowledge\n   * @property {number} [timeout=30000] The time to await a response in ms\n   * @property {Compression.Types} [compression=Compression.Types.None] Compression codec\n   */\n  const send = async ({ acks, timeout, compression, topic, messages }) => {\n    const topicMessage = { topic, messages }\n    return sendBatch({\n      acks,\n      timeout,\n      compression,\n      topicMessages: [topicMessage],\n    })\n  }\n\n  return {\n    send,\n    sendBatch,\n  }\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('./partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = 0x9747b28c\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = 0x5bd1e995\nconst R = 24\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = SEED ^ length\n  let length4 = length / 4\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k *= M\n    k ^= k >>> R\n    k *= M\n    h *= M\n    h ^= k\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h ^= (data[(length & ~3) + 2] & 0xff) << 16\n    case 2:\n      h ^= (data[(length & ~3) + 1] & 0xff) << 8\n    case 1:\n      h ^= data[length & ~3] & 0xff\n      h *= M\n  }\n\n  h ^= h >>> 13\n  h *= M\n  h ^= h >>> 15\n\n  return h\n}\n","const randomBytes = require('./randomBytes')\n\n// Based on the java client 0.10.2\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java\n\n/**\n * A cheap way to deterministically convert a number to a positive value. When the input is\n * positive, the original value is returned. When the input number is negative, the returned\n * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n * value.\n */\nconst toPositive = x => x & 0x7fffffff\n\n/**\n * The default partitioning strategy:\n *  - If a partition is specified in the message, use it\n *  - If no partition is specified but a key is present choose a partition based on a hash of the key\n *  - If no partition or key is present choose a partition in a round-robin fashion\n */\nmodule.exports = murmur2 => () => {\n  const counters = {}\n\n  return ({ topic, partitionMetadata, message }) => {\n    if (!(topic in counters)) {\n      counters[topic] = randomBytes(32).readUInt32BE(0)\n    }\n    const numPartitions = partitionMetadata.length\n    const availablePartitions = partitionMetadata.filter(p => p.leader >= 0)\n    const numAvailablePartitions = availablePartitions.length\n\n    if (message.partition !== null && message.partition !== undefined) {\n      return message.partition\n    }\n\n    if (message.key !== null && message.key !== undefined) {\n      return toPositive(murmur2(message.key)) % numPartitions\n    }\n\n    if (numAvailablePartitions > 0) {\n      const i = toPositive(++counters[topic]) % numAvailablePartitions\n      return availablePartitions[i].partitionId\n    }\n\n    // no partitions are available, give a non-available partition\n    return toPositive(++counters[topic]) % numPartitions\n  }\n}\n","const { KafkaJSNonRetriableError } = require('../../../errors')\n\nconst toNodeCompatible = crypto => ({\n  randomBytes: size => crypto.getRandomValues(Buffer.allocUnsafe(size)),\n})\n\nlet cryptoImplementation = null\nif (global && global.crypto) {\n  cryptoImplementation =\n    global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto\n} else if (global && global.msCrypto) {\n  cryptoImplementation = toNodeCompatible(global.msCrypto)\n} else if (global && !global.crypto) {\n  cryptoImplementation = require('crypto')\n}\n\nconst MAX_BYTES = 65536\n\nmodule.exports = size => {\n  if (size > MAX_BYTES) {\n    throw new KafkaJSNonRetriableError(\n      `Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`\n    )\n  }\n\n  if (!cryptoImplementation) {\n    throw new KafkaJSNonRetriableError('No available crypto implementation')\n  }\n\n  return cryptoImplementation.randomBytes(size)\n}\n","const murmur2 = require('./murmur2')\nconst createDefaultPartitioner = require('../default/partitioner')\n\nmodule.exports = createDefaultPartitioner(murmur2)\n","/* eslint-disable */\nconst Long = require('../../../utils/long')\n\n// Based on the kafka client 0.10.2 murmur2 implementation\n// https://github.com/apache/kafka/blob/0.10.2/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L364\n\nconst SEED = Long.fromValue(0x9747b28c)\n\n// 'm' and 'r' are mixing constants generated offline.\n// They're not really 'magic', they just happen to work well.\nconst M = Long.fromValue(0x5bd1e995)\nconst R = Long.fromValue(24)\n\nmodule.exports = key => {\n  const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key))\n  const length = data.length\n\n  // Initialize the hash to a random value\n  let h = Long.fromValue(SEED.xor(length))\n  let length4 = Math.floor(length / 4)\n\n  for (let i = 0; i < length4; i++) {\n    const i4 = i * 4\n    let k =\n      (data[i4 + 0] & 0xff) +\n      ((data[i4 + 1] & 0xff) << 8) +\n      ((data[i4 + 2] & 0xff) << 16) +\n      ((data[i4 + 3] & 0xff) << 24)\n    k = Long.fromValue(k)\n    k = k.multiply(M)\n    k = k.xor(k.toInt() >>> R)\n    k = Long.fromValue(k).multiply(M)\n    h = h.multiply(M)\n    h = h.xor(k)\n  }\n\n  // Handle the last few bytes of the input array\n  switch (length % 4) {\n    case 3:\n      h = h.xor((data[(length & ~3) + 2] & 0xff) << 16)\n    case 2:\n      h = h.xor((data[(length & ~3) + 1] & 0xff) << 8)\n    case 1:\n      h = h.xor(data[length & ~3] & 0xff)\n      h = h.multiply(M)\n  }\n\n  h = h.xor(h.toInt() >>> 13)\n  h = h.multiply(M)\n  h = h.xor(h.toInt() >>> 15)\n\n  return h.toInt()\n}\n","const DefaultPartitioner = require('./default')\nconst JavaCompatiblePartitioner = require('./defaultJava')\n\nmodule.exports = {\n  DefaultPartitioner,\n  JavaCompatiblePartitioner,\n}\n","const flatten = require('../utils/flatten')\n\nmodule.exports = ({ topics }) => {\n  const partitions = topics.map(({ topicName, partitions }) =>\n    partitions.map(partition => ({ topicName, ...partition }))\n  )\n\n  return flatten(partitions)\n}\n","const flatten = require('../utils/flatten')\nconst { KafkaJSMetadataNotLoaded } = require('../errors')\nconst { staleMetadata } = require('../protocol/error')\nconst groupMessagesPerPartition = require('./groupMessagesPerPartition')\nconst createTopicData = require('./createTopicData')\nconst responseSerializer = require('./responseSerializer')\n\nconst { keys } = Object\n\n/**\n * @param {Object} options\n * @param {import(\"../../types\").Logger} options.logger\n * @param {import(\"../../types\").Cluster} options.cluster\n * @param {ReturnType<import(\"../../types\").ICustomPartitioner>} options.partitioner\n * @param {import(\"./eosManager\").EosManager} options.eosManager\n * @param {import(\"../retry\").Retrier} options.retrier\n */\nmodule.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {\n  return async ({ acks, timeout, compression, topicMessages }) => {\n    /** @type {Map<import(\"../../types\").Broker, any[]>} */\n    const responsePerBroker = new Map()\n\n    /** @param {Map<import(\"../../types\").Broker, any[]>} responsePerBroker */\n    const createProducerRequests = async responsePerBroker => {\n      const topicMetadata = new Map()\n\n      await cluster.refreshMetadataIfNecessary()\n\n      for (const { topic, messages } of topicMessages) {\n        const partitionMetadata = cluster.findTopicPartitionMetadata(topic)\n\n        if (partitionMetadata.length === 0) {\n          logger.debug('Producing to topic without metadata', {\n            topic,\n            targetTopics: Array.from(cluster.targetTopics),\n          })\n\n          throw new KafkaJSMetadataNotLoaded('Producing to topic without metadata')\n        }\n\n        const messagesPerPartition = groupMessagesPerPartition({\n          topic,\n          partitionMetadata,\n          messages,\n          partitioner,\n        })\n\n        const partitions = keys(messagesPerPartition)\n        const sequencePerPartition = partitions.reduce((result, partition) => {\n          result[partition] = eosManager.getSequence(topic, partition)\n          return result\n        }, {})\n\n        const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions)\n        const leaders = keys(partitionsPerLeader)\n\n        topicMetadata.set(topic, {\n          partitionsPerLeader,\n          messagesPerPartition,\n          sequencePerPartition,\n        })\n\n        for (const nodeId of leaders) {\n          const broker = await cluster.findBroker({ nodeId })\n          if (!responsePerBroker.has(broker)) {\n            responsePerBroker.set(broker, null)\n          }\n        }\n      }\n\n      const brokers = Array.from(responsePerBroker.keys())\n      const brokersWithoutResponse = brokers.filter(broker => !responsePerBroker.get(broker))\n\n      return brokersWithoutResponse.map(async broker => {\n        const entries = Array.from(topicMetadata.entries())\n        const topicDataForBroker = entries\n          .filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId])\n          .map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({\n            topic,\n            partitions: partitionsPerLeader[broker.nodeId],\n            sequencePerPartition,\n            messagesPerPartition,\n          }))\n\n        const topicData = createTopicData(topicDataForBroker)\n\n        try {\n          if (eosManager.isTransactional()) {\n            await eosManager.addPartitionsToTransaction(topicData)\n          }\n\n          const response = await broker.produce({\n            transactionalId: eosManager.isTransactional()\n              ? eosManager.getTransactionalId()\n              : undefined,\n            producerId: eosManager.getProducerId(),\n            producerEpoch: eosManager.getProducerEpoch(),\n            acks,\n            timeout,\n            compression,\n            topicData,\n          })\n\n          const expectResponse = acks !== 0\n          const formattedResponse = expectResponse ? responseSerializer(response) : []\n\n          formattedResponse.forEach(({ topicName, partition }) => {\n            const increment = topicMetadata.get(topicName).messagesPerPartition[partition].length\n\n            eosManager.updateSequence(topicName, partition, increment)\n          })\n\n          responsePerBroker.set(broker, formattedResponse)\n        } catch (e) {\n          responsePerBroker.delete(broker)\n          throw e\n        }\n      })\n    }\n\n    return retrier(async (bail, retryCount, retryTime) => {\n      const topics = topicMessages.map(({ topic }) => topic)\n      await cluster.addMultipleTargetTopics(topics)\n\n      try {\n        const requests = await createProducerRequests(responsePerBroker)\n        await Promise.all(requests)\n        const responses = Array.from(responsePerBroker.values())\n        return flatten(responses)\n      } catch (e) {\n        if (e.name === 'KafkaJSConnectionClosedError') {\n          cluster.removeBroker({ host: e.host, port: e.port })\n        }\n\n        if (!cluster.isConnected()) {\n          logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {\n            retryCount,\n            retryTime,\n          })\n          await cluster.connect()\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        // This is necessary in case the metadata is stale and the number of partitions\n        // for this topic has increased in the meantime\n        if (\n          staleMetadata(e) ||\n          e.name === 'KafkaJSMetadataNotLoaded' ||\n          e.name === 'KafkaJSConnectionError' ||\n          e.name === 'KafkaJSConnectionClosedError' ||\n          (e.name === 'KafkaJSProtocolError' && e.retriable)\n        ) {\n          logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime })\n          await cluster.refreshMetadata()\n          throw e\n        }\n\n        logger.error(`${e.message}`, { retryCount, retryTime })\n        if (e.retriable) throw e\n        bail(e)\n      }\n    })\n  }\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java#L44\n\n/**\n * @typedef {number} ACLOperationTypes\n *\n * Enum for ACL Operations Types\n * @readonly\n * @enum {ACLOperationTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclOperation which this client cannot understand, perhaps because this\n   * client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclOperation.\n   */\n  ANY: 1,\n  /**\n   * ALL operation.\n   */\n  ALL: 2,\n  /**\n   * READ operation.\n   */\n  READ: 3,\n  /**\n   * WRITE operation.\n   */\n  WRITE: 4,\n  /**\n   * CREATE operation.\n   */\n  CREATE: 5,\n  /**\n   * DELETE operation.\n   */\n  DELETE: 6,\n  /**\n   * ALTER operation.\n   */\n  ALTER: 7,\n  /**\n   * DESCRIBE operation.\n   */\n  DESCRIBE: 8,\n  /**\n   * CLUSTER_ACTION operation.\n   */\n  CLUSTER_ACTION: 9,\n  /**\n   * DESCRIBE_CONFIGS operation.\n   */\n  DESCRIBE_CONFIGS: 10,\n  /**\n   * ALTER_CONFIGS operation.\n   */\n  ALTER_CONFIGS: 11,\n  /**\n   * IDEMPOTENT_WRITE operation.\n   */\n  IDEMPOTENT_WRITE: 12,\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java/#L31\n\n/**\n * @typedef {number} ACLPermissionTypes\n *\n * Enum for Permission Types\n * @readonly\n * @enum {ACLPermissionTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any AclPermissionType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any AclPermissionType.\n   */\n  ANY: 1,\n  /**\n   * Disallows access.\n   */\n  DENY: 2,\n  /**\n   * Grants access.\n   */\n  ALLOW: 3,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/a15387f34d142684859c2a57fcbef25edcdce25a/clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java#L25-L31\n * @typedef {number} ACLResourceTypes\n *\n * Enum for ACL Resource Types\n * @readonly\n * @enum {ACLResourceTypes}\n */\n\nmodule.exports = {\n  /**\n   * Represents any ResourceType which this client cannot understand,\n   * perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any ResourceType.\n   */\n  ANY: 1,\n  /**\n   * A Kafka topic.\n   * @see http://kafka.apache.org/documentation/#topicconfigs\n   */\n  TOPIC: 2,\n  /**\n   * A consumer group.\n   * @see http://kafka.apache.org/documentation/#consumerconfigs\n   */\n  GROUP: 3,\n  /**\n   * The cluster as a whole.\n   */\n  CLUSTER: 4,\n  /**\n   * A transactional ID.\n   */\n  TRANSACTIONAL_ID: 5,\n  /**\n   * A token ID.\n   */\n  DELEGATION_TOKEN: 6,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigResource.java\n */\nmodule.exports = {\n  UNKNOWN: 0,\n  TOPIC: 2,\n  BROKER: 4,\n  BROKER_LOGGER: 8,\n}\n","/**\n * @see https://github.com/apache/kafka/blob/1f240ce1793cab09e1c4823e17436d2b030df2bc/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java#L115-L122\n */\nmodule.exports = {\n  UNKNOWN: 0,\n  TOPIC_CONFIG: 1,\n  DYNAMIC_BROKER_CONFIG: 2,\n  DYNAMIC_DEFAULT_BROKER_CONFIG: 3,\n  STATIC_BROKER_CONFIG: 4,\n  DEFAULT_CONFIG: 5,\n  DYNAMIC_BROKER_LOGGER_CONFIG: 6,\n}\n","// From: https://kafka.apache.org/protocol.html#The_Messages_FindCoordinator\n\n/**\n * @typedef {number} CoordinatorType\n *\n * Enum for the types of coordinator to find.\n * @enum {CoordinatorType}\n */\nmodule.exports = {\n  GROUP: 0,\n  TRANSACTION: 1,\n}\n","// Based on https://github.com/brianloveswords/buffer-crc32/blob/master/index.js\n\nvar CRC_TABLE = new Int32Array([\n  0x00000000,\n  0x77073096,\n  0xee0e612c,\n  0x990951ba,\n  0x076dc419,\n  0x706af48f,\n  0xe963a535,\n  0x9e6495a3,\n  0x0edb8832,\n  0x79dcb8a4,\n  0xe0d5e91e,\n  0x97d2d988,\n  0x09b64c2b,\n  0x7eb17cbd,\n  0xe7b82d07,\n  0x90bf1d91,\n  0x1db71064,\n  0x6ab020f2,\n  0xf3b97148,\n  0x84be41de,\n  0x1adad47d,\n  0x6ddde4eb,\n  0xf4d4b551,\n  0x83d385c7,\n  0x136c9856,\n  0x646ba8c0,\n  0xfd62f97a,\n  0x8a65c9ec,\n  0x14015c4f,\n  0x63066cd9,\n  0xfa0f3d63,\n  0x8d080df5,\n  0x3b6e20c8,\n  0x4c69105e,\n  0xd56041e4,\n  0xa2677172,\n  0x3c03e4d1,\n  0x4b04d447,\n  0xd20d85fd,\n  0xa50ab56b,\n  0x35b5a8fa,\n  0x42b2986c,\n  0xdbbbc9d6,\n  0xacbcf940,\n  0x32d86ce3,\n  0x45df5c75,\n  0xdcd60dcf,\n  0xabd13d59,\n  0x26d930ac,\n  0x51de003a,\n  0xc8d75180,\n  0xbfd06116,\n  0x21b4f4b5,\n  0x56b3c423,\n  0xcfba9599,\n  0xb8bda50f,\n  0x2802b89e,\n  0x5f058808,\n  0xc60cd9b2,\n  0xb10be924,\n  0x2f6f7c87,\n  0x58684c11,\n  0xc1611dab,\n  0xb6662d3d,\n  0x76dc4190,\n  0x01db7106,\n  0x98d220bc,\n  0xefd5102a,\n  0x71b18589,\n  0x06b6b51f,\n  0x9fbfe4a5,\n  0xe8b8d433,\n  0x7807c9a2,\n  0x0f00f934,\n  0x9609a88e,\n  0xe10e9818,\n  0x7f6a0dbb,\n  0x086d3d2d,\n  0x91646c97,\n  0xe6635c01,\n  0x6b6b51f4,\n  0x1c6c6162,\n  0x856530d8,\n  0xf262004e,\n  0x6c0695ed,\n  0x1b01a57b,\n  0x8208f4c1,\n  0xf50fc457,\n  0x65b0d9c6,\n  0x12b7e950,\n  0x8bbeb8ea,\n  0xfcb9887c,\n  0x62dd1ddf,\n  0x15da2d49,\n  0x8cd37cf3,\n  0xfbd44c65,\n  0x4db26158,\n  0x3ab551ce,\n  0xa3bc0074,\n  0xd4bb30e2,\n  0x4adfa541,\n  0x3dd895d7,\n  0xa4d1c46d,\n  0xd3d6f4fb,\n  0x4369e96a,\n  0x346ed9fc,\n  0xad678846,\n  0xda60b8d0,\n  0x44042d73,\n  0x33031de5,\n  0xaa0a4c5f,\n  0xdd0d7cc9,\n  0x5005713c,\n  0x270241aa,\n  0xbe0b1010,\n  0xc90c2086,\n  0x5768b525,\n  0x206f85b3,\n  0xb966d409,\n  0xce61e49f,\n  0x5edef90e,\n  0x29d9c998,\n  0xb0d09822,\n  0xc7d7a8b4,\n  0x59b33d17,\n  0x2eb40d81,\n  0xb7bd5c3b,\n  0xc0ba6cad,\n  0xedb88320,\n  0x9abfb3b6,\n  0x03b6e20c,\n  0x74b1d29a,\n  0xead54739,\n  0x9dd277af,\n  0x04db2615,\n  0x73dc1683,\n  0xe3630b12,\n  0x94643b84,\n  0x0d6d6a3e,\n  0x7a6a5aa8,\n  0xe40ecf0b,\n  0x9309ff9d,\n  0x0a00ae27,\n  0x7d079eb1,\n  0xf00f9344,\n  0x8708a3d2,\n  0x1e01f268,\n  0x6906c2fe,\n  0xf762575d,\n  0x806567cb,\n  0x196c3671,\n  0x6e6b06e7,\n  0xfed41b76,\n  0x89d32be0,\n  0x10da7a5a,\n  0x67dd4acc,\n  0xf9b9df6f,\n  0x8ebeeff9,\n  0x17b7be43,\n  0x60b08ed5,\n  0xd6d6a3e8,\n  0xa1d1937e,\n  0x38d8c2c4,\n  0x4fdff252,\n  0xd1bb67f1,\n  0xa6bc5767,\n  0x3fb506dd,\n  0x48b2364b,\n  0xd80d2bda,\n  0xaf0a1b4c,\n  0x36034af6,\n  0x41047a60,\n  0xdf60efc3,\n  0xa867df55,\n  0x316e8eef,\n  0x4669be79,\n  0xcb61b38c,\n  0xbc66831a,\n  0x256fd2a0,\n  0x5268e236,\n  0xcc0c7795,\n  0xbb0b4703,\n  0x220216b9,\n  0x5505262f,\n  0xc5ba3bbe,\n  0xb2bd0b28,\n  0x2bb45a92,\n  0x5cb36a04,\n  0xc2d7ffa7,\n  0xb5d0cf31,\n  0x2cd99e8b,\n  0x5bdeae1d,\n  0x9b64c2b0,\n  0xec63f226,\n  0x756aa39c,\n  0x026d930a,\n  0x9c0906a9,\n  0xeb0e363f,\n  0x72076785,\n  0x05005713,\n  0x95bf4a82,\n  0xe2b87a14,\n  0x7bb12bae,\n  0x0cb61b38,\n  0x92d28e9b,\n  0xe5d5be0d,\n  0x7cdcefb7,\n  0x0bdbdf21,\n  0x86d3d2d4,\n  0xf1d4e242,\n  0x68ddb3f8,\n  0x1fda836e,\n  0x81be16cd,\n  0xf6b9265b,\n  0x6fb077e1,\n  0x18b74777,\n  0x88085ae6,\n  0xff0f6a70,\n  0x66063bca,\n  0x11010b5c,\n  0x8f659eff,\n  0xf862ae69,\n  0x616bffd3,\n  0x166ccf45,\n  0xa00ae278,\n  0xd70dd2ee,\n  0x4e048354,\n  0x3903b3c2,\n  0xa7672661,\n  0xd06016f7,\n  0x4969474d,\n  0x3e6e77db,\n  0xaed16a4a,\n  0xd9d65adc,\n  0x40df0b66,\n  0x37d83bf0,\n  0xa9bcae53,\n  0xdebb9ec5,\n  0x47b2cf7f,\n  0x30b5ffe9,\n  0xbdbdf21c,\n  0xcabac28a,\n  0x53b39330,\n  0x24b4a3a6,\n  0xbad03605,\n  0xcdd70693,\n  0x54de5729,\n  0x23d967bf,\n  0xb3667a2e,\n  0xc4614ab8,\n  0x5d681b02,\n  0x2a6f2b94,\n  0xb40bbe37,\n  0xc30c8ea1,\n  0x5a05df1b,\n  0x2d02ef8d,\n])\n\nmodule.exports = encoder => {\n  const { buffer } = encoder\n  const l = buffer.length\n  let crc = -1\n  for (let n = 0; n < l; n++) {\n    crc = CRC_TABLE[(crc ^ buffer[n]) & 0xff] ^ (crc >>> 8)\n  }\n  return crc ^ -1\n}\n","const { KafkaJSInvalidVarIntError, KafkaJSInvalidLongError } = require('../errors')\nconst Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\nconst DOUBLE_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\n\nmodule.exports = class Decoder {\n  static int32Size() {\n    return INT32_SIZE\n  }\n\n  static decodeZigZag(value) {\n    return (value >>> 1) ^ -(value & 1)\n  }\n\n  static decodeZigZag64(longValue) {\n    return longValue.shiftRightUnsigned(1).xor(longValue.and(Long.fromInt(1)).negate())\n  }\n\n  constructor(buffer) {\n    this.buffer = buffer\n    this.offset = 0\n  }\n\n  readInt8() {\n    const value = this.buffer.readInt8(this.offset)\n    this.offset += INT8_SIZE\n    return value\n  }\n\n  canReadInt16() {\n    return this.canReadBytes(INT16_SIZE)\n  }\n\n  readInt16() {\n    const value = this.buffer.readInt16BE(this.offset)\n    this.offset += INT16_SIZE\n    return value\n  }\n\n  canReadInt32() {\n    return this.canReadBytes(INT32_SIZE)\n  }\n\n  readInt32() {\n    const value = this.buffer.readInt32BE(this.offset)\n    this.offset += INT32_SIZE\n    return value\n  }\n\n  canReadInt64() {\n    return this.canReadBytes(INT64_SIZE)\n  }\n\n  readInt64() {\n    const first = this.buffer[this.offset]\n    const last = this.buffer[this.offset + 7]\n\n    const low =\n      (first << 24) + // Overflow\n      this.buffer[this.offset + 1] * 2 ** 16 +\n      this.buffer[this.offset + 2] * 2 ** 8 +\n      this.buffer[this.offset + 3]\n    const high =\n      this.buffer[this.offset + 4] * 2 ** 24 +\n      this.buffer[this.offset + 5] * 2 ** 16 +\n      this.buffer[this.offset + 6] * 2 ** 8 +\n      last\n    this.offset += INT64_SIZE\n\n    return (BigInt(low) << 32n) + BigInt(high)\n  }\n\n  readDouble() {\n    const value = this.buffer.readDoubleBE(this.offset)\n    this.offset += DOUBLE_SIZE\n    return value\n  }\n\n  readString() {\n    const byteLength = this.readInt16()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  readVarIntString() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  readUVarIntString() {\n    const byteLength = this.readUVarInt()\n\n    if (byteLength === 0) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    const value = stringBuffer.toString('utf8')\n    this.offset += byteLength\n    return value\n  }\n\n  canReadBytes(length) {\n    return Buffer.byteLength(this.buffer) - this.offset >= length\n  }\n\n  readBytes(byteLength = this.readInt32()) {\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readVarIntBytes() {\n    const byteLength = this.readVarInt()\n\n    if (byteLength === -1) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readUVarIntBytes() {\n    const byteLength = this.readUVarInt()\n\n    if (byteLength === 0) {\n      return null\n    }\n\n    const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength)\n    this.offset += byteLength\n    return stringBuffer\n  }\n\n  readBoolean() {\n    return this.readInt8() === 1\n  }\n\n  readAll() {\n    const result = this.buffer.slice(this.offset)\n    this.offset += Buffer.byteLength(this.buffer)\n    return result\n  }\n\n  readArray(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  readVarIntArray(reader) {\n    const length = this.readVarInt()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  readUVarIntArray(reader) {\n    const length = this.readUVarInt()\n\n    if (length === 0) {\n      return []\n    }\n\n    const array = new Array(length - 1)\n    for (let i = 0; i < length - 1; i++) {\n      array[i] = reader(this)\n    }\n\n    return array\n  }\n\n  async readArrayAsync(reader) {\n    const length = this.readInt32()\n\n    if (length === -1) {\n      return []\n    }\n\n    const array = new Array(length)\n    for (let i = 0; i < length; i++) {\n      array[i] = await reader(this)\n    }\n\n    return array\n  }\n\n  readVarInt() {\n    let currentByte\n    let result = 0\n    let i = 0\n\n    do {\n      currentByte = this.buffer[this.offset++]\n      result += (currentByte & OTHER_BITS) << i\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag(result)\n  }\n\n  // By default JavaScript's numbers are of type float64, performing bitwise operations converts the numbers to a signed 32-bit integer\n  // Unsigned Right Shift Operator >>> ensures the returned value is an unsigned 32-bit integer\n  readUVarInt() {\n    let currentByte\n    let result = 0\n    let i = 0\n    while (((currentByte = this.buffer[this.offset++]) & MOST_SIGNIFICANT_BIT) !== 0) {\n      result |= (currentByte & OTHER_BITS) << i\n      i += 7\n      if (i > 28) {\n        throw new KafkaJSInvalidVarIntError('Invalid VarInt, must contain 5 bytes or less')\n      }\n    }\n    result |= currentByte << i\n    return result >>> 0\n  }\n\n  readVarLong() {\n    let currentByte\n    let result = Long.fromInt(0)\n    let i = 0\n\n    do {\n      if (i > 63) {\n        throw new KafkaJSInvalidLongError('Invalid Long, must contain 9 bytes or less')\n      }\n      currentByte = this.buffer[this.offset++]\n      result = result.add(Long.fromInt(currentByte & OTHER_BITS).shiftLeft(i))\n      i += 7\n    } while (currentByte >= MOST_SIGNIFICANT_BIT)\n\n    return Decoder.decodeZigZag64(result)\n  }\n\n  slice(size) {\n    return new Decoder(this.buffer.slice(this.offset, this.offset + size))\n  }\n\n  forward(size) {\n    this.offset += size\n  }\n}\n","const Long = require('../utils/long')\n\nconst INT8_SIZE = 1\nconst INT16_SIZE = 2\nconst INT32_SIZE = 4\nconst INT64_SIZE = 8\nconst DOUBLE_SIZE = 8\n\nconst MOST_SIGNIFICANT_BIT = 0x80 // 128\nconst OTHER_BITS = 0x7f // 127\nconst UNSIGNED_INT32_MAX_NUMBER = 0xffffff80\nconst UNSIGNED_INT64_MAX_NUMBER = 0xffffffffffffff80n\n\nmodule.exports = class Encoder {\n  static encodeZigZag(value) {\n    return (value << 1) ^ (value >> 31)\n  }\n\n  static encodeZigZag64(value) {\n    const longValue = Long.fromValue(value)\n    return longValue.shiftLeft(1).xor(longValue.shiftRight(63))\n  }\n\n  static sizeOfVarInt(value) {\n    let encodedValue = this.encodeZigZag(value)\n    let bytes = 1\n\n    while ((encodedValue & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      bytes += 1\n      encodedValue >>>= 7\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarLong(value) {\n    let longValue = Encoder.encodeZigZag64(value)\n    let bytes = 1\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      bytes += 1\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    return bytes\n  }\n\n  static sizeOfVarIntBytes(value) {\n    const size = value == null ? -1 : Buffer.byteLength(value)\n\n    if (size < 0) {\n      return Encoder.sizeOfVarInt(-1)\n    }\n\n    return Encoder.sizeOfVarInt(size) + size\n  }\n\n  static nextPowerOfTwo(value) {\n    return 1 << (31 - Math.clz32(value) + 1)\n  }\n\n  /**\n   * Construct a new encoder with the given initial size\n   *\n   * @param {number} [initialSize] initial size\n   */\n  constructor(initialSize = 511) {\n    this.buf = Buffer.alloc(Encoder.nextPowerOfTwo(initialSize))\n    this.offset = 0\n  }\n\n  /**\n   * @param {Buffer} buffer\n   */\n  writeBufferInternal(buffer) {\n    const bufferLength = buffer.length\n    this.ensureAvailable(bufferLength)\n    buffer.copy(this.buf, this.offset, 0)\n    this.offset += bufferLength\n  }\n\n  ensureAvailable(length) {\n    if (this.offset + length > this.buf.length) {\n      const newLength = Encoder.nextPowerOfTwo(this.offset + length)\n      const newBuffer = Buffer.alloc(newLength)\n      this.buf.copy(newBuffer, 0, 0, this.offset)\n      this.buf = newBuffer\n    }\n  }\n\n  get buffer() {\n    return this.buf.slice(0, this.offset)\n  }\n\n  writeInt8(value) {\n    this.ensureAvailable(INT8_SIZE)\n    this.buf.writeInt8(value, this.offset)\n    this.offset += INT8_SIZE\n    return this\n  }\n\n  writeInt16(value) {\n    this.ensureAvailable(INT16_SIZE)\n    this.buf.writeInt16BE(value, this.offset)\n    this.offset += INT16_SIZE\n    return this\n  }\n\n  writeInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeUInt32(value) {\n    this.ensureAvailable(INT32_SIZE)\n    this.buf.writeUInt32BE(value, this.offset)\n    this.offset += INT32_SIZE\n    return this\n  }\n\n  writeInt64(value) {\n    this.ensureAvailable(INT64_SIZE)\n    const longValue = Long.fromValue(value)\n    this.buf.writeInt32BE(longValue.getHighBits(), this.offset)\n    this.buf.writeInt32BE(longValue.getLowBits(), this.offset + INT32_SIZE)\n    this.offset += INT64_SIZE\n    return this\n  }\n\n  writeDouble(value) {\n    this.ensureAvailable(DOUBLE_SIZE)\n    this.buf.writeDoubleBE(value, this.offset)\n    this.offset += DOUBLE_SIZE\n    return this\n  }\n\n  writeBoolean(value) {\n    value ? this.writeInt8(1) : this.writeInt8(0)\n    return this\n  }\n\n  writeString(value) {\n    if (value == null) {\n      this.writeInt16(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.ensureAvailable(INT16_SIZE + byteLength)\n    this.writeInt16(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeVarIntString(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.writeVarInt(byteLength)\n    this.ensureAvailable(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeUVarIntString(value) {\n    if (value == null) {\n      this.writeUVarInt(0)\n      return this\n    }\n\n    const byteLength = Buffer.byteLength(value, 'utf8')\n    this.writeUVarInt(byteLength + 1)\n    this.ensureAvailable(byteLength)\n    this.buf.write(value, this.offset, byteLength, 'utf8')\n    this.offset += byteLength\n    return this\n  }\n\n  writeBytes(value) {\n    if (value == null) {\n      this.writeInt32(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.ensureAvailable(INT32_SIZE + value.length)\n      this.writeInt32(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.ensureAvailable(INT32_SIZE + byteLength)\n      this.writeInt32(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeVarIntBytes(value) {\n    if (value == null) {\n      this.writeVarInt(-1)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.writeVarInt(value.length)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.writeVarInt(byteLength)\n      this.ensureAvailable(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeUVarIntBytes(value) {\n    if (value == null) {\n      this.writeVarInt(0)\n      return this\n    }\n\n    if (Buffer.isBuffer(value)) {\n      // raw bytes\n      this.writeUVarInt(value.length + 1)\n      this.writeBufferInternal(value)\n    } else {\n      const valueToWrite = String(value)\n      const byteLength = Buffer.byteLength(valueToWrite, 'utf8')\n      this.writeUVarInt(byteLength + 1)\n      this.ensureAvailable(byteLength)\n      this.buf.write(valueToWrite, this.offset, byteLength, 'utf8')\n      this.offset += byteLength\n    }\n\n    return this\n  }\n\n  writeEncoder(value) {\n    if (value == null || !Buffer.isBuffer(value.buf)) {\n      throw new Error('value should be an instance of Encoder')\n    }\n\n    this.writeBufferInternal(value.buffer)\n    return this\n  }\n\n  writeEncoderArray(value) {\n    if (!Array.isArray(value) || value.some(v => v == null || !Buffer.isBuffer(v.buf))) {\n      throw new Error('all values should be an instance of Encoder[]')\n    }\n\n    value.forEach(v => {\n      this.writeBufferInternal(v.buffer)\n    })\n    return this\n  }\n\n  writeBuffer(value) {\n    if (!Buffer.isBuffer(value)) {\n      throw new Error('value should be an instance of Buffer')\n    }\n\n    this.writeBufferInternal(value)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   */\n  writeNullableArray(array, type) {\n    // A null value is encoded with length of -1 and there are no following bytes\n    // On the context of this library, empty array and null are the same thing\n    const length = array.length !== 0 ? array.length : -1\n    this.writeArray(array, type, length)\n    return this\n  }\n\n  /**\n   * @param {any[]} array\n   * @param {'int32'|'number'|'string'|'object'} [type]\n   * @param {number} [length]\n   */\n  writeArray(array, type, length) {\n    const arrayLength = length == null ? array.length : length\n    this.writeInt32(arrayLength)\n    if (type !== undefined) {\n      switch (type) {\n        case 'int32':\n        case 'number':\n          array.forEach(value => this.writeInt32(value))\n          break\n        case 'string':\n          array.forEach(value => this.writeString(value))\n          break\n        case 'object':\n          this.writeEncoderArray(array)\n          break\n      }\n    } else {\n      array.forEach(value => {\n        switch (typeof value) {\n          case 'number':\n            this.writeInt32(value)\n            break\n          case 'string':\n            this.writeString(value)\n            break\n          case 'object':\n            this.writeEncoder(value)\n            break\n        }\n      })\n    }\n    return this\n  }\n\n  writeVarIntArray(array, type) {\n    if (type === 'object') {\n      this.writeVarInt(array.length)\n      this.writeEncoderArray(array)\n    } else {\n      const objectArray = array.filter(v => typeof v === 'object')\n      this.writeVarInt(objectArray.length)\n      this.writeEncoderArray(objectArray)\n    }\n    return this\n  }\n\n  writeUVarIntArray(array, type) {\n    if (type === 'object') {\n      this.writeUVarInt(array.length + 1)\n      this.writeEncoderArray(array)\n    } else {\n      const objectArray = array.filter(v => typeof v === 'object')\n      this.writeUVarInt(objectArray.length + 1)\n      this.writeEncoderArray(objectArray)\n    }\n    return this\n  }\n\n  // Based on:\n  // https://en.wikipedia.org/wiki/LEB128 Using LEB128 format similar to VLQ.\n  // https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/util/Varint.java#L106\n  writeVarInt(value) {\n    return this.writeUVarInt(Encoder.encodeZigZag(value))\n  }\n\n  writeUVarInt(value) {\n    const byteArray = []\n    while ((value & UNSIGNED_INT32_MAX_NUMBER) !== 0) {\n      byteArray.push((value & OTHER_BITS) | MOST_SIGNIFICANT_BIT)\n      value >>>= 7\n    }\n    byteArray.push(value & OTHER_BITS)\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  writeVarLong(value) {\n    const byteArray = []\n    let longValue = Encoder.encodeZigZag64(value)\n\n    while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {\n      byteArray.push(\n        longValue\n          .and(OTHER_BITS)\n          .or(MOST_SIGNIFICANT_BIT)\n          .toInt()\n      )\n      longValue = longValue.shiftRightUnsigned(7)\n    }\n\n    byteArray.push(longValue.toInt())\n\n    this.writeBufferInternal(Buffer.from(byteArray))\n    return this\n  }\n\n  size() {\n    // We can use the offset here directly, because we anyways will not re-encode the buffer when writing\n    return this.offset\n  }\n\n  toJSON() {\n    return this.buffer.toJSON()\n  }\n}\n","const { KafkaJSProtocolError } = require('../errors')\nconst websiteUrl = require('../utils/websiteUrl')\n\nconst errorCodes = [\n  {\n    type: 'UNKNOWN',\n    code: -1,\n    retriable: false,\n    message: 'The server experienced an unexpected error when processing the request',\n  },\n  {\n    type: 'OFFSET_OUT_OF_RANGE',\n    code: 1,\n    retriable: false,\n    message: 'The requested offset is not within the range of offsets maintained by the server',\n  },\n  {\n    type: 'CORRUPT_MESSAGE',\n    code: 2,\n    retriable: true,\n    message:\n      'This message has failed its CRC checksum, exceeds the valid size, or is otherwise corrupt',\n  },\n  {\n    type: 'UNKNOWN_TOPIC_OR_PARTITION',\n    code: 3,\n    retriable: true,\n    message: 'This server does not host this topic-partition',\n  },\n  {\n    type: 'INVALID_FETCH_SIZE',\n    code: 4,\n    retriable: false,\n    message: 'The requested fetch size is invalid',\n  },\n  {\n    type: 'LEADER_NOT_AVAILABLE',\n    code: 5,\n    retriable: true,\n    message:\n      'There is no leader for this topic-partition as we are in the middle of a leadership election',\n  },\n  {\n    type: 'NOT_LEADER_FOR_PARTITION',\n    code: 6,\n    retriable: true,\n    message: 'This server is not the leader for that topic-partition',\n  },\n  {\n    type: 'REQUEST_TIMED_OUT',\n    code: 7,\n    retriable: true,\n    message: 'The request timed out',\n  },\n  {\n    type: 'BROKER_NOT_AVAILABLE',\n    code: 8,\n    retriable: false,\n    message: 'The broker is not available',\n  },\n  {\n    type: 'REPLICA_NOT_AVAILABLE',\n    code: 9,\n    retriable: false,\n    message: 'The replica is not available for the requested topic-partition',\n  },\n  {\n    type: 'MESSAGE_TOO_LARGE',\n    code: 10,\n    retriable: false,\n    message:\n      'The request included a message larger than the max message size the server will accept',\n  },\n  {\n    type: 'STALE_CONTROLLER_EPOCH',\n    code: 11,\n    retriable: false,\n    message: 'The controller moved to another broker',\n  },\n  {\n    type: 'OFFSET_METADATA_TOO_LARGE',\n    code: 12,\n    retriable: false,\n    message: 'The metadata field of the offset request was too large',\n  },\n  {\n    type: 'NETWORK_EXCEPTION',\n    code: 13,\n    retriable: true,\n    message: 'The server disconnected before a response was received',\n  },\n  {\n    type: 'GROUP_LOAD_IN_PROGRESS',\n    code: 14,\n    retriable: true,\n    message: \"The coordinator is loading and hence can't process requests for this group\",\n  },\n  {\n    type: 'GROUP_COORDINATOR_NOT_AVAILABLE',\n    code: 15,\n    retriable: true,\n    message: 'The group coordinator is not available',\n  },\n  {\n    type: 'NOT_COORDINATOR_FOR_GROUP',\n    code: 16,\n    retriable: true,\n    message: 'This is not the correct coordinator for this group',\n  },\n  {\n    type: 'INVALID_TOPIC_EXCEPTION',\n    code: 17,\n    retriable: false,\n    message: 'The request attempted to perform an operation on an invalid topic',\n  },\n  {\n    type: 'RECORD_LIST_TOO_LARGE',\n    code: 18,\n    retriable: false,\n    message:\n      'The request included message batch larger than the configured segment size on the server',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS',\n    code: 19,\n    retriable: true,\n    message: 'Messages are rejected since there are fewer in-sync replicas than required',\n  },\n  {\n    type: 'NOT_ENOUGH_REPLICAS_AFTER_APPEND',\n    code: 20,\n    retriable: true,\n    message: 'Messages are written to the log, but to fewer in-sync replicas than required',\n  },\n  {\n    type: 'INVALID_REQUIRED_ACKS',\n    code: 21,\n    retriable: false,\n    message: 'Produce request specified an invalid value for required acks',\n  },\n  {\n    type: 'ILLEGAL_GENERATION',\n    code: 22,\n    retriable: false,\n    message: 'Specified group generation id is not valid',\n  },\n  {\n    type: 'INCONSISTENT_GROUP_PROTOCOL',\n    code: 23,\n    retriable: false,\n    message:\n      \"The group member's supported protocols are incompatible with those of existing members\",\n  },\n  {\n    type: 'INVALID_GROUP_ID',\n    code: 24,\n    retriable: false,\n    message: 'The configured groupId is invalid',\n  },\n  {\n    type: 'UNKNOWN_MEMBER_ID',\n    code: 25,\n    retriable: false,\n    message: 'The coordinator is not aware of this member',\n  },\n  {\n    type: 'INVALID_SESSION_TIMEOUT',\n    code: 26,\n    retriable: false,\n    message:\n      'The session timeout is not within the range allowed by the broker (as configured by group.min.session.timeout.ms and group.max.session.timeout.ms)',\n  },\n  {\n    type: 'REBALANCE_IN_PROGRESS',\n    code: 27,\n    retriable: false,\n    message: 'The group is rebalancing, so a rejoin is needed',\n    helpUrl: websiteUrl('docs/faq', 'what-does-it-mean-to-get-rebalance-in-progress-errors'),\n  },\n  {\n    type: 'INVALID_COMMIT_OFFSET_SIZE',\n    code: 28,\n    retriable: false,\n    message: 'The committing offset data size is not valid',\n  },\n  {\n    type: 'TOPIC_AUTHORIZATION_FAILED',\n    code: 29,\n    retriable: false,\n    message: 'Not authorized to access topics: [Topic authorization failed]',\n  },\n  {\n    type: 'GROUP_AUTHORIZATION_FAILED',\n    code: 30,\n    retriable: false,\n    message: 'Not authorized to access group: Group authorization failed',\n  },\n  {\n    type: 'CLUSTER_AUTHORIZATION_FAILED',\n    code: 31,\n    retriable: false,\n    message: 'Cluster authorization failed',\n  },\n  {\n    type: 'INVALID_TIMESTAMP',\n    code: 32,\n    retriable: false,\n    message: 'The timestamp of the message is out of acceptable range',\n  },\n  {\n    type: 'UNSUPPORTED_SASL_MECHANISM',\n    code: 33,\n    retriable: false,\n    message: 'The broker does not support the requested SASL mechanism',\n  },\n  {\n    type: 'ILLEGAL_SASL_STATE',\n    code: 34,\n    retriable: false,\n    message: 'Request is not valid given the current SASL state',\n  },\n  {\n    type: 'UNSUPPORTED_VERSION',\n    code: 35,\n    retriable: false,\n    message: 'The version of API is not supported',\n  },\n  {\n    type: 'TOPIC_ALREADY_EXISTS',\n    code: 36,\n    retriable: false,\n    message: 'Topic with this name already exists',\n  },\n  {\n    type: 'INVALID_PARTITIONS',\n    code: 37,\n    retriable: false,\n    message: 'Number of partitions is invalid',\n  },\n  {\n    type: 'INVALID_REPLICATION_FACTOR',\n    code: 38,\n    retriable: false,\n    message: 'Replication-factor is invalid',\n  },\n  {\n    type: 'INVALID_REPLICA_ASSIGNMENT',\n    code: 39,\n    retriable: false,\n    message: 'Replica assignment is invalid',\n  },\n  {\n    type: 'INVALID_CONFIG',\n    code: 40,\n    retriable: false,\n    message: 'Configuration is invalid',\n  },\n  {\n    type: 'NOT_CONTROLLER',\n    code: 41,\n    retriable: true,\n    message: 'This is not the correct controller for this cluster',\n  },\n  {\n    type: 'INVALID_REQUEST',\n    code: 42,\n    retriable: false,\n    message:\n      'This most likely occurs because of a request being malformed by the client library or the message was sent to an incompatible broker. See the broker logs for more details',\n  },\n  {\n    type: 'UNSUPPORTED_FOR_MESSAGE_FORMAT',\n    code: 43,\n    retriable: false,\n    message: 'The message format version on the broker does not support the request',\n  },\n  {\n    type: 'POLICY_VIOLATION',\n    code: 44,\n    retriable: false,\n    message: 'Request parameters do not satisfy the configured policy',\n  },\n  {\n    type: 'OUT_OF_ORDER_SEQUENCE_NUMBER',\n    code: 45,\n    retriable: false,\n    message: 'The broker received an out of order sequence number',\n  },\n  {\n    type: 'DUPLICATE_SEQUENCE_NUMBER',\n    code: 46,\n    retriable: false,\n    message: 'The broker received a duplicate sequence number',\n  },\n  {\n    type: 'INVALID_PRODUCER_EPOCH',\n    code: 47,\n    retriable: false,\n    message:\n      \"Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker\",\n  },\n  {\n    type: 'INVALID_TXN_STATE',\n    code: 48,\n    retriable: false,\n    message: 'The producer attempted a transactional operation in an invalid state',\n  },\n  {\n    type: 'INVALID_PRODUCER_ID_MAPPING',\n    code: 49,\n    retriable: false,\n    message:\n      'The producer attempted to use a producer id which is not currently assigned to its transactional id',\n  },\n  {\n    type: 'INVALID_TRANSACTION_TIMEOUT',\n    code: 50,\n    retriable: false,\n    message:\n      'The transaction timeout is larger than the maximum value allowed by the broker (as configured by max.transaction.timeout.ms)',\n  },\n  {\n    type: 'CONCURRENT_TRANSACTIONS',\n    code: 51,\n    /**\n     * The concurrent transactions error has \"retriable\" set to false on the protocol documentation (https://kafka.apache.org/protocol.html#protocol_error_codes)\n     * but the server expects the clients to retry. PR #223\n     * @see https://github.com/apache/kafka/blob/12f310d50e7f5b1c18c4f61a119a6cd830da3bc0/core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala#L153\n     */\n    retriable: true,\n    message:\n      'The producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing',\n  },\n  {\n    type: 'TRANSACTION_COORDINATOR_FENCED',\n    code: 52,\n    retriable: false,\n    message:\n      'Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer',\n  },\n  {\n    type: 'TRANSACTIONAL_ID_AUTHORIZATION_FAILED',\n    code: 53,\n    retriable: false,\n    message: 'Transactional Id authorization failed',\n  },\n  {\n    type: 'SECURITY_DISABLED',\n    code: 54,\n    retriable: false,\n    message: 'Security features are disabled',\n  },\n  {\n    type: 'OPERATION_NOT_ATTEMPTED',\n    code: 55,\n    retriable: false,\n    message:\n      'The broker did not attempt to execute this operation. This may happen for batched RPCs where some operations in the batch failed, causing the broker to respond without trying the rest',\n  },\n  {\n    type: 'KAFKA_STORAGE_ERROR',\n    code: 56,\n    retriable: true,\n    message: 'Disk error when trying to access log file on the disk',\n  },\n  {\n    type: 'LOG_DIR_NOT_FOUND',\n    code: 57,\n    retriable: false,\n    message: 'The user-specified log directory is not found in the broker config',\n  },\n  {\n    type: 'SASL_AUTHENTICATION_FAILED',\n    code: 58,\n    retriable: false,\n    message: 'SASL Authentication failed',\n    helpUrl: websiteUrl('docs/configuration', 'sasl'),\n  },\n  {\n    type: 'UNKNOWN_PRODUCER_ID',\n    code: 59,\n    retriable: false,\n    message:\n      \"This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception\",\n  },\n  {\n    type: 'REASSIGNMENT_IN_PROGRESS',\n    code: 60,\n    retriable: false,\n    message: 'A partition reassignment is in progress',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTH_DISABLED',\n    code: 61,\n    retriable: false,\n    message: 'Delegation Token feature is not enabled',\n  },\n  {\n    type: 'DELEGATION_TOKEN_NOT_FOUND',\n    code: 62,\n    retriable: false,\n    message: 'Delegation Token is not found on server',\n  },\n  {\n    type: 'DELEGATION_TOKEN_OWNER_MISMATCH',\n    code: 63,\n    retriable: false,\n    message: 'Specified Principal is not valid Owner/Renewer',\n  },\n  {\n    type: 'DELEGATION_TOKEN_REQUEST_NOT_ALLOWED',\n    code: 64,\n    retriable: false,\n    message:\n      'Delegation Token requests are not allowed on PLAINTEXT/1-way SSL channels and on delegation token authenticated channels',\n  },\n  {\n    type: 'DELEGATION_TOKEN_AUTHORIZATION_FAILED',\n    code: 65,\n    retriable: false,\n    message: 'Delegation Token authorization failed',\n  },\n  {\n    type: 'DELEGATION_TOKEN_EXPIRED',\n    code: 66,\n    retriable: false,\n    message: 'Delegation Token is expired',\n  },\n  {\n    type: 'INVALID_PRINCIPAL_TYPE',\n    code: 67,\n    retriable: false,\n    message: 'Supplied principalType is not supported',\n  },\n  {\n    type: 'NON_EMPTY_GROUP',\n    code: 68,\n    retriable: false,\n    message: 'The group is not empty',\n  },\n  {\n    type: 'GROUP_ID_NOT_FOUND',\n    code: 69,\n    retriable: false,\n    message: 'The group id was not found',\n  },\n  {\n    type: 'FETCH_SESSION_ID_NOT_FOUND',\n    code: 70,\n    retriable: true,\n    message: 'The fetch session ID was not found',\n  },\n  {\n    type: 'INVALID_FETCH_SESSION_EPOCH',\n    code: 71,\n    retriable: true,\n    message: 'The fetch session epoch is invalid',\n  },\n  {\n    type: 'LISTENER_NOT_FOUND',\n    code: 72,\n    retriable: true,\n    message:\n      'There is no listener on the leader broker that matches the listener on which metadata request was processed',\n  },\n  {\n    type: 'TOPIC_DELETION_DISABLED',\n    code: 73,\n    retriable: false,\n    message: 'Topic deletion is disabled',\n  },\n  {\n    type: 'FENCED_LEADER_EPOCH',\n    code: 74,\n    retriable: true,\n    message: 'The leader epoch in the request is older than the epoch on the broker',\n  },\n  {\n    type: 'UNKNOWN_LEADER_EPOCH',\n    code: 75,\n    retriable: true,\n    message: 'The leader epoch in the request is newer than the epoch on the broker',\n  },\n  {\n    type: 'UNSUPPORTED_COMPRESSION_TYPE',\n    code: 76,\n    retriable: false,\n    message: 'The requesting client does not support the compression type of given partition',\n  },\n  {\n    type: 'STALE_BROKER_EPOCH',\n    code: 77,\n    retriable: false,\n    message: 'Broker epoch has changed',\n  },\n  {\n    type: 'OFFSET_NOT_AVAILABLE',\n    code: 78,\n    retriable: true,\n    message:\n      'The leader high watermark has not caught up from a recent leader election so the offsets cannot be guaranteed to be monotonically increasing',\n  },\n  {\n    type: 'MEMBER_ID_REQUIRED',\n    code: 79,\n    retriable: false,\n    message:\n      'The group member needs to have a valid member id before actually entering a consumer group',\n  },\n  {\n    type: 'PREFERRED_LEADER_NOT_AVAILABLE',\n    code: 80,\n    retriable: true,\n    message: 'The preferred leader was not available',\n  },\n  {\n    type: 'GROUP_MAX_SIZE_REACHED',\n    code: 81,\n    retriable: false,\n    message:\n      'The consumer group has reached its max size. It already has the configured maximum number of members',\n  },\n  {\n    type: 'FENCED_INSTANCE_ID',\n    code: 82,\n    retriable: false,\n    message:\n      'The broker rejected this static consumer since another consumer with the same group instance id has registered with a different member id',\n  },\n  {\n    type: 'ELIGIBLE_LEADERS_NOT_AVAILABLE',\n    code: 83,\n    retriable: true,\n    message: 'Eligible topic partition leaders are not available',\n  },\n  {\n    type: 'ELECTION_NOT_NEEDED',\n    code: 84,\n    retriable: true,\n    message: 'Leader election not needed for topic partition',\n  },\n  {\n    type: 'NO_REASSIGNMENT_IN_PROGRESS',\n    code: 85,\n    retriable: false,\n    message: 'No partition reassignment is in progress',\n  },\n  {\n    type: 'GROUP_SUBSCRIBED_TO_TOPIC',\n    code: 86,\n    retriable: false,\n    message:\n      'Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it',\n  },\n  {\n    type: 'INVALID_RECORD',\n    code: 87,\n    retriable: false,\n    message: 'This record has failed the validation on broker and hence be rejected',\n  },\n  {\n    type: 'UNSTABLE_OFFSET_COMMIT',\n    code: 88,\n    retriable: true,\n    message: 'There are unstable offsets that need to be cleared',\n  },\n]\n\nconst unknownErrorCode = errorCode => ({\n  type: 'KAFKAJS_UNKNOWN_ERROR_CODE',\n  code: -99,\n  retriable: false,\n  message: `Unknown error code ${errorCode}`,\n})\n\nconst SUCCESS_CODE = 0\nconst UNSUPPORTED_VERSION_CODE = 35\n\nconst failure = code => code !== SUCCESS_CODE\nconst createErrorFromCode = code => {\n  return new KafkaJSProtocolError(errorCodes.find(e => e.code === code) || unknownErrorCode(code))\n}\n\nconst failIfVersionNotSupported = code => {\n  if (code === UNSUPPORTED_VERSION_CODE) {\n    throw createErrorFromCode(UNSUPPORTED_VERSION_CODE)\n  }\n}\n\nconst staleMetadata = e =>\n  ['UNKNOWN_TOPIC_OR_PARTITION', 'LEADER_NOT_AVAILABLE', 'NOT_LEADER_FOR_PARTITION'].includes(\n    e.type\n  )\n\nmodule.exports = {\n  failure,\n  errorCodes,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n  staleMetadata,\n}\n","/**\n * Enum for isolation levels\n * @readonly\n * @enum {number}\n */\nmodule.exports = {\n  // Makes all records visible\n  READ_UNCOMMITTED: 0,\n\n  // non-transactional and COMMITTED transactional records are visible. It returns all data\n  // from offsets smaller than the current LSO (last stable offset), and enables the inclusion of\n  // the list of aborted transactions in the result, which allows consumers to discard ABORTED\n  // transactional records\n  READ_COMMITTED: 1,\n}\n","const { promisify } = require('util')\nconst zlib = require('zlib')\n\nconst gzip = promisify(zlib.gzip)\nconst unzip = promisify(zlib.unzip)\n\nmodule.exports = {\n  /**\n   * @param {Encoder} encoder\n   * @returns {Promise}\n   */\n  async compress(encoder) {\n    return await gzip(encoder.buffer)\n  },\n\n  /**\n   * @param {Buffer} buffer\n   * @returns {Promise}\n   */\n  async decompress(buffer) {\n    return await unzip(buffer)\n  },\n}\n","const { KafkaJSNotImplemented } = require('../../../errors')\n\nconst COMPRESSION_CODEC_MASK = 0x07\n\nconst Types = {\n  None: 0,\n  GZIP: 1,\n  Snappy: 2,\n  LZ4: 3,\n  ZSTD: 4,\n}\n\nconst Codecs = {\n  [Types.GZIP]: () => require('./gzip'),\n  [Types.Snappy]: () => {\n    throw new KafkaJSNotImplemented('Snappy compression not implemented')\n  },\n  [Types.LZ4]: () => {\n    throw new KafkaJSNotImplemented('LZ4 compression not implemented')\n  },\n  [Types.ZSTD]: () => {\n    throw new KafkaJSNotImplemented('ZSTD compression not implemented')\n  },\n}\n\nconst lookupCodec = type => (Codecs[type] ? Codecs[type]() : null)\nconst lookupCodecByAttributes = attributes => {\n  const codec = Codecs[attributes & COMPRESSION_CODEC_MASK]\n  return codec ? codec() : null\n}\n\nmodule.exports = {\n  Types,\n  Codecs,\n  lookupCodec,\n  lookupCodecByAttributes,\n  COMPRESSION_CODEC_MASK,\n}\n","const {\n  KafkaJSPartialMessageError,\n  KafkaJSUnsupportedMagicByteInMessageSet,\n} = require('../../errors')\n\nconst V0Decoder = require('./v0/decoder')\nconst V1Decoder = require('./v1/decoder')\n\nconst decodeMessage = (decoder, magicByte) => {\n  switch (magicByte) {\n    case 0:\n      return V0Decoder(decoder)\n    case 1:\n      return V1Decoder(decoder)\n    default:\n      throw new KafkaJSUnsupportedMagicByteInMessageSet(\n        `Unsupported MessageSet message version, magic byte: ${magicByte}`\n      )\n  }\n}\n\nmodule.exports = (offset, size, decoder) => {\n  // Don't decrement decoder.offset because slice is already considering the current\n  // offset of the decoder\n  const remainingBytes = Buffer.byteLength(decoder.slice(size).buffer)\n\n  if (remainingBytes < size) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: remainingBytes(${remainingBytes}) < messageSize(${size})`\n    )\n  }\n\n  const crc = decoder.readInt32()\n  const magicByte = decoder.readInt8()\n  const message = decodeMessage(decoder, magicByte)\n  return Object.assign({ offset, size, crc, magicByte }, message)\n}\n","const versions = {\n  0: require('./v0'),\n  1: require('./v1'),\n}\n\nmodule.exports = ({ version = 0 }) => versions[version]\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression, COMPRESSION_CODEC_MASK } = require('../compression')\n\n/**\n * v0\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, key, value }) => {\n  const content = new Encoder()\n    .writeInt8(0) // magicByte\n    .writeInt8(compression & COMPRESSION_CODEC_MASK)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","module.exports = decoder => ({\n  attributes: decoder.readInt8(),\n  timestamp: decoder.readInt64().toString(),\n  key: decoder.readBytes(),\n  value: decoder.readBytes(),\n})\n","const Encoder = require('../../encoder')\nconst crc32 = require('../../crc32')\nconst { Types: Compression, COMPRESSION_CODEC_MASK } = require('../compression')\n\n/**\n * v1 (supported since 0.10.0)\n * Message => Crc MagicByte Attributes Key Value\n *   Crc => int32\n *   MagicByte => int8\n *   Attributes => int8\n *   Timestamp => int64\n *   Key => bytes\n *   Value => bytes\n */\n\nmodule.exports = ({ compression = Compression.None, timestamp = Date.now(), key, value }) => {\n  const content = new Encoder()\n    .writeInt8(1) // magicByte\n    .writeInt8(compression & COMPRESSION_CODEC_MASK)\n    .writeInt64(timestamp)\n    .writeBytes(key)\n    .writeBytes(value)\n\n  const crc = crc32(content)\n  return new Encoder().writeInt32(crc).writeEncoder(content)\n}\n","const Long = require('../../utils/long')\nconst Decoder = require('../decoder')\nconst MessageDecoder = require('../message/decoder')\nconst { lookupCodecByAttributes } = require('../message/compression')\nconst { KafkaJSPartialMessageError } = require('../../errors')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\nmodule.exports = async (primaryDecoder, size = null) => {\n  const messages = []\n  const messageSetSize = size || primaryDecoder.readInt32()\n  const messageSetDecoder = primaryDecoder.slice(messageSetSize)\n\n  while (messageSetDecoder.offset < messageSetSize) {\n    try {\n      const message = EntryDecoder(messageSetDecoder)\n      const codec = lookupCodecByAttributes(message.attributes)\n\n      if (codec) {\n        const buffer = await codec.decompress(message.value)\n        messages.push(...EntriesDecoder(new Decoder(buffer), message))\n      } else {\n        messages.push(message)\n      }\n    } catch (e) {\n      if (e.name === 'KafkaJSPartialMessageError') {\n        // We tried to decode a partial message, it means that minBytes\n        // is probably too low\n        break\n      }\n\n      if (e.name === 'KafkaJSUnsupportedMagicByteInMessageSet') {\n        // Received a MessageSet and a RecordBatch on the same response, the cluster is probably\n        // upgrading the message format from 0.10 to 0.11. Stop processing this message set to\n        // receive the full record batch on the next request\n        break\n      }\n\n      throw e\n    }\n  }\n\n  primaryDecoder.forward(messageSetSize)\n  return messages\n}\n\nconst EntriesDecoder = (decoder, compressedMessage) => {\n  const messages = []\n\n  while (decoder.offset < decoder.buffer.length) {\n    messages.push(EntryDecoder(decoder))\n  }\n\n  if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {\n    const compressedOffset = Long.fromValue(compressedMessage.offset)\n    const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset)\n    const baseOffset = compressedOffset - lastMessageOffset\n\n    for (const message of messages) {\n      message.offset = Long.fromValue(message.offset)\n        .add(baseOffset)\n        .toString()\n    }\n  }\n\n  return messages\n}\n\nconst EntryDecoder = decoder => {\n  if (!decoder.canReadInt64()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the offset`\n    )\n  }\n\n  const offset = decoder.readInt64().toString()\n\n  if (!decoder.canReadInt32()) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial message: There isn't enough bytes to read the message size`\n    )\n  }\n\n  const size = decoder.readInt32()\n  return MessageDecoder(offset, size, decoder)\n}\n","const Encoder = require('../encoder')\nconst MessageProtocol = require('../message')\nconst { Types } = require('../message/compression')\n\n/**\n * MessageSet => [Offset MessageSize Message]\n *  Offset => int64\n *  MessageSize => int32\n *  Message => Bytes\n */\n\n/**\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n */\nmodule.exports = ({ messageVersion = 0, compression, entries }) => {\n  const isCompressed = compression !== Types.None\n  const Message = MessageProtocol({ version: messageVersion })\n  const encoder = new Encoder()\n\n  // Messages in a message set are __not__ encoded as an array.\n  // They are written in sequence.\n  // https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Messagesets\n\n  entries.forEach((entry, i) => {\n    const message = Message(entry)\n\n    // This is the offset used in kafka as the log sequence number.\n    // When the producer is sending non compressed messages, it can set the offsets to anything\n    // When the producer is sending compressed messages, to avoid server side recompression, each compressed message\n    // should have offset starting from 0 and increasing by one for each inner message in the compressed message\n    encoder.writeInt64(isCompressed ? i : -1)\n    encoder.writeInt32(message.size())\n\n    encoder.writeEncoder(message)\n  })\n\n  return encoder\n}\n","/**\n * A javascript implementation of the CRC32 checksum that uses\n * the CRC32-C polynomial, the same polynomial used by iSCSI\n *\n * also known as CRC32 Castagnoli\n * based on: https://github.com/ashi009/node-fast-crc32c/blob/master/impls/js_crc32c.js\n */\nconst crc32C = buffer => {\n  let crc = 0 ^ -1\n  for (let i = 0; i < buffer.length; i++) {\n    crc = T[(crc ^ buffer[i]) & 0xff] ^ (crc >>> 8)\n  }\n\n  return (crc ^ -1) >>> 0\n}\n\nmodule.exports = crc32C\n\n// prettier-ignore\nvar T = new Int32Array([\n  0x00000000, 0xf26b8303, 0xe13b70f7, 0x1350f3f4,\n  0xc79a971f, 0x35f1141c, 0x26a1e7e8, 0xd4ca64eb,\n  0x8ad958cf, 0x78b2dbcc, 0x6be22838, 0x9989ab3b,\n  0x4d43cfd0, 0xbf284cd3, 0xac78bf27, 0x5e133c24,\n  0x105ec76f, 0xe235446c, 0xf165b798, 0x030e349b,\n  0xd7c45070, 0x25afd373, 0x36ff2087, 0xc494a384,\n  0x9a879fa0, 0x68ec1ca3, 0x7bbcef57, 0x89d76c54,\n  0x5d1d08bf, 0xaf768bbc, 0xbc267848, 0x4e4dfb4b,\n  0x20bd8ede, 0xd2d60ddd, 0xc186fe29, 0x33ed7d2a,\n  0xe72719c1, 0x154c9ac2, 0x061c6936, 0xf477ea35,\n  0xaa64d611, 0x580f5512, 0x4b5fa6e6, 0xb93425e5,\n  0x6dfe410e, 0x9f95c20d, 0x8cc531f9, 0x7eaeb2fa,\n  0x30e349b1, 0xc288cab2, 0xd1d83946, 0x23b3ba45,\n  0xf779deae, 0x05125dad, 0x1642ae59, 0xe4292d5a,\n  0xba3a117e, 0x4851927d, 0x5b016189, 0xa96ae28a,\n  0x7da08661, 0x8fcb0562, 0x9c9bf696, 0x6ef07595,\n  0x417b1dbc, 0xb3109ebf, 0xa0406d4b, 0x522bee48,\n  0x86e18aa3, 0x748a09a0, 0x67dafa54, 0x95b17957,\n  0xcba24573, 0x39c9c670, 0x2a993584, 0xd8f2b687,\n  0x0c38d26c, 0xfe53516f, 0xed03a29b, 0x1f682198,\n  0x5125dad3, 0xa34e59d0, 0xb01eaa24, 0x42752927,\n  0x96bf4dcc, 0x64d4cecf, 0x77843d3b, 0x85efbe38,\n  0xdbfc821c, 0x2997011f, 0x3ac7f2eb, 0xc8ac71e8,\n  0x1c661503, 0xee0d9600, 0xfd5d65f4, 0x0f36e6f7,\n  0x61c69362, 0x93ad1061, 0x80fde395, 0x72966096,\n  0xa65c047d, 0x5437877e, 0x4767748a, 0xb50cf789,\n  0xeb1fcbad, 0x197448ae, 0x0a24bb5a, 0xf84f3859,\n  0x2c855cb2, 0xdeeedfb1, 0xcdbe2c45, 0x3fd5af46,\n  0x7198540d, 0x83f3d70e, 0x90a324fa, 0x62c8a7f9,\n  0xb602c312, 0x44694011, 0x5739b3e5, 0xa55230e6,\n  0xfb410cc2, 0x092a8fc1, 0x1a7a7c35, 0xe811ff36,\n  0x3cdb9bdd, 0xceb018de, 0xdde0eb2a, 0x2f8b6829,\n  0x82f63b78, 0x709db87b, 0x63cd4b8f, 0x91a6c88c,\n  0x456cac67, 0xb7072f64, 0xa457dc90, 0x563c5f93,\n  0x082f63b7, 0xfa44e0b4, 0xe9141340, 0x1b7f9043,\n  0xcfb5f4a8, 0x3dde77ab, 0x2e8e845f, 0xdce5075c,\n  0x92a8fc17, 0x60c37f14, 0x73938ce0, 0x81f80fe3,\n  0x55326b08, 0xa759e80b, 0xb4091bff, 0x466298fc,\n  0x1871a4d8, 0xea1a27db, 0xf94ad42f, 0x0b21572c,\n  0xdfeb33c7, 0x2d80b0c4, 0x3ed04330, 0xccbbc033,\n  0xa24bb5a6, 0x502036a5, 0x4370c551, 0xb11b4652,\n  0x65d122b9, 0x97baa1ba, 0x84ea524e, 0x7681d14d,\n  0x2892ed69, 0xdaf96e6a, 0xc9a99d9e, 0x3bc21e9d,\n  0xef087a76, 0x1d63f975, 0x0e330a81, 0xfc588982,\n  0xb21572c9, 0x407ef1ca, 0x532e023e, 0xa145813d,\n  0x758fe5d6, 0x87e466d5, 0x94b49521, 0x66df1622,\n  0x38cc2a06, 0xcaa7a905, 0xd9f75af1, 0x2b9cd9f2,\n  0xff56bd19, 0x0d3d3e1a, 0x1e6dcdee, 0xec064eed,\n  0xc38d26c4, 0x31e6a5c7, 0x22b65633, 0xd0ddd530,\n  0x0417b1db, 0xf67c32d8, 0xe52cc12c, 0x1747422f,\n  0x49547e0b, 0xbb3ffd08, 0xa86f0efc, 0x5a048dff,\n  0x8ecee914, 0x7ca56a17, 0x6ff599e3, 0x9d9e1ae0,\n  0xd3d3e1ab, 0x21b862a8, 0x32e8915c, 0xc083125f,\n  0x144976b4, 0xe622f5b7, 0xf5720643, 0x07198540,\n  0x590ab964, 0xab613a67, 0xb831c993, 0x4a5a4a90,\n  0x9e902e7b, 0x6cfbad78, 0x7fab5e8c, 0x8dc0dd8f,\n  0xe330a81a, 0x115b2b19, 0x020bd8ed, 0xf0605bee,\n  0x24aa3f05, 0xd6c1bc06, 0xc5914ff2, 0x37faccf1,\n  0x69e9f0d5, 0x9b8273d6, 0x88d28022, 0x7ab90321,\n  0xae7367ca, 0x5c18e4c9, 0x4f48173d, 0xbd23943e,\n  0xf36e6f75, 0x0105ec76, 0x12551f82, 0xe03e9c81,\n  0x34f4f86a, 0xc69f7b69, 0xd5cf889d, 0x27a40b9e,\n  0x79b737ba, 0x8bdcb4b9, 0x988c474d, 0x6ae7c44e,\n  0xbe2da0a5, 0x4c4623a6, 0x5f16d052, 0xad7d5351\n]);\n","const crc32C = require('./crc32C')\nconst unsigned = value => Uint32Array.from([value])[0]\n\nmodule.exports = buffer => unsigned(crc32C(buffer))\n","module.exports = decoder => ({\n  key: decoder.readVarIntString(),\n  value: decoder.readVarIntBytes(),\n})\n","const Encoder = require('../../../encoder')\n\n/**\n * v0\n * Header => Key Value\n *   Key => varInt|string\n *   Value => varInt|bytes\n */\n\nmodule.exports = ({ key, value }) => {\n  return new Encoder().writeVarIntString(key).writeVarIntBytes(value)\n}\n","const Long = require('../../../../utils/long')\nconst HeaderDecoder = require('../../header/v0/decoder')\nconst TimestampTypes = require('../../../timestampTypes')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\nmodule.exports = (decoder, batchContext = {}) => {\n  const {\n    firstOffset,\n    firstTimestamp,\n    magicByte,\n    isControlBatch = false,\n    timestampType,\n    maxTimestamp,\n  } = batchContext\n  const attributes = decoder.readInt8()\n\n  const timestampDelta = decoder.readVarLong()\n  const timestamp =\n    timestampType === TimestampTypes.LOG_APPEND_TIME && maxTimestamp\n      ? maxTimestamp\n      : Long.fromValue(firstTimestamp)\n          .add(timestampDelta)\n          .toString()\n\n  const offsetDelta = decoder.readVarInt()\n  const offset = Long.fromValue(firstOffset)\n    .add(offsetDelta)\n    .toString()\n\n  const key = decoder.readVarIntBytes()\n  const value = decoder.readVarIntBytes()\n  const headers = decoder\n    .readVarIntArray(HeaderDecoder)\n    .reduce((obj, { key, value }) => ({ ...obj, [key]: value }), {})\n\n  return {\n    magicByte,\n    attributes, // Record level attributes are presently unused\n    timestamp,\n    offset,\n    key,\n    value,\n    headers,\n    isControlRecord: isControlBatch,\n    batchContext,\n  }\n}\n","const Encoder = require('../../../encoder')\nconst Header = require('../../header/v0')\n\n/**\n * v0\n * Record =>\n *   Length => Varint\n *   Attributes => Int8\n *   TimestampDelta => Varlong\n *   OffsetDelta => Varint\n *   Key => varInt|Bytes\n *   Value => varInt|Bytes\n *   Headers => [HeaderKey HeaderValue]\n *     HeaderKey => VarInt|String\n *     HeaderValue => VarInt|Bytes\n */\n\n/**\n * @param [offsetDelta=0] {Integer}\n * @param [timestampDelta=0] {Long}\n * @param key {Buffer}\n * @param value {Buffer}\n * @param [headers={}] {Object}\n */\nmodule.exports = ({ offsetDelta = 0, timestampDelta = 0, key, value, headers = {} }) => {\n  const headersArray = Object.keys(headers).map(headerKey => ({\n    key: headerKey,\n    value: headers[headerKey],\n  }))\n\n  const sizeOfBody =\n    1 + // always one byte for attributes\n    Encoder.sizeOfVarLong(timestampDelta) +\n    Encoder.sizeOfVarInt(offsetDelta) +\n    Encoder.sizeOfVarIntBytes(key) +\n    Encoder.sizeOfVarIntBytes(value) +\n    sizeOfHeaders(headersArray)\n\n  return new Encoder()\n    .writeVarInt(sizeOfBody)\n    .writeInt8(0) // no used record attributes at the moment\n    .writeVarLong(timestampDelta)\n    .writeVarInt(offsetDelta)\n    .writeVarIntBytes(key)\n    .writeVarIntBytes(value)\n    .writeVarIntArray(headersArray.map(Header))\n}\n\nconst sizeOfHeaders = headersArray => {\n  let size = Encoder.sizeOfVarInt(headersArray.length)\n\n  for (const header of headersArray) {\n    const keySize = Buffer.byteLength(header.key)\n    const valueSize = Buffer.byteLength(header.value)\n\n    size += Encoder.sizeOfVarInt(keySize) + keySize\n\n    if (header.value === null) {\n      size += Encoder.sizeOfVarInt(-1)\n    } else {\n      size += Encoder.sizeOfVarInt(valueSize) + valueSize\n    }\n  }\n\n  return size\n}\n","const Decoder = require('../../decoder')\nconst { KafkaJSPartialMessageError } = require('../../../errors')\nconst { lookupCodecByAttributes } = require('../../message/compression')\nconst RecordDecoder = require('../record/v0/decoder')\nconst TimestampTypes = require('../../timestampTypes')\n\nconst TIMESTAMP_TYPE_FLAG_MASK = 0x8\nconst TRANSACTIONAL_FLAG_MASK = 0x10\nconst CONTROL_FLAG_MASK = 0x20\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nmodule.exports = async fetchDecoder => {\n  const firstOffset = fetchDecoder.readInt64().toString()\n  const length = fetchDecoder.readInt32()\n  const decoder = fetchDecoder.slice(length)\n  fetchDecoder.forward(length)\n\n  const remainingBytes = Buffer.byteLength(decoder.buffer)\n\n  if (remainingBytes < length) {\n    throw new KafkaJSPartialMessageError(\n      `Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`\n    )\n  }\n\n  const partitionLeaderEpoch = decoder.readInt32()\n\n  // The magic byte was read by the Fetch protocol to distinguish between\n  // the record batch and the legacy message set. It's not used here but\n  // it has to be read.\n  const magicByte = decoder.readInt8() // eslint-disable-line no-unused-vars\n\n  // The library is currently not performing CRC validations\n  const crc = decoder.readInt32() // eslint-disable-line no-unused-vars\n\n  const attributes = decoder.readInt16()\n  const lastOffsetDelta = decoder.readInt32()\n  const firstTimestamp = decoder.readInt64().toString()\n  const maxTimestamp = decoder.readInt64().toString()\n  const producerId = decoder.readInt64().toString()\n  const producerEpoch = decoder.readInt16()\n  const firstSequence = decoder.readInt32()\n\n  const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0\n  const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0\n  const timestampType =\n    (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0\n      ? TimestampTypes.LOG_APPEND_TIME\n      : TimestampTypes.CREATE_TIME\n\n  const codec = lookupCodecByAttributes(attributes)\n\n  const recordContext = {\n    firstOffset,\n    firstTimestamp,\n    partitionLeaderEpoch,\n    inTransaction,\n    isControlBatch,\n    lastOffsetDelta,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    maxTimestamp,\n    timestampType,\n  }\n\n  const records = await decodeRecords(codec, decoder, { ...recordContext, magicByte })\n\n  return {\n    ...recordContext,\n    records,\n  }\n}\n\nconst decodeRecords = async (codec, recordsDecoder, recordContext) => {\n  if (!codec) {\n    return recordsDecoder.readArray(decoder => decodeRecord(decoder, recordContext))\n  }\n\n  const length = recordsDecoder.readInt32()\n\n  if (length <= 0) {\n    return []\n  }\n\n  const compressedRecordsBuffer = recordsDecoder.readAll()\n  const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer)\n  const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer)\n  const records = new Array(length)\n\n  for (let i = 0; i < length; i++) {\n    records[i] = decodeRecord(decompressedRecordDecoder, recordContext)\n  }\n\n  return records\n}\n\nconst decodeRecord = (decoder, recordContext) => {\n  const recordBuffer = decoder.readVarIntBytes()\n  return RecordDecoder(new Decoder(recordBuffer), recordContext)\n}\n","const Long = require('../../../utils/long')\nconst Encoder = require('../../encoder')\nconst crc32C = require('../crc32C')\nconst {\n  Types: Compression,\n  lookupCodec,\n  COMPRESSION_CODEC_MASK,\n} = require('../../message/compression')\n\nconst MAGIC_BYTE = 2\nconst TIMESTAMP_MASK = 0 // The fourth lowest bit, always set this bit to 0 (since 0.10.0)\nconst TRANSACTIONAL_MASK = 16 // The fifth lowest bit\n\n/**\n * v0\n * RecordBatch =>\n *  FirstOffset => int64\n *  Length => int32\n *  PartitionLeaderEpoch => int32\n *  Magic => int8\n *  CRC => int32\n *  Attributes => int16\n *  LastOffsetDelta => int32\n *  FirstTimestamp => int64\n *  MaxTimestamp => int64\n *  ProducerId => int64\n *  ProducerEpoch => int16\n *  FirstSequence => int32\n *  Records => [Record]\n */\n\nconst RecordBatch = async ({\n  compression = Compression.None,\n  firstOffset = Long.fromInt(0),\n  firstTimestamp = Date.now(),\n  maxTimestamp = Date.now(),\n  partitionLeaderEpoch = 0,\n  lastOffsetDelta = 0,\n  transactional = false,\n  producerId = Long.fromValue(-1), // for idempotent messages\n  producerEpoch = 0, // for idempotent messages\n  firstSequence = 0, // for idempotent messages\n  records = [],\n}) => {\n  const COMPRESSION_CODEC = compression & COMPRESSION_CODEC_MASK\n  const IN_TRANSACTION = transactional ? TRANSACTIONAL_MASK : 0\n  const attributes = COMPRESSION_CODEC | TIMESTAMP_MASK | IN_TRANSACTION\n\n  const batchBody = new Encoder()\n    .writeInt16(attributes)\n    .writeInt32(lastOffsetDelta)\n    .writeInt64(firstTimestamp)\n    .writeInt64(maxTimestamp)\n    .writeInt64(producerId)\n    .writeInt16(producerEpoch)\n    .writeInt32(firstSequence)\n\n  if (compression === Compression.None) {\n    if (records.every(v => typeof v === typeof records[0])) {\n      batchBody.writeArray(records, typeof records[0])\n    } else {\n      batchBody.writeArray(records)\n    }\n  } else {\n    const compressedRecords = await compressRecords(compression, records)\n    batchBody.writeInt32(records.length).writeBuffer(compressedRecords)\n  }\n\n  // CRC32C validation is happening here:\n  // https://github.com/apache/kafka/blob/0.11.0.1/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L148\n\n  const batch = new Encoder()\n    .writeInt32(partitionLeaderEpoch)\n    .writeInt8(MAGIC_BYTE)\n    .writeUInt32(crc32C(batchBody.buffer))\n    .writeEncoder(batchBody)\n\n  return new Encoder().writeInt64(firstOffset).writeBytes(batch.buffer)\n}\n\nconst compressRecords = async (compression, records) => {\n  const codec = lookupCodec(compression)\n  const recordsEncoder = new Encoder()\n\n  recordsEncoder.writeEncoderArray(records)\n\n  return codec.compress(recordsEncoder)\n}\n\nmodule.exports = {\n  RecordBatch,\n  MAGIC_BYTE,\n}\n","const Encoder = require('./encoder')\n\nmodule.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {\n  const payload = await encode()\n  const requestPayload = new Encoder()\n    .writeInt16(apiKey)\n    .writeInt16(apiVersion)\n    .writeInt32(correlationId)\n    .writeString(clientId)\n    .writeEncoder(payload)\n\n  return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload)\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, groupId }), response }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, groupId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, groupId }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddOffsetsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddOffsetsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch group_id\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   group_id => STRING\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddOffsetsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * AddOffsetsToTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AddOffsetsToTxn Request (Version: 1) => transactional_id producer_id producer_epoch group_id\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   group_id => STRING\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, groupId }) =>\n  Object.assign(\n    requestV0({\n      transactionalId,\n      producerId,\n      producerEpoch,\n      groupId,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AddOffsetsToTxn Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, topics }), response }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, producerId, producerEpoch, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AddPartitionsToTxn: apiKey } = require('../../apiKeys')\n\n/**\n * AddPartitionsToTxn Request (Version: 0) => transactional_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AddPartitionsToTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = partition => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AddPartitionsToTxn Response (Version: 0) => throttle_time_ms [errors]\n *   throttle_time_ms => INT32\n *   errors => topic [partition_errors]\n *     topic => STRING\n *     partition_errors => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errors = await decoder.readArrayAsync(decodeError)\n\n  return {\n    throttleTime,\n    errors,\n  }\n}\n\nconst decodeError = async decoder => ({\n  topic: decoder.readString(),\n  partitionErrors: await decoder.readArrayAsync(decodePartitionError),\n})\n\nconst decodePartitionError = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.errors\n    .map(({ partitionErrors }) => ({\n      partitionsWithErrors: partitionErrors.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AddPartitionsToTxn Request (Version: 1) => transactional_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, topics }) =>\n  Object.assign(\n    requestV0({\n      transactionalId,\n      producerId,\n      producerEpoch,\n      topics,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AddPartitionsToTxn Response (Version: 1) => throttle_time_ms [errors]\n *   throttle_time_ms => INT32\n *   errors => topic [partition_errors]\n *     topic => STRING\n *     partition_errors => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ resources, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources, validateOnly }), response }\n  },\n  1: ({ resources, validateOnly }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ resources, validateOnly }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { AlterConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * AlterConfigs Request (Version: 0) => [resources] validate_only\n *   resources => resource_type resource_name [config_entries]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   validate_only => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of resources to change\n * @param {boolean} [validateOnly=false]\n */\nmodule.exports = ({ resources, validateOnly = false }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'AlterConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(validateOnly)\n  },\n})\n\nconst encodeResource = ({ type, name, configEntries }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * AlterConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n */\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * AlterConfigs Request (Version: 1) => [resources] validate_only\n *   resources => resource_type resource_name [config_entries]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   validate_only => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of resources to change\n * @param {boolean} [validateOnly=false]\n */\nmodule.exports = ({ resources, validateOnly }) =>\n  Object.assign(\n    requestV0({\n      resources,\n      validateOnly,\n    }),\n    { apiVersion: 1 }\n  )\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * AlterConfigs Response (Version: 1) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","module.exports = {\n  Produce: 0,\n  Fetch: 1,\n  ListOffsets: 2,\n  Metadata: 3,\n  LeaderAndIsr: 4,\n  StopReplica: 5,\n  UpdateMetadata: 6,\n  ControlledShutdown: 7,\n  OffsetCommit: 8,\n  OffsetFetch: 9,\n  GroupCoordinator: 10,\n  JoinGroup: 11,\n  Heartbeat: 12,\n  LeaveGroup: 13,\n  SyncGroup: 14,\n  DescribeGroups: 15,\n  ListGroups: 16,\n  SaslHandshake: 17,\n  ApiVersions: 18, // ApiVersions v0 on Kafka 0.10\n  CreateTopics: 19,\n  DeleteTopics: 20,\n  DeleteRecords: 21,\n  InitProducerId: 22,\n  OffsetForLeaderEpoch: 23,\n  AddPartitionsToTxn: 24,\n  AddOffsetsToTxn: 25,\n  EndTxn: 26,\n  WriteTxnMarkers: 27,\n  TxnOffsetCommit: 28,\n  DescribeAcls: 29,\n  CreateAcls: 30,\n  DeleteAcls: 31,\n  DescribeConfigs: 32,\n  AlterConfigs: 33, // ApiVersions v0 and v1 on Kafka 0.11\n  AlterReplicaLogDirs: 34,\n  DescribeLogDirs: 35,\n  SaslAuthenticate: 36,\n  CreatePartitions: 37,\n  CreateDelegationToken: 38,\n  RenewDelegationToken: 39,\n  ExpireDelegationToken: 40,\n  DescribeDelegationToken: 41,\n  DeleteGroups: 42, // ApiVersions v2 on Kafka 1.0\n  ElectPreferredLeaders: 43,\n}\n","const logResponseError = false\n\nconst versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response, logResponseError: true }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response, logResponseError }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response, logResponseError }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ApiVersions: apiKey } = require('../../apiKeys')\n\n/**\n * ApiVersionRequest => ApiKeys\n */\n\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ApiVersions',\n  encode: async () => new Encoder(),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * ApiVersionResponse => ApiVersions\n *   ErrorCode = INT16\n *   ApiVersions = [ApiVersion]\n *     ApiVersion = ApiKey MinVersion MaxVersion\n *       ApiKey = INT16\n *       MinVersion = INT16\n *       MaxVersion = INT16\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    apiVersions: decoder.readArray(apiVersion),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * ApiVersions Response (Version: 1) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nconst apiVersion = decoder => ({\n  apiKey: decoder.readInt16(),\n  minVersion: decoder.readInt16(),\n  maxVersion: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const apiVersions = decoder.readArray(apiVersion)\n\n  /**\n   * The Java client defaults this value to 0 if not present,\n   * even though it is required in the protocol. This is to\n   * work around https://github.com/tulios/kafkajs/issues/491\n   *\n   * See:\n   * https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java#L23-L25\n   */\n  const throttleTime = decoder.canReadInt32() ? decoder.readInt32() : 0\n\n  return {\n    errorCode,\n    apiVersions,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV0 = require('../v0/request')\n\n// ApiVersions Request after v1 indicates the client can parse throttle_time_ms\n\nmodule.exports = () => ({ ...requestV0(), apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ApiVersions Response (Version: 2) => error_code [api_versions] throttle_time_ms\n *   error_code => INT16\n *   api_versions => api_key min_version max_version\n *     api_key => INT16\n *     min_version => INT16\n *     max_version => INT16\n *   throttle_time_ms => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ creations }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ creations }), response }\n  },\n  1: ({ creations }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ creations }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreateAcls: apiKey } = require('../../apiKeys')\n\n/**\n * CreateAcls Request (Version: 0) => [creations]\n *   creations => resource_type resource_name principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => STRING\n *     principal => STRING\n *     host => STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeCreations = ({\n  resourceType,\n  resourceName,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ creations }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreateAcls',\n  encode: async () => {\n    return new Encoder().writeArray(creations.map(encodeCreations))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * CreateAcls Response (Version: 0) => throttle_time_ms [creation_responses]\n *   throttle_time_ms => INT32\n *   creation_responses => error_code error_message\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decodeCreationResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const creationResponses = decoder.readArray(decodeCreationResponse)\n\n  return {\n    throttleTime,\n    creationResponses,\n  }\n}\n\nconst parse = async data => {\n  const creationResponsesWithError = data.creationResponses.filter(({ errorCode }) =>\n    failure(errorCode)\n  )\n\n  if (creationResponsesWithError.length > 0) {\n    throw createErrorFromCode(creationResponsesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { CreateAcls: apiKey } = require('../../apiKeys')\n\n/**\n * CreateAcls Request (Version: 1) => [creations]\n *   creations => resource_type resource_name resource_pattern_type principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => STRING\n *     resource_pattern_type => INT8\n *     principal => STRING\n *     host => STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeCreations = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeInt8(resourcePatternType)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ creations }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'CreateAcls',\n  encode: async () => {\n    return new Encoder().writeArray(creations.map(encodeCreations))\n  },\n})\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreateAcls Response (Version: 1) => throttle_time_ms [creation_responses]\n *   throttle_time_ms => INT32\n *   creation_responses => error_code error_message\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topicPartitions, timeout, validateOnly }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topicPartitions, timeout, validateOnly }), response }\n  },\n  1: ({ topicPartitions, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topicPartitions, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreatePartitions: apiKey } = require('../../apiKeys')\n\n/**\n * CreatePartitions Request (Version: 0) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreatePartitions',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topicPartitions.map(encodeTopicPartitions))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopicPartitions = ({ topic, count, assignments = [] }) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(count)\n    .writeNullableArray(assignments.map(encodeAssignments))\n}\n\nconst encodeAssignments = brokerIds => {\n  return new Encoder().writeNullableArray(brokerIds)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/*\n * CreatePartitions Response (Version: 0) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  return {\n    throttleTime,\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * CreatePartitions Request (Version: 1) => [topic_partitions] timeout validate_only\n *   topic_partitions => topic new_partitions\n *     topic => STRING\n *     new_partitions => count [assignment]\n *       count => INT32\n *       assignment => ARRAY(INT32)\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topicPartitions, validateOnly, timeout }) =>\n  Object.assign(requestV0({ topicPartitions, validateOnly, timeout }), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreatePartitions Response (Version: 0) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n  2: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n  3: ({ topics, validateOnly, timeout }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ topics, validateOnly, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n * CreateTopics Request (Version: 0) => [create_topic_requests] timeout\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics.map(encodeTopics)).writeInt32(timeout)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst { KafkaJSAggregateError, KafkaJSCreateTopicError } = require('../../../../errors')\n\n/**\n * CreateTopics Response (Version: 0) => [topic_errors]\n *   topic_errors => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw new KafkaJSAggregateError(\n      'Topic creation errors',\n      topicsWithError.map(\n        error => new KafkaJSCreateTopicError(createErrorFromCode(error.errorCode), error.topic)\n      )\n    )\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { CreateTopics: apiKey } = require('../../apiKeys')\n\n/**\n *CreateTopics Request (Version: 1) => [create_topic_requests] timeout validate_only\n *  create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *    topic => STRING\n *    num_partitions => INT32\n *    replication_factor => INT16\n *    replica_assignment => partition [replicas]\n *      partition => INT32\n *      replicas => INT32\n *    config_entries => config_name config_value\n *      config_name => STRING\n *      config_value => NULLABLE_STRING\n *  timeout => INT32\n *  validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly = false, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'CreateTopics',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(topics.map(encodeTopics))\n      .writeInt32(timeout)\n      .writeBoolean(validateOnly)\n  },\n})\n\nconst encodeTopics = ({\n  topic,\n  numPartitions = 1,\n  replicationFactor = 1,\n  replicaAssignment = [],\n  configEntries = [],\n}) => {\n  return new Encoder()\n    .writeString(topic)\n    .writeInt32(numPartitions)\n    .writeInt16(replicationFactor)\n    .writeArray(replicaAssignment.map(encodeReplicaAssignment))\n    .writeArray(configEntries.map(encodeConfigEntries))\n}\n\nconst encodeReplicaAssignment = ({ partition, replicas }) => {\n  return new Encoder().writeInt32(partition).writeArray(replicas)\n}\n\nconst encodeConfigEntries = ({ name, value }) => {\n  return new Encoder().writeString(name).writeString(value)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * CreateTopics Response (Version: 1) => [topic_errors]\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * CreateTopics Request (Version: 2) => [create_topic_requests] timeout validate_only\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly, timeout }) =>\n  Object.assign(requestV1({ topics, validateOnly, timeout }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\n\n/**\n * CreateTopics Response (Version: 2) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * CreateTopics Request (Version: 3) => [create_topic_requests] timeout validate_only\n *   create_topic_requests => topic num_partitions replication_factor [replica_assignment] [config_entries]\n *     topic => STRING\n *     num_partitions => INT32\n *     replication_factor => INT16\n *     replica_assignment => partition [replicas]\n *       partition => INT32\n *       replicas => INT32\n *     config_entries => config_name config_value\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *   timeout => INT32\n *   validate_only => BOOLEAN\n */\n\nmodule.exports = ({ topics, validateOnly, timeout }) =>\n  Object.assign(requestV2({ topics, validateOnly, timeout }), { apiVersion: 3 })\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * Starting in version 3, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * CreateTopics Response (Version: 3) => throttle_time_ms [topic_errors]\n *   throttle_time_ms => INT32\n *   topic_errors => topic error_code error_message\n *     topic => STRING\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ filters }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ filters }), response }\n  },\n  1: ({ filters }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ filters }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteAcls Request (Version: 0) => [filters]\n *   filters => resource_type resource_name principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => NULLABLE_STRING\n *     principal => NULLABLE_STRING\n *     host => NULLABLE_STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeFilters = ({\n  resourceType,\n  resourceName,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ filters }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteAcls',\n  encode: async () => {\n    return new Encoder().writeArray(filters.map(encodeFilters))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteAcls Response (Version: 0) => throttle_time_ms [filter_responses]\n *   throttle_time_ms => INT32\n *   filter_responses => error_code error_message [matching_acls]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     matching_acls => error_code error_message resource_type resource_name principal host operation permission_type\n *       error_code => INT16\n *       error_message => NULLABLE_STRING\n *       resource_type => INT8\n *       resource_name => STRING\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeMatchingAcls = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeFilterResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  matchingAcls: decoder.readArray(decodeMatchingAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const filterResponses = decoder.readArray(decodeFilterResponse)\n\n  return {\n    throttleTime,\n    filterResponses,\n  }\n}\n\nconst parse = async data => {\n  const filterResponsesWithError = data.filterResponses.filter(({ errorCode }) =>\n    failure(errorCode)\n  )\n\n  if (filterResponsesWithError.length > 0) {\n    throw createErrorFromCode(filterResponsesWithError[0].errorCode)\n  }\n\n  for (const filterResponse of data.filterResponses) {\n    const matchingAcls = filterResponse.matchingAcls\n    const matchingAclsWithError = matchingAcls.filter(({ errorCode }) => failure(errorCode))\n\n    if (matchingAclsWithError.length > 0) {\n      throw createErrorFromCode(matchingAclsWithError[0].errorCode)\n    }\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decodeMatchingAcls,\n  decodeFilterResponse,\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteAcls Request (Version: 1) => [filters]\n *   filters => resource_type resource_name resource_pattern_type_filter principal host operation permission_type\n *     resource_type => INT8\n *     resource_name => NULLABLE_STRING\n *     resource_pattern_type_filter => INT8\n *     principal => NULLABLE_STRING\n *     host => NULLABLE_STRING\n *     operation => INT8\n *     permission_type => INT8\n */\n\nconst encodeFilters = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => {\n  return new Encoder()\n    .writeInt8(resourceType)\n    .writeString(resourceName)\n    .writeInt8(resourcePatternType)\n    .writeString(principal)\n    .writeString(host)\n    .writeInt8(operation)\n    .writeInt8(permissionType)\n}\n\nmodule.exports = ({ filters }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DeleteAcls',\n  encode: async () => {\n    return new Encoder().writeArray(filters.map(encodeFilters))\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n * Version 1 also introduces a new resource pattern type field.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs\n *\n * DeleteAcls Response (Version: 1) => throttle_time_ms [filter_responses]\n *   throttle_time_ms => INT32\n *   filter_responses => error_code error_message [matching_acls]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     matching_acls => error_code error_message resource_type resource_name resource_pattern_type principal host operation permission_type\n *       error_code => INT16\n *       error_message => NULLABLE_STRING\n *       resource_type => INT8\n *       resource_name => STRING\n *       resource_pattern_type => INT8\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeMatchingAcls = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  resourcePatternType: decoder.readInt8(),\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeFilterResponse = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  matchingAcls: decoder.readArray(decodeMatchingAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const filterResponses = decoder.readArray(decodeFilterResponse)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    filterResponses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: groupIds => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(groupIds), response }\n  },\n  1: groupIds => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(groupIds), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteGroups Request (Version: 0) => [groups_names]\n *   groups_names => STRING\n */\n\n/**\n */\nmodule.exports = groupIds => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds.map(encodeGroups))\n  },\n})\n\nconst encodeGroups = group => {\n  return new Encoder().writeString(group)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n/**\n * DeleteGroups Response (Version: 0) => throttle_time_ms [results]\n *  throttle_time_ms => INT32\n *  results => group_id error_code\n *    group_id => STRING\n *    error_code => INT16\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTimeMs = decoder.readInt32()\n  const results = decoder.readArray(decodeGroup)\n\n  for (const result of results) {\n    if (failure(result.errorCode)) {\n      result.error = createErrorFromCode(result.errorCode)\n    }\n  }\n  return {\n    throttleTimeMs,\n    results,\n  }\n}\n\nconst parse = async data => {\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteGroups Request (Version: 1)\n */\n\nmodule.exports = groupIds => Object.assign(requestV0(groupIds), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteGroups Response (Version: 1) => throttle_time_ms [results]\n *  throttle_time_ms => INT32\n *  results => group_id error_code\n *    group_id => STRING\n *    error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response: response({ topics }) }\n  },\n  1: ({ topics, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, timeout }), response: response({ topics }) }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteRecords: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteRecords Request (Version: 0) => [topics] timeout_ms\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset\n *       partition => INT32\n *       offset => INT64\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteRecords',\n  encode: async () => {\n    return new Encoder()\n      .writeArray(\n        topics.map(({ topic, partitions }) => {\n          return new Encoder().writeString(topic).writeArray(\n            partitions.map(({ partition, offset }) => {\n              return new Encoder().writeInt32(partition).writeInt64(offset)\n            })\n          )\n        })\n      )\n      .writeInt32(timeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { KafkaJSDeleteTopicRecordsError } = require('../../../../errors')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteRecords Response (Version: 0) => throttle_time_ms [topics]\n *  throttle_time_ms => INT32\n *  topics => name [partitions]\n *    name => STRING\n *    partitions => partition low_watermark error_code\n *      partition => INT32\n *      low_watermark => INT64\n *      error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    topics: decoder\n      .readArray(decoder => ({\n        topic: decoder.readString(),\n        partitions: decoder.readArray(decoder => ({\n          partition: decoder.readInt32(),\n          lowWatermark: decoder.readInt64(),\n          errorCode: decoder.readInt16(),\n        })),\n      }))\n      .sort(topicNameComparator),\n  }\n}\n\nconst parse = requestTopics => async data => {\n  const topicsWithErrors = data.topics\n    .map(({ partitions }) => ({\n      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    // at present we only ever request one topic at a time, so can destructure the arrays\n    const [{ topic }] = data.topics // topic name\n    const [{ partitions: requestPartitions }] = requestTopics // requested offset(s)\n    const [{ partitionsWithErrors }] = topicsWithErrors // partition(s) + error(s)\n\n    throw new KafkaJSDeleteTopicRecordsError({\n      topic,\n      partitions: partitionsWithErrors.map(({ partition, errorCode }) => ({\n        partition,\n        error: createErrorFromCode(errorCode),\n        // attach the original offset from the request, onto the error response\n        offset: requestPartitions.find(p => p.partition === partition).offset,\n      })),\n    })\n  }\n\n  return data\n}\n\nmodule.exports = ({ topics }) => ({\n  decode,\n  parse: parse(topics),\n})\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteRecords Request (Version: 1) => [topics] timeout_ms\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset\n *       partition => INT32\n *       offset => INT64\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout }) =>\n  Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteRecords Response (Version: 1) => throttle_time_ms [topics]\n *  throttle_time_ms => INT32\n *  topics => name [partitions]\n *    name => STRING\n *    partitions => partition_index low_watermark error_code\n *      partition_index => INT32\n *      low_watermark => INT64\n *      error_code => INT16\n */\n\nmodule.exports = ({ topics }) => {\n  const { parse, decode: decodeV0 } = responseV0({ topics })\n\n  const decode = async rawData => {\n    const decoded = await decodeV0(rawData)\n\n    return {\n      ...decoded,\n      throttleTime: 0,\n      clientSideThrottleTime: decoded.throttleTime,\n    }\n  }\n\n  return {\n    decode,\n    parse,\n  }\n}\n","const versions = {\n  0: ({ topics, timeout }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics, timeout }), response }\n  },\n  1: ({ topics, timeout }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics, timeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DeleteTopics: apiKey } = require('../../apiKeys')\n\n/**\n * DeleteTopics Request (Version: 0) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\nmodule.exports = ({ topics, timeout = 5000 }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DeleteTopics',\n  encode: async () => {\n    return new Encoder().writeArray(topics).writeInt32(timeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DeleteTopics Response (Version: 0) => [topic_error_codes]\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode))\n  if (topicsWithError.length > 0) {\n    throw createErrorFromCode(topicsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DeleteTopics Request (Version: 1) => [topics] timeout\n *   topics => STRING\n *   timeout => INT32\n */\n\nmodule.exports = ({ topics, timeout }) =>\n  Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DeleteTopics Response (Version: 1) => throttle_time_ms [topic_error_codes]\n *   throttle_time_ms => INT32\n *   topic_error_codes => topic error_code\n *     topic => STRING\n *     error_code => INT16\n */\n\nconst topicNameComparator = (a, b) => a.topic.localeCompare(b.topic)\n\nconst topicErrors = decoder => ({\n  topic: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ resourceType, resourceName, principal, host, operation, permissionType }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ resourceType, resourceName, principal, host, operation, permissionType }),\n      response,\n    }\n  },\n  1: ({\n    resourceType,\n    resourceName,\n    resourcePatternType,\n    principal,\n    host,\n    operation,\n    permissionType,\n  }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({\n        resourceType,\n        resourceName,\n        resourcePatternType,\n        principal,\n        host,\n        operation,\n        permissionType,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeAcls Request (Version: 0) => resource_type resource_name principal host operation permission_type\n *   resource_type => INT8\n *   resource_name => NULLABLE_STRING\n *   principal => NULLABLE_STRING\n *   host => NULLABLE_STRING\n *   operation => INT8\n *   permission_type => INT8\n */\n\nmodule.exports = ({ resourceType, resourceName, principal, host, operation, permissionType }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeAcls',\n  encode: async () => {\n    return new Encoder()\n      .writeInt8(resourceType)\n      .writeString(resourceName)\n      .writeString(principal)\n      .writeString(host)\n      .writeInt8(operation)\n      .writeInt8(permissionType)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeAcls Response (Version: 0) => throttle_time_ms error_code error_message [resources]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   resources => resource_type resource_name [acls]\n *     resource_type => INT8\n *     resource_name => STRING\n *     acls => principal host operation permission_type\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\n\nconst decodeAcls = decoder => ({\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeResources = decoder => ({\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  acls: decoder.readArray(decodeAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    errorCode,\n    errorMessage,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeAcls: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeAcls Request (Version: 1) => resource_type resource_name resource_pattern_type_filter principal host operation permission_type\n *   resource_type => INT8\n *   resource_name => NULLABLE_STRING\n *   resource_pattern_type_filter => INT8\n *   principal => NULLABLE_STRING\n *   host => NULLABLE_STRING\n *   operation => INT8\n *   permission_type => INT8\n */\n\nmodule.exports = ({\n  resourceType,\n  resourceName,\n  resourcePatternType,\n  principal,\n  host,\n  operation,\n  permissionType,\n}) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DescribeAcls',\n  encode: async () => {\n    return new Encoder()\n      .writeInt8(resourceType)\n      .writeString(resourceName)\n      .writeInt8(resourcePatternType)\n      .writeString(principal)\n      .writeString(host)\n      .writeInt8(operation)\n      .writeInt8(permissionType)\n  },\n})\n","const { parse } = require('../v0/response')\nconst Decoder = require('../../../decoder')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n * Version 1 also introduces a new resource pattern type field.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+Prefixed+ACLs\n *\n * DescribeAcls Response (Version: 1) => throttle_time_ms error_code error_message [resources]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   resources => resource_type resource_name resource_pattern_type [acls]\n *     resource_type => INT8\n *     resource_name => STRING\n *     resource_pattern_type => INT8\n *     acls => principal host operation permission_type\n *       principal => STRING\n *       host => STRING\n *       operation => INT8\n *       permission_type => INT8\n */\nconst decodeAcls = decoder => ({\n  principal: decoder.readString(),\n  host: decoder.readString(),\n  operation: decoder.readInt8(),\n  permissionType: decoder.readInt8(),\n})\n\nconst decodeResources = decoder => ({\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  resourcePatternType: decoder.readInt8(),\n  acls: decoder.readArray(decodeAcls),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    errorCode,\n    errorMessage,\n    resources,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ resources }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ resources }), response }\n  },\n  1: ({ resources, includeSynonyms }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ resources, includeSynonyms }), response }\n  },\n  2: ({ resources, includeSynonyms }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ resources, includeSynonyms }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 0) => [resources]\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n */\nmodule.exports = ({ resources }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource))\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst ConfigSource = require('../../../configSource')\nconst ConfigResourceTypes = require('../../../configResourceTypes')\n\n/**\n * DescribeConfigs Response (Version: 0) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only is_default is_sensitive\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       is_default => BOOLEAN\n *       is_sensitive => BOOLEAN\n */\n\nconst decodeConfigEntries = (decoder, resourceType) => {\n  const configName = decoder.readString()\n  const configValue = decoder.readString()\n  const readOnly = decoder.readBoolean()\n  const isDefault = decoder.readBoolean()\n  const isSensitive = decoder.readBoolean()\n\n  /**\n   * Backporting ConfigSource value to v0\n   * @see https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java#L232-L242\n   */\n  let configSource\n  if (isDefault) {\n    configSource = ConfigSource.DEFAULT_CONFIG\n  } else {\n    switch (resourceType) {\n      case ConfigResourceTypes.BROKER:\n        configSource = ConfigSource.STATIC_BROKER_CONFIG\n        break\n      case ConfigResourceTypes.TOPIC:\n        configSource = ConfigSource.TOPIC_CONFIG\n        break\n      default:\n        configSource = ConfigSource.UNKNOWN\n    }\n  }\n\n  return {\n    configName,\n    configValue,\n    readOnly,\n    isDefault,\n    configSource,\n    isSensitive,\n  }\n}\n\nconst decodeResources = decoder => {\n  const errorCode = decoder.readInt16()\n  const errorMessage = decoder.readString()\n  const resourceType = decoder.readInt8()\n  const resourceName = decoder.readString()\n  const configEntries = decoder.readArray(decoder => decodeConfigEntries(decoder, resourceType))\n\n  return {\n    errorCode,\n    errorMessage,\n    resourceType,\n    resourceName,\n    configEntries,\n  }\n}\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nconst parse = async data => {\n  const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode))\n  if (resourcesWithError.length > 0) {\n    throw createErrorFromCode(resourcesWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeConfigs: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeConfigs Request (Version: 1) => [resources] include_synonyms\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n *   include_synonyms => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n * @param [includeSynonyms=false]\n */\nmodule.exports = ({ resources, includeSynonyms = false }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'DescribeConfigs',\n  encode: async () => {\n    return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(includeSynonyms)\n  },\n})\n\nconst encodeResource = ({ type, name, configNames = [] }) => {\n  return new Encoder()\n    .writeInt8(type)\n    .writeString(name)\n    .writeNullableArray(configNames)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst { DEFAULT_CONFIG } = require('../../../configSource')\n\n/**\n * DescribeConfigs Response (Version: 1) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only config_source is_sensitive [config_synonyms]\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       config_source => INT8\n *       is_sensitive => BOOLEAN\n *       config_synonyms => config_name config_value config_source\n *         config_name => STRING\n *         config_value => NULLABLE_STRING\n *         config_source => INT8\n */\n\nconst decodeSynonyms = decoder => ({\n  configName: decoder.readString(),\n  configValue: decoder.readString(),\n  configSource: decoder.readInt8(),\n})\n\nconst decodeConfigEntries = decoder => {\n  const configName = decoder.readString()\n  const configValue = decoder.readString()\n  const readOnly = decoder.readBoolean()\n  const configSource = decoder.readInt8()\n  const isSensitive = decoder.readBoolean()\n  const configSynonyms = decoder.readArray(decodeSynonyms)\n\n  return {\n    configName,\n    configValue,\n    readOnly,\n    isDefault: configSource === DEFAULT_CONFIG,\n    configSource,\n    isSensitive,\n    configSynonyms,\n  }\n}\n\nconst decodeResources = decoder => ({\n  errorCode: decoder.readInt16(),\n  errorMessage: decoder.readString(),\n  resourceType: decoder.readInt8(),\n  resourceName: decoder.readString(),\n  configEntries: decoder.readArray(decodeConfigEntries),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const resources = decoder.readArray(decodeResources)\n\n  return {\n    throttleTime,\n    resources,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * DescribeConfigs Request (Version: 1) => [resources] include_synonyms\n *   resources => resource_type resource_name [config_names]\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_names => STRING\n *   include_synonyms => BOOLEAN\n */\n\n/**\n * @param {Array} resources An array of config resources to be returned\n * @param [includeSynonyms=false]\n */\nmodule.exports = ({ resources, includeSynonyms }) =>\n  Object.assign(requestV1({ resources, includeSynonyms }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DescribeConfigs Response (Version: 2) => throttle_time_ms [resources]\n *   throttle_time_ms => INT32\n *   resources => error_code error_message resource_type resource_name [config_entries]\n *     error_code => INT16\n *     error_message => NULLABLE_STRING\n *     resource_type => INT8\n *     resource_name => STRING\n *     config_entries => config_name config_value read_only config_source is_sensitive [config_synonyms]\n *       config_name => STRING\n *       config_value => NULLABLE_STRING\n *       read_only => BOOLEAN\n *       config_source => INT8\n *       is_sensitive => BOOLEAN\n *       config_synonyms => config_name config_value config_source\n *         config_name => STRING\n *         config_value => NULLABLE_STRING\n *         config_source => INT8\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupIds }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupIds }), response }\n  },\n  1: ({ groupIds }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupIds }), response }\n  },\n  2: ({ groupIds }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ groupIds }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { DescribeGroups: apiKey } = require('../../apiKeys')\n\n/**\n * DescribeGroups Request (Version: 0) => [group_ids]\n *   group_ids => STRING\n */\n\n/**\n * @param {Array} groupIds List of groupIds to request metadata for (an empty groupId array will return empty group metadata)\n */\nmodule.exports = ({ groupIds }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'DescribeGroups',\n  encode: async () => {\n    return new Encoder().writeArray(groupIds)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * DescribeGroups Response (Version: 0) => [groups]\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    groups,\n  }\n}\n\nconst parse = async data => {\n  const groupsWithError = data.groups.filter(({ errorCode }) => failure(errorCode))\n  if (groupsWithError.length > 0) {\n    throw createErrorFromCode(groupsWithError[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * DescribeGroups Request (Version: 1) => [group_ids]\n *   group_ids => STRING\n */\n\nmodule.exports = ({ groupIds }) => Object.assign(requestV0({ groupIds }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * DescribeGroups Response (Version: 1) => throttle_time_ms [groups]\n *   throttle_time_ms => INT32\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decoderMember = decoder => ({\n  memberId: decoder.readString(),\n  clientId: decoder.readString(),\n  clientHost: decoder.readString(),\n  memberMetadata: decoder.readBytes(),\n  memberAssignment: decoder.readBytes(),\n})\n\nconst decodeGroup = decoder => ({\n  errorCode: decoder.readInt16(),\n  groupId: decoder.readString(),\n  state: decoder.readString(),\n  protocolType: decoder.readString(),\n  protocol: decoder.readString(),\n  members: decoder.readArray(decoderMember),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    throttleTime,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * DescribeGroups Request (Version: 2) => [group_ids]\n *   group_ids => STRING\n */\n\nmodule.exports = ({ groupIds }) => Object.assign(requestV1({ groupIds }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * DescribeGroups Response (Version: 2) => throttle_time_ms [groups]\n *   throttle_time_ms => INT32\n *   groups => error_code group_id state protocol_type protocol [members]\n *     error_code => INT16\n *     group_id => STRING\n *     state => STRING\n *     protocol_type => STRING\n *     protocol => STRING\n *     members => member_id client_id client_host member_metadata member_assignment\n *       member_id => STRING\n *       client_id => STRING\n *       client_host => STRING\n *       member_metadata => BYTES\n *       member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, producerId, producerEpoch, transactionResult }),\n      response,\n    }\n  },\n  1: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ transactionalId, producerId, producerEpoch, transactionResult }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { EndTxn: apiKey } = require('../../apiKeys')\n\n/**\n * EndTxn Request (Version: 0) => transactional_id producer_id producer_epoch transaction_result\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   transaction_result => BOOLEAN\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'EndTxn',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeBoolean(transactionResult)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * EndTxn Response (Version: 0) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * EndTxn Request (Version: 1) => transactional_id producer_id producer_epoch transaction_result\n *   transactional_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   transaction_result => BOOLEAN\n */\n\nmodule.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) =>\n  Object.assign(requestV0({ transactionalId, producerId, producerEpoch, transactionResult }), {\n    apiVersion: 1,\n  })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * EndTxn Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\nconst NETWORK_DELAY = 100\n\n/**\n * The FETCH request can block up to maxWaitTime, which can be bigger than the configured\n * request timeout. It's safer to always use the maxWaitTime\n **/\nconst requestTimeout = timeout =>\n  Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  1: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  2: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  3: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, maxBytes, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ replicaId, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  4: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  5: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  6: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  7: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  8: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v8/request')\n    const response = require('./v8/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  9: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v9/request')\n    const response = require('./v9/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  10: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n  }) => {\n    const request = require('./v10/request')\n    const response = require('./v10/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n  11: ({\n    replicaId = REPLICA_ID,\n    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n    sessionId = 0,\n    sessionEpoch = -1,\n    forgottenTopics = [],\n    maxWaitTime,\n    minBytes,\n    maxBytes,\n    topics,\n    rackId,\n  }) => {\n    const request = require('./v11/request')\n    const response = require('./v11/response')\n    return {\n      request: request({\n        replicaId,\n        isolationLevel,\n        sessionId,\n        sessionEpoch,\n        forgottenTopics,\n        maxWaitTime,\n        minBytes,\n        maxBytes,\n        topics,\n        rackId,\n      }),\n      response,\n      requestTimeout: requestTimeout(maxWaitTime),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 0) => replica_id max_wait_time min_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { KafkaJSOffsetOutOfRange } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    responses,\n  }\n}\n\nconst { code: OFFSET_OUT_OF_RANGE_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'OFFSET_OUT_OF_RANGE'\n)\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(({ topicName, partitions }) => {\n    return partitions\n      .filter(partition => failure(partition.errorCode))\n      .map(partition => Object.assign({}, partition, { topic: topicName }))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode, topic, partition } = errors[0]\n    if (errorCode === OFFSET_OUT_OF_RANGE_ERROR_CODE) {\n      throw new KafkaJSOffsetOutOfRange(createErrorFromCode(errorCode), { topic, partition })\n    }\n\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\n\n/**\n * Fetch Response (Version: 1) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  messages: await MessageSetDecoder(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV9 = require('../v9/request')\n\n/**\n * ZStd Compression\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression\n */\n\n/**\n * Fetch Request (Version: 10) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV9({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 10 }\n  )\n","const { decode, parse } = require('../v9/response')\n\n/**\n * Fetch Response (Version: 10) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow consumers to fetch from closest replica\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica\n */\n\n/**\n * Fetch Request (Version: 11) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n *   rack_id => STRING\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  rackId = '',\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 11,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n      .writeString(rackId)\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 11) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *         preferred_read_replica => INT32\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  preferredReadReplica: decoder.readInt32(),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {\n  return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 2 })\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 2) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *      record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\n\n/**\n * Fetch Request (Version: 3) => replica_id max_wait_time min_bytes max_bytes [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\n/**\n * @param {number} replicaId Broker id of the follower\n * @param {number} maxWaitTime Maximum time in ms to wait for the response\n * @param {number} minBytes Minimum bytes to accumulate in the response.\n * @param {number} maxBytes Maximum bytes to accumulate in the response. Note that this is not an absolute maximum,\n *                          if the first message in the first non-empty partition of the fetch is larger than this value,\n *                          the message will still be returned to ensure that progress can be made.\n * @param {Array} topics Topics to fetch\n *                        [\n *                          {\n *                            topic: 'topic-name',\n *                            partitions: [\n *                              {\n *                                partition: 0,\n *                                fetchOffset: '4124',\n *                                maxBytes: 2048\n *                              }\n *                            ]\n *                          }\n *                        ]\n */\nmodule.exports = ({ replicaId, maxWaitTime, minBytes, maxBytes, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v1/response')\n\n/**\n * Fetch Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Decoder = require('../../../decoder')\nconst MessageSetDecoder = require('../../../messageSet/decoder')\nconst RecordBatchDecoder = require('../../../recordBatch/v0/decoder')\nconst { MAGIC_BYTE } = require('../../../recordBatch/v0')\n\n// the magic offset is at the same offset for all current message formats, but the 4 bytes\n// between the size and the magic is dependent on the version.\nconst MAGIC_OFFSET = 16\nconst RECORD_BATCH_OVERHEAD = 49\n\nconst decodeMessages = async decoder => {\n  const messagesSize = decoder.readInt32()\n\n  if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {\n    return []\n  }\n\n  const messagesBuffer = decoder.readBytes(messagesSize)\n  const messagesDecoder = new Decoder(messagesBuffer)\n  const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0)\n\n  if (magicByte === MAGIC_BYTE) {\n    const records = []\n\n    while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {\n      try {\n        const recordBatch = await RecordBatchDecoder(messagesDecoder)\n        records.push(...recordBatch.records)\n      } catch (e) {\n        // The tail of the record batches can have incomplete records\n        // due to how maxBytes works. See https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI\n        if (e.name === 'KafkaJSPartialMessageError') {\n          break\n        }\n\n        throw e\n      }\n    }\n\n    return records\n  }\n\n  return MessageSetDecoder(messagesDecoder, messagesSize)\n}\n\nmodule.exports = decodeMessages\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 4) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('./decodeMessages')\n\n/**\n * Fetch Response (Version: 4) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Fetch Request (Version: 5) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 5) => throttle_time_ms [responses]\n *  throttle_time_ms => INT32\n *  responses => topic [partition_responses]\n *    topic => STRING\n *    partition_responses => partition_header record_set\n *      partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *        partition => INT32\n *        error_code => INT16\n *        high_watermark => INT64\n *        last_stable_offset => INT64\n *        log_start_offset => INT64\n *        aborted_transactions => producer_id first_offset\n *          producer_id => INT64\n *          first_offset => INT64\n *      record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV5 = require('../v5/request')\n\n/**\n * Fetch Request (Version: 6) => replica_id max_wait_time min_bytes max_bytes isolation_level [topics]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n}) =>\n  Object.assign(\n    requestV5({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n    }),\n    { apiVersion: 6 }\n  )\n","const { decode, parse } = require('../v5/response')\n\n/**\n * Fetch Response (Version: 6) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Sessions are only used by followers\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability\n */\n\n/**\n * Fetch Request (Version: 7) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 7,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 7) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  return {\n    throttleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const ISOLATION_LEVEL = require('../../../isolationLevel')\nconst requestV7 = require('../v7/request')\n\n/**\n * Quota violation brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n */\n\n/**\n * Fetch Request (Version: 8) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) =>\n  Object.assign(\n    requestV7({\n      replicaId,\n      maxWaitTime,\n      minBytes,\n      maxBytes,\n      topics,\n      isolationLevel,\n      sessionId,\n      sessionEpoch,\n      forgottenTopics,\n    }),\n    { apiVersion: 8 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV1 } = require('../v1/response')\nconst decodeMessages = require('../v4/decodeMessages')\n\n/**\n * Fetch Response (Version: 8) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nconst decodeAbortedTransactions = decoder => ({\n  producerId: decoder.readInt64().toString(),\n  firstOffset: decoder.readInt64().toString(),\n})\n\nconst decodePartition = async decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  highWatermark: decoder.readInt64().toString(),\n  lastStableOffset: decoder.readInt64().toString(),\n  lastStartOffset: decoder.readInt64().toString(),\n  abortedTransactions: decoder.readArray(decodeAbortedTransactions),\n  messages: await decodeMessages(decoder),\n})\n\nconst decodeResponse = async decoder => ({\n  topicName: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const clientSideThrottleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const sessionId = decoder.readInt32()\n  const responses = await decoder.readArrayAsync(decodeResponse)\n\n  // Report a `throttleTime` of 0: The broker will not have throttled\n  // this request, but if the `clientSideThrottleTime` is >0 then it\n  // expects us to do that -- and it will ignore requests.\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime,\n    errorCode,\n    sessionId,\n    responses,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV1,\n}\n","const Encoder = require('../../../encoder')\nconst { Fetch: apiKey } = require('../../apiKeys')\nconst ISOLATION_LEVEL = require('../../../isolationLevel')\n\n/**\n * Allow fetchers to detect and handle log truncation\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation\n */\n\n/**\n * Fetch Request (Version: 9) => replica_id max_wait_time min_bytes max_bytes isolation_level session_id session_epoch [topics] [forgotten_topics_data]\n *   replica_id => INT32\n *   max_wait_time => INT32\n *   min_bytes => INT32\n *   max_bytes => INT32\n *   isolation_level => INT8\n *   session_id => INT32\n *   session_epoch => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition current_leader_epoch fetch_offset log_start_offset partition_max_bytes\n *       partition => INT32\n *       current_leader_epoch => INT32\n *       fetch_offset => INT64\n *       log_start_offset => INT64\n *       partition_max_bytes => INT32\n *   forgotten_topics_data => topic [partitions]\n *     topic => STRING\n *     partitions => INT32\n */\n\nmodule.exports = ({\n  replicaId,\n  maxWaitTime,\n  minBytes,\n  maxBytes,\n  topics,\n  isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,\n  sessionId = 0,\n  sessionEpoch = -1,\n  forgottenTopics = [], // Topics to remove from the fetch session\n}) => ({\n  apiKey,\n  apiVersion: 9,\n  apiName: 'Fetch',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt32(maxWaitTime)\n      .writeInt32(minBytes)\n      .writeInt32(maxBytes)\n      .writeInt8(isolationLevel)\n      .writeInt32(sessionId)\n      .writeInt32(sessionEpoch)\n      .writeArray(topics.map(encodeTopic))\n      .writeArray(forgottenTopics.map(encodeForgottenTopics))\n  },\n})\n\nconst encodeForgottenTopics = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions)\n}\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({\n  partition,\n  currentLeaderEpoch = -1,\n  fetchOffset,\n  logStartOffset = -1,\n  maxBytes,\n}) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(currentLeaderEpoch)\n    .writeInt64(fetchOffset)\n    .writeInt64(logStartOffset)\n    .writeInt32(maxBytes)\n}\n","const { decode, parse } = require('../v8/response')\n\n/**\n * Fetch Response (Version: 9) => throttle_time_ms error_code session_id [responses]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   session_id => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition_header record_set\n *       partition_header => partition error_code high_watermark last_stable_offset log_start_offset [aborted_transactions]\n *         partition => INT32\n *         error_code => INT16\n *         high_watermark => INT64\n *         last_stable_offset => INT64\n *         log_start_offset => INT64\n *         aborted_transactions => producer_id first_offset\n *           producer_id => INT64\n *           first_offset => INT64\n *       record_set => RECORDS\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const COORDINATOR_TYPES = require('../../coordinatorTypes')\n\nconst versions = {\n  0: ({ groupId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId }), response }\n  },\n  1: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ coordinatorKey: groupId, coordinatorType }), response }\n  },\n  2: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ coordinatorKey: groupId, coordinatorType }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 0) => group_id\n *   group_id => STRING\n */\n\nmodule.exports = ({ groupId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(groupId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 0) => error_code coordinator\n *  error_code => INT16\n *  coordinator => node_id host port\n *    node_id => INT32\n *    host => STRING\n *    port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    errorCode,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { GroupCoordinator: apiKey } = require('../../apiKeys')\n\n/**\n * FindCoordinator Request (Version: 1) => coordinator_key coordinator_type\n *   coordinator_key => STRING\n *   coordinator_type => INT8\n */\n\nmodule.exports = ({ coordinatorKey, coordinatorType }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'GroupCoordinator',\n  encode: async () => {\n    return new Encoder().writeString(coordinatorKey).writeInt8(coordinatorType)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * FindCoordinator Response (Version: 1) => throttle_time_ms error_code error_message coordinator\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   coordinator => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  const errorMessage = decoder.readString()\n  const coordinator = {\n    nodeId: decoder.readInt32(),\n    host: decoder.readString(),\n    port: decoder.readInt32(),\n  }\n\n  return {\n    throttleTime,\n    errorCode,\n    errorMessage,\n    coordinator,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * FindCoordinator Request (Version: 2) => coordinator_key coordinator_type\n *   coordinator_key => STRING\n *   coordinator_type => INT8\n */\n\nmodule.exports = ({ coordinatorKey, coordinatorType }) =>\n  Object.assign(requestV1({ coordinatorKey, coordinatorType }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * Starting in version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * FindCoordinator Response (Version: 1) => throttle_time_ms error_code error_message coordinator\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   coordinator => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  2: ({ groupId, groupGenerationId, memberId }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId }),\n      response,\n    }\n  },\n  3: ({ groupId, groupGenerationId, memberId, groupInstanceId }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, groupGenerationId, memberId, groupInstanceId }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Heartbeat: apiKey } = require('../../apiKeys')\n\n/**\n * Heartbeat Request (Version: 0) => group_id group_generation_id member_id\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Heartbeat',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * Heartbeat Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * Heartbeat Request (Version: 1) => group_id generation_id member_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) =>\n  Object.assign(requestV0({ groupId, groupGenerationId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Heartbeat Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Heartbeat Request (Version: 2) => group_id generation_id member_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId }) =>\n  Object.assign(requestV1({ groupId, groupGenerationId, memberId }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * Heartbeat Response (Version: 2) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Heartbeat: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 adds group_instance_id to indicate member identity across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * Heartbeat Request (Version: 3) => group_id generation_id member_id group_instance_id\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, groupInstanceId }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Heartbeat',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n  },\n})\n","const { parse, decode } = require('../v2/response')\n\n/**\n * Heartbeat Response (Version: 3) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const apiKeys = require('./apiKeys')\nconst { KafkaJSServerDoesNotSupportApiKey, KafkaJSNotImplemented } = require('../../errors')\n\n/**\n * @typedef {(options?: Object) => { request: any, response: any, logResponseErrors?: boolean }} Request\n */\n\n/**\n * @typedef {Object} RequestDefinitions\n * @property {string[]} versions\n * @property {({ version: number }) => Request} protocol\n */\n\n/**\n * @typedef {(apiKey: number, definitions: RequestDefinitions) => Request} Lookup\n */\n\n/** @type {RequestDefinitions} */\nconst noImplementedRequestDefinitions = {\n  versions: [],\n  protocol: () => {\n    throw new KafkaJSNotImplemented()\n  },\n}\n\n/**\n * @type {{[apiName: string]: RequestDefinitions}}\n */\nconst requests = {\n  Produce: require('./produce'),\n  Fetch: require('./fetch'),\n  ListOffsets: require('./listOffsets'),\n  Metadata: require('./metadata'),\n  LeaderAndIsr: noImplementedRequestDefinitions,\n  StopReplica: noImplementedRequestDefinitions,\n  UpdateMetadata: noImplementedRequestDefinitions,\n  ControlledShutdown: noImplementedRequestDefinitions,\n  OffsetCommit: require('./offsetCommit'),\n  OffsetFetch: require('./offsetFetch'),\n  GroupCoordinator: require('./findCoordinator'),\n  JoinGroup: require('./joinGroup'),\n  Heartbeat: require('./heartbeat'),\n  LeaveGroup: require('./leaveGroup'),\n  SyncGroup: require('./syncGroup'),\n  DescribeGroups: require('./describeGroups'),\n  ListGroups: require('./listGroups'),\n  SaslHandshake: require('./saslHandshake'),\n  ApiVersions: require('./apiVersions'),\n  CreateTopics: require('./createTopics'),\n  DeleteTopics: require('./deleteTopics'),\n  DeleteRecords: require('./deleteRecords'),\n  InitProducerId: require('./initProducerId'),\n  OffsetForLeaderEpoch: noImplementedRequestDefinitions,\n  AddPartitionsToTxn: require('./addPartitionsToTxn'),\n  AddOffsetsToTxn: require('./addOffsetsToTxn'),\n  EndTxn: require('./endTxn'),\n  WriteTxnMarkers: noImplementedRequestDefinitions,\n  TxnOffsetCommit: require('./txnOffsetCommit'),\n  DescribeAcls: require('./describeAcls'),\n  CreateAcls: require('./createAcls'),\n  DeleteAcls: require('./deleteAcls'),\n  DescribeConfigs: require('./describeConfigs'),\n  AlterConfigs: require('./alterConfigs'),\n  AlterReplicaLogDirs: noImplementedRequestDefinitions,\n  DescribeLogDirs: noImplementedRequestDefinitions,\n  SaslAuthenticate: require('./saslAuthenticate'),\n  CreatePartitions: require('./createPartitions'),\n  CreateDelegationToken: noImplementedRequestDefinitions,\n  RenewDelegationToken: noImplementedRequestDefinitions,\n  ExpireDelegationToken: noImplementedRequestDefinitions,\n  DescribeDelegationToken: noImplementedRequestDefinitions,\n  DeleteGroups: require('./deleteGroups'),\n}\n\nconst names = Object.keys(apiKeys)\nconst keys = Object.values(apiKeys)\nconst findApiName = apiKey => names[keys.indexOf(apiKey)]\n\n/**\n * @param {import(\"../../../types\").ApiVersions} versions\n * @returns {Lookup}\n */\nconst lookup = versions => (apiKey, definition) => {\n  const version = versions[apiKey]\n  const availableVersions = definition.versions.map(Number)\n  const bestImplementedVersion = Math.max(...availableVersions)\n\n  if (!version || version.maxVersion == null) {\n    throw new KafkaJSServerDoesNotSupportApiKey(\n      `The Kafka server does not support the requested API version`,\n      { apiKey, apiName: findApiName(apiKey) }\n    )\n  }\n\n  const bestSupportedVersion = Math.min(bestImplementedVersion, version.maxVersion)\n  return definition.protocol({ version: bestSupportedVersion })\n}\n\nmodule.exports = {\n  requests,\n  lookup,\n}\n","const versions = {\n  0: ({ transactionalId, transactionTimeout = 5000 }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ transactionalId, transactionTimeout }), response }\n  },\n  1: ({ transactionalId, transactionTimeout = 5000 }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ transactionalId, transactionTimeout }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { InitProducerId: apiKey } = require('../../apiKeys')\n\n/**\n * InitProducerId Request (Version: 0) => transactional_id transaction_timeout_ms\n *   transactional_id => NULLABLE_STRING\n *   transaction_timeout_ms => INT32\n */\n\nmodule.exports = ({ transactionalId, transactionTimeout }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'InitProducerId',\n  encode: async () => {\n    return new Encoder().writeString(transactionalId).writeInt32(transactionTimeout)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * InitProducerId Response (Version: 0) => throttle_time_ms error_code producer_id producer_epoch\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   producer_id => INT64\n *   producer_epoch => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    producerId: decoder.readInt64().toString(),\n    producerEpoch: decoder.readInt16(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * InitProducerId Request (Version: 1) => transactional_id transaction_timeout_ms\n *   transactional_id => NULLABLE_STRING\n *   transaction_timeout_ms => INT32\n */\n\nmodule.exports = ({ transactionalId, transactionTimeout }) =>\n  Object.assign(requestV0({ transactionalId, transactionTimeout }), { apiVersion: 1 })\n","const { parse, decode: decodeV0 } = require('../v0/response')\n\n/**\n * Starting in version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * InitProducerId Response (Version: 0) => throttle_time_ms error_code producer_id producer_epoch\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   producer_id => INT64\n *   producer_epoch => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV0(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const NETWORK_DELAY = 5000\n\n/**\n * @see https://github.com/apache/kafka/pull/5203\n * The JOIN_GROUP request may block up to sessionTimeout (or rebalanceTimeout in JoinGroupV1),\n * so we should override the requestTimeout to be a bit more than the sessionTimeout\n * NOTE: the sessionTimeout can be configured as Number.MAX_SAFE_INTEGER and overflow when\n * increased, so we have to check for potential overflows\n **/\nconst requestTimeout = ({ rebalanceTimeout, sessionTimeout }) => {\n  const timeout = rebalanceTimeout || sessionTimeout\n  return Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout\n}\n\nconst logResponseError = memberId => memberId != null && memberId !== ''\n\nconst versions = {\n  0: ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout: null, sessionTimeout }),\n    }\n  },\n  1: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  2: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  3: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n    }\n  },\n  4: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n      logResponseError: logResponseError(memberId),\n    }\n  },\n  5: ({\n    groupId,\n    sessionTimeout,\n    rebalanceTimeout,\n    memberId,\n    groupInstanceId,\n    protocolType,\n    groupProtocols,\n  }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n\n    return {\n      request: request({\n        groupId,\n        sessionTimeout,\n        rebalanceTimeout,\n        memberId,\n        groupInstanceId,\n        protocolType,\n        groupProtocols,\n      }),\n      response,\n      requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),\n      logResponseError: logResponseError(memberId),\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 0) => group_id session_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 0) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * JoinGroup Request (Version: 1) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeInt32(rebalanceTimeout)\n      .writeString(memberId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 1) => error_code generation_id group_protocol leader_id member_id [members]\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * JoinGroup Request (Version: 2) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV1({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 2 }\n  )\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * JoinGroup Response (Version: 2) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * JoinGroup Request (Version: 3) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV2({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 3 }\n  )\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * Starting in version 3, on quota violation, brokers send out responses\n * before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * JoinGroup Response (Version: 3) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Starting in version 4, the client needs to issue a second request to join group\n * with assigned id.\n *\n * JoinGroup Request (Version: 4) => group_id session_timeout rebalance_timeout member_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  protocolType,\n  groupProtocols,\n}) =>\n  Object.assign(\n    requestV3({\n      groupId,\n      sessionTimeout,\n      rebalanceTimeout,\n      memberId,\n      protocolType,\n      groupProtocols,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode } = require('../v3/response')\nconst { KafkaJSMemberIdRequired } = require('../../../../errors')\nconst { failure, createErrorFromCode, errorCodes } = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 4) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id member_metadata\n *     member_id => STRING\n *     member_metadata => BYTES\n */\n\nconst { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'MEMBER_ID_REQUIRED'\n)\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {\n      throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {\n        memberId: data.memberId,\n      })\n    }\n\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { JoinGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 5 adds group_instance_id to identify members across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * JoinGroup Request (Version: 5) => group_id session_timeout rebalance_timeout member_id group_instance_id protocol_type [group_protocols]\n *   group_id => STRING\n *   session_timeout => INT32\n *   rebalance_timeout => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n *   protocol_type => STRING\n *   group_protocols => protocol_name protocol_metadata\n *     protocol_name => STRING\n *     protocol_metadata => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  sessionTimeout,\n  rebalanceTimeout,\n  memberId,\n  groupInstanceId = null,\n  protocolType,\n  groupProtocols,\n}) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'JoinGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(sessionTimeout)\n      .writeInt32(rebalanceTimeout)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n      .writeString(protocolType)\n      .writeArray(groupProtocols.map(encodeGroupProtocols))\n  },\n})\n\nconst encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {\n  return new Encoder().writeString(name).writeBytes(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { KafkaJSMemberIdRequired } = require('../../../../errors')\nconst {\n  failure,\n  createErrorFromCode,\n  errorCodes,\n  failIfVersionNotSupported,\n} = require('../../../error')\n\n/**\n * JoinGroup Response (Version: 5) => throttle_time_ms error_code generation_id group_protocol leader_id member_id [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   generation_id => INT32\n *   group_protocol => STRING\n *   leader_id => STRING\n *   member_id => STRING\n *   members => member_id group_instance_id metadata\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n *     member_metadata => BYTES\n */\nconst { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find(\n  e => e.type === 'MEMBER_ID_REQUIRED'\n)\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {\n      throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {\n        memberId: data.memberId,\n      })\n    }\n\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime: 0,\n    clientSideThrottleTime: throttleTime,\n    errorCode,\n    generationId: decoder.readInt32(),\n    groupProtocol: decoder.readString(),\n    leaderId: decoder.readString(),\n    memberId: decoder.readString(),\n    members: decoder.readArray(decoder => ({\n      memberId: decoder.readString(),\n      groupInstanceId: decoder.readString(),\n      memberMetadata: decoder.readBytes(),\n    })),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ groupId, memberId }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  1: ({ groupId, memberId }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  2: ({ groupId, memberId }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, memberId }),\n      response,\n    }\n  },\n  3: ({ groupId, memberId, groupInstanceId }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, members: [{ memberId, groupInstanceId }] }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { LeaveGroup: apiKey } = require('../../apiKeys')\n\n/**\n * LeaveGroup Request (Version: 0) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'LeaveGroup',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeString(memberId)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * LeaveGroup Response (Version: 0) => error_code\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { errorCode }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * LeaveGroup Request (Version: 1) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) =>\n  Object.assign(requestV0({ groupId, memberId }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * LeaveGroup Response (Version: 1) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime, errorCode }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * LeaveGroup Request (Version: 2) => group_id member_id\n *   group_id => STRING\n *   member_id => STRING\n */\n\nmodule.exports = ({ groupId, memberId }) =>\n  Object.assign(requestV1({ groupId, memberId }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * LeaveGroup Response (Version: 2) => throttle_time_ms error_code\n *   throttle_time_ms => INT32\n *   error_code => INT16\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { LeaveGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 changes leavegroup to operate on a batch of members\n * and adds group_instance_id to identify members across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * LeaveGroup Request (Version: 3) => group_id [members]\n *   group_id => STRING\n *   members => member_id group_instance_id\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, members }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'LeaveGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeArray(members.map(member => encodeMember(member)))\n  },\n})\n\nconst encodeMember = ({ memberId, groupInstanceId = null }) => {\n  return new Encoder().writeString(memberId).writeString(groupInstanceId)\n}\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported, failure, createErrorFromCode } = require('../../../error')\nconst { parse: parseV2 } = require('../v2/response')\n\n/**\n * LeaveGroup Response (Version: 3) => throttle_time_ms error_code [members]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   members => member_id group_instance_id error_code\n *     member_id => STRING\n *     group_instance_id => NULLABLE_STRING\n *     error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const members = decoder.readArray(decodeMembers)\n\n  failIfVersionNotSupported(errorCode)\n\n  return { throttleTime: 0, clientSideThrottleTime: throttleTime, errorCode, members }\n}\n\nconst decodeMembers = decoder => ({\n  memberId: decoder.readString(),\n  groupInstanceId: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const parsed = parseV2(data)\n\n  const memberWithError = data.members.find(member => failure(member.errorCode))\n  if (memberWithError) {\n    throw createErrorFromCode(memberWithError.errorCode)\n  }\n\n  return parsed\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: () => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request(), response }\n  },\n  1: () => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request(), response }\n  },\n  2: () => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request(), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListGroups: apiKey } = require('../../apiKeys')\n\n/**\n * ListGroups Request (Version: 0)\n */\n\n/**\n */\nmodule.exports = () => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListGroups',\n  encode: async () => {\n    return new Encoder()\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * ListGroups Response (Version: 0) => error_code [groups]\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decodeGroup = decoder => ({\n  groupId: decoder.readString(),\n  protocolType: decoder.readString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(decodeGroup)\n\n  return {\n    errorCode,\n    groups,\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decodeGroup,\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * ListGroups Request (Version: 1)\n */\n\nmodule.exports = () => Object.assign(requestV0(), { apiVersion: 1 })\n","const responseV0 = require('../v0/response')\n\nconst Decoder = require('../../../decoder')\n\n/**\n * ListGroups Response (Version: 1) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n  const groups = decoder.readArray(responseV0.decodeGroup)\n\n  return {\n    throttleTime,\n    errorCode,\n    groups,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: responseV0.parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * ListGroups Request (Version: 2)\n */\n\nmodule.exports = () => Object.assign(requestV1(), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ListGroups Response (Version: 2) => error_code [groups]\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   groups => group_id protocol_type\n *     group_id => STRING\n *     protocol_type => STRING\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const ISOLATION_LEVEL = require('../../isolationLevel')\n\n// For normal consumers, use -1\nconst REPLICA_ID = -1\n\nconst versions = {\n  0: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  1: ({ replicaId = REPLICA_ID, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ replicaId, topics }), response }\n  },\n  2: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ replicaId, isolationLevel, topics }), response }\n  },\n  3: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ replicaId, isolationLevel, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 0) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp max_num_offsets\n *       partition => INT32\n *       timestamp => INT64\n *       max_num_offsets => INT32\n */\n\n/**\n * @param {number} replicaId\n * @param {object} topics use timestamp=-1 for latest offsets and timestamp=-2 for earliest.\n *                        Default timestamp=-1. Example:\n *                          {\n *                            topics: [\n *                              {\n *                                topic: 'topic-name',\n *                                partitions: [{ partition: 0, timestamp: -1 }]\n *                              }\n *                            ]\n *                          }\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1, maxNumOffsets = 1 }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(timestamp)\n    .writeInt32(maxNumOffsets)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Offsets Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code [offsets]\n *       partition => INT32\n *       error_code => INT16\n *       offsets => INT64\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offsets: decoder.readArray(decodeOffsets),\n})\n\nconst decodeOffsets = decoder => decoder.readInt64().toString()\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 1) => replica_id [topics]\n *   replica_id => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { ListOffsets: apiKey } = require('../../apiKeys')\n\n/**\n * ListOffsets Request (Version: 2) => replica_id isolation_level [topics]\n *   replica_id => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, isolationLevel, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'ListOffsets',\n  encode: async () => {\n    return new Encoder()\n      .writeInt32(replicaId)\n      .writeInt8(isolationLevel)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, timestamp = -1 }) => {\n  return new Encoder().writeInt32(partition).writeInt64(timestamp)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * ListOffsets Response (Version: 2) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  timestamp: decoder.readInt64().toString(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * ListOffsets Request (Version: 3) => replica_id isolation_level [topics]\n *   replica_id => INT32\n *   isolation_level => INT8\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition timestamp\n *       partition => INT32\n *       timestamp => INT64\n */\nmodule.exports = ({ replicaId, isolationLevel, topics }) =>\n  Object.assign(requestV2({ replicaId, isolationLevel, topics }), { apiVersion: 3 })\n","const { parse, decode: decodeV2 } = require('../v2/response')\n\n/**\n * In version 3 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * ListOffsets Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code timestamp offset\n *       partition => INT32\n *       error_code => INT16\n *       timestamp => INT64\n *       offset => INT64\n */\nconst decode = async rawData => {\n  const decoded = await decodeV2(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ topics }), response }\n  },\n  1: ({ topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ topics }), response }\n  },\n  2: ({ topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ topics }), response }\n  },\n  3: ({ topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ topics }), response }\n  },\n  4: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n  5: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n  6: ({ topics, allowAutoTopicCreation }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return { request: request({ topics, allowAutoTopicCreation }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 0) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeArray(topics)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Metadata Response (Version: 0) => [brokers] [topic_metadata]\n *   brokers => node_id host port\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *   topic_metadata => topic_error_code topic [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  // leader: The node id for the kafka broker currently acting as leader\n  // for this partition\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nconst parse = async data => {\n  const topicsWithErrors = data.topicMetadata.filter(topic => failure(topic.topicErrorCode))\n  if (topicsWithErrors.length > 0) {\n    const { topicErrorCode } = topicsWithErrors[0]\n    throw createErrorFromCode(topicErrorCode)\n  }\n\n  const partitionsWithErrors = data.topicMetadata.map(topic => {\n    return topic.partitionMetadata.filter(partition => failure(partition.partitionErrorCode))\n  })\n\n  const errors = flatten(partitionsWithErrors)\n  if (errors.length > 0) {\n    const { partitionErrorCode } = errors[0]\n    throw createErrorFromCode(partitionErrorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 1) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeNullableArray(topics)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 1) => [brokers] controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Metadata Request (Version: 2) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 2) => [brokers] cluster_id controller_id [topic_metadata]\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => topic_error_code topic is_internal [partition_metadata]\n *     topic_error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => partition_error_code partition_id leader [replicas] [isr]\n *       partition_error_code => INT16\n *       partition_id => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * Metadata Request (Version: 3) => [topics]\n *   topics => STRING\n */\n\nmodule.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 3 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 3) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Metadata: apiKey } = require('../../apiKeys')\n\n/**\n * Metadata Request (Version: 4) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) => ({\n  apiKey,\n  apiVersion: 4,\n  apiName: 'Metadata',\n  encode: async () => {\n    return new Encoder().writeNullableArray(topics).writeBoolean(allowAutoTopicCreation)\n  },\n})\n","const { parse: parseV3, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Metadata Response (Version: 4) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n */\n\nmodule.exports = {\n  parse: parseV3,\n  decode: decodeV3,\n}\n","const requestV4 = require('../v4/request')\n\n/**\n * Metadata Request (Version: 5) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) =>\n  Object.assign(requestV4({ topics, allowAutoTopicCreation }), { apiVersion: 5 })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * Metadata Response (Version: 5) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr] [offline_replicas]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n *       offline_replicas => INT32\n */\n\nconst broker = decoder => ({\n  nodeId: decoder.readInt32(),\n  host: decoder.readString(),\n  port: decoder.readInt32(),\n  rack: decoder.readString(),\n})\n\nconst topicMetadata = decoder => ({\n  topicErrorCode: decoder.readInt16(),\n  topic: decoder.readString(),\n  isInternal: decoder.readBoolean(),\n  partitionMetadata: decoder.readArray(partitionMetadata),\n})\n\nconst partitionMetadata = decoder => ({\n  partitionErrorCode: decoder.readInt16(),\n  partitionId: decoder.readInt32(),\n  leader: decoder.readInt32(),\n  replicas: decoder.readArray(d => d.readInt32()),\n  isr: decoder.readArray(d => d.readInt32()),\n  offlineReplicas: decoder.readArray(d => d.readInt32()),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    brokers: decoder.readArray(broker),\n    clusterId: decoder.readString(),\n    controllerId: decoder.readInt32(),\n    topicMetadata: decoder.readArray(topicMetadata),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV5 = require('../v5/request')\n\n/**\n * Metadata Request (Version: 6) => [topics] allow_auto_topic_creation\n *   topics => STRING\n *   allow_auto_topic_creation => BOOLEAN\n */\n\nmodule.exports = ({ topics, allowAutoTopicCreation = true }) =>\n  Object.assign(requestV5({ topics, allowAutoTopicCreation }), { apiVersion: 6 })\n","const { parse, decode: decodeV1 } = require('../v5/response')\n\n/**\n * In version 6 on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * Metadata Response (Version: 6) => throttle_time_ms [brokers] cluster_id controller_id [topic_metadata]\n *   throttle_time_ms => INT32\n *   brokers => node_id host port rack\n *     node_id => INT32\n *     host => STRING\n *     port => INT32\n *     rack => NULLABLE_STRING\n *   cluster_id => NULLABLE_STRING\n *   controller_id => INT32\n *   topic_metadata => error_code topic is_internal [partition_metadata]\n *     error_code => INT16\n *     topic => STRING\n *     is_internal => BOOLEAN\n *     partition_metadata => error_code partition leader [replicas] [isr] [offline_replicas]\n *       error_code => INT16\n *       partition => INT32\n *       leader => INT32\n *       replicas => INT32\n *       isr => INT32\n *       offline_replicas => INT32\n */\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","// This value signals to the broker that its default configuration should be used.\nconst RETENTION_TIME = -1\n\nconst versions = {\n  0: ({ groupId, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  1: ({ groupId, groupGenerationId, memberId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, groupGenerationId, memberId, topics }), response }\n  },\n  2: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  3: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  4: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        retentionTime,\n        topics,\n      }),\n      response,\n    }\n  },\n  5: ({ groupId, groupGenerationId, memberId, topics }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({\n        groupId,\n        groupGenerationId,\n        memberId,\n        topics,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 0) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetCommit Response (Version: 0) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 1) => group_id group_generation_id member_id [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset timestamp metadata\n *       partition => INT32\n *       offset => INT64\n *       timestamp => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, timestamp = Date.now(), metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeInt64(timestamp)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetCommit Request (Version: 2) => group_id group_generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   group_generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeInt64(retentionTime)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV2 = require('../v2/request')\n\n/**\n * OffsetCommit Request (Version: 3) => group_id generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) =>\n  Object.assign(requestV2({ groupId, groupGenerationId, memberId, retentionTime, topics }), {\n    apiVersion: 3,\n  })\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * OffsetCommit Response (Version: 3) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * OffsetCommit Request (Version: 4) => group_id generation_id member_id retention_time [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   retention_time => INT64\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) =>\n  Object.assign(requestV3({ groupId, groupGenerationId, memberId, retentionTime, topics }), {\n    apiVersion: 4,\n  })\n","const { parse, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Starting in version 4, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * OffsetCommit Response (Version: 4) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV3(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * Version 5 removes retention_time, as this is controlled by a broker setting\n *\n * OffsetCommit Request (Version: 4) => group_id generation_id member_id [topics]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({\n  apiKey,\n  apiVersion: 5,\n  apiName: 'OffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(groupGenerationId)\n      .writeString(memberId)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata = null }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const { parse, decode } = require('../v4/response')\n\n/**\n * OffsetCommit Response (Version: 5) => throttle_time_ms [responses]\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  1: ({ groupId, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  2: ({ groupId, topics }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  3: ({ groupId, topics }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return { request: request({ groupId, topics }), response }\n  },\n  4: ({ groupId, topics }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return { request: request({ groupId, topics }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 1) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 1,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 1) => [responses]\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * OffsetFetch Request (Version: 2) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) =>\n  Object.assign(requestV1({ groupId, topics }), { apiVersion: 2 })\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * OffsetFetch Response (Version: 2) => [responses] error_code\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  const partitionsWithError = data.responses.map(response =>\n    response.partitions.filter(partition => failure(partition.errorCode))\n  )\n  const partitionWithError = flatten(partitionsWithError)[0]\n  if (partitionWithError) {\n    throw createErrorFromCode(partitionWithError.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { OffsetFetch: apiKey } = require('../../apiKeys')\n\n/**\n * OffsetFetch Request (Version: 3) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'OffsetFetch',\n  encode: async () => {\n    return new Encoder().writeString(groupId).writeNullableArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition }) => {\n  return new Encoder().writeInt32(partition)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV2 } = require('../v2/response')\n\n/**\n * OffsetFetch Response (Version: 3) => throttle_time_ms [responses] error_code\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  return {\n    throttleTime: decoder.readInt32(),\n    responses: decoder.readArray(decodeResponses),\n    errorCode: decoder.readInt16(),\n  }\n}\n\nconst decodeResponses = decoder => ({\n  topic: decoder.readString(),\n  partitions: decoder.readArray(decodePartitions),\n})\n\nconst decodePartitions = decoder => ({\n  partition: decoder.readInt32(),\n  offset: decoder.readInt64().toString(),\n  metadata: decoder.readString(),\n  errorCode: decoder.readInt16(),\n})\n\nmodule.exports = {\n  decode,\n  parse: parseV2,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * OffsetFetch Request (Version: 4) => group_id [topics]\n *   group_id => STRING\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition\n *       partition => INT32\n */\n\nmodule.exports = ({ groupId, topics }) =>\n  Object.assign(requestV3({ groupId, topics }), { apiVersion: 4 })\n","const { parse, decode: decodeV3 } = require('../v3/response')\n\n/**\n * Starting in version 4, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * OffsetFetch Response (Version: 4) => throttle_time_ms [responses] error_code\n *   throttle_time_ms => INT32\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition offset metadata error_code\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n *       error_code => INT16\n *   error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV3(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ acks, timeout, topicData }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  1: ({ acks, timeout, topicData }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ acks, timeout, topicData }), response }\n  },\n  2: ({ acks, timeout, topicData, compression }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return { request: request({ acks, timeout, compression, topicData }), response }\n  },\n  3: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  4: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v4/request')\n    const response = require('./v4/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  5: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v5/request')\n    const response = require('./v5/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  6: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v6/request')\n    const response = require('./v6/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n  7: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {\n    const request = require('./v7/request')\n    const response = require('./v7/response')\n    return {\n      request: request({\n        acks,\n        timeout,\n        compression,\n        topicData,\n        transactionalId,\n        producerId,\n        producerEpoch,\n      }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\n\n/**\n * Produce Request (Version: 0) => acks timeout [topic_data]\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set record_set_size\n *       partition => INT32\n *       record_set_size => INT32\n *       record_set => RECORDS\n */\n\n/**\n * MessageV0:\n * {\n *   key: bytes,\n *   value: bytes\n * }\n *\n * MessageSet:\n * [\n *   { key: \"<value>\", value: \"<value>\" },\n *   { key: \"<value>\", value: \"<value>\" },\n * ]\n *\n * TopicData:\n * [\n *   {\n *     topic: 'name1',\n *     partitions: [\n *       {\n *         partition: 0,\n *         messages: [<MessageSet>]\n *       }\n *     ]\n *   }\n * ]\n */\n\n/**\n * @param acks {Integer} This field indicates how many acknowledgements the servers should receive before\n *                       responding to the request. If it is 0 the server will not send any response\n *                       (this is the only case where the server will not reply to a request). If it is 1,\n *                       the server will wait the data is written to the local log before sending a response.\n *                       If it is -1 the server will block until the message is committed by all in sync replicas\n *                       before sending a response.\n *\n * @param timeout {Integer} This provides a maximum time in milliseconds the server can await the receipt of the number\n *                          of acknowledgements in RequiredAcks. The timeout is not an exact limit on the request time\n *                          for a few reasons:\n *                          (1) it does not include network latency,\n *                          (2) the timer begins at the beginning of the processing of this request so if many requests are\n *                              queued due to server overload that wait time will not be included,\n *                          (3) we will not terminate a local write so if the local write time exceeds this timeout it will not\n *                              be respected. To get a hard timeout of this type the client should use the socket timeout.\n *\n * @param topicData {Array}\n */\nmodule.exports = ({ acks, timeout, topicData }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(topicData.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartitions))\n}\n\nconst encodePartitions = ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 0, entries: messages })\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(messageSet.size())\n    .writeEncoder(messageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * v0\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]]\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  return {\n    topics,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(topic => {\n    return topic.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n// Produce Request on or after v1 indicates the client can parse the quota throttle time\n// in the Produce Response.\n\nmodule.exports = ({ acks, timeout, topicData }) => {\n  return Object.assign(requestV0({ acks, timeout, topicData }), { apiVersion: 1 })\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v1 (supported in 0.9.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst MessageSet = require('../../../messageSet')\nconst { Types, lookupCodec } = require('../../../message/compression')\n\n// Produce Request on or after v2 indicates the client can parse the timestamp field\n// in the produce Response.\n\nmodule.exports = ({ acks, timeout, compression = Types.None, topicData }) => ({\n  apiKey,\n  apiVersion: 2,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(await encodeTopic(data))\n    }\n\n    return new Encoder()\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => {\n  const encodePartitions = partitionsEncoder(compression)\n\n  return async ({ topic, partitions }) => {\n    const encodedPartitions = []\n\n    for (const data of partitions) {\n      encodedPartitions.push(await encodePartitions(data))\n    }\n\n    return new Encoder().writeString(topic).writeArray(encodedPartitions)\n  }\n}\n\nconst partitionsEncoder = compression => async ({ partition, messages }) => {\n  const messageSet = MessageSet({ messageVersion: 1, compression, entries: messages })\n\n  if (compression === Types.None) {\n    return new Encoder()\n      .writeInt32(partition)\n      .writeInt32(messageSet.size())\n      .writeEncoder(messageSet)\n  }\n\n  const timestamp = messages[0].timestamp || Date.now()\n\n  const codec = lookupCodec(compression)\n  const compressedValue = await codec.compress(messageSet)\n  const compressedMessageSet = MessageSet({\n    messageVersion: 1,\n    entries: [{ compression, timestamp, value: compressedValue }],\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(compressedMessageSet.size())\n    .writeEncoder(compressedMessageSet)\n}\n","const Decoder = require('../../../decoder')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * v2 (supported in 0.10.0 or later)\n * ProduceResponse => [TopicName [Partition ErrorCode Offset Timestamp]] ThrottleTime\n *   TopicName => string\n *   Partition => int32\n *   ErrorCode => int16\n *   Offset => int64\n *   Timestamp => int64\n *   ThrottleTime => int32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  offset: decoder.readInt64().toString(),\n  timestamp: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const Long = require('../../../../utils/long')\nconst Encoder = require('../../../encoder')\nconst { Produce: apiKey } = require('../../apiKeys')\nconst { Types } = require('../../../message/compression')\nconst Record = require('../../../recordBatch/record/v0')\nconst { RecordBatch } = require('../../../recordBatch/v0')\n\n/**\n * Produce Request (Version: 3) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\n/**\n * @param [transactionalId=null] {String} The transactional id or null if the producer is not transactional\n * @param acks {Integer} See producer request v0\n * @param timeout {Integer} See producer request v0\n * @param [compression=CompressionTypes.None] {CompressionTypes}\n * @param topicData {Array}\n */\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId = null,\n  producerId = Long.fromInt(-1),\n  producerEpoch = 0,\n  compression = Types.None,\n  topicData,\n}) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'Produce',\n  expectResponse: () => acks !== 0,\n  encode: async () => {\n    const encodeTopic = topicEncoder(compression)\n    const encodedTopicData = []\n\n    for (const data of topicData) {\n      encodedTopicData.push(\n        await encodeTopic({ ...data, transactionalId, producerId, producerEpoch })\n      )\n    }\n\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeInt16(acks)\n      .writeInt32(timeout)\n      .writeArray(encodedTopicData)\n  },\n})\n\nconst topicEncoder = compression => async ({\n  topic,\n  partitions,\n  transactionalId,\n  producerId,\n  producerEpoch,\n}) => {\n  const encodePartitions = partitionsEncoder(compression)\n  const encodedPartitions = []\n\n  for (const data of partitions) {\n    encodedPartitions.push(\n      await encodePartitions({ ...data, transactionalId, producerId, producerEpoch })\n    )\n  }\n\n  return new Encoder().writeString(topic).writeArray(encodedPartitions)\n}\n\nconst partitionsEncoder = compression => async ({\n  partition,\n  messages,\n  transactionalId,\n  firstSequence,\n  producerId,\n  producerEpoch,\n}) => {\n  const dateNow = Date.now()\n  const messageTimestamps = messages\n    .map(m => m.timestamp)\n    .filter(timestamp => timestamp != null)\n    .sort()\n\n  const timestamps = messageTimestamps.length === 0 ? [dateNow] : messageTimestamps\n  const firstTimestamp = timestamps[0]\n  const maxTimestamp = timestamps[timestamps.length - 1]\n\n  const records = messages.map((message, i) =>\n    Record({\n      ...message,\n      offsetDelta: i,\n      timestampDelta: (message.timestamp || dateNow) - firstTimestamp,\n    })\n  )\n\n  const recordBatch = await RecordBatch({\n    compression,\n    records,\n    firstTimestamp,\n    maxTimestamp,\n    producerId,\n    producerEpoch,\n    firstSequence,\n    transactional: !!transactionalId,\n    lastOffsetDelta: records.length - 1,\n  })\n\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt32(recordBatch.size())\n    .writeEncoder(recordBatch)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\nconst flatten = require('../../../../utils/flatten')\n\n/**\n * Produce Response (Version: 3) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nconst parse = async data => {\n  const partitionsWithError = data.topics.map(response => {\n    return response.partitions.filter(partition => failure(partition.errorCode))\n  })\n\n  const errors = flatten(partitionsWithError)\n  if (errors.length > 0) {\n    const { errorCode } = errors[0]\n    throw createErrorFromCode(errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 4) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 4 }\n  )\n","const { decode, parse } = require('../v3/response')\n\n/**\n * Produce Response (Version: 4) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV3 = require('../v3/request')\n\n/**\n * Produce Request (Version: 5) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV3({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 5 }\n  )\n","const Decoder = require('../../../decoder')\nconst { parse: parseV3 } = require('../v3/response')\n\n/**\n * Produce Response (Version: 5) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst partition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n  baseOffset: decoder.readInt64().toString(),\n  logAppendTime: decoder.readInt64().toString(),\n  logStartOffset: decoder.readInt64().toString(),\n})\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const topics = decoder.readArray(decoder => ({\n    topicName: decoder.readString(),\n    partitions: decoder.readArray(partition),\n  }))\n\n  const throttleTime = decoder.readInt32()\n\n  return {\n    topics,\n    throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV3,\n}\n","const requestV5 = require('../v5/request')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L113-L117\n *\n * Produce Request (Version: 6) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV5({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 6 }\n  )\n","const { parse, decode: decodeV5 } = require('../v5/response')\n\n/**\n * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java#L152-L156\n *\n * Produce Response (Version: 6) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV5(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV6 = require('../v6/request')\n\n/**\n * V7 indicates ZStandard capability (see KIP-110)\n * @see https://github.com/apache/kafka/blob/9c8f75c4b624084c954b4da69f092211a9ac4689/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L118-L121\n *\n * Produce Request (Version: 7) => transactional_id acks timeout [topic_data]\n *   transactional_id => NULLABLE_STRING\n *   acks => INT16\n *   timeout => INT32\n *   topic_data => topic [data]\n *     topic => STRING\n *     data => partition record_set\n *       partition => INT32\n *       record_set => RECORDS\n */\n\nmodule.exports = ({\n  acks,\n  timeout,\n  transactionalId,\n  producerId,\n  producerEpoch,\n  compression,\n  topicData,\n}) =>\n  Object.assign(\n    requestV6({\n      acks,\n      timeout,\n      transactionalId,\n      producerId,\n      producerEpoch,\n      compression,\n      topicData,\n    }),\n    { apiVersion: 7 }\n  )\n","const { decode, parse } = require('../v6/response')\n\n/**\n * Produce Response (Version: 7) => [responses] throttle_time_ms\n *   responses => topic [partition_responses]\n *     topic => STRING\n *     partition_responses => partition error_code base_offset log_append_time log_start_offset\n *       partition => INT32\n *       error_code => INT16\n *       base_offset => INT64\n *       log_append_time => INT64\n *       log_start_offset => INT64\n *   throttle_time_ms => INT32\n */\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ authBytes }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ authBytes }), response }\n  },\n  1: ({ authBytes }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ authBytes }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslAuthenticate: apiKey } = require('../../apiKeys')\n\n/**\n * SaslAuthenticate Request (Version: 0) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslAuthenticate',\n  encode: async () => {\n    return new Encoder().writeBuffer(authBytes)\n  },\n})\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst {\n  failure,\n  createErrorFromCode,\n  failIfVersionNotSupported,\n  errorCodes,\n} = require('../../../error')\n\nconst { KafkaJSProtocolError } = require('../../../../errors')\nconst SASL_AUTHENTICATION_FAILED = 58\nconst protocolAuthError = errorCodes.find(e => e.code === SASL_AUTHENTICATION_FAILED)\n\n/**\n * SaslAuthenticate Response (Version: 0) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n  }\n}\n\nconst parse = async data => {\n  if (data.errorCode === SASL_AUTHENTICATION_FAILED && data.errorMessage) {\n    throw new KafkaJSProtocolError({\n      ...protocolAuthError,\n      message: data.errorMessage,\n    })\n  }\n\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SaslAuthenticate Request (Version: 1) => sasl_auth_bytes\n *   sasl_auth_bytes => BYTES\n */\n\n/**\n * @param {Buffer} authBytes - SASL authentication bytes from client as defined by the SASL mechanism\n */\nmodule.exports = ({ authBytes }) => Object.assign(requestV0({ authBytes }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst Encoder = require('../../../encoder')\nconst { parse: parseV0 } = require('../v0/response')\nconst { failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslAuthenticate Response (Version: 1) => error_code error_message sasl_auth_bytes\n *   error_code => INT16\n *   error_message => NULLABLE_STRING\n *   sasl_auth_bytes => BYTES\n *   session_lifetime_ms => INT64\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n  const errorMessage = decoder.readString()\n\n  // This is necessary to make the response compatible with the original\n  // mechanism protocols. They expect a byte response, which starts with\n  // the size\n  const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes())\n  const authBytes = authBytesEncoder.buffer\n  const sessionLifetimeMs = decoder.readInt64().toString()\n\n  return {\n    errorCode,\n    errorMessage,\n    authBytes,\n    sessionLifetimeMs,\n  }\n}\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ mechanism }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return { request: request({ mechanism }), response }\n  },\n  1: ({ mechanism }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return { request: request({ mechanism }), response }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SaslHandshake: apiKey } = require('../../apiKeys')\n\n/**\n * SaslHandshake Request (Version: 0) => mechanism\n *    mechanism => STRING\n */\n\n/**\n * @param {string} mechanism - SASL Mechanism chosen by the client\n */\nmodule.exports = ({ mechanism }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SaslHandshake',\n  encode: async () => new Encoder().writeString(mechanism),\n})\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SaslHandshake Response (Version: 0) => error_code [enabled_mechanisms]\n *    error_code => INT16\n *    enabled_mechanisms => STRING\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    enabledMechanisms: decoder.readArray(decoder => decoder.readString()),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\nmodule.exports = ({ mechanism }) => ({ ...requestV0({ mechanism }), apiVersion: 1 })\n","const { decode: decodeV0, parse: parseV0 } = require('../v0/response')\n\nmodule.exports = {\n  decode: decodeV0,\n  parse: parseV0,\n}\n","const versions = {\n  0: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  1: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  2: ({ groupId, generationId, memberId, groupAssignment }) => {\n    const request = require('./v2/request')\n    const response = require('./v2/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupAssignment }),\n      response,\n    }\n  },\n  3: ({ groupId, generationId, memberId, groupInstanceId, groupAssignment }) => {\n    const request = require('./v3/request')\n    const response = require('./v3/response')\n    return {\n      request: request({ groupId, generationId, memberId, groupInstanceId, groupAssignment }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { SyncGroup: apiKey } = require('../../apiKeys')\n\n/**\n * SyncGroup Request (Version: 0) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'SyncGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(generationId)\n      .writeString(memberId)\n      .writeArray(groupAssignment.map(encodeGroupAssignment))\n  },\n})\n\nconst encodeGroupAssignment = ({ memberId, memberAssignment }) => {\n  return new Encoder().writeString(memberId).writeBytes(memberAssignment)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode, failIfVersionNotSupported } = require('../../../error')\n\n/**\n * SyncGroup Response (Version: 0) => error_code member_assignment\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nconst parse = async data => {\n  if (failure(data.errorCode)) {\n    throw createErrorFromCode(data.errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * SyncGroup Request (Version: 1) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) =>\n  Object.assign(requestV0({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 1 })\n","const Decoder = require('../../../decoder')\nconst { failIfVersionNotSupported } = require('../../../error')\nconst { parse: parseV0 } = require('../v0/response')\n\n/**\n * SyncGroup Response (Version: 1) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const errorCode = decoder.readInt16()\n\n  failIfVersionNotSupported(errorCode)\n\n  return {\n    throttleTime,\n    errorCode,\n    memberAssignment: decoder.readBytes(),\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse: parseV0,\n}\n","const requestV1 = require('../v1/request')\n\n/**\n * SyncGroup Request (Version: 2) => group_id generation_id member_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({ groupId, generationId, memberId, groupAssignment }) =>\n  Object.assign(requestV1({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 2 })\n","const { parse, decode: decodeV1 } = require('../v1/response')\n\n/**\n * In version 2, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * SyncGroup Response (Version: 2) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const Encoder = require('../../../encoder')\nconst { SyncGroup: apiKey } = require('../../apiKeys')\n\n/**\n * Version 3 adds group_instance_id to indicate member identity across restarts.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances\n *\n * SyncGroup Request (Version: 3) => group_id generation_id member_id group_instance_id [group_assignment]\n *   group_id => STRING\n *   generation_id => INT32\n *   member_id => STRING\n *   group_instance_id => NULLABLE_STRING\n *   group_assignment => member_id member_assignment\n *     member_id => STRING\n *     member_assignment => BYTES\n */\n\nmodule.exports = ({\n  groupId,\n  generationId,\n  memberId,\n  groupInstanceId = null,\n  groupAssignment,\n}) => ({\n  apiKey,\n  apiVersion: 3,\n  apiName: 'SyncGroup',\n  encode: async () => {\n    return new Encoder()\n      .writeString(groupId)\n      .writeInt32(generationId)\n      .writeString(memberId)\n      .writeString(groupInstanceId)\n      .writeArray(groupAssignment.map(encodeGroupAssignment))\n  },\n})\n\nconst encodeGroupAssignment = ({ memberId, memberAssignment }) => {\n  return new Encoder().writeString(memberId).writeBytes(memberAssignment)\n}\n","const { decode, parse } = require('../v2/response')\n\n/**\n * SyncGroup Response (Version: 2) => throttle_time_ms error_code member_assignment\n *   throttle_time_ms => INT32\n *   error_code => INT16\n *   member_assignment => BYTES\n */\nmodule.exports = {\n  decode,\n  parse,\n}\n","const versions = {\n  0: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {\n    const request = require('./v0/request')\n    const response = require('./v0/response')\n    return {\n      request: request({ transactionalId, groupId, producerId, producerEpoch, topics }),\n      response,\n    }\n  },\n  1: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {\n    const request = require('./v1/request')\n    const response = require('./v1/response')\n    return {\n      request: request({ transactionalId, groupId, producerId, producerEpoch, topics }),\n      response,\n    }\n  },\n}\n\nmodule.exports = {\n  versions: Object.keys(versions),\n  protocol: ({ version }) => versions[version],\n}\n","const Encoder = require('../../../encoder')\nconst { TxnOffsetCommit: apiKey } = require('../../apiKeys')\n\n/**\n * TxnOffsetCommit Request (Version: 0) => transactional_id group_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   group_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) => ({\n  apiKey,\n  apiVersion: 0,\n  apiName: 'TxnOffsetCommit',\n  encode: async () => {\n    return new Encoder()\n      .writeString(transactionalId)\n      .writeString(groupId)\n      .writeInt64(producerId)\n      .writeInt16(producerEpoch)\n      .writeArray(topics.map(encodeTopic))\n  },\n})\n\nconst encodeTopic = ({ topic, partitions }) => {\n  return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition))\n}\n\nconst encodePartition = ({ partition, offset, metadata }) => {\n  return new Encoder()\n    .writeInt32(partition)\n    .writeInt64(offset)\n    .writeString(metadata)\n}\n","const Decoder = require('../../../decoder')\nconst { failure, createErrorFromCode } = require('../../../error')\n\n/**\n * TxnOffsetCommit Response (Version: 0) => throttle_time_ms [topics]\n *   throttle_time_ms => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\nconst decode = async rawData => {\n  const decoder = new Decoder(rawData)\n  const throttleTime = decoder.readInt32()\n  const topics = await decoder.readArrayAsync(decodeTopic)\n\n  return {\n    throttleTime,\n    topics,\n  }\n}\n\nconst decodeTopic = async decoder => ({\n  topic: decoder.readString(),\n  partitions: await decoder.readArrayAsync(decodePartition),\n})\n\nconst decodePartition = decoder => ({\n  partition: decoder.readInt32(),\n  errorCode: decoder.readInt16(),\n})\n\nconst parse = async data => {\n  const topicsWithErrors = data.topics\n    .map(({ partitions }) => ({\n      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode)),\n    }))\n    .filter(({ partitionsWithErrors }) => partitionsWithErrors.length)\n\n  if (topicsWithErrors.length > 0) {\n    throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode)\n  }\n\n  return data\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","const requestV0 = require('../v0/request')\n\n/**\n * TxnOffsetCommit Request (Version: 1) => transactional_id group_id producer_id producer_epoch [topics]\n *   transactional_id => STRING\n *   group_id => STRING\n *   producer_id => INT64\n *   producer_epoch => INT16\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition offset metadata\n *       partition => INT32\n *       offset => INT64\n *       metadata => NULLABLE_STRING\n */\n\nmodule.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) =>\n  Object.assign(requestV0({ transactionalId, groupId, producerId, producerEpoch, topics }), {\n    apiVersion: 1,\n  })\n","const { parse, decode: decodeV1 } = require('../v0/response')\n\n/**\n * In version 1, on quota violation, brokers send out responses before throttling.\n * @see https://cwiki.apache.org/confluence/display/KAFKA/KIP-219+-+Improve+quota+communication\n *\n * TxnOffsetCommit Response (Version: 1) => throttle_time_ms [topics]\n *   throttle_time_ms => INT32\n *   topics => topic [partitions]\n *     topic => STRING\n *     partitions => partition error_code\n *       partition => INT32\n *       error_code => INT16\n */\n\nconst decode = async rawData => {\n  const decoded = await decodeV1(rawData)\n\n  return {\n    ...decoded,\n    throttleTime: 0,\n    clientSideThrottleTime: decoded.throttleTime,\n  }\n}\n\nmodule.exports = {\n  decode,\n  parse,\n}\n","// From:\n// https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/resource/PatternType.java#L32\n\n/**\n * @typedef {number} ACLResourcePatternTypes\n *\n * Enum for ACL Resource Pattern Type\n * @readonly\n * @enum {ACLResourcePatternTypes}\n */\nmodule.exports = {\n  /**\n   * Represents any PatternType which this client cannot understand, perhaps because this client is too old.\n   */\n  UNKNOWN: 0,\n  /**\n   * In a filter, matches any resource pattern type.\n   */\n  ANY: 1,\n  /**\n   * In a filter, will perform pattern matching.\n   *\n   * e.g. Given a filter of {@code ResourcePatternFilter(TOPIC, \"payments.received\", MATCH)`}, the filter match\n   * any {@link ResourcePattern} that matches topic 'payments.received'. This might include:\n   * <ul>\n   *     <li>A Literal pattern with the same type and name, e.g. {@code ResourcePattern(TOPIC, \"payments.received\", LITERAL)}</li>\n   *     <li>A Wildcard pattern with the same type, e.g. {@code ResourcePattern(TOPIC, \"*\", LITERAL)}</li>\n   *     <li>A Prefixed pattern with the same type and where the name is a matching prefix, e.g. {@code ResourcePattern(TOPIC, \"payments.\", PREFIXED)}</li>\n   * </ul>\n   */\n  MATCH: 2,\n  /**\n   * A literal resource name.\n   *\n   * A literal name defines the full name of a resource, e.g. topic with name 'foo', or group with name 'bob'.\n   *\n   * The special wildcard character {@code *} can be used to represent a resource with any name.\n   */\n  LITERAL: 3,\n  /**\n   * A prefixed resource name.\n   *\n   * A prefixed name defines a prefix for a resource, e.g. topics with names that start with 'foo'.\n   */\n  PREFIXED: 4,\n}\n","const ACLResourceTypes = require('./aclResourceTypes')\n\n/**\n * @deprecated\n * @see https://github.com/tulios/kafkajs/issues/649\n *\n * Use ConfigResourceTypes or AclResourceTypes instead.\n */\nmodule.exports = ACLResourceTypes\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","const Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = '' }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc5801.txt\n *\n * See org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse\n * for official Java client implementation.\n *\n * The mechanism consists of a message from the client to the server.\n * The client sends the \"n,\"\" GS header, followed by the authorizationIdentitty\n * prefixed by \"a=\" (if present), followed by \",\", followed by a US-ASCII SOH\n * character, followed by \"auth=Bearer \", followed by the token value, followed\n * by US-ASCII SOH character, followed by SASL extensions in OAuth \"friendly\"\n * format and then closed by two additionals US-ASCII SOH characters.\n *\n * SASL extensions are optional an must be expressed as key-value pairs in an\n * object. Each expression is converted as, the extension entry key, followed\n * by \"=\", followed by extension entry value. Each extension is separated by a\n * US-ASCII SOH character. If extensions are not present, their relative part\n * in the message, including the US-ASCII SOH character, is omitted.\n *\n * The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication token and verify that the\n * authentication credentials permit the client to login as the authorization\n * identity. If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst SEPARATOR = '\\u0001' // SOH - Start Of Header ASCII\n\nfunction formatExtensions(extensions) {\n  let msg = ''\n\n  if (extensions == null) {\n    return msg\n  }\n\n  let prefix = ''\n  for (const k in extensions) {\n    msg += `${prefix}${k}=${extensions[k]}`\n    prefix = SEPARATOR\n  }\n\n  return msg\n}\n\nmodule.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {\n  const authzid = authorizationIdentity == null ? '' : `\"a=${authorizationIdentity}`\n  let ext = formatExtensions(oauthBearerToken.extensions)\n  if (ext.length > 0) {\n    ext = `${SEPARATOR}${ext}`\n  }\n\n  const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`\n\n  return {\n    encode: async () => {\n      return new Encoder().writeBytes(Buffer.from(oauthMsg))\n    },\n  }\n}\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","module.exports = {\n  request: require('./request'),\n  response: require('./response'),\n}\n","/**\n * http://www.ietf.org/rfc/rfc2595.txt\n *\n * The mechanism consists of a single message from the client to the\n * server.  The client sends the authorization identity (identity to\n * login as), followed by a US-ASCII NUL character, followed by the\n * authentication identity (identity whose password will be used),\n * followed by a US-ASCII NUL character, followed by the clear-text\n * password.  The client may leave the authorization identity empty to\n * indicate that it is the same as the authentication identity.\n *\n * The server will verify the authentication identity and password with\n * the system authentication database and verify that the authentication\n * credentials permit the client to login as the authorization identity.\n * If both steps succeed, the user is logged in.\n */\n\nconst Encoder = require('../../encoder')\n\nconst US_ASCII_NULL_CHAR = '\\u0000'\n\nmodule.exports = ({ authorizationIdentity = null, username, password }) => ({\n  encode: async () => {\n    return new Encoder().writeBytes(\n      [authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)\n    )\n  },\n})\n","module.exports = {\n  decode: async () => true,\n  parse: async () => true,\n}\n","const Encoder = require('../../../encoder')\n\nmodule.exports = ({ finalMessage }) => ({\n  encode: async () => new Encoder().writeBytes(finalMessage),\n})\n","module.exports = require('../firstMessage/response')\n","/**\n * https://tools.ietf.org/html/rfc5802\n *\n * First, the client sends the \"client-first-message\" containing:\n *\n *  -> a GS2 header consisting of a flag indicating whether channel\n * binding is supported-but-not-used, not supported, or used, and an\n * optional SASL authorization identity;\n *\n *  -> SCRAM username and a random, unique nonce attributes.\n *\n * Note that the client's first message will always start with \"n\", \"y\",\n * or \"p\"; otherwise, the message is invalid and authentication MUST\n * fail.  This is important, as it allows for GS2 extensibility (e.g.,\n * to add support for security layers).\n */\n\nconst Encoder = require('../../../encoder')\n\nmodule.exports = ({ clientFirstMessage }) => ({\n  encode: async () => new Encoder().writeBytes(clientFirstMessage),\n})\n","/* eslint no-unused-vars: [\"error\", { \"varsIgnorePattern\": \"_\" }] */\n\nconst Decoder = require('../../../decoder')\n\nconst ENTRY_REGEX = /^([rsiev])=(.*)$/\n\nmodule.exports = {\n  decode: async rawData => {\n    return new Decoder(rawData).readBytes()\n  },\n  parse: async data => {\n    const processed = data\n      .toString()\n      .split(',')\n      .map(str => {\n        const [_, key, value] = str.match(ENTRY_REGEX)\n        return [key, value]\n      })\n      .reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {})\n\n    return { original: data.toString(), ...processed }\n  },\n}\n","module.exports = {\n  firstMessage: {\n    request: require('./firstMessage/request'),\n    response: require('./firstMessage/response'),\n  },\n  finalMessage: {\n    request: require('./finalMessage/request'),\n    response: require('./finalMessage/response'),\n  },\n}\n","/**\n * Enum for timestamp types\n * @readonly\n * @enum {TimestampType}\n */\nmodule.exports = {\n  // Timestamp type is unknown\n  NO_TIMESTAMP: -1,\n\n  // Timestamp relates to message creation time as set by a Kafka client\n  CREATE_TIME: 0,\n\n  // Timestamp relates to the time a message was appended to a Kafka log\n  LOG_APPEND_TIME: 1,\n}\n","module.exports = {\n  maxRetryTime: 30 * 1000,\n  initialRetryTime: 300,\n  factor: 0.2, // randomization factor\n  multiplier: 2, // exponential factor\n  retries: 5, // max retries\n}\n","module.exports = {\n  maxRetryTime: 1000,\n  initialRetryTime: 50,\n  factor: 0.02, // randomization factor\n  multiplier: 1.5, // exponential factor\n  retries: 15, // max retries\n}\n","const { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require('../errors')\n\nconst isTestMode = process.env.NODE_ENV === 'test'\nconst RETRY_DEFAULT = isTestMode ? require('./defaults.test') : require('./defaults')\n\nconst random = (min, max) => {\n  return Math.random() * (max - min) + min\n}\n\nconst randomFromRetryTime = (factor, retryTime) => {\n  const delta = factor * retryTime\n  return Math.ceil(random(retryTime - delta, retryTime + delta))\n}\n\nconst UNRECOVERABLE_ERRORS = ['RangeError', 'ReferenceError', 'SyntaxError', 'TypeError']\nconst isErrorUnrecoverable = e => UNRECOVERABLE_ERRORS.includes(e.name)\nconst isErrorRetriable = error =>\n  (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error)\n\nconst createRetriable = (configs, resolve, reject, fn) => {\n  let aborted = false\n  const { factor, multiplier, maxRetryTime, retries } = configs\n\n  const bail = error => {\n    aborted = true\n    reject(error || new Error('Aborted'))\n  }\n\n  const calculateExponentialRetryTime = retryTime => {\n    return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime)\n  }\n\n  const retry = (retryTime, retryCount = 0) => {\n    if (aborted) return\n\n    const nextRetryTime = calculateExponentialRetryTime(retryTime)\n    const shouldRetry = retryCount < retries\n\n    const scheduleRetry = () => {\n      setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime)\n    }\n\n    fn(bail, retryCount, retryTime)\n      .then(resolve)\n      .catch(e => {\n        if (isErrorRetriable(e)) {\n          if (shouldRetry) {\n            scheduleRetry()\n          } else {\n            reject(new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime }))\n          }\n        } else {\n          reject(new KafkaJSNonRetriableError(e))\n        }\n      })\n  }\n\n  return retry\n}\n\n/**\n * @typedef {(fn: (bail: (err: Error) => void, retryCount: number, retryTime: number) => any) => Promise<ReturnType<fn>>} Retrier\n */\n\n/**\n * @param {import(\"../../types\").RetryOptions} [opts]\n * @returns {Retrier}\n */\nmodule.exports = (opts = {}) => fn => {\n  return new Promise((resolve, reject) => {\n    const configs = Object.assign({}, RETRY_DEFAULT, opts)\n    const start = createRetriable(configs, resolve, reject, fn)\n    start(randomFromRetryTime(configs.factor, configs.initialRetryTime))\n  })\n}\n","module.exports = (a, b) => {\n  const result = []\n  const length = a.length\n  let i = 0\n\n  while (i < length) {\n    if (b.indexOf(a[i]) === -1) {\n      result.push(a[i])\n    }\n    i += 1\n  }\n\n  return result\n}\n","const defaultErrorHandler = e => {\n  throw e\n}\n\n/**\n * Generator that processes the given promises, and yields their result in the order of them resolving.\n *\n * @template T\n * @param {Promise<T>[]} promises promises to process\n * @param {(err: Error) => any} [handleError] optional error handler\n * @returns {Generator<Promise<T>>}\n */\nfunction* BufferedAsyncIterator(promises, handleError = defaultErrorHandler) {\n  /** Queue of promises in order of resolution */\n  const promisesQueue = []\n  /** Queue of {resolve, reject} in the same order as `promisesQueue` */\n  const resolveRejectQueue = []\n\n  promises.forEach(promise => {\n    // Create a new promise into the promises queue, and keep the {resolve,reject}\n    // in the resolveRejectQueue\n    let resolvePromise\n    let rejectPromise\n    promisesQueue.push(\n      new Promise((resolve, reject) => {\n        resolvePromise = resolve\n        rejectPromise = reject\n      })\n    )\n    resolveRejectQueue.push({ resolve: resolvePromise, reject: rejectPromise })\n\n    // When the promise resolves pick the next available {resolve, reject}, and\n    // through that resolve the next promise in the queue\n    promise.then(\n      result => {\n        const { resolve } = resolveRejectQueue.pop()\n        resolve(result)\n      },\n      async err => {\n        const { reject } = resolveRejectQueue.pop()\n        try {\n          await handleError(err)\n          reject(err)\n        } catch (newError) {\n          reject(newError)\n        }\n      }\n    )\n  })\n\n  // While there are promises left pick the next one to yield\n  // The caller will then wait for the value to resolve.\n  while (promisesQueue.length > 0) {\n    const nextPromise = promisesQueue.pop()\n    yield nextPromise\n  }\n}\n\nmodule.exports = BufferedAsyncIterator\n","const { KafkaJSNonRetriableError } = require('../errors')\n\nconst REJECTED_ERROR = new KafkaJSNonRetriableError(\n  'Queued function aborted due to earlier promise rejection'\n)\nfunction NOOP() {}\n\nconst concurrency = ({ limit, onChange = NOOP } = {}) => {\n  if (isNaN(limit) || typeof limit !== 'number' || limit < 1) {\n    throw new KafkaJSNonRetriableError(`\"limit\" cannot be less than 1`)\n  }\n\n  let waiting = []\n  let semaphore = 0\n\n  const clear = () => {\n    for (const lazyAction of waiting) {\n      lazyAction((_1, _2, reject) => reject(REJECTED_ERROR))\n    }\n    waiting = []\n    semaphore = 0\n  }\n\n  const next = () => {\n    semaphore--\n    onChange(semaphore)\n\n    if (waiting.length > 0) {\n      const lazyAction = waiting.shift()\n      lazyAction()\n    }\n  }\n\n  const invoke = (action, resolve, reject) => {\n    semaphore++\n    onChange(semaphore)\n\n    action()\n      .then(result => {\n        resolve(result)\n        next()\n      })\n      .catch(error => {\n        reject(error)\n        clear()\n      })\n  }\n\n  const push = (action, resolve, reject) => {\n    if (semaphore < limit) {\n      invoke(action, resolve, reject)\n    } else {\n      waiting.push(override => {\n        const execute = override || invoke\n        execute(action, resolve, reject)\n      })\n    }\n  }\n\n  return action => new Promise((resolve, reject) => push(action, resolve, reject))\n}\n\nmodule.exports = concurrency\n","/**\n * Flatten the given arrays into a new array\n *\n * @param {Array<Array<T>>} arrays\n * @returns {Array<T>}\n * @template T\n */\nfunction flatten(arrays) {\n  return [].concat.apply([], arrays)\n}\n\nmodule.exports = flatten\n","module.exports = async (array, groupFn) => {\n  const result = new Map()\n\n  for (const item of array) {\n    const group = await Promise.resolve(groupFn(item))\n    result.set(group, result.has(group) ? [...result.get(group), item] : [item])\n  }\n\n  return result\n}\n","const { format } = require('util')\nconst { KafkaJSLockTimeout } = require('../errors')\n\nconst PRIVATE = {\n  LOCKED: Symbol('private:Lock:locked'),\n  TIMEOUT: Symbol('private:Lock:timeout'),\n  WAITING: Symbol('private:Lock:waiting'),\n  TIMEOUT_ERROR_MESSAGE: Symbol('private:Lock:timeoutErrorMessage'),\n}\n\nconst TIMEOUT_MESSAGE = 'Timeout while acquiring lock (%d waiting locks)'\n\nmodule.exports = class Lock {\n  constructor({ timeout, description = null } = {}) {\n    if (typeof timeout !== 'number') {\n      throw new TypeError(`'timeout' is not a number, received '${typeof timeout}'`)\n    }\n\n    this[PRIVATE.LOCKED] = false\n    this[PRIVATE.TIMEOUT] = timeout\n    this[PRIVATE.WAITING] = new Set()\n    this[PRIVATE.TIMEOUT_ERROR_MESSAGE] = () => {\n      const timeoutMessage = format(TIMEOUT_MESSAGE, this[PRIVATE.WAITING].size)\n      return description ? `${timeoutMessage}: \"${description}\"` : timeoutMessage\n    }\n  }\n\n  async acquire() {\n    return new Promise((resolve, reject) => {\n      if (!this[PRIVATE.LOCKED]) {\n        this[PRIVATE.LOCKED] = true\n        return resolve()\n      }\n\n      let timeoutId = null\n      const tryToAcquire = async () => {\n        if (!this[PRIVATE.LOCKED]) {\n          this[PRIVATE.LOCKED] = true\n          clearTimeout(timeoutId)\n          this[PRIVATE.WAITING].delete(tryToAcquire)\n          return resolve()\n        }\n      }\n\n      this[PRIVATE.WAITING].add(tryToAcquire)\n      timeoutId = setTimeout(() => {\n        // The message should contain the number of waiters _including_ this one\n        const error = new KafkaJSLockTimeout(this[PRIVATE.TIMEOUT_ERROR_MESSAGE]())\n        this[PRIVATE.WAITING].delete(tryToAcquire)\n        reject(error)\n      }, this[PRIVATE.TIMEOUT])\n    })\n  }\n\n  async release() {\n    this[PRIVATE.LOCKED] = false\n    const waitingLock = this[PRIVATE.WAITING].values().next().value\n\n    if (waitingLock) {\n      return waitingLock()\n    }\n  }\n}\n","/**\n * @exports Long\n * @class A Long class for representing a 64 bit int (BigInt)\n * @param {bigint} value The value of the 64 bit int\n * @constructor\n */\nclass Long {\n  constructor(value) {\n    this.value = value\n  }\n\n  /**\n   * @function isLong\n   * @param {*} obj Object\n   * @returns {boolean}\n   * @inner\n   */\n  static isLong(obj) {\n    return typeof obj.value === 'bigint'\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromBits(value) {\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromInt(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt.asIntN(64, BigInt(value)))\n  }\n\n  /**\n   * @param {number} value\n   * @returns {!Long}\n   * @inner\n   */\n  static fromNumber(value) {\n    if (isNaN(value)) return Long.ZERO\n\n    return new Long(BigInt(value))\n  }\n\n  /**\n   * @function\n   * @param {bigint|number|string|Long} val\n   * @returns {!Long}\n   * @inner\n   */\n  static fromValue(val) {\n    if (typeof val === 'number') return this.fromNumber(val)\n    if (typeof val === 'string') return this.fromString(val)\n    if (typeof val === 'bigint') return new Long(val)\n    if (this.isLong(val)) return new Long(BigInt(val.value))\n\n    return new Long(BigInt(val))\n  }\n\n  /**\n   * @param {string} str\n   * @returns {!Long}\n   * @inner\n   */\n  static fromString(str) {\n    if (str.length === 0) throw Error('empty string')\n    if (str === 'NaN' || str === 'Infinity' || str === '+Infinity' || str === '-Infinity')\n      return Long.ZERO\n    return new Long(BigInt(str))\n  }\n\n  /**\n   * Tests if this Long's value equals zero.\n   * @returns {boolean}\n   */\n  isZero() {\n    return this.value === BigInt(0)\n  }\n\n  /**\n   * Tests if this Long's value is negative.\n   * @returns {boolean}\n   */\n  isNegative() {\n    return this.value < BigInt(0)\n  }\n\n  /**\n   * Converts the Long to a string.\n   * @returns {string}\n   * @override\n   */\n  toString() {\n    return String(this.value)\n  }\n\n  /**\n   * Converts the Long to the nearest floating-point representation (double, 53-bit mantissa)\n   * @returns {number}\n   * @override\n   */\n  toNumber() {\n    return Number(this.value)\n  }\n\n  /**\n   * Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.\n   * @returns {number}\n   */\n  toInt() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n\n  /**\n   * Converts the Long to JSON\n   * @returns {string}\n   * @override\n   */\n  toJSON() {\n    return this.toString()\n  }\n\n  /**\n   * Returns this Long with bits shifted to the left by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftLeft(numBits) {\n    return new Long(this.value << BigInt(numBits))\n  }\n\n  /**\n   * Returns this Long with bits arithmetically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRight(numBits) {\n    return new Long(this.value >> BigInt(numBits))\n  }\n\n  /**\n   * Returns the bitwise OR of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  or(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return Long.fromBits(this.value | other.value)\n  }\n\n  /**\n   * Returns the bitwise XOR of this Long and the given one.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  xor(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value ^ other.value)\n  }\n\n  /**\n   * Returns the bitwise AND of this Long and the specified.\n   * @param {bigint|number|string} other Other Long\n   * @returns {!Long}\n   */\n  and(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return new Long(this.value & other.value)\n  }\n\n  /**\n   * Returns the bitwise NOT of this Long.\n   * @returns {!Long}\n   */\n  not() {\n    return new Long(~this.value)\n  }\n\n  /**\n   * Returns this Long with bits logically shifted to the right by the given amount.\n   * @param {number|bigint} numBits Number of bits\n   * @returns {!Long} Shifted bigint\n   */\n  shiftRightUnsigned(numBits) {\n    return new Long(this.value >> BigInt.asUintN(64, BigInt(numBits)))\n  }\n\n  /**\n   * Tests if this Long's value equals the specified's.\n   * @param {bigint|number|string} other Other value\n   * @returns {boolean}\n   */\n  equals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value === other.value\n  }\n\n  /**\n   * Tests if this Long's value is greater than or equal the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  greaterThanOrEqual(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value >= other.value\n  }\n\n  gte(other) {\n    return this.greaterThanOrEqual(other)\n  }\n\n  notEquals(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return !this.equals(/* validates */ other)\n  }\n\n  /**\n   * Returns the sum of this and the specified Long.\n   * @param {!Long|number|string} addend Addend\n   * @returns {!Long} Sum\n   */\n  add(addend) {\n    if (!Long.isLong(addend)) addend = Long.fromValue(addend)\n    return new Long(this.value + addend.value)\n  }\n\n  /**\n   * Returns the difference of this and the specified Long.\n   * @param {!Long|number|string} subtrahend Subtrahend\n   * @returns {!Long} Difference\n   */\n  subtract(subtrahend) {\n    if (!Long.isLong(subtrahend)) subtrahend = Long.fromValue(subtrahend)\n    return this.add(subtrahend.negate())\n  }\n\n  /**\n   * Returns the product of this and the specified Long.\n   * @param {!Long|number|string} multiplier Multiplier\n   * @returns {!Long} Product\n   */\n  multiply(multiplier) {\n    if (this.isZero()) return Long.ZERO\n    if (!Long.isLong(multiplier)) multiplier = Long.fromValue(multiplier)\n    return new Long(this.value * multiplier.value)\n  }\n\n  /**\n   * Returns this Long divided by the specified. The result is signed if this Long is signed or\n   *  unsigned if this Long is unsigned.\n   * @param {!Long|number|string} divisor Divisor\n   * @returns {!Long} Quotient\n   */\n  divide(divisor) {\n    if (!Long.isLong(divisor)) divisor = Long.fromValue(divisor)\n    if (divisor.isZero()) throw Error('division by zero')\n    return new Long(this.value / divisor.value)\n  }\n\n  /**\n   * Compares this Long's value with the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {number} 0 if they are the same, 1 if the this is greater and -1\n   *  if the given one is greater\n   */\n  compare(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    if (this.value === other.value) return 0\n    if (this.value > other.value) return 1\n    if (other.value > this.value) return -1\n  }\n\n  /**\n   * Tests if this Long's value is less than the specified's.\n   * @param {!Long|number|string} other Other value\n   * @returns {boolean}\n   */\n  lessThan(other) {\n    if (!Long.isLong(other)) other = Long.fromValue(other)\n    return this.value < other.value\n  }\n\n  /**\n   * Negates this Long's value.\n   * @returns {!Long} Negated Long\n   */\n  negate() {\n    if (this.equals(Long.MIN_VALUE)) {\n      return Long.MIN_VALUE\n    }\n    return this.not().add(Long.ONE)\n  }\n\n  /**\n   * Gets the high 32 bits as a signed integer.\n   * @returns {number} Signed high bits\n   */\n  getHighBits() {\n    return Number(BigInt.asIntN(32, this.value >> BigInt(32)))\n  }\n\n  /**\n   * Gets the low 32 bits as a signed integer.\n   * @returns {number} Signed low bits\n   */\n  getLowBits() {\n    return Number(BigInt.asIntN(32, this.value))\n  }\n}\n\n/**\n * Minimum signed value.\n * @type {bigint}\n */\nLong.MIN_VALUE = new Long(BigInt('-9223372036854775808'))\n\n/**\n * Maximum signed value.\n * @type {bigint}\n */\nLong.MAX_VALUE = new Long(BigInt('9223372036854775807'))\n\n/**\n * Signed zero.\n * @type {Long}\n */\nLong.ZERO = Long.fromInt(0)\n\n/**\n * Signed one.\n * @type {!Long}\n */\nLong.ONE = Long.fromInt(1)\n\nmodule.exports = Long\n","/**\n * @template T\n * @param { (...args: any) => Promise<T> } [asyncFunction]\n * Promise returning function that will only ever be invoked sequentially.\n * @returns { (...args: any) => Promise<T> }\n * Function that may invoke asyncFunction if there is not a currently executing invocation.\n * Returns promise from the currently executing invocation.\n */\nmodule.exports = asyncFunction => {\n  let promise = null\n\n  return (...args) => {\n    if (promise == null) {\n      promise = asyncFunction(...args).finally(() => (promise = null))\n    }\n    return promise\n  }\n}\n","/**\n * @param {T[]} array\n * @returns T[]\n * @template T\n */\nmodule.exports = array => {\n  if (!Array.isArray(array)) {\n    throw new TypeError(\"'array' is not an array\")\n  }\n\n  if (array.length < 2) {\n    return array\n  }\n\n  const copy = array.slice()\n\n  for (let i = copy.length - 1; i > 0; i--) {\n    const j = Math.floor(Math.random() * (i + 1))\n    const temp = copy[i]\n    copy[i] = copy[j]\n    copy[j] = temp\n  }\n\n  return copy\n}\n","module.exports = timeInMs =>\n  new Promise(resolve => {\n    setTimeout(resolve, timeInMs)\n  })\n","const { keys } = Object\nmodule.exports = object =>\n  keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {})\n","const sleep = require('./sleep')\nconst { KafkaJSTimeout } = require('../errors')\n\nmodule.exports = (\n  fn,\n  { delay = 50, maxWait = 10000, timeoutMessage = 'Timeout', ignoreTimeout = false } = {}\n) => {\n  let timeoutId\n  let totalWait = 0\n  let fulfilled = false\n\n  const checkCondition = async (resolve, reject) => {\n    totalWait += delay\n    await sleep(delay)\n\n    try {\n      const result = await fn(totalWait)\n      if (result) {\n        fulfilled = true\n        clearTimeout(timeoutId)\n        return resolve(result)\n      }\n\n      checkCondition(resolve, reject)\n    } catch (e) {\n      fulfilled = true\n      clearTimeout(timeoutId)\n      reject(e)\n    }\n  }\n\n  return new Promise((resolve, reject) => {\n    checkCondition(resolve, reject)\n\n    if (ignoreTimeout) {\n      return\n    }\n\n    timeoutId = setTimeout(() => {\n      if (!fulfilled) {\n        return reject(new KafkaJSTimeout(timeoutMessage))\n      }\n    }, maxWait)\n  })\n}\n","const BASE_URL = 'https://kafka.js.org'\n\nmodule.exports = (path, hash) => `${BASE_URL}/${path}${hash ? '#' + hash : ''}`\n","var packet = require('dns-packet')\nvar dgram = require('dgram')\nvar thunky = require('thunky')\nvar events = require('events')\nvar os = require('os')\n\nvar noop = function () {}\n\nmodule.exports = function (opts) {\n  if (!opts) opts = {}\n\n  var that = new events.EventEmitter()\n  var port = typeof opts.port === 'number' ? opts.port : 5353\n  var type = opts.type || 'udp4'\n  var ip = opts.ip || opts.host || (type === 'udp4' ? '224.0.0.251' : null)\n  var me = {address: ip, port: port}\n  var memberships = {}\n  var destroyed = false\n  var interval = null\n\n  if (type === 'udp6' && (!ip || !opts.interface)) {\n    throw new Error('For IPv6 multicast you must specify `ip` and `interface`')\n  }\n\n  var socket = opts.socket || dgram.createSocket({\n    type: type,\n    reuseAddr: opts.reuseAddr !== false,\n    toString: function () {\n      return type\n    }\n  })\n\n  socket.on('error', function (err) {\n    if (err.code === 'EACCES' || err.code === 'EADDRINUSE') that.emit('error', err)\n    else that.emit('warning', err)\n  })\n\n  socket.on('message', function (message, rinfo) {\n    try {\n      message = packet.decode(message)\n    } catch (err) {\n      that.emit('warning', err)\n      return\n    }\n\n    that.emit('packet', message, rinfo)\n\n    if (message.type === 'query') that.emit('query', message, rinfo)\n    if (message.type === 'response') that.emit('response', message, rinfo)\n  })\n\n  socket.on('listening', function () {\n    if (!port) port = me.port = socket.address().port\n    if (opts.multicast !== false) {\n      that.update()\n      interval = setInterval(that.update, 5000)\n      socket.setMulticastTTL(opts.ttl || 255)\n      socket.setMulticastLoopback(opts.loopback !== false)\n    }\n  })\n\n  var bind = thunky(function (cb) {\n    if (!port || opts.bind === false) return cb(null)\n    socket.once('error', cb)\n    socket.bind(port, opts.bind || opts.interface, function () {\n      socket.removeListener('error', cb)\n      cb(null)\n    })\n  })\n\n  bind(function (err) {\n    if (err) return that.emit('error', err)\n    that.emit('ready')\n  })\n\n  that.send = function (value, rinfo, cb) {\n    if (typeof rinfo === 'function') return that.send(value, null, rinfo)\n    if (!cb) cb = noop\n    if (!rinfo) rinfo = me\n    else if (!rinfo.host && !rinfo.address) rinfo.address = me.address\n\n    bind(onbind)\n\n    function onbind (err) {\n      if (destroyed) return cb()\n      if (err) return cb(err)\n      var message = packet.encode(value)\n      socket.send(message, 0, message.length, rinfo.port, rinfo.address || rinfo.host, cb)\n    }\n  }\n\n  that.response =\n  that.respond = function (res, rinfo, cb) {\n    if (Array.isArray(res)) res = {answers: res}\n\n    res.type = 'response'\n    res.flags = (res.flags || 0) | packet.AUTHORITATIVE_ANSWER\n    that.send(res, rinfo, cb)\n  }\n\n  that.query = function (q, type, rinfo, cb) {\n    if (typeof type === 'function') return that.query(q, null, null, type)\n    if (typeof type === 'object' && type && type.port) return that.query(q, null, type, rinfo)\n    if (typeof rinfo === 'function') return that.query(q, type, null, rinfo)\n    if (!cb) cb = noop\n\n    if (typeof q === 'string') q = [{name: q, type: type || 'ANY'}]\n    if (Array.isArray(q)) q = {type: 'query', questions: q}\n\n    q.type = 'query'\n    that.send(q, rinfo, cb)\n  }\n\n  that.destroy = function (cb) {\n    if (!cb) cb = noop\n    if (destroyed) return process.nextTick(cb)\n    destroyed = true\n    clearInterval(interval)\n\n    // Need to drop memberships by hand and ignore errors.\n    // socket.close() does not cope with errors.\n    for (var iface in memberships) {\n      try {\n        socket.dropMembership(ip, iface)\n      } catch (e) {\n        // eat it\n      }\n    }\n    memberships = {}\n    socket.close(cb)\n  }\n\n  that.update = function () {\n    var ifaces = opts.interface ? [].concat(opts.interface) : allInterfaces()\n    var updated = false\n\n    for (var i = 0; i < ifaces.length; i++) {\n      var addr = ifaces[i]\n      if (memberships[addr]) continue\n\n      try {\n        socket.addMembership(ip, addr)\n        memberships[addr] = true\n        updated = true\n      } catch (err) {\n        that.emit('warning', err)\n      }\n    }\n\n    if (updated) {\n      if (socket.setMulticastInterface) {\n        try {\n          socket.setMulticastInterface(opts.interface || defaultInterface())\n        } catch (err) {\n          that.emit('warning', err)\n        }\n      }\n      that.emit('networkInterface')\n    }\n  }\n\n  return that\n}\n\nfunction defaultInterface () {\n  var networks = os.networkInterfaces()\n  var names = Object.keys(networks)\n\n  for (var i = 0; i < names.length; i++) {\n    var net = networks[names[i]]\n    for (var j = 0; j < net.length; j++) {\n      var iface = net[j]\n      if (isIPv4(iface.family) && !iface.internal) {\n        if (os.platform() === 'darwin' && names[i] === 'en0') return iface.address\n        return '0.0.0.0'\n      }\n    }\n  }\n\n  return '127.0.0.1'\n}\n\nfunction allInterfaces () {\n  var networks = os.networkInterfaces()\n  var names = Object.keys(networks)\n  var res = []\n\n  for (var i = 0; i < names.length; i++) {\n    var net = networks[names[i]]\n    for (var j = 0; j < net.length; j++) {\n      var iface = net[j]\n      if (isIPv4(iface.family)) {\n        res.push(iface.address)\n        // could only addMembership once per interface (https://nodejs.org/api/dgram.html#dgram_socket_addmembership_multicastaddress_multicastinterface)\n        break\n      }\n    }\n  }\n\n  return res\n}\n\nfunction isIPv4 (family) { // for backwards compat\n  return family === 4 || family === 'IPv4'\n}\n","import crypto from 'crypto'\nimport { urlAlphabet } from './url-alphabet/index.js'\nconst POOL_SIZE_MULTIPLIER = 128\nlet pool, poolOffset\nlet fillPool = bytes => {\n  if (!pool || pool.length < bytes) {\n    pool = Buffer.allocUnsafe(bytes * POOL_SIZE_MULTIPLIER)\n    crypto.randomFillSync(pool)\n    poolOffset = 0\n  } else if (poolOffset + bytes > pool.length) {\n    crypto.randomFillSync(pool)\n    poolOffset = 0\n  }\n  poolOffset += bytes\n}\nlet random = bytes => {\n  fillPool((bytes -= 0))\n  return pool.subarray(poolOffset - bytes, poolOffset)\n}\nlet customRandom = (alphabet, defaultSize, getRandom) => {\n  let mask = (2 << (31 - Math.clz32((alphabet.length - 1) | 1))) - 1\n  let step = Math.ceil((1.6 * mask * defaultSize) / alphabet.length)\n  return (size = defaultSize) => {\n    let id = ''\n    while (true) {\n      let bytes = getRandom(step)\n      let i = step\n      while (i--) {\n        id += alphabet[bytes[i] & mask] || ''\n        if (id.length === size) return id\n      }\n    }\n  }\n}\nlet customAlphabet = (alphabet, size = 21) =>\n  customRandom(alphabet, size, random)\nlet nanoid = (size = 21) => {\n  fillPool((size -= 0))\n  let id = ''\n  for (let i = poolOffset - size; i < poolOffset; i++) {\n    id += urlAlphabet[pool[i] & 63]\n  }\n  return id\n}\nexport { nanoid, customAlphabet, customRandom, urlAlphabet, random }\n","let urlAlphabet =\n  'useandom-26T198340PX75pxJACKVERYMINDBUSHWOLF_GQZbfghjklqvwyzrict'\nexport { urlAlphabet }\n","module.exports = {\n\tcore: {\n\t\tBatch: require(\"./src/Batch\"),\n\t\tClientBuilder: require(\"./src/ClientBuilder\"),\n\t\tbuildClient: require(\"./src/util/buildClients\"),\n\t\tSharedCredentials: require(\"./src/SharedCredentials\"),\n\t\tStaticCredentials: require(\"./src/StaticCredentials\"),\n\t\tErrors: require(\"./src/Errors\"),\n\t},\n\tusStreet: {\n\t\tLookup: require(\"./src/us_street/Lookup\"),\n\t\tCandidate: require(\"./src/us_street/Candidate\"),\n\t},\n\tusZipcode: {\n\t\tLookup: require(\"./src/us_zipcode/Lookup\"),\n\t\tResult: require(\"./src/us_zipcode/Result\"),\n\t},\n\tusAutocomplete: {\n\t\tLookup: require(\"./src/us_autocomplete/Lookup\"),\n\t\tSuggestion: require(\"./src/us_autocomplete/Suggestion\"),\n\t},\n\tusAutocompletePro: {\n\t\tLookup: require(\"./src/us_autocomplete_pro/Lookup\"),\n\t\tSuggestion: require(\"./src/us_autocomplete_pro/Suggestion\"),\n\t},\n\tusExtract: {\n\t\tLookup: require(\"./src/us_extract/Lookup\"),\n\t\tResult: require(\"./src/us_extract/Result\"),\n\t},\n\tinternationalStreet: {\n\t\tLookup: require(\"./src/international_street/Lookup\"),\n\t\tCandidate: require(\"./src/international_street/Candidate\"),\n\t},\n\tusReverseGeo: {\n\t\tLookup: require(\"./src/us_reverse_geo/Lookup\"),\n\t},\n\tinternationalAddressAutocomplete: {\n\t\tLookup: require(\"./src/international_address_autocomplete/Lookup\"),\n\t\tSuggestion: require(\"./src/international_address_autocomplete/Suggestion\"),\n\t},\n};\n","module.exports = require('./lib/axios');","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar buildFullPath = require('../core/buildFullPath');\nvar buildURL = require('./../helpers/buildURL');\nvar http = require('http');\nvar https = require('https');\nvar httpFollow = require('follow-redirects').http;\nvar httpsFollow = require('follow-redirects').https;\nvar url = require('url');\nvar zlib = require('zlib');\nvar VERSION = require('./../env/data').version;\nvar createError = require('../core/createError');\nvar enhanceError = require('../core/enhanceError');\nvar transitionalDefaults = require('../defaults/transitional');\nvar Cancel = require('../cancel/Cancel');\n\nvar isHttps = /https:?/;\n\n/**\n *\n * @param {http.ClientRequestArgs} options\n * @param {AxiosProxyConfig} proxy\n * @param {string} location\n */\nfunction setProxy(options, proxy, location) {\n  options.hostname = proxy.host;\n  options.host = proxy.host;\n  options.port = proxy.port;\n  options.path = location;\n\n  // Basic proxy authorization\n  if (proxy.auth) {\n    var base64 = Buffer.from(proxy.auth.username + ':' + proxy.auth.password, 'utf8').toString('base64');\n    options.headers['Proxy-Authorization'] = 'Basic ' + base64;\n  }\n\n  // If a proxy is used, any redirects must also pass through the proxy\n  options.beforeRedirect = function beforeRedirect(redirection) {\n    redirection.headers.host = redirection.host;\n    setProxy(redirection, proxy, redirection.href);\n  };\n}\n\n/*eslint consistent-return:0*/\nmodule.exports = function httpAdapter(config) {\n  return new Promise(function dispatchHttpRequest(resolvePromise, rejectPromise) {\n    var onCanceled;\n    function done() {\n      if (config.cancelToken) {\n        config.cancelToken.unsubscribe(onCanceled);\n      }\n\n      if (config.signal) {\n        config.signal.removeEventListener('abort', onCanceled);\n      }\n    }\n    var resolve = function resolve(value) {\n      done();\n      resolvePromise(value);\n    };\n    var rejected = false;\n    var reject = function reject(value) {\n      done();\n      rejected = true;\n      rejectPromise(value);\n    };\n    var data = config.data;\n    var headers = config.headers;\n    var headerNames = {};\n\n    Object.keys(headers).forEach(function storeLowerName(name) {\n      headerNames[name.toLowerCase()] = name;\n    });\n\n    // Set User-Agent (required by some servers)\n    // See https://github.com/axios/axios/issues/69\n    if ('user-agent' in headerNames) {\n      // User-Agent is specified; handle case where no UA header is desired\n      if (!headers[headerNames['user-agent']]) {\n        delete headers[headerNames['user-agent']];\n      }\n      // Otherwise, use specified value\n    } else {\n      // Only set header if it hasn't been set in config\n      headers['User-Agent'] = 'axios/' + VERSION;\n    }\n\n    if (data && !utils.isStream(data)) {\n      if (Buffer.isBuffer(data)) {\n        // Nothing to do...\n      } else if (utils.isArrayBuffer(data)) {\n        data = Buffer.from(new Uint8Array(data));\n      } else if (utils.isString(data)) {\n        data = Buffer.from(data, 'utf-8');\n      } else {\n        return reject(createError(\n          'Data after transformation must be a string, an ArrayBuffer, a Buffer, or a Stream',\n          config\n        ));\n      }\n\n      if (config.maxBodyLength > -1 && data.length > config.maxBodyLength) {\n        return reject(createError('Request body larger than maxBodyLength limit', config));\n      }\n\n      // Add Content-Length header if data exists\n      if (!headerNames['content-length']) {\n        headers['Content-Length'] = data.length;\n      }\n    }\n\n    // HTTP basic authentication\n    var auth = undefined;\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password || '';\n      auth = username + ':' + password;\n    }\n\n    // Parse url\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    var parsed = url.parse(fullPath);\n    var protocol = parsed.protocol || 'http:';\n\n    if (!auth && parsed.auth) {\n      var urlAuth = parsed.auth.split(':');\n      var urlUsername = urlAuth[0] || '';\n      var urlPassword = urlAuth[1] || '';\n      auth = urlUsername + ':' + urlPassword;\n    }\n\n    if (auth && headerNames.authorization) {\n      delete headers[headerNames.authorization];\n    }\n\n    var isHttpsRequest = isHttps.test(protocol);\n    var agent = isHttpsRequest ? config.httpsAgent : config.httpAgent;\n\n    try {\n      buildURL(parsed.path, config.params, config.paramsSerializer).replace(/^\\?/, '');\n    } catch (err) {\n      var customErr = new Error(err.message);\n      customErr.config = config;\n      customErr.url = config.url;\n      customErr.exists = true;\n      reject(customErr);\n    }\n\n    var options = {\n      path: buildURL(parsed.path, config.params, config.paramsSerializer).replace(/^\\?/, ''),\n      method: config.method.toUpperCase(),\n      headers: headers,\n      agent: agent,\n      agents: { http: config.httpAgent, https: config.httpsAgent },\n      auth: auth\n    };\n\n    if (config.socketPath) {\n      options.socketPath = config.socketPath;\n    } else {\n      options.hostname = parsed.hostname;\n      options.port = parsed.port;\n    }\n\n    var proxy = config.proxy;\n    if (!proxy && proxy !== false) {\n      var proxyEnv = protocol.slice(0, -1) + '_proxy';\n      var proxyUrl = process.env[proxyEnv] || process.env[proxyEnv.toUpperCase()];\n      if (proxyUrl) {\n        var parsedProxyUrl = url.parse(proxyUrl);\n        var noProxyEnv = process.env.no_proxy || process.env.NO_PROXY;\n        var shouldProxy = true;\n\n        if (noProxyEnv) {\n          var noProxy = noProxyEnv.split(',').map(function trim(s) {\n            return s.trim();\n          });\n\n          shouldProxy = !noProxy.some(function proxyMatch(proxyElement) {\n            if (!proxyElement) {\n              return false;\n            }\n            if (proxyElement === '*') {\n              return true;\n            }\n            if (proxyElement[0] === '.' &&\n                parsed.hostname.substr(parsed.hostname.length - proxyElement.length) === proxyElement) {\n              return true;\n            }\n\n            return parsed.hostname === proxyElement;\n          });\n        }\n\n        if (shouldProxy) {\n          proxy = {\n            host: parsedProxyUrl.hostname,\n            port: parsedProxyUrl.port,\n            protocol: parsedProxyUrl.protocol\n          };\n\n          if (parsedProxyUrl.auth) {\n            var proxyUrlAuth = parsedProxyUrl.auth.split(':');\n            proxy.auth = {\n              username: proxyUrlAuth[0],\n              password: proxyUrlAuth[1]\n            };\n          }\n        }\n      }\n    }\n\n    if (proxy) {\n      options.headers.host = parsed.hostname + (parsed.port ? ':' + parsed.port : '');\n      setProxy(options, proxy, protocol + '//' + parsed.hostname + (parsed.port ? ':' + parsed.port : '') + options.path);\n    }\n\n    var transport;\n    var isHttpsProxy = isHttpsRequest && (proxy ? isHttps.test(proxy.protocol) : true);\n    if (config.transport) {\n      transport = config.transport;\n    } else if (config.maxRedirects === 0) {\n      transport = isHttpsProxy ? https : http;\n    } else {\n      if (config.maxRedirects) {\n        options.maxRedirects = config.maxRedirects;\n      }\n      transport = isHttpsProxy ? httpsFollow : httpFollow;\n    }\n\n    if (config.maxBodyLength > -1) {\n      options.maxBodyLength = config.maxBodyLength;\n    }\n\n    if (config.insecureHTTPParser) {\n      options.insecureHTTPParser = config.insecureHTTPParser;\n    }\n\n    // Create the request\n    var req = transport.request(options, function handleResponse(res) {\n      if (req.aborted) return;\n\n      // uncompress the response body transparently if required\n      var stream = res;\n\n      // return the last request in case of redirects\n      var lastRequest = res.req || req;\n\n\n      // if no content, is HEAD request or decompress disabled we should not decompress\n      if (res.statusCode !== 204 && lastRequest.method !== 'HEAD' && config.decompress !== false) {\n        switch (res.headers['content-encoding']) {\n        /*eslint default-case:0*/\n        case 'gzip':\n        case 'compress':\n        case 'deflate':\n        // add the unzipper to the body stream processing pipeline\n          stream = stream.pipe(zlib.createUnzip());\n\n          // remove the content-encoding in order to not confuse downstream operations\n          delete res.headers['content-encoding'];\n          break;\n        }\n      }\n\n      var response = {\n        status: res.statusCode,\n        statusText: res.statusMessage,\n        headers: res.headers,\n        config: config,\n        request: lastRequest\n      };\n\n      if (config.responseType === 'stream') {\n        response.data = stream;\n        settle(resolve, reject, response);\n      } else {\n        var responseBuffer = [];\n        var totalResponseBytes = 0;\n        stream.on('data', function handleStreamData(chunk) {\n          responseBuffer.push(chunk);\n          totalResponseBytes += chunk.length;\n\n          // make sure the content length is not over the maxContentLength if specified\n          if (config.maxContentLength > -1 && totalResponseBytes > config.maxContentLength) {\n            // stream.destoy() emit aborted event before calling reject() on Node.js v16\n            rejected = true;\n            stream.destroy();\n            reject(createError('maxContentLength size of ' + config.maxContentLength + ' exceeded',\n              config, null, lastRequest));\n          }\n        });\n\n        stream.on('aborted', function handlerStreamAborted() {\n          if (rejected) {\n            return;\n          }\n          stream.destroy();\n          reject(createError('error request aborted', config, 'ERR_REQUEST_ABORTED', lastRequest));\n        });\n\n        stream.on('error', function handleStreamError(err) {\n          if (req.aborted) return;\n          reject(enhanceError(err, config, null, lastRequest));\n        });\n\n        stream.on('end', function handleStreamEnd() {\n          try {\n            var responseData = responseBuffer.length === 1 ? responseBuffer[0] : Buffer.concat(responseBuffer);\n            if (config.responseType !== 'arraybuffer') {\n              responseData = responseData.toString(config.responseEncoding);\n              if (!config.responseEncoding || config.responseEncoding === 'utf8') {\n                responseData = utils.stripBOM(responseData);\n              }\n            }\n            response.data = responseData;\n          } catch (err) {\n            reject(enhanceError(err, config, err.code, response.request, response));\n          }\n          settle(resolve, reject, response);\n        });\n      }\n    });\n\n    // Handle errors\n    req.on('error', function handleRequestError(err) {\n      if (req.aborted && err.code !== 'ERR_FR_TOO_MANY_REDIRECTS') return;\n      reject(enhanceError(err, config, null, req));\n    });\n\n    // set tcp keep alive to prevent drop connection by peer\n    req.on('socket', function handleRequestSocket(socket) {\n      // default interval of sending ack packet is 1 minute\n      socket.setKeepAlive(true, 1000 * 60);\n    });\n\n    // Handle request timeout\n    if (config.timeout) {\n      // This is forcing a int timeout to avoid problems if the `req` interface doesn't handle other types.\n      var timeout = parseInt(config.timeout, 10);\n\n      if (isNaN(timeout)) {\n        reject(createError(\n          'error trying to parse `config.timeout` to int',\n          config,\n          'ERR_PARSE_TIMEOUT',\n          req\n        ));\n\n        return;\n      }\n\n      // Sometime, the response will be very slow, and does not respond, the connect event will be block by event loop system.\n      // And timer callback will be fired, and abort() will be invoked before connection, then get \"socket hang up\" and code ECONNRESET.\n      // At this time, if we have a large number of request, nodejs will hang up some socket on background. and the number will up and up.\n      // And then these socket which be hang up will devoring CPU little by little.\n      // ClientRequest.setTimeout will be fired on the specify milliseconds, and can make sure that abort() will be fired after connect.\n      req.setTimeout(timeout, function handleRequestTimeout() {\n        req.abort();\n        var timeoutErrorMessage = '';\n        if (config.timeoutErrorMessage) {\n          timeoutErrorMessage = config.timeoutErrorMessage;\n        } else {\n          timeoutErrorMessage = 'timeout of ' + config.timeout + 'ms exceeded';\n        }\n        var transitional = config.transitional || transitionalDefaults;\n        reject(createError(\n          timeoutErrorMessage,\n          config,\n          transitional.clarifyTimeoutError ? 'ETIMEDOUT' : 'ECONNABORTED',\n          req\n        ));\n      });\n    }\n\n    if (config.cancelToken || config.signal) {\n      // Handle cancellation\n      // eslint-disable-next-line func-names\n      onCanceled = function(cancel) {\n        if (req.aborted) return;\n\n        req.abort();\n        reject(!cancel || (cancel && cancel.type) ? new Cancel('canceled') : cancel);\n      };\n\n      config.cancelToken && config.cancelToken.subscribe(onCanceled);\n      if (config.signal) {\n        config.signal.aborted ? onCanceled() : config.signal.addEventListener('abort', onCanceled);\n      }\n    }\n\n\n    // Send the request\n    if (utils.isStream(data)) {\n      data.on('error', function handleStreamError(err) {\n        reject(enhanceError(err, config, null, req));\n      }).pipe(req);\n    } else {\n      req.end(data);\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar settle = require('./../core/settle');\nvar cookies = require('./../helpers/cookies');\nvar buildURL = require('./../helpers/buildURL');\nvar buildFullPath = require('../core/buildFullPath');\nvar parseHeaders = require('./../helpers/parseHeaders');\nvar isURLSameOrigin = require('./../helpers/isURLSameOrigin');\nvar createError = require('../core/createError');\nvar transitionalDefaults = require('../defaults/transitional');\nvar Cancel = require('../cancel/Cancel');\n\nmodule.exports = function xhrAdapter(config) {\n  return new Promise(function dispatchXhrRequest(resolve, reject) {\n    var requestData = config.data;\n    var requestHeaders = config.headers;\n    var responseType = config.responseType;\n    var onCanceled;\n    function done() {\n      if (config.cancelToken) {\n        config.cancelToken.unsubscribe(onCanceled);\n      }\n\n      if (config.signal) {\n        config.signal.removeEventListener('abort', onCanceled);\n      }\n    }\n\n    if (utils.isFormData(requestData)) {\n      delete requestHeaders['Content-Type']; // Let the browser set it\n    }\n\n    var request = new XMLHttpRequest();\n\n    // HTTP basic authentication\n    if (config.auth) {\n      var username = config.auth.username || '';\n      var password = config.auth.password ? unescape(encodeURIComponent(config.auth.password)) : '';\n      requestHeaders.Authorization = 'Basic ' + btoa(username + ':' + password);\n    }\n\n    var fullPath = buildFullPath(config.baseURL, config.url);\n    request.open(config.method.toUpperCase(), buildURL(fullPath, config.params, config.paramsSerializer), true);\n\n    // Set the request timeout in MS\n    request.timeout = config.timeout;\n\n    function onloadend() {\n      if (!request) {\n        return;\n      }\n      // Prepare the response\n      var responseHeaders = 'getAllResponseHeaders' in request ? parseHeaders(request.getAllResponseHeaders()) : null;\n      var responseData = !responseType || responseType === 'text' ||  responseType === 'json' ?\n        request.responseText : request.response;\n      var response = {\n        data: responseData,\n        status: request.status,\n        statusText: request.statusText,\n        headers: responseHeaders,\n        config: config,\n        request: request\n      };\n\n      settle(function _resolve(value) {\n        resolve(value);\n        done();\n      }, function _reject(err) {\n        reject(err);\n        done();\n      }, response);\n\n      // Clean up request\n      request = null;\n    }\n\n    if ('onloadend' in request) {\n      // Use onloadend if available\n      request.onloadend = onloadend;\n    } else {\n      // Listen for ready state to emulate onloadend\n      request.onreadystatechange = function handleLoad() {\n        if (!request || request.readyState !== 4) {\n          return;\n        }\n\n        // The request errored out and we didn't get a response, this will be\n        // handled by onerror instead\n        // With one exception: request that using file: protocol, most browsers\n        // will return status as 0 even though it's a successful request\n        if (request.status === 0 && !(request.responseURL && request.responseURL.indexOf('file:') === 0)) {\n          return;\n        }\n        // readystate handler is calling before onerror or ontimeout handlers,\n        // so we should call onloadend on the next 'tick'\n        setTimeout(onloadend);\n      };\n    }\n\n    // Handle browser request cancellation (as opposed to a manual cancellation)\n    request.onabort = function handleAbort() {\n      if (!request) {\n        return;\n      }\n\n      reject(createError('Request aborted', config, 'ECONNABORTED', request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle low level network errors\n    request.onerror = function handleError() {\n      // Real errors are hidden from us by the browser\n      // onerror should only fire if it's a network error\n      reject(createError('Network Error', config, null, request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Handle timeout\n    request.ontimeout = function handleTimeout() {\n      var timeoutErrorMessage = config.timeout ? 'timeout of ' + config.timeout + 'ms exceeded' : 'timeout exceeded';\n      var transitional = config.transitional || transitionalDefaults;\n      if (config.timeoutErrorMessage) {\n        timeoutErrorMessage = config.timeoutErrorMessage;\n      }\n      reject(createError(\n        timeoutErrorMessage,\n        config,\n        transitional.clarifyTimeoutError ? 'ETIMEDOUT' : 'ECONNABORTED',\n        request));\n\n      // Clean up request\n      request = null;\n    };\n\n    // Add xsrf header\n    // This is only done if running in a standard browser environment.\n    // Specifically not if we're in a web worker, or react-native.\n    if (utils.isStandardBrowserEnv()) {\n      // Add xsrf header\n      var xsrfValue = (config.withCredentials || isURLSameOrigin(fullPath)) && config.xsrfCookieName ?\n        cookies.read(config.xsrfCookieName) :\n        undefined;\n\n      if (xsrfValue) {\n        requestHeaders[config.xsrfHeaderName] = xsrfValue;\n      }\n    }\n\n    // Add headers to the request\n    if ('setRequestHeader' in request) {\n      utils.forEach(requestHeaders, function setRequestHeader(val, key) {\n        if (typeof requestData === 'undefined' && key.toLowerCase() === 'content-type') {\n          // Remove Content-Type if data is undefined\n          delete requestHeaders[key];\n        } else {\n          // Otherwise add header to the request\n          request.setRequestHeader(key, val);\n        }\n      });\n    }\n\n    // Add withCredentials to request if needed\n    if (!utils.isUndefined(config.withCredentials)) {\n      request.withCredentials = !!config.withCredentials;\n    }\n\n    // Add responseType to request if needed\n    if (responseType && responseType !== 'json') {\n      request.responseType = config.responseType;\n    }\n\n    // Handle progress if needed\n    if (typeof config.onDownloadProgress === 'function') {\n      request.addEventListener('progress', config.onDownloadProgress);\n    }\n\n    // Not all browsers support upload events\n    if (typeof config.onUploadProgress === 'function' && request.upload) {\n      request.upload.addEventListener('progress', config.onUploadProgress);\n    }\n\n    if (config.cancelToken || config.signal) {\n      // Handle cancellation\n      // eslint-disable-next-line func-names\n      onCanceled = function(cancel) {\n        if (!request) {\n          return;\n        }\n        reject(!cancel || (cancel && cancel.type) ? new Cancel('canceled') : cancel);\n        request.abort();\n        request = null;\n      };\n\n      config.cancelToken && config.cancelToken.subscribe(onCanceled);\n      if (config.signal) {\n        config.signal.aborted ? onCanceled() : config.signal.addEventListener('abort', onCanceled);\n      }\n    }\n\n    if (!requestData) {\n      requestData = null;\n    }\n\n    // Send the request\n    request.send(requestData);\n  });\n};\n","'use strict';\n\nvar utils = require('./utils');\nvar bind = require('./helpers/bind');\nvar Axios = require('./core/Axios');\nvar mergeConfig = require('./core/mergeConfig');\nvar defaults = require('./defaults');\n\n/**\n * Create an instance of Axios\n *\n * @param {Object} defaultConfig The default config for the instance\n * @return {Axios} A new instance of Axios\n */\nfunction createInstance(defaultConfig) {\n  var context = new Axios(defaultConfig);\n  var instance = bind(Axios.prototype.request, context);\n\n  // Copy axios.prototype to instance\n  utils.extend(instance, Axios.prototype, context);\n\n  // Copy context to instance\n  utils.extend(instance, context);\n\n  // Factory for creating new instances\n  instance.create = function create(instanceConfig) {\n    return createInstance(mergeConfig(defaultConfig, instanceConfig));\n  };\n\n  return instance;\n}\n\n// Create the default instance to be exported\nvar axios = createInstance(defaults);\n\n// Expose Axios class to allow class inheritance\naxios.Axios = Axios;\n\n// Expose Cancel & CancelToken\naxios.Cancel = require('./cancel/Cancel');\naxios.CancelToken = require('./cancel/CancelToken');\naxios.isCancel = require('./cancel/isCancel');\naxios.VERSION = require('./env/data').version;\n\n// Expose all/spread\naxios.all = function all(promises) {\n  return Promise.all(promises);\n};\naxios.spread = require('./helpers/spread');\n\n// Expose isAxiosError\naxios.isAxiosError = require('./helpers/isAxiosError');\n\nmodule.exports = axios;\n\n// Allow use of default import syntax in TypeScript\nmodule.exports.default = axios;\n","'use strict';\n\n/**\n * A `Cancel` is an object that is thrown when an operation is canceled.\n *\n * @class\n * @param {string=} message The message.\n */\nfunction Cancel(message) {\n  this.message = message;\n}\n\nCancel.prototype.toString = function toString() {\n  return 'Cancel' + (this.message ? ': ' + this.message : '');\n};\n\nCancel.prototype.__CANCEL__ = true;\n\nmodule.exports = Cancel;\n","'use strict';\n\nvar Cancel = require('./Cancel');\n\n/**\n * A `CancelToken` is an object that can be used to request cancellation of an operation.\n *\n * @class\n * @param {Function} executor The executor function.\n */\nfunction CancelToken(executor) {\n  if (typeof executor !== 'function') {\n    throw new TypeError('executor must be a function.');\n  }\n\n  var resolvePromise;\n\n  this.promise = new Promise(function promiseExecutor(resolve) {\n    resolvePromise = resolve;\n  });\n\n  var token = this;\n\n  // eslint-disable-next-line func-names\n  this.promise.then(function(cancel) {\n    if (!token._listeners) return;\n\n    var i;\n    var l = token._listeners.length;\n\n    for (i = 0; i < l; i++) {\n      token._listeners[i](cancel);\n    }\n    token._listeners = null;\n  });\n\n  // eslint-disable-next-line func-names\n  this.promise.then = function(onfulfilled) {\n    var _resolve;\n    // eslint-disable-next-line func-names\n    var promise = new Promise(function(resolve) {\n      token.subscribe(resolve);\n      _resolve = resolve;\n    }).then(onfulfilled);\n\n    promise.cancel = function reject() {\n      token.unsubscribe(_resolve);\n    };\n\n    return promise;\n  };\n\n  executor(function cancel(message) {\n    if (token.reason) {\n      // Cancellation has already been requested\n      return;\n    }\n\n    token.reason = new Cancel(message);\n    resolvePromise(token.reason);\n  });\n}\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nCancelToken.prototype.throwIfRequested = function throwIfRequested() {\n  if (this.reason) {\n    throw this.reason;\n  }\n};\n\n/**\n * Subscribe to the cancel signal\n */\n\nCancelToken.prototype.subscribe = function subscribe(listener) {\n  if (this.reason) {\n    listener(this.reason);\n    return;\n  }\n\n  if (this._listeners) {\n    this._listeners.push(listener);\n  } else {\n    this._listeners = [listener];\n  }\n};\n\n/**\n * Unsubscribe from the cancel signal\n */\n\nCancelToken.prototype.unsubscribe = function unsubscribe(listener) {\n  if (!this._listeners) {\n    return;\n  }\n  var index = this._listeners.indexOf(listener);\n  if (index !== -1) {\n    this._listeners.splice(index, 1);\n  }\n};\n\n/**\n * Returns an object that contains a new `CancelToken` and a function that, when called,\n * cancels the `CancelToken`.\n */\nCancelToken.source = function source() {\n  var cancel;\n  var token = new CancelToken(function executor(c) {\n    cancel = c;\n  });\n  return {\n    token: token,\n    cancel: cancel\n  };\n};\n\nmodule.exports = CancelToken;\n","'use strict';\n\nmodule.exports = function isCancel(value) {\n  return !!(value && value.__CANCEL__);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar buildURL = require('../helpers/buildURL');\nvar InterceptorManager = require('./InterceptorManager');\nvar dispatchRequest = require('./dispatchRequest');\nvar mergeConfig = require('./mergeConfig');\nvar validator = require('../helpers/validator');\n\nvar validators = validator.validators;\n/**\n * Create a new instance of Axios\n *\n * @param {Object} instanceConfig The default config for the instance\n */\nfunction Axios(instanceConfig) {\n  this.defaults = instanceConfig;\n  this.interceptors = {\n    request: new InterceptorManager(),\n    response: new InterceptorManager()\n  };\n}\n\n/**\n * Dispatch a request\n *\n * @param {Object} config The config specific for this request (merged with this.defaults)\n */\nAxios.prototype.request = function request(configOrUrl, config) {\n  /*eslint no-param-reassign:0*/\n  // Allow for axios('example/url'[, config]) a la fetch API\n  if (typeof configOrUrl === 'string') {\n    config = config || {};\n    config.url = configOrUrl;\n  } else {\n    config = configOrUrl || {};\n  }\n\n  config = mergeConfig(this.defaults, config);\n\n  // Set config.method\n  if (config.method) {\n    config.method = config.method.toLowerCase();\n  } else if (this.defaults.method) {\n    config.method = this.defaults.method.toLowerCase();\n  } else {\n    config.method = 'get';\n  }\n\n  var transitional = config.transitional;\n\n  if (transitional !== undefined) {\n    validator.assertOptions(transitional, {\n      silentJSONParsing: validators.transitional(validators.boolean),\n      forcedJSONParsing: validators.transitional(validators.boolean),\n      clarifyTimeoutError: validators.transitional(validators.boolean)\n    }, false);\n  }\n\n  // filter out skipped interceptors\n  var requestInterceptorChain = [];\n  var synchronousRequestInterceptors = true;\n  this.interceptors.request.forEach(function unshiftRequestInterceptors(interceptor) {\n    if (typeof interceptor.runWhen === 'function' && interceptor.runWhen(config) === false) {\n      return;\n    }\n\n    synchronousRequestInterceptors = synchronousRequestInterceptors && interceptor.synchronous;\n\n    requestInterceptorChain.unshift(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  var responseInterceptorChain = [];\n  this.interceptors.response.forEach(function pushResponseInterceptors(interceptor) {\n    responseInterceptorChain.push(interceptor.fulfilled, interceptor.rejected);\n  });\n\n  var promise;\n\n  if (!synchronousRequestInterceptors) {\n    var chain = [dispatchRequest, undefined];\n\n    Array.prototype.unshift.apply(chain, requestInterceptorChain);\n    chain = chain.concat(responseInterceptorChain);\n\n    promise = Promise.resolve(config);\n    while (chain.length) {\n      promise = promise.then(chain.shift(), chain.shift());\n    }\n\n    return promise;\n  }\n\n\n  var newConfig = config;\n  while (requestInterceptorChain.length) {\n    var onFulfilled = requestInterceptorChain.shift();\n    var onRejected = requestInterceptorChain.shift();\n    try {\n      newConfig = onFulfilled(newConfig);\n    } catch (error) {\n      onRejected(error);\n      break;\n    }\n  }\n\n  try {\n    promise = dispatchRequest(newConfig);\n  } catch (error) {\n    return Promise.reject(error);\n  }\n\n  while (responseInterceptorChain.length) {\n    promise = promise.then(responseInterceptorChain.shift(), responseInterceptorChain.shift());\n  }\n\n  return promise;\n};\n\nAxios.prototype.getUri = function getUri(config) {\n  config = mergeConfig(this.defaults, config);\n  return buildURL(config.url, config.params, config.paramsSerializer).replace(/^\\?/, '');\n};\n\n// Provide aliases for supported request methods\nutils.forEach(['delete', 'get', 'head', 'options'], function forEachMethodNoData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: (config || {}).data\n    }));\n  };\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  /*eslint func-names:0*/\n  Axios.prototype[method] = function(url, data, config) {\n    return this.request(mergeConfig(config || {}, {\n      method: method,\n      url: url,\n      data: data\n    }));\n  };\n});\n\nmodule.exports = Axios;\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction InterceptorManager() {\n  this.handlers = [];\n}\n\n/**\n * Add a new interceptor to the stack\n *\n * @param {Function} fulfilled The function to handle `then` for a `Promise`\n * @param {Function} rejected The function to handle `reject` for a `Promise`\n *\n * @return {Number} An ID used to remove interceptor later\n */\nInterceptorManager.prototype.use = function use(fulfilled, rejected, options) {\n  this.handlers.push({\n    fulfilled: fulfilled,\n    rejected: rejected,\n    synchronous: options ? options.synchronous : false,\n    runWhen: options ? options.runWhen : null\n  });\n  return this.handlers.length - 1;\n};\n\n/**\n * Remove an interceptor from the stack\n *\n * @param {Number} id The ID that was returned by `use`\n */\nInterceptorManager.prototype.eject = function eject(id) {\n  if (this.handlers[id]) {\n    this.handlers[id] = null;\n  }\n};\n\n/**\n * Iterate over all the registered interceptors\n *\n * This method is particularly useful for skipping over any\n * interceptors that may have become `null` calling `eject`.\n *\n * @param {Function} fn The function to call for each interceptor\n */\nInterceptorManager.prototype.forEach = function forEach(fn) {\n  utils.forEach(this.handlers, function forEachHandler(h) {\n    if (h !== null) {\n      fn(h);\n    }\n  });\n};\n\nmodule.exports = InterceptorManager;\n","'use strict';\n\nvar isAbsoluteURL = require('../helpers/isAbsoluteURL');\nvar combineURLs = require('../helpers/combineURLs');\n\n/**\n * Creates a new URL by combining the baseURL with the requestedURL,\n * only when the requestedURL is not already an absolute URL.\n * If the requestURL is absolute, this function returns the requestedURL untouched.\n *\n * @param {string} baseURL The base URL\n * @param {string} requestedURL Absolute or relative URL to combine\n * @returns {string} The combined full path\n */\nmodule.exports = function buildFullPath(baseURL, requestedURL) {\n  if (baseURL && !isAbsoluteURL(requestedURL)) {\n    return combineURLs(baseURL, requestedURL);\n  }\n  return requestedURL;\n};\n","'use strict';\n\nvar enhanceError = require('./enhanceError');\n\n/**\n * Create an Error with the specified message, config, error code, request and response.\n *\n * @param {string} message The error message.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The created error.\n */\nmodule.exports = function createError(message, config, code, request, response) {\n  var error = new Error(message);\n  return enhanceError(error, config, code, request, response);\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar transformData = require('./transformData');\nvar isCancel = require('../cancel/isCancel');\nvar defaults = require('../defaults');\nvar Cancel = require('../cancel/Cancel');\n\n/**\n * Throws a `Cancel` if cancellation has been requested.\n */\nfunction throwIfCancellationRequested(config) {\n  if (config.cancelToken) {\n    config.cancelToken.throwIfRequested();\n  }\n\n  if (config.signal && config.signal.aborted) {\n    throw new Cancel('canceled');\n  }\n}\n\n/**\n * Dispatch a request to the server using the configured adapter.\n *\n * @param {object} config The config that is to be used for the request\n * @returns {Promise} The Promise to be fulfilled\n */\nmodule.exports = function dispatchRequest(config) {\n  throwIfCancellationRequested(config);\n\n  // Ensure headers exist\n  config.headers = config.headers || {};\n\n  // Transform request data\n  config.data = transformData.call(\n    config,\n    config.data,\n    config.headers,\n    config.transformRequest\n  );\n\n  // Flatten headers\n  config.headers = utils.merge(\n    config.headers.common || {},\n    config.headers[config.method] || {},\n    config.headers\n  );\n\n  utils.forEach(\n    ['delete', 'get', 'head', 'post', 'put', 'patch', 'common'],\n    function cleanHeaderConfig(method) {\n      delete config.headers[method];\n    }\n  );\n\n  var adapter = config.adapter || defaults.adapter;\n\n  return adapter(config).then(function onAdapterResolution(response) {\n    throwIfCancellationRequested(config);\n\n    // Transform response data\n    response.data = transformData.call(\n      config,\n      response.data,\n      response.headers,\n      config.transformResponse\n    );\n\n    return response;\n  }, function onAdapterRejection(reason) {\n    if (!isCancel(reason)) {\n      throwIfCancellationRequested(config);\n\n      // Transform response data\n      if (reason && reason.response) {\n        reason.response.data = transformData.call(\n          config,\n          reason.response.data,\n          reason.response.headers,\n          config.transformResponse\n        );\n      }\n    }\n\n    return Promise.reject(reason);\n  });\n};\n","'use strict';\n\n/**\n * Update an Error with the specified config, error code, and response.\n *\n * @param {Error} error The error to update.\n * @param {Object} config The config.\n * @param {string} [code] The error code (for example, 'ECONNABORTED').\n * @param {Object} [request] The request.\n * @param {Object} [response] The response.\n * @returns {Error} The error.\n */\nmodule.exports = function enhanceError(error, config, code, request, response) {\n  error.config = config;\n  if (code) {\n    error.code = code;\n  }\n\n  error.request = request;\n  error.response = response;\n  error.isAxiosError = true;\n\n  error.toJSON = function toJSON() {\n    return {\n      // Standard\n      message: this.message,\n      name: this.name,\n      // Microsoft\n      description: this.description,\n      number: this.number,\n      // Mozilla\n      fileName: this.fileName,\n      lineNumber: this.lineNumber,\n      columnNumber: this.columnNumber,\n      stack: this.stack,\n      // Axios\n      config: this.config,\n      code: this.code,\n      status: this.response && this.response.status ? this.response.status : null\n    };\n  };\n  return error;\n};\n","'use strict';\n\nvar utils = require('../utils');\n\n/**\n * Config-specific merge-function which creates a new config-object\n * by merging two configuration objects together.\n *\n * @param {Object} config1\n * @param {Object} config2\n * @returns {Object} New object resulting from merging config2 to config1\n */\nmodule.exports = function mergeConfig(config1, config2) {\n  // eslint-disable-next-line no-param-reassign\n  config2 = config2 || {};\n  var config = {};\n\n  function getMergedValue(target, source) {\n    if (utils.isPlainObject(target) && utils.isPlainObject(source)) {\n      return utils.merge(target, source);\n    } else if (utils.isPlainObject(source)) {\n      return utils.merge({}, source);\n    } else if (utils.isArray(source)) {\n      return source.slice();\n    }\n    return source;\n  }\n\n  // eslint-disable-next-line consistent-return\n  function mergeDeepProperties(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      return getMergedValue(config1[prop], config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      return getMergedValue(undefined, config1[prop]);\n    }\n  }\n\n  // eslint-disable-next-line consistent-return\n  function valueFromConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      return getMergedValue(undefined, config2[prop]);\n    }\n  }\n\n  // eslint-disable-next-line consistent-return\n  function defaultToConfig2(prop) {\n    if (!utils.isUndefined(config2[prop])) {\n      return getMergedValue(undefined, config2[prop]);\n    } else if (!utils.isUndefined(config1[prop])) {\n      return getMergedValue(undefined, config1[prop]);\n    }\n  }\n\n  // eslint-disable-next-line consistent-return\n  function mergeDirectKeys(prop) {\n    if (prop in config2) {\n      return getMergedValue(config1[prop], config2[prop]);\n    } else if (prop in config1) {\n      return getMergedValue(undefined, config1[prop]);\n    }\n  }\n\n  var mergeMap = {\n    'url': valueFromConfig2,\n    'method': valueFromConfig2,\n    'data': valueFromConfig2,\n    'baseURL': defaultToConfig2,\n    'transformRequest': defaultToConfig2,\n    'transformResponse': defaultToConfig2,\n    'paramsSerializer': defaultToConfig2,\n    'timeout': defaultToConfig2,\n    'timeoutMessage': defaultToConfig2,\n    'withCredentials': defaultToConfig2,\n    'adapter': defaultToConfig2,\n    'responseType': defaultToConfig2,\n    'xsrfCookieName': defaultToConfig2,\n    'xsrfHeaderName': defaultToConfig2,\n    'onUploadProgress': defaultToConfig2,\n    'onDownloadProgress': defaultToConfig2,\n    'decompress': defaultToConfig2,\n    'maxContentLength': defaultToConfig2,\n    'maxBodyLength': defaultToConfig2,\n    'transport': defaultToConfig2,\n    'httpAgent': defaultToConfig2,\n    'httpsAgent': defaultToConfig2,\n    'cancelToken': defaultToConfig2,\n    'socketPath': defaultToConfig2,\n    'responseEncoding': defaultToConfig2,\n    'validateStatus': mergeDirectKeys\n  };\n\n  utils.forEach(Object.keys(config1).concat(Object.keys(config2)), function computeConfigValue(prop) {\n    var merge = mergeMap[prop] || mergeDeepProperties;\n    var configValue = merge(prop);\n    (utils.isUndefined(configValue) && merge !== mergeDirectKeys) || (config[prop] = configValue);\n  });\n\n  return config;\n};\n","'use strict';\n\nvar createError = require('./createError');\n\n/**\n * Resolve or reject a Promise based on response status.\n *\n * @param {Function} resolve A function that resolves the promise.\n * @param {Function} reject A function that rejects the promise.\n * @param {object} response The response.\n */\nmodule.exports = function settle(resolve, reject, response) {\n  var validateStatus = response.config.validateStatus;\n  if (!response.status || !validateStatus || validateStatus(response.status)) {\n    resolve(response);\n  } else {\n    reject(createError(\n      'Request failed with status code ' + response.status,\n      response.config,\n      null,\n      response.request,\n      response\n    ));\n  }\n};\n","'use strict';\n\nvar utils = require('./../utils');\nvar defaults = require('../defaults');\n\n/**\n * Transform the data for a request or a response\n *\n * @param {Object|String} data The data to be transformed\n * @param {Array} headers The headers for the request or response\n * @param {Array|Function} fns A single function or Array of functions\n * @returns {*} The resulting transformed data\n */\nmodule.exports = function transformData(data, headers, fns) {\n  var context = this || defaults;\n  /*eslint no-param-reassign:0*/\n  utils.forEach(fns, function transform(fn) {\n    data = fn.call(context, data, headers);\n  });\n\n  return data;\n};\n","'use strict';\n\nvar utils = require('../utils');\nvar normalizeHeaderName = require('../helpers/normalizeHeaderName');\nvar enhanceError = require('../core/enhanceError');\nvar transitionalDefaults = require('./transitional');\n\nvar DEFAULT_CONTENT_TYPE = {\n  'Content-Type': 'application/x-www-form-urlencoded'\n};\n\nfunction setContentTypeIfUnset(headers, value) {\n  if (!utils.isUndefined(headers) && utils.isUndefined(headers['Content-Type'])) {\n    headers['Content-Type'] = value;\n  }\n}\n\nfunction getDefaultAdapter() {\n  var adapter;\n  if (typeof XMLHttpRequest !== 'undefined') {\n    // For browsers use XHR adapter\n    adapter = require('../adapters/xhr');\n  } else if (typeof process !== 'undefined' && Object.prototype.toString.call(process) === '[object process]') {\n    // For node use HTTP adapter\n    adapter = require('../adapters/http');\n  }\n  return adapter;\n}\n\nfunction stringifySafely(rawValue, parser, encoder) {\n  if (utils.isString(rawValue)) {\n    try {\n      (parser || JSON.parse)(rawValue);\n      return utils.trim(rawValue);\n    } catch (e) {\n      if (e.name !== 'SyntaxError') {\n        throw e;\n      }\n    }\n  }\n\n  return (encoder || JSON.stringify)(rawValue);\n}\n\nvar defaults = {\n\n  transitional: transitionalDefaults,\n\n  adapter: getDefaultAdapter(),\n\n  transformRequest: [function transformRequest(data, headers) {\n    normalizeHeaderName(headers, 'Accept');\n    normalizeHeaderName(headers, 'Content-Type');\n\n    if (utils.isFormData(data) ||\n      utils.isArrayBuffer(data) ||\n      utils.isBuffer(data) ||\n      utils.isStream(data) ||\n      utils.isFile(data) ||\n      utils.isBlob(data)\n    ) {\n      return data;\n    }\n    if (utils.isArrayBufferView(data)) {\n      return data.buffer;\n    }\n    if (utils.isURLSearchParams(data)) {\n      setContentTypeIfUnset(headers, 'application/x-www-form-urlencoded;charset=utf-8');\n      return data.toString();\n    }\n    if (utils.isObject(data) || (headers && headers['Content-Type'] === 'application/json')) {\n      setContentTypeIfUnset(headers, 'application/json');\n      return stringifySafely(data);\n    }\n    return data;\n  }],\n\n  transformResponse: [function transformResponse(data) {\n    var transitional = this.transitional || defaults.transitional;\n    var silentJSONParsing = transitional && transitional.silentJSONParsing;\n    var forcedJSONParsing = transitional && transitional.forcedJSONParsing;\n    var strictJSONParsing = !silentJSONParsing && this.responseType === 'json';\n\n    if (strictJSONParsing || (forcedJSONParsing && utils.isString(data) && data.length)) {\n      try {\n        return JSON.parse(data);\n      } catch (e) {\n        if (strictJSONParsing) {\n          if (e.name === 'SyntaxError') {\n            throw enhanceError(e, this, 'E_JSON_PARSE');\n          }\n          throw e;\n        }\n      }\n    }\n\n    return data;\n  }],\n\n  /**\n   * A timeout in milliseconds to abort a request. If set to 0 (default) a\n   * timeout is not created.\n   */\n  timeout: 0,\n\n  xsrfCookieName: 'XSRF-TOKEN',\n  xsrfHeaderName: 'X-XSRF-TOKEN',\n\n  maxContentLength: -1,\n  maxBodyLength: -1,\n\n  validateStatus: function validateStatus(status) {\n    return status >= 200 && status < 300;\n  },\n\n  headers: {\n    common: {\n      'Accept': 'application/json, text/plain, */*'\n    }\n  }\n};\n\nutils.forEach(['delete', 'get', 'head'], function forEachMethodNoData(method) {\n  defaults.headers[method] = {};\n});\n\nutils.forEach(['post', 'put', 'patch'], function forEachMethodWithData(method) {\n  defaults.headers[method] = utils.merge(DEFAULT_CONTENT_TYPE);\n});\n\nmodule.exports = defaults;\n","'use strict';\n\nmodule.exports = {\n  silentJSONParsing: true,\n  forcedJSONParsing: true,\n  clarifyTimeoutError: false\n};\n","module.exports = {\n  \"version\": \"0.26.1\"\n};","'use strict';\n\nmodule.exports = function bind(fn, thisArg) {\n  return function wrap() {\n    var args = new Array(arguments.length);\n    for (var i = 0; i < args.length; i++) {\n      args[i] = arguments[i];\n    }\n    return fn.apply(thisArg, args);\n  };\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nfunction encode(val) {\n  return encodeURIComponent(val).\n    replace(/%3A/gi, ':').\n    replace(/%24/g, '$').\n    replace(/%2C/gi, ',').\n    replace(/%20/g, '+').\n    replace(/%5B/gi, '[').\n    replace(/%5D/gi, ']');\n}\n\n/**\n * Build a URL by appending params to the end\n *\n * @param {string} url The base of the url (e.g., http://www.google.com)\n * @param {object} [params] The params to be appended\n * @returns {string} The formatted url\n */\nmodule.exports = function buildURL(url, params, paramsSerializer) {\n  /*eslint no-param-reassign:0*/\n  if (!params) {\n    return url;\n  }\n\n  var serializedParams;\n  if (paramsSerializer) {\n    serializedParams = paramsSerializer(params);\n  } else if (utils.isURLSearchParams(params)) {\n    serializedParams = params.toString();\n  } else {\n    var parts = [];\n\n    utils.forEach(params, function serialize(val, key) {\n      if (val === null || typeof val === 'undefined') {\n        return;\n      }\n\n      if (utils.isArray(val)) {\n        key = key + '[]';\n      } else {\n        val = [val];\n      }\n\n      utils.forEach(val, function parseValue(v) {\n        if (utils.isDate(v)) {\n          v = v.toISOString();\n        } else if (utils.isObject(v)) {\n          v = JSON.stringify(v);\n        }\n        parts.push(encode(key) + '=' + encode(v));\n      });\n    });\n\n    serializedParams = parts.join('&');\n  }\n\n  if (serializedParams) {\n    var hashmarkIndex = url.indexOf('#');\n    if (hashmarkIndex !== -1) {\n      url = url.slice(0, hashmarkIndex);\n    }\n\n    url += (url.indexOf('?') === -1 ? '?' : '&') + serializedParams;\n  }\n\n  return url;\n};\n","'use strict';\n\n/**\n * Creates a new URL by combining the specified URLs\n *\n * @param {string} baseURL The base URL\n * @param {string} relativeURL The relative URL\n * @returns {string} The combined URL\n */\nmodule.exports = function combineURLs(baseURL, relativeURL) {\n  return relativeURL\n    ? baseURL.replace(/\\/+$/, '') + '/' + relativeURL.replace(/^\\/+/, '')\n    : baseURL;\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs support document.cookie\n    (function standardBrowserEnv() {\n      return {\n        write: function write(name, value, expires, path, domain, secure) {\n          var cookie = [];\n          cookie.push(name + '=' + encodeURIComponent(value));\n\n          if (utils.isNumber(expires)) {\n            cookie.push('expires=' + new Date(expires).toGMTString());\n          }\n\n          if (utils.isString(path)) {\n            cookie.push('path=' + path);\n          }\n\n          if (utils.isString(domain)) {\n            cookie.push('domain=' + domain);\n          }\n\n          if (secure === true) {\n            cookie.push('secure');\n          }\n\n          document.cookie = cookie.join('; ');\n        },\n\n        read: function read(name) {\n          var match = document.cookie.match(new RegExp('(^|;\\\\s*)(' + name + ')=([^;]*)'));\n          return (match ? decodeURIComponent(match[3]) : null);\n        },\n\n        remove: function remove(name) {\n          this.write(name, '', Date.now() - 86400000);\n        }\n      };\n    })() :\n\n  // Non standard browser env (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return {\n        write: function write() {},\n        read: function read() { return null; },\n        remove: function remove() {}\n      };\n    })()\n);\n","'use strict';\n\n/**\n * Determines whether the specified URL is absolute\n *\n * @param {string} url The URL to test\n * @returns {boolean} True if the specified URL is absolute, otherwise false\n */\nmodule.exports = function isAbsoluteURL(url) {\n  // A URL is considered absolute if it begins with \"<scheme>://\" or \"//\" (protocol-relative URL).\n  // RFC 3986 defines scheme name as a sequence of characters beginning with a letter and followed\n  // by any combination of letters, digits, plus, period, or hyphen.\n  return /^([a-z][a-z\\d+\\-.]*:)?\\/\\//i.test(url);\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n/**\n * Determines whether the payload is an error thrown by Axios\n *\n * @param {*} payload The value to test\n * @returns {boolean} True if the payload is an error thrown by Axios, otherwise false\n */\nmodule.exports = function isAxiosError(payload) {\n  return utils.isObject(payload) && (payload.isAxiosError === true);\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\nmodule.exports = (\n  utils.isStandardBrowserEnv() ?\n\n  // Standard browser envs have full support of the APIs needed to test\n  // whether the request URL is of the same origin as current location.\n    (function standardBrowserEnv() {\n      var msie = /(msie|trident)/i.test(navigator.userAgent);\n      var urlParsingNode = document.createElement('a');\n      var originURL;\n\n      /**\n    * Parse a URL to discover it's components\n    *\n    * @param {String} url The URL to be parsed\n    * @returns {Object}\n    */\n      function resolveURL(url) {\n        var href = url;\n\n        if (msie) {\n        // IE needs attribute set twice to normalize properties\n          urlParsingNode.setAttribute('href', href);\n          href = urlParsingNode.href;\n        }\n\n        urlParsingNode.setAttribute('href', href);\n\n        // urlParsingNode provides the UrlUtils interface - http://url.spec.whatwg.org/#urlutils\n        return {\n          href: urlParsingNode.href,\n          protocol: urlParsingNode.protocol ? urlParsingNode.protocol.replace(/:$/, '') : '',\n          host: urlParsingNode.host,\n          search: urlParsingNode.search ? urlParsingNode.search.replace(/^\\?/, '') : '',\n          hash: urlParsingNode.hash ? urlParsingNode.hash.replace(/^#/, '') : '',\n          hostname: urlParsingNode.hostname,\n          port: urlParsingNode.port,\n          pathname: (urlParsingNode.pathname.charAt(0) === '/') ?\n            urlParsingNode.pathname :\n            '/' + urlParsingNode.pathname\n        };\n      }\n\n      originURL = resolveURL(window.location.href);\n\n      /**\n    * Determine if a URL shares the same origin as the current location\n    *\n    * @param {String} requestURL The URL to test\n    * @returns {boolean} True if URL shares the same origin, otherwise false\n    */\n      return function isURLSameOrigin(requestURL) {\n        var parsed = (utils.isString(requestURL)) ? resolveURL(requestURL) : requestURL;\n        return (parsed.protocol === originURL.protocol &&\n            parsed.host === originURL.host);\n      };\n    })() :\n\n  // Non standard browser envs (web workers, react-native) lack needed support.\n    (function nonStandardBrowserEnv() {\n      return function isURLSameOrigin() {\n        return true;\n      };\n    })()\n);\n","'use strict';\n\nvar utils = require('../utils');\n\nmodule.exports = function normalizeHeaderName(headers, normalizedName) {\n  utils.forEach(headers, function processHeader(value, name) {\n    if (name !== normalizedName && name.toUpperCase() === normalizedName.toUpperCase()) {\n      headers[normalizedName] = value;\n      delete headers[name];\n    }\n  });\n};\n","'use strict';\n\nvar utils = require('./../utils');\n\n// Headers whose duplicates are ignored by node\n// c.f. https://nodejs.org/api/http.html#http_message_headers\nvar ignoreDuplicateOf = [\n  'age', 'authorization', 'content-length', 'content-type', 'etag',\n  'expires', 'from', 'host', 'if-modified-since', 'if-unmodified-since',\n  'last-modified', 'location', 'max-forwards', 'proxy-authorization',\n  'referer', 'retry-after', 'user-agent'\n];\n\n/**\n * Parse headers into an object\n *\n * ```\n * Date: Wed, 27 Aug 2014 08:58:49 GMT\n * Content-Type: application/json\n * Connection: keep-alive\n * Transfer-Encoding: chunked\n * ```\n *\n * @param {String} headers Headers needing to be parsed\n * @returns {Object} Headers parsed into an object\n */\nmodule.exports = function parseHeaders(headers) {\n  var parsed = {};\n  var key;\n  var val;\n  var i;\n\n  if (!headers) { return parsed; }\n\n  utils.forEach(headers.split('\\n'), function parser(line) {\n    i = line.indexOf(':');\n    key = utils.trim(line.substr(0, i)).toLowerCase();\n    val = utils.trim(line.substr(i + 1));\n\n    if (key) {\n      if (parsed[key] && ignoreDuplicateOf.indexOf(key) >= 0) {\n        return;\n      }\n      if (key === 'set-cookie') {\n        parsed[key] = (parsed[key] ? parsed[key] : []).concat([val]);\n      } else {\n        parsed[key] = parsed[key] ? parsed[key] + ', ' + val : val;\n      }\n    }\n  });\n\n  return parsed;\n};\n","'use strict';\n\n/**\n * Syntactic sugar for invoking a function and expanding an array for arguments.\n *\n * Common use case would be to use `Function.prototype.apply`.\n *\n *  ```js\n *  function f(x, y, z) {}\n *  var args = [1, 2, 3];\n *  f.apply(null, args);\n *  ```\n *\n * With `spread` this example can be re-written.\n *\n *  ```js\n *  spread(function(x, y, z) {})([1, 2, 3]);\n *  ```\n *\n * @param {Function} callback\n * @returns {Function}\n */\nmodule.exports = function spread(callback) {\n  return function wrap(arr) {\n    return callback.apply(null, arr);\n  };\n};\n","'use strict';\n\nvar VERSION = require('../env/data').version;\n\nvar validators = {};\n\n// eslint-disable-next-line func-names\n['object', 'boolean', 'number', 'function', 'string', 'symbol'].forEach(function(type, i) {\n  validators[type] = function validator(thing) {\n    return typeof thing === type || 'a' + (i < 1 ? 'n ' : ' ') + type;\n  };\n});\n\nvar deprecatedWarnings = {};\n\n/**\n * Transitional option validator\n * @param {function|boolean?} validator - set to false if the transitional option has been removed\n * @param {string?} version - deprecated version / removed since version\n * @param {string?} message - some message with additional info\n * @returns {function}\n */\nvalidators.transitional = function transitional(validator, version, message) {\n  function formatMessage(opt, desc) {\n    return '[Axios v' + VERSION + '] Transitional option \\'' + opt + '\\'' + desc + (message ? '. ' + message : '');\n  }\n\n  // eslint-disable-next-line func-names\n  return function(value, opt, opts) {\n    if (validator === false) {\n      throw new Error(formatMessage(opt, ' has been removed' + (version ? ' in ' + version : '')));\n    }\n\n    if (version && !deprecatedWarnings[opt]) {\n      deprecatedWarnings[opt] = true;\n      // eslint-disable-next-line no-console\n      console.warn(\n        formatMessage(\n          opt,\n          ' has been deprecated since v' + version + ' and will be removed in the near future'\n        )\n      );\n    }\n\n    return validator ? validator(value, opt, opts) : true;\n  };\n};\n\n/**\n * Assert object's properties type\n * @param {object} options\n * @param {object} schema\n * @param {boolean?} allowUnknown\n */\n\nfunction assertOptions(options, schema, allowUnknown) {\n  if (typeof options !== 'object') {\n    throw new TypeError('options must be an object');\n  }\n  var keys = Object.keys(options);\n  var i = keys.length;\n  while (i-- > 0) {\n    var opt = keys[i];\n    var validator = schema[opt];\n    if (validator) {\n      var value = options[opt];\n      var result = value === undefined || validator(value, opt, options);\n      if (result !== true) {\n        throw new TypeError('option ' + opt + ' must be ' + result);\n      }\n      continue;\n    }\n    if (allowUnknown !== true) {\n      throw Error('Unknown option ' + opt);\n    }\n  }\n}\n\nmodule.exports = {\n  assertOptions: assertOptions,\n  validators: validators\n};\n","'use strict';\n\nvar bind = require('./helpers/bind');\n\n// utils is a library of generic helper functions non-specific to axios\n\nvar toString = Object.prototype.toString;\n\n/**\n * Determine if a value is an Array\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Array, otherwise false\n */\nfunction isArray(val) {\n  return Array.isArray(val);\n}\n\n/**\n * Determine if a value is undefined\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if the value is undefined, otherwise false\n */\nfunction isUndefined(val) {\n  return typeof val === 'undefined';\n}\n\n/**\n * Determine if a value is a Buffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Buffer, otherwise false\n */\nfunction isBuffer(val) {\n  return val !== null && !isUndefined(val) && val.constructor !== null && !isUndefined(val.constructor)\n    && typeof val.constructor.isBuffer === 'function' && val.constructor.isBuffer(val);\n}\n\n/**\n * Determine if a value is an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an ArrayBuffer, otherwise false\n */\nfunction isArrayBuffer(val) {\n  return toString.call(val) === '[object ArrayBuffer]';\n}\n\n/**\n * Determine if a value is a FormData\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an FormData, otherwise false\n */\nfunction isFormData(val) {\n  return toString.call(val) === '[object FormData]';\n}\n\n/**\n * Determine if a value is a view on an ArrayBuffer\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a view on an ArrayBuffer, otherwise false\n */\nfunction isArrayBufferView(val) {\n  var result;\n  if ((typeof ArrayBuffer !== 'undefined') && (ArrayBuffer.isView)) {\n    result = ArrayBuffer.isView(val);\n  } else {\n    result = (val) && (val.buffer) && (isArrayBuffer(val.buffer));\n  }\n  return result;\n}\n\n/**\n * Determine if a value is a String\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a String, otherwise false\n */\nfunction isString(val) {\n  return typeof val === 'string';\n}\n\n/**\n * Determine if a value is a Number\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Number, otherwise false\n */\nfunction isNumber(val) {\n  return typeof val === 'number';\n}\n\n/**\n * Determine if a value is an Object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is an Object, otherwise false\n */\nfunction isObject(val) {\n  return val !== null && typeof val === 'object';\n}\n\n/**\n * Determine if a value is a plain Object\n *\n * @param {Object} val The value to test\n * @return {boolean} True if value is a plain Object, otherwise false\n */\nfunction isPlainObject(val) {\n  if (toString.call(val) !== '[object Object]') {\n    return false;\n  }\n\n  var prototype = Object.getPrototypeOf(val);\n  return prototype === null || prototype === Object.prototype;\n}\n\n/**\n * Determine if a value is a Date\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Date, otherwise false\n */\nfunction isDate(val) {\n  return toString.call(val) === '[object Date]';\n}\n\n/**\n * Determine if a value is a File\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a File, otherwise false\n */\nfunction isFile(val) {\n  return toString.call(val) === '[object File]';\n}\n\n/**\n * Determine if a value is a Blob\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Blob, otherwise false\n */\nfunction isBlob(val) {\n  return toString.call(val) === '[object Blob]';\n}\n\n/**\n * Determine if a value is a Function\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Function, otherwise false\n */\nfunction isFunction(val) {\n  return toString.call(val) === '[object Function]';\n}\n\n/**\n * Determine if a value is a Stream\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a Stream, otherwise false\n */\nfunction isStream(val) {\n  return isObject(val) && isFunction(val.pipe);\n}\n\n/**\n * Determine if a value is a URLSearchParams object\n *\n * @param {Object} val The value to test\n * @returns {boolean} True if value is a URLSearchParams object, otherwise false\n */\nfunction isURLSearchParams(val) {\n  return toString.call(val) === '[object URLSearchParams]';\n}\n\n/**\n * Trim excess whitespace off the beginning and end of a string\n *\n * @param {String} str The String to trim\n * @returns {String} The String freed of excess whitespace\n */\nfunction trim(str) {\n  return str.trim ? str.trim() : str.replace(/^\\s+|\\s+$/g, '');\n}\n\n/**\n * Determine if we're running in a standard browser environment\n *\n * This allows axios to run in a web worker, and react-native.\n * Both environments support XMLHttpRequest, but not fully standard globals.\n *\n * web workers:\n *  typeof window -> undefined\n *  typeof document -> undefined\n *\n * react-native:\n *  navigator.product -> 'ReactNative'\n * nativescript\n *  navigator.product -> 'NativeScript' or 'NS'\n */\nfunction isStandardBrowserEnv() {\n  if (typeof navigator !== 'undefined' && (navigator.product === 'ReactNative' ||\n                                           navigator.product === 'NativeScript' ||\n                                           navigator.product === 'NS')) {\n    return false;\n  }\n  return (\n    typeof window !== 'undefined' &&\n    typeof document !== 'undefined'\n  );\n}\n\n/**\n * Iterate over an Array or an Object invoking a function for each item.\n *\n * If `obj` is an Array callback will be called passing\n * the value, index, and complete array for each item.\n *\n * If 'obj' is an Object callback will be called passing\n * the value, key, and complete object for each property.\n *\n * @param {Object|Array} obj The object to iterate\n * @param {Function} fn The callback to invoke for each item\n */\nfunction forEach(obj, fn) {\n  // Don't bother if no value provided\n  if (obj === null || typeof obj === 'undefined') {\n    return;\n  }\n\n  // Force an array if not already something iterable\n  if (typeof obj !== 'object') {\n    /*eslint no-param-reassign:0*/\n    obj = [obj];\n  }\n\n  if (isArray(obj)) {\n    // Iterate over array values\n    for (var i = 0, l = obj.length; i < l; i++) {\n      fn.call(null, obj[i], i, obj);\n    }\n  } else {\n    // Iterate over object keys\n    for (var key in obj) {\n      if (Object.prototype.hasOwnProperty.call(obj, key)) {\n        fn.call(null, obj[key], key, obj);\n      }\n    }\n  }\n}\n\n/**\n * Accepts varargs expecting each argument to be an object, then\n * immutably merges the properties of each object and returns result.\n *\n * When multiple objects contain the same key the later object in\n * the arguments list will take precedence.\n *\n * Example:\n *\n * ```js\n * var result = merge({foo: 123}, {foo: 456});\n * console.log(result.foo); // outputs 456\n * ```\n *\n * @param {Object} obj1 Object to merge\n * @returns {Object} Result of all merge properties\n */\nfunction merge(/* obj1, obj2, obj3, ... */) {\n  var result = {};\n  function assignValue(val, key) {\n    if (isPlainObject(result[key]) && isPlainObject(val)) {\n      result[key] = merge(result[key], val);\n    } else if (isPlainObject(val)) {\n      result[key] = merge({}, val);\n    } else if (isArray(val)) {\n      result[key] = val.slice();\n    } else {\n      result[key] = val;\n    }\n  }\n\n  for (var i = 0, l = arguments.length; i < l; i++) {\n    forEach(arguments[i], assignValue);\n  }\n  return result;\n}\n\n/**\n * Extends object a by mutably adding to it the properties of object b.\n *\n * @param {Object} a The object to be extended\n * @param {Object} b The object to copy properties from\n * @param {Object} thisArg The object to bind function to\n * @return {Object} The resulting value of object a\n */\nfunction extend(a, b, thisArg) {\n  forEach(b, function assignValue(val, key) {\n    if (thisArg && typeof val === 'function') {\n      a[key] = bind(val, thisArg);\n    } else {\n      a[key] = val;\n    }\n  });\n  return a;\n}\n\n/**\n * Remove byte order marker. This catches EF BB BF (the UTF-8 BOM)\n *\n * @param {string} content with BOM\n * @return {string} content value without BOM\n */\nfunction stripBOM(content) {\n  if (content.charCodeAt(0) === 0xFEFF) {\n    content = content.slice(1);\n  }\n  return content;\n}\n\nmodule.exports = {\n  isArray: isArray,\n  isArrayBuffer: isArrayBuffer,\n  isBuffer: isBuffer,\n  isFormData: isFormData,\n  isArrayBufferView: isArrayBufferView,\n  isString: isString,\n  isNumber: isNumber,\n  isObject: isObject,\n  isPlainObject: isPlainObject,\n  isUndefined: isUndefined,\n  isDate: isDate,\n  isFile: isFile,\n  isBlob: isBlob,\n  isFunction: isFunction,\n  isStream: isStream,\n  isURLSearchParams: isURLSearchParams,\n  isStandardBrowserEnv: isStandardBrowserEnv,\n  forEach: forEach,\n  merge: merge,\n  extend: extend,\n  trim: trim,\n  stripBOM: stripBOM\n};\n","class AgentSender {\n\tconstructor(innerSender) {\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\trequest.parameters.agent = \"smarty (sdk:javascript@\" + require(\"../package.json\").version + \")\";\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = AgentSender;","class BaseUrlSender {\n\tconstructor(innerSender, urlOverride) {\n\t\tthis.urlOverride = urlOverride;\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\trequest.baseUrl = this.urlOverride;\n\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = BaseUrlSender;","const BatchFullError = require(\"./Errors\").BatchFullError;\n\n/**\n * This class contains a collection of up to 100 lookups to be sent to one of the Smarty APIs<br>\n *     all at once. This is more efficient than sending them one at a time.\n */\nclass Batch {\n\tconstructor () {\n\t\tthis.lookups = [];\n\t}\n\n\tadd (lookup) {\n\t\tif (this.lookupsHasRoomForLookup()) this.lookups.push(lookup);\n\t\telse throw new BatchFullError();\n\t}\n\n\tlookupsHasRoomForLookup() {\n\t\tconst maxNumberOfLookups = 100;\n\t\treturn this.lookups.length < maxNumberOfLookups;\n\t}\n\n\tlength() {\n\t\treturn this.lookups.length;\n\t}\n\n\tgetByIndex(index) {\n\t\treturn this.lookups[index];\n\t}\n\n\tgetByInputId(inputId) {\n\t\treturn this.lookups.filter(lookup => {\n\t\t\treturn lookup.inputId === inputId;\n\t\t})[0];\n\t}\n\n\t/**\n\t * Clears the lookups stored in the batch so it can be used again.<br>\n\t *     This helps avoid the overhead of building a new Batch object for each group of lookups.\n\t */\n\tclear () {\n\t\tthis.lookups = [];\n\t}\n\n\tisEmpty () {\n\t\treturn this.length() === 0;\n\t}\n}\n\nmodule.exports = Batch;","const HttpSender = require(\"./HttpSender\");\nconst SigningSender = require(\"./SigningSender\");\nconst BaseUrlSender = require(\"./BaseUrlSender\");\nconst AgentSender = require(\"./AgentSender\");\nconst StaticCredentials = require(\"./StaticCredentials\");\nconst SharedCredentials = require(\"./SharedCredentials\");\nconst CustomHeaderSender = require(\"./CustomHeaderSender\");\nconst StatusCodeSender = require(\"./StatusCodeSender\");\nconst LicenseSender = require(\"./LicenseSender\");\nconst BadCredentialsError = require(\"./Errors\").BadCredentialsError;\n\n//TODO: refactor this to work more cleanly with a bundler.\nconst UsStreetClient = require(\"./us_street/Client\");\nconst UsZipcodeClient = require(\"./us_zipcode/Client\");\nconst UsAutocompleteClient = require(\"./us_autocomplete/Client\");\nconst UsAutocompleteProClient = require(\"./us_autocomplete_pro/Client\");\nconst UsExtractClient = require(\"./us_extract/Client\");\nconst InternationalStreetClient = require(\"./international_street/Client\");\nconst UsReverseGeoClient = require(\"./us_reverse_geo/Client\");\nconst InternationalAddressAutocompleteClient = require(\"./international_address_autocomplete/Client\");\n\nconst INTERNATIONAL_STREET_API_URI = \"https://international-street.api.smartystreets.com/verify\";\nconst US_AUTOCOMPLETE_API_URL = \"https://us-autocomplete.api.smartystreets.com/suggest\";\nconst US_AUTOCOMPLETE_PRO_API_URL = \"https://us-autocomplete-pro.api.smartystreets.com/lookup\";\nconst US_EXTRACT_API_URL = \"https://us-extract.api.smartystreets.com/\";\nconst US_STREET_API_URL = \"https://us-street.api.smartystreets.com/street-address\";\nconst US_ZIP_CODE_API_URL = \"https://us-zipcode.api.smartystreets.com/lookup\";\nconst US_REVERSE_GEO_API_URL = \"https://us-reverse-geo.api.smartystreets.com/lookup\";\nconst INTERNATIONAL_ADDRESS_AUTOCOMPLETE_API_URL = \"https://international-autocomplete.api.smartystreets.com/lookup\";\n\n/**\n * The ClientBuilder class helps you build a client object for one of the supported Smarty APIs.<br>\n * You can use ClientBuilder's methods to customize settings like maximum retries or timeout duration. These methods<br>\n * are chainable, so you can usually get set up with one line of code.\n */\nclass ClientBuilder {\n\tconstructor(signer) {\n\t\tif (noCredentialsProvided()) throw new BadCredentialsError();\n\n\t\tthis.signer = signer;\n\t\tthis.httpSender = undefined;\n\t\tthis.maxRetries = 5;\n\t\tthis.maxTimeout = 10000;\n\t\tthis.baseUrl = undefined;\n\t\tthis.proxy = undefined;\n\t\tthis.customHeaders = {};\n\t\tthis.debug = undefined;\n\t\tthis.licenses = [];\n\n\t\tfunction noCredentialsProvided() {\n\t\t\treturn !signer instanceof StaticCredentials || !signer instanceof SharedCredentials;\n\t\t}\n\t}\n\n\t/**\n\t * @param retries The maximum number of times to retry sending the request to the API. (Default is 5)\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithMaxRetries(retries) {\n\t\tthis.maxRetries = retries;\n\t\treturn this;\n\t}\n\n\t/**\n\t * @param timeout The maximum time (in milliseconds) to wait for a connection, and also to wait for <br>\n\t *                   the response to be read. (Default is 10000)\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithMaxTimeout(timeout) {\n\t\tthis.maxTimeout = timeout;\n\t\treturn this;\n\t}\n\n\t/**\n\t * @param sender Default is a series of nested senders. See <b>buildSender()</b>.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithSender(sender) {\n\t\tthis.httpSender = sender;\n\t\treturn this;\n\t}\n\n\t/**\n\t * This may be useful when using a local installation of the Smarty APIs.\n\t * @param url Defaults to the URL for the API corresponding to the <b>Client</b> object being built.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithBaseUrl(url) {\n\t\tthis.baseUrl = url;\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use this to specify a proxy through which to send all lookups.\n\t * @param host The host of the proxy server (do not include the port).\n\t * @param port The port on the proxy server to which you wish to connect.\n\t * @param protocol The protocol on the proxy server to which you wish to connect. If the proxy server uses HTTPS, then you must set the protocol to 'https'.\n\t * @param username The username to login to the proxy.\n\t * @param password The password to login to the proxy.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithProxy(host, port, protocol, username, password) {\n\t\tthis.proxy = {\n\t\t\thost: host,\n\t\t\tport: port,\n\t\t\tprotocol: protocol,\n\t\t};\n\n\t\tif (username && password) {\n\t\t\tthis.proxy.auth = {\n\t\t\t\tusername: username,\n\t\t\t\tpassword: password,\n\t\t\t};\n\t\t}\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Use this to add any additional headers you need.\n\t * @param customHeaders A String to Object <b>Map</b> of header name/value pairs.\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithCustomHeaders(customHeaders) {\n\t\tthis.customHeaders = customHeaders;\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Enables debug mode, which will print information about the HTTP request and response to console.log\n\t * @return Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithDebug() {\n\t\tthis.debug = true;\n\n\t\treturn this;\n\t}\n\n\t/**\n\t * Allows the caller to specify the subscription license (aka \"track\") they wish to use.\n\t * @param licenses A String Array of licenses.\n\t * @returns Returns <b>this</b> to accommodate method chaining.\n\t */\n\twithLicenses(licenses) {\n\t\tthis.licenses = licenses;\n\n\t\treturn this;\n\t}\n\n\tbuildSender() {\n\t\tif (this.httpSender) return this.httpSender;\n\n\t\tconst httpSender = new HttpSender(this.maxTimeout, this.maxRetries, this.proxy, this.debug);\n\t\tconst statusCodeSender = new StatusCodeSender(httpSender);\n\t\tconst signingSender = new SigningSender(statusCodeSender, this.signer);\n\t\tconst agentSender = new AgentSender(signingSender);\n\t\tconst customHeaderSender = new CustomHeaderSender(agentSender, this.customHeaders);\n\t\tconst baseUrlSender = new BaseUrlSender(customHeaderSender, this.baseUrl);\n\t\tconst licenseSender = new LicenseSender(baseUrlSender, this.licenses);\n\n\t\treturn licenseSender;\n\t}\n\n\tbuildClient(baseUrl, Client) {\n\t\tif (!this.baseUrl) {\n\t\t\tthis.baseUrl = baseUrl;\n\t\t}\n\n\t\treturn new Client(this.buildSender());\n\t}\n\n\tbuildUsStreetApiClient() {\n\t\treturn this.buildClient(US_STREET_API_URL, UsStreetClient);\n\t}\n\n\tbuildUsZipcodeClient() {\n\t\treturn this.buildClient(US_ZIP_CODE_API_URL, UsZipcodeClient);\n\t}\n\n\tbuildUsAutocompleteClient() { // Deprecated\n\t\treturn this.buildClient(US_AUTOCOMPLETE_API_URL, UsAutocompleteClient);\n\t}\n\n\tbuildUsAutocompleteProClient() {\n\t\treturn this.buildClient(US_AUTOCOMPLETE_PRO_API_URL, UsAutocompleteProClient);\n\t}\n\n\tbuildUsExtractClient() {\n\t\treturn this.buildClient(US_EXTRACT_API_URL, UsExtractClient);\n\t}\n\n\tbuildInternationalStreetClient() {\n\t\treturn this.buildClient(INTERNATIONAL_STREET_API_URI, InternationalStreetClient);\n\t}\n\n\tbuildUsReverseGeoClient() {\n\t\treturn this.buildClient(US_REVERSE_GEO_API_URL, UsReverseGeoClient);\n\t}\n\n\tbuildInternationalAddressAutocompleteClient() {\n\t\treturn this.buildClient(INTERNATIONAL_ADDRESS_AUTOCOMPLETE_API_URL, InternationalAddressAutocompleteClient);\n\t}\n}\n\nmodule.exports = ClientBuilder;","class CustomHeaderSender {\n\tconstructor(innerSender, customHeaders) {\n\t\tthis.sender = innerSender;\n\t\tthis.customHeaders = customHeaders;\n\t}\n\n\tsend(request) {\n\t\tfor (let key in this.customHeaders) {\n\t\t\trequest.headers[key] = this.customHeaders[key];\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = CustomHeaderSender;","class SmartyError extends Error {\n\tconstructor(message) {\n\t\tsuper(message);\n\t}\n}\n\nclass BatchFullError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"A batch can contain a max of 100 lookups.\");\n\t}\n}\n\nclass BatchEmptyError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"A batch must contain at least 1 lookup.\");\n\t}\n}\n\nclass UndefinedLookupError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"The lookup provided is missing or undefined. Make sure you're passing a Lookup object.\");\n\t}\n}\n\nclass BadCredentialsError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Unauthorized: The credentials were provided incorrectly or did not match any existing active credentials.\");\n\t}\n}\n\nclass PaymentRequiredError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Payment Required: There is no active subscription for the account associated with the credentials submitted with the request.\");\n\t}\n}\n\nclass RequestEntityTooLargeError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Request Entity Too Large: The request body has exceeded the maximum size.\");\n\t}\n}\n\nclass BadRequestError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Bad Request (Malformed Payload): A GET request lacked a street field or the request body of a POST request contained malformed JSON.\");\n\t}\n}\n\nclass UnprocessableEntityError extends SmartyError {\n\tconstructor(message) {\n\t\tsuper(message);\n\t}\n}\n\nclass TooManyRequestsError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"When using the public 'embedded key' authentication, we restrict the number of requests coming from a given source over too short of a time.\");\n\t}\n}\n\nclass InternalServerError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Internal Server Error.\");\n\t}\n}\n\nclass ServiceUnavailableError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"Service Unavailable. Try again later.\");\n\t}\n}\n\nclass GatewayTimeoutError extends SmartyError {\n\tconstructor() {\n\t\tsuper(\"The upstream data provider did not respond in a timely fashion and the request failed. A serious, yet rare occurrence indeed.\");\n\t}\n}\n\nmodule.exports = {\n\tBatchFullError: BatchFullError,\n\tBatchEmptyError: BatchEmptyError,\n\tUndefinedLookupError: UndefinedLookupError,\n\tBadCredentialsError: BadCredentialsError,\n\tPaymentRequiredError: PaymentRequiredError,\n\tRequestEntityTooLargeError: RequestEntityTooLargeError,\n\tBadRequestError: BadRequestError,\n\tUnprocessableEntityError: UnprocessableEntityError,\n\tTooManyRequestsError: TooManyRequestsError,\n\tInternalServerError: InternalServerError,\n\tServiceUnavailableError: ServiceUnavailableError,\n\tGatewayTimeoutError: GatewayTimeoutError\n};","const Response = require(\"./Response\");\nconst Axios = require(\"axios\");\nconst axiosRetry = require(\"axios-retry\");\n\nclass HttpSender {\n\tconstructor(timeout = 10000, retries = 5, proxyConfig, debug = false) {\n\t\taxiosRetry(Axios, {\n\t\t\tretries: retries,\n\t\t});\n\t\tthis.timeout = timeout;\n\t\tthis.proxyConfig = proxyConfig;\n\t\tif (debug) this.enableDebug();\n\t}\n\n\tbuildRequestConfig({payload, parameters, headers, baseUrl}) {\n\t\tlet config = {\n\t\t\tmethod: \"GET\",\n\t\t\ttimeout: this.timeout,\n\t\t\tparams: parameters,\n\t\t\theaders: headers,\n\t\t\tbaseURL: baseUrl,\n\t\t\tvalidateStatus: function (status) {\n\t\t\t\treturn status < 500;\n\t\t\t},\n\t\t};\n\n\t\tif (payload) {\n\t\t\tconfig.method = \"POST\";\n\t\t\tconfig.data = payload;\n\t\t}\n\n\t\tif (this.proxyConfig) config.proxy = this.proxyConfig;\n\t\treturn config;\n\t}\n\n\tbuildSmartyResponse(response, error) {\n\t\tif (response) return new Response(response.status, response.data);\n\t\treturn new Response(undefined, undefined, error)\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tlet requestConfig = this.buildRequestConfig(request);\n\n\t\t\tAxios(requestConfig)\n\t\t\t\t.then(response => {\n\t\t\t\t\tlet smartyResponse = this.buildSmartyResponse(response);\n\n\t\t\t\t\tif (smartyResponse.statusCode >= 400) reject(smartyResponse);\n\n\t\t\t\t\tresolve(smartyResponse);\n\t\t\t\t})\n\t\t\t\t.catch(error => reject(this.buildSmartyResponse(undefined, error)));\n\t\t});\n\t}\n\n\tenableDebug() {\n\t\tAxios.interceptors.request.use(request => {\n\t\t\tconsole.log('Request:\\r\\n', request);\n\t\t\tconsole.log('\\r\\n*******************************************\\r\\n');\n\t\t\treturn request\n\t\t});\n\n\t\tAxios.interceptors.response.use(response => {\n\t\t\tconsole.log('Response:\\r\\n');\n\t\t\tconsole.log('Status:', response.status, response.statusText);\n\t\t\tconsole.log('Headers:', response.headers);\n\t\t\tconsole.log('Data:', response.data);\n\t\t\treturn response\n\t\t})\n\t}\n}\n\nmodule.exports = HttpSender;","class InputData {\n\tconstructor(lookup) {\n\t\tthis.lookup = lookup;\n\t\tthis.data = {};\n\t}\n\n\tadd(apiField, lookupField) {\n\t\tif (this.lookupFieldIsPopulated(lookupField)) this.data[apiField] = this.lookup[lookupField];\n\t}\n\n\tlookupFieldIsPopulated(lookupField) {\n\t\treturn this.lookup[lookupField] !== \"\" && this.lookup[lookupField] !== undefined;\n\t}\n}\n\nmodule.exports = InputData;","class LicenseSender {\n\tconstructor(innerSender, licenses) {\n\t\tthis.sender = innerSender;\n\t\tthis.licenses = licenses;\n\t}\n\n\tsend(request) {\n\t\tif (this.licenses.length !== 0) {\n\t\t\trequest.parameters[\"license\"] = this.licenses.join(\",\");\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = LicenseSender;","class Request {\n\tconstructor(payload) {\n\t\tthis.baseUrl = \"\";\n\t\tthis.payload = payload;\n\t\tthis.headers = {\n\t\t\t\"Content-Type\": \"application/json; charset=utf-8\",\n\t\t};\n\n\t\tthis.parameters = {};\n\t}\n}\n\nmodule.exports = Request;","class Response {\n\tconstructor (statusCode, payload, error = undefined) {\n\t\tthis.statusCode = statusCode;\n\t\tthis.payload = payload;\n\t\tthis.error = error;\n\t}\n}\n\nmodule.exports = Response;","class SharedCredentials {\n\tconstructor(authId, hostName) {\n\t\tthis.authId = authId;\n\t\tthis.hostName = hostName;\n\t}\n\n\tsign(request) {\n\t\trequest.parameters[\"key\"] = this.authId;\n\t\tif (this.hostName) request.headers[\"Referer\"] = \"https://\" + this.hostName;\n\t}\n}\n\nmodule.exports = SharedCredentials;","const UnprocessableEntityError = require(\"./Errors\").UnprocessableEntityError;\nconst SharedCredentials = require(\"./SharedCredentials\");\n\nclass SigningSender {\n\tconstructor(innerSender, signer) {\n\t\tthis.signer = signer;\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\tconst sendingPostWithSharedCredentials = request.payload && this.signer instanceof SharedCredentials;\n\t\tif (sendingPostWithSharedCredentials) {\n\t\t\tconst message = \"Shared credentials cannot be used in batches with a length greater than 1 or when using the US Extract API.\";\n\t\t\tthrow new UnprocessableEntityError(message);\n\t\t}\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.signer.sign(request);\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(reject);\n\t\t});\n\t}\n}\n\nmodule.exports = SigningSender;","class StaticCredentials {\n\tconstructor (authId, authToken) {\n\t\tthis.authId = authId;\n\t\tthis.authToken = authToken;\n\t}\n\n\tsign (request) {\n\t\trequest.parameters[\"auth-id\"] = this.authId;\n\t\trequest.parameters[\"auth-token\"] = this.authToken;\n\t}\n}\n\nmodule.exports = StaticCredentials;","const Errors = require(\"./Errors\");\n\nclass StatusCodeSender {\n\tconstructor(innerSender) {\n\t\tthis.sender = innerSender;\n\t}\n\n\tsend(request) {\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(resolve)\n\t\t\t\t.catch(error => {\n\t\t\t\t\tswitch (error.statusCode) {\n\t\t\t\t\t\tcase 400:\n\t\t\t\t\t\t\terror.error = new Errors.BadRequestError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 401:\n\t\t\t\t\t\t\terror.error = new Errors.BadCredentialsError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 402:\n\t\t\t\t\t\t\terror.error = new Errors.PaymentRequiredError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 413:\n\t\t\t\t\t\t\terror.error = new Errors.RequestEntityTooLargeError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 422:\n\t\t\t\t\t\t\terror.error = new Errors.UnprocessableEntityError(\"GET request lacked required fields.\");\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 429:\n\t\t\t\t\t\t\terror.error = new Errors.TooManyRequestsError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 500:\n\t\t\t\t\t\t\terror.error = new Errors.InternalServerError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 503:\n\t\t\t\t\t\t\terror.error = new Errors.ServiceUnavailableError();\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase 504:\n\t\t\t\t\t\t\terror.error = new Errors.GatewayTimeoutError();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\n\t\t\t\t\treject(error);\n\t\t\t\t});\n\t\t});\n\t}\n}\n\nmodule.exports = StatusCodeSender;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Suggestion = require(\"./Suggestion\");\n\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = {\n\t\t\tsearch: lookup.search,\n\t\t\tcountry: lookup.country,\n\t\t\tmax_results: lookup.max_results,\n\t\t\tinclude_only_administrative_area: lookup.include_only_administrative_area,\n\t\t\tinclude_only_locality: lookup.include_only_locality,\n\t\t\tinclude_only_postal_code: lookup.include_only_postal_code,\n\t\t};\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = buildSuggestionsFromResponse(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildSuggestionsFromResponse(payload) {\n\t\t\tif (payload && payload.candidates === null) return [];\n\n\t\t\treturn payload.candidates.map(suggestion => new Suggestion(suggestion));\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","class Lookup {\n\tconstructor(search = \"\", country = \"United States\", max_results = undefined, include_only_administrative_area = \"\", include_only_locality = \"\", include_only_postal_code = \"\") {\n\t\tthis.result = [];\n\n\t\tthis.search = search;\n\t\tthis.country = country;\n\t\tthis.max_results = max_results;\n\t\tthis.include_only_administrative_area = include_only_administrative_area;\n\t\tthis.include_only_locality = include_only_locality;\n\t\tthis.include_only_postal_code = include_only_postal_code;\n\t}\n}\n\nmodule.exports = Lookup;","class Suggestion {\n\tconstructor(responseData) {\n\t\tthis.street = responseData.street;\n\t\tthis.locality = responseData.locality;\n\t\tthis.administrativeArea = responseData.administrative_area;\n\t\tthis.postalCode = responseData.postal_code;\n\t\tthis.countryIso3 = responseData.country_iso3;\n\t}\n}\n\nmodule.exports = Suggestion;","/**\n * A candidate is a possible match for an address that was submitted.<br>\n *     A lookup can have multiple candidates if the address was ambiguous.\n *\n * @see \"https://www.smarty.com/docs/cloud/international-street-api#root\"\n */\nclass Candidate {\n\tconstructor(responseData) {\n\t\tthis.organization = responseData.organization;\n\t\tthis.address1 = responseData.address1;\n\t\tthis.address2 = responseData.address2;\n\t\tthis.address3 = responseData.address3;\n\t\tthis.address4 = responseData.address4;\n\t\tthis.address5 = responseData.address5;\n\t\tthis.address6 = responseData.address6;\n\t\tthis.address7 = responseData.address7;\n\t\tthis.address8 = responseData.address8;\n\t\tthis.address9 = responseData.address9;\n\t\tthis.address10 = responseData.address10;\n\t\tthis.address11 = responseData.address11;\n\t\tthis.address12 = responseData.address12;\n\n\t\tthis.components = {};\n\t\tif (responseData.components !== undefined) {\n\t\t\tthis.components.countryIso3 = responseData.components.country_iso_3;\n\t\t\tthis.components.superAdministrativeArea = responseData.components.super_administrative_area;\n\t\t\tthis.components.administrativeArea = responseData.components.administrative_area;\n\t\t\tthis.components.subAdministrativeArea = responseData.components.sub_administrative_area;\n\t\t\tthis.components.dependentLocality = responseData.components.dependent_locality;\n\t\t\tthis.components.dependentLocalityName = responseData.components.dependent_locality_name;\n\t\t\tthis.components.doubleDependentLocality = responseData.components.double_dependent_locality;\n\t\t\tthis.components.locality = responseData.components.locality;\n\t\t\tthis.components.postalCode = responseData.components.postal_code;\n\t\t\tthis.components.postalCodeShort = responseData.components.postal_code_short;\n\t\t\tthis.components.postalCodeExtra = responseData.components.postal_code_extra;\n\t\t\tthis.components.premise = responseData.components.premise;\n\t\t\tthis.components.premiseExtra = responseData.components.premise_extra;\n\t\t\tthis.components.premisePrefixNumber = responseData.components.premise_prefix_number;\n\t\t\tthis.components.premiseNumber = responseData.components.premise_number;\n\t\t\tthis.components.premiseType = responseData.components.premise_type;\n\t\t\tthis.components.thoroughfare = responseData.components.thoroughfare;\n\t\t\tthis.components.thoroughfarePredirection = responseData.components.thoroughfare_predirection;\n\t\t\tthis.components.thoroughfarePostdirection = responseData.components.thoroughfare_postdirection;\n\t\t\tthis.components.thoroughfareName = responseData.components.thoroughfare_name;\n\t\t\tthis.components.thoroughfareTrailingType = responseData.components.thoroughfare_trailing_type;\n\t\t\tthis.components.thoroughfareType = responseData.components.thoroughfare_type;\n\t\t\tthis.components.dependentThoroughfare = responseData.components.dependent_thoroughfare;\n\t\t\tthis.components.dependentThoroughfarePredirection = responseData.components.dependent_thoroughfare_predirection;\n\t\t\tthis.components.dependentThoroughfarePostdirection = responseData.components.dependent_thoroughfare_postdirection;\n\t\t\tthis.components.dependentThoroughfareName = responseData.components.dependent_thoroughfare_name;\n\t\t\tthis.components.dependentThoroughfareTrailingType = responseData.components.dependent_thoroughfare_trailing_type;\n\t\t\tthis.components.dependentThoroughfareType = responseData.components.dependent_thoroughfare_type;\n\t\t\tthis.components.building = responseData.components.building;\n\t\t\tthis.components.buildingLeadingType = responseData.components.building_leading_type;\n\t\t\tthis.components.buildingName = responseData.components.building_name;\n\t\t\tthis.components.buildingTrailingType = responseData.components.building_trailing_type;\n\t\t\tthis.components.subBuildingType = responseData.components.sub_building_type;\n\t\t\tthis.components.subBuildingNumber = responseData.components.sub_building_number;\n\t\t\tthis.components.subBuildingName = responseData.components.sub_building_name;\n\t\t\tthis.components.subBuilding = responseData.components.sub_building;\n\t\t\tthis.components.postBox = responseData.components.post_box;\n\t\t\tthis.components.postBoxType = responseData.components.post_box_type;\n\t\t\tthis.components.postBoxNumber = responseData.components.post_box_number;\n\t\t}\n\n\t\tthis.analysis = {};\n\t\tif (responseData.analysis !== undefined) {\n\t\t\tthis.analysis.verificationStatus = responseData.analysis.verification_status;\n\t\t\tthis.analysis.addressPrecision = responseData.analysis.address_precision;\n\t\t\tthis.analysis.maxAddressPrecision = responseData.analysis.max_address_precision;\n\n\t\t\tthis.analysis.changes = {};\n\t\t\tif (responseData.analysis.changes !== undefined) {\n\t\t\t\tthis.analysis.changes.organization = responseData.analysis.changes.organization;\n\t\t\t\tthis.analysis.changes.address1 = responseData.analysis.changes.address1;\n\t\t\t\tthis.analysis.changes.address2 = responseData.analysis.changes.address2;\n\t\t\t\tthis.analysis.changes.address3 = responseData.analysis.changes.address3;\n\t\t\t\tthis.analysis.changes.address4 = responseData.analysis.changes.address4;\n\t\t\t\tthis.analysis.changes.address5 = responseData.analysis.changes.address5;\n\t\t\t\tthis.analysis.changes.address6 = responseData.analysis.changes.address6;\n\t\t\t\tthis.analysis.changes.address7 = responseData.analysis.changes.address7;\n\t\t\t\tthis.analysis.changes.address8 = responseData.analysis.changes.address8;\n\t\t\t\tthis.analysis.changes.address9 = responseData.analysis.changes.address9;\n\t\t\t\tthis.analysis.changes.address10 = responseData.analysis.changes.address10;\n\t\t\t\tthis.analysis.changes.address11 = responseData.analysis.changes.address11;\n\t\t\t\tthis.analysis.changes.address12 = responseData.analysis.changes.address12;\n\n\t\t\t\tthis.analysis.changes.components = {};\n\t\t\t\tif (responseData.analysis.changes.components !== undefined) {\n\t\t\t\t\tthis.analysis.changes.components.countryIso3 = responseData.analysis.changes.components.country_iso_3;\n\t\t\t\t\tthis.analysis.changes.components.superAdministrativeArea = responseData.analysis.changes.components.super_administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.administrativeArea = responseData.analysis.changes.components.administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.subAdministrativeArea = responseData.analysis.changes.components.sub_administrative_area;\n\t\t\t\t\tthis.analysis.changes.components.dependentLocality = responseData.analysis.changes.components.dependent_locality;\n\t\t\t\t\tthis.analysis.changes.components.dependentLocalityName = responseData.analysis.changes.components.dependent_locality_name;\n\t\t\t\t\tthis.analysis.changes.components.doubleDependentLocality = responseData.analysis.changes.components.double_dependent_locality;\n\t\t\t\t\tthis.analysis.changes.components.locality = responseData.analysis.changes.components.locality;\n\t\t\t\t\tthis.analysis.changes.components.postalCode = responseData.analysis.changes.components.postal_code;\n\t\t\t\t\tthis.analysis.changes.components.postalCodeShort = responseData.analysis.changes.components.postal_code_short;\n\t\t\t\t\tthis.analysis.changes.components.postalCodeExtra = responseData.analysis.changes.components.postal_code_extra;\n\t\t\t\t\tthis.analysis.changes.components.premise = responseData.analysis.changes.components.premise;\n\t\t\t\t\tthis.analysis.changes.components.premiseExtra = responseData.analysis.changes.components.premise_extra;\n\t\t\t\t\tthis.analysis.changes.components.premisePrefixNumber = responseData.analysis.changes.components.premise_prefix_number;\n\t\t\t\t\tthis.analysis.changes.components.premiseNumber = responseData.analysis.changes.components.premise_number;\n\t\t\t\t\tthis.analysis.changes.components.premiseType = responseData.analysis.changes.components.premise_type;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfare = responseData.analysis.changes.components.thoroughfare;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfarePredirection = responseData.analysis.changes.components.thoroughfare_predirection;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfarePostdirection = responseData.analysis.changes.components.thoroughfare_postdirection;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareName = responseData.analysis.changes.components.thoroughfare_name;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareTrailingType = responseData.analysis.changes.components.thoroughfare_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.thoroughfareType = responseData.analysis.changes.components.thoroughfare_type;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfare = responseData.analysis.changes.components.dependent_thoroughfare;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfarePredirection = responseData.analysis.changes.components.dependent_thoroughfare_predirection;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfarePostdirection = responseData.analysis.changes.components.dependent_thoroughfare_postdirection;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareName = responseData.analysis.changes.components.dependent_thoroughfare_name;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareTrailingType = responseData.analysis.changes.components.dependent_thoroughfare_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.dependentThoroughfareType = responseData.analysis.changes.components.dependent_thoroughfare_type;\n\t\t\t\t\tthis.analysis.changes.components.building = responseData.analysis.changes.components.building;\n\t\t\t\t\tthis.analysis.changes.components.buildingLeadingType = responseData.analysis.changes.components.building_leading_type;\n\t\t\t\t\tthis.analysis.changes.components.buildingName = responseData.analysis.changes.components.building_name;\n\t\t\t\t\tthis.analysis.changes.components.buildingTrailingType = responseData.analysis.changes.components.building_trailing_type;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingType = responseData.analysis.changes.components.sub_building_type;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingNumber = responseData.analysis.changes.components.sub_building_number;\n\t\t\t\t\tthis.analysis.changes.components.subBuildingName = responseData.analysis.changes.components.sub_building_name;\n\t\t\t\t\tthis.analysis.changes.components.subBuilding = responseData.analysis.changes.components.sub_building;\n\t\t\t\t\tthis.analysis.changes.components.postBox = responseData.analysis.changes.components.post_box;\n\t\t\t\t\tthis.analysis.changes.components.postBoxType = responseData.analysis.changes.components.post_box_type;\n\t\t\t\t\tthis.analysis.changes.components.postBoxNumber = responseData.analysis.changes.components.post_box_number;\n\t\t\t\t}\n\t\t\t\t//TODO: Fill in the rest of these fields and their corresponding tests.\n\t\t\t}\n\t\t}\n\n\t\tthis.metadata = {};\n\t\tif (responseData.metadata !== undefined) {\n\t\t\tthis.metadata.latitude = responseData.metadata.latitude;\n\t\t\tthis.metadata.longitude = responseData.metadata.longitude;\n\t\t\tthis.metadata.geocodePrecision = responseData.metadata.geocode_precision;\n\t\t\tthis.metadata.maxGeocodePrecision = responseData.metadata.max_geocode_precision;\n\t\t\tthis.metadata.addressFormat = responseData.metadata.address_format;\n\t\t}\n\t}\n}\n\nmodule.exports = Candidate;","const Request = require(\"../Request\");\nconst Errors = require(\"../Errors\");\nconst Candidate = require(\"./Candidate\");\nconst buildInputData = require(\"../util/buildInputData\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").internationalStreet;\n\n/**\n * This client sends lookups to the Smarty International Street API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildInputData(lookup, keyTranslationFormat);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tresolve(attachLookupCandidates(response, lookup));\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction attachLookupCandidates(response, lookup) {\n\t\t\tresponse.payload.map(rawCandidate => {\n\t\t\t\tlookup.result.push(new Candidate(rawCandidate));\n\t\t\t});\n\n\t\t\treturn lookup;\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","const UnprocessableEntityError = require(\"../Errors\").UnprocessableEntityError;\nconst messages = {\n\tcountryRequired: \"Country field is required.\",\n\tfreeformOrAddress1Required: \"Either freeform or address1 is required.\",\n\tinsufficientInformation: \"Insufficient information: One or more required fields were not set on the lookup.\",\n\tbadGeocode: \"Invalid input: geocode can only be set to 'true' (default is 'false'.\",\n\tinvalidLanguage: \"Invalid input: language can only be set to 'latin' or 'native'. When not set, the the output language will match the language of the input values.\"\n};\n\n\n/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     <p><b>Note: </b><i>Lookups must have certain required fields set with non-blank values. <br>\n *         These can be found at the URL below.</i></p>\n *     @see \"https://www.smarty.com/docs/cloud/international-street-api#http-input-fields\"\n */\nclass Lookup {\n\tconstructor(country, freeform) {\n\t\tthis.result = [];\n\n\t\tthis.country = country;\n\t\tthis.freeform = freeform;\n\t\tthis.address1 = undefined;\n\t\tthis.address2 = undefined;\n\t\tthis.address3 = undefined;\n\t\tthis.address4 = undefined;\n\t\tthis.organization = undefined;\n\t\tthis.locality = undefined;\n\t\tthis.administrativeArea = undefined;\n\t\tthis.postalCode = undefined;\n\t\tthis.geocode = undefined;\n\t\tthis.language = undefined;\n\t\tthis.inputId = undefined;\n\n\t\tthis.ensureEnoughInfo = this.ensureEnoughInfo.bind(this);\n\t\tthis.ensureValidData = this.ensureValidData.bind(this);\n\t}\n\n\tensureEnoughInfo() {\n\t\tif (fieldIsMissing(this.country)) throw new UnprocessableEntityError(messages.countryRequired);\n\n\t\tif (fieldIsSet(this.freeform)) return true;\n\n\t\tif (fieldIsMissing(this.address1)) throw new UnprocessableEntityError(messages.freeformOrAddress1Required);\n\n\t\tif (fieldIsSet(this.postalCode)) return true;\n\n\t\tif (fieldIsMissing(this.locality) || fieldIsMissing(this.administrativeArea)) throw new UnprocessableEntityError(messages.insufficientInformation);\n\n\t\treturn true;\n\t}\n\n\tensureValidData() {\n\t\tlet languageIsSetIncorrectly = () => {\n\t\t\tlet isLanguage = language => this.language.toLowerCase() === language;\n\n\t\t\treturn fieldIsSet(this.language) && !(isLanguage(\"latin\") || isLanguage(\"native\"));\n\t\t};\n\n\t\tlet geocodeIsSetIncorrectly = () => {\n\t\t\treturn fieldIsSet(this.geocode) && this.geocode.toLowerCase() !== \"true\";\n\t\t};\n\n\t\tif (geocodeIsSetIncorrectly()) throw new UnprocessableEntityError(messages.badGeocode);\n\n\t\tif (languageIsSetIncorrectly()) throw new UnprocessableEntityError(messages.invalidLanguage);\n\n\t\treturn true;\n\t}\n}\n\nfunction fieldIsMissing (field) {\n\tif (!field) return true;\n\n\tconst whitespaceCharacters = /\\s/g;\n\n\treturn field.replace(whitespaceCharacters, \"\").length < 1;\n}\n\nfunction fieldIsSet (field) {\n\treturn !fieldIsMissing(field);\n}\n\nmodule.exports = Lookup;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Suggestion = require(\"./Suggestion\");\n\n/**\n * This client sends lookups to the Smarty US Autocomplete API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildRequestParameters(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = buildSuggestionsFromResponse(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParameters(lookup) {\n\t\t\treturn {\n\t\t\t\tprefix: lookup.prefix,\n\t\t\t\tsuggestions: lookup.maxSuggestions,\n\t\t\t\tcity_filter: joinFieldWith(lookup.cityFilter, \",\"),\n\t\t\t\tstate_filter: joinFieldWith(lookup.stateFilter, \",\"),\n\t\t\t\tprefer: joinFieldWith(lookup.prefer, \";\"),\n\t\t\t\tprefer_ratio: lookup.preferRatio,\n\t\t\t\tgeolocate: lookup.geolocate,\n\t\t\t\tgeolocate_precision: lookup.geolocatePrecision,\n\t\t\t};\n\n\t\t\tfunction joinFieldWith(field, delimiter) {\n\t\t\t\tif (field.length) return field.join(delimiter);\n\t\t\t}\n\t\t}\n\n\t\tfunction buildSuggestionsFromResponse(payload) {\n\t\t\tif (payload.suggestions === null) return [];\n\n\t\t\treturn payload.suggestions.map(suggestion => new Suggestion(suggestion));\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-autocomplete-api#http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param prefix The beginning of an address. This is required to be set.\n\t */\n\tconstructor(prefix) {\n\t\tthis.result = [];\n\n\t\tthis.prefix = prefix;\n\t\tthis.maxSuggestions = undefined;\n\t\tthis.cityFilter = [];\n\t\tthis.stateFilter = [];\n\t\tthis.prefer = [];\n\t\tthis.preferRatio = undefined;\n\t\tthis.geolocate = undefined;\n\t\tthis.geolocatePrecision = undefined;\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://www.smarty.com/docs/cloud/us-autocomplete-api#http-response\"\n */\nclass Suggestion {\n\tconstructor(responseData) {\n\t\tthis.text = responseData.text;\n\t\tthis.streetLine = responseData.street_line;\n\t\tthis.city = responseData.city;\n\t\tthis.state = responseData.state;\n\t}\n}\n\nmodule.exports = Suggestion;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Suggestion = require(\"./Suggestion\");\n\n/**\n * This client sends lookups to the Smarty US Autocomplete Pro API, <br>\n *     and attaches the suggestions to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildRequestParameters(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = buildSuggestionsFromResponse(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParameters(lookup) {\n\t\t\treturn {\n\t\t\t\tsearch: lookup.search,\n\t\t\t\tselected: lookup.selected,\n\t\t\t\tmax_results: lookup.maxResults,\n\t\t\t\tinclude_only_cities: joinFieldWith(lookup.includeOnlyCities, \";\"),\n\t\t\t\tinclude_only_states: joinFieldWith(lookup.includeOnlyStates, \";\"),\n\t\t\t\tinclude_only_zip_codes: joinFieldWith(lookup.includeOnlyZIPCodes, \";\"),\n\t\t\t\texclude_states: joinFieldWith(lookup.excludeStates, \";\"),\n\t\t\t\tprefer_cities: joinFieldWith(lookup.preferCities, \";\"),\n\t\t\t\tprefer_states: joinFieldWith(lookup.preferStates, \";\"),\n\t\t\t\tprefer_zip_codes: joinFieldWith(lookup.preferZIPCodes, \";\"),\n\t\t\t\tprefer_ratio: lookup.preferRatio,\n\t\t\t\tprefer_geolocation: lookup.preferGeolocation,\n\t\t\t\tsource: lookup.source,\n\t\t\t};\n\n\t\t\tfunction joinFieldWith(field, delimiter) {\n\t\t\t\tif (field.length) return field.join(delimiter);\n\t\t\t}\n\t\t}\n\n\t\tfunction buildSuggestionsFromResponse(payload) {\n\t\t\tif (payload.suggestions === null) return [];\n\n\t\t\treturn payload.suggestions.map(suggestion => new Suggestion(suggestion));\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-autocomplete-api#pro-http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param search The beginning of an address. This is required to be set.\n\t */\n\tconstructor(search) {\n\t\tthis.result = [];\n\n\t\tthis.search = search;\n\t\tthis.selected = undefined;\n\t\tthis.maxResults = undefined;\n\t\tthis.includeOnlyCities = [];\n\t\tthis.includeOnlyStates = [];\n\t\tthis.includeOnlyZIPCodes = [];\n\t\tthis.excludeStates = [];\n\t\tthis.preferCities = [];\n\t\tthis.preferStates = [];\n\t\tthis.preferZIPCodes = [];\n\t\tthis.preferRatio = undefined;\n\t\tthis.preferGeolocation = undefined;\n\t\tthis.source = undefined\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://www.smarty.com/docs/cloud/us-autocomplete-api#pro-http-response\"\n */\nclass Suggestion {\n\tconstructor(responseData) {\n\t\tthis.streetLine = responseData.street_line;\n\t\tthis.secondary = responseData.secondary;\n\t\tthis.city = responseData.city;\n\t\tthis.state = responseData.state;\n\t\tthis.zipcode = responseData.zipcode;\n\t\tthis.entries = responseData.entries;\n\t}\n}\n\nmodule.exports = Suggestion;","const Candidate = require(\"../us_street/Candidate\");\n\n/**\n * @see <a href=\"https://www.smarty.com/docs/cloud/us-extract-api#http-response-status\">Smarty US Extract API docs</a>\n */\nclass Address {\n\tconstructor (responseData) {\n\t\tthis.text = responseData.text;\n\t\tthis.verified = responseData.verified;\n\t\tthis.line = responseData.line;\n\t\tthis.start = responseData.start;\n\t\tthis.end = responseData.end;\n\t\tthis.candidates = responseData.api_output.map(rawAddress => new Candidate(rawAddress));\n\t}\n}\n\nmodule.exports = Address;","const Errors = require(\"../Errors\");\nconst Request = require(\"../Request\");\nconst Result = require(\"./Result\");\n\n/**\n * This client sends lookups to the Smarty US Extract API, <br>\n *     and attaches the results to the Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new Errors.UndefinedLookupError();\n\n\t\tlet request = new Request(lookup.text);\n\t\trequest.parameters = buildRequestParams(lookup);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tlookup.result = new Result(response.payload);\n\t\t\t\t\tresolve(lookup);\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction buildRequestParams(lookup) {\n\t\t\treturn {\n\t\t\t\thtml: lookup.html,\n\t\t\t\taggressive: lookup.aggressive,\n\t\t\t\taddr_line_breaks: lookup.addressesHaveLineBreaks,\n\t\t\t\taddr_per_line: lookup.addressesPerLine,\n\t\t\t};\n\t\t}\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-extract-api#http-request-input-fields\"\n */\nclass Lookup {\n\t/**\n\t * @param text The text that is to have addresses extracted out of it for verification (required)\n\t */\n\tconstructor(text) {\n\t\tthis.result = {\n\t\t\tmeta: {},\n\t\t\taddresses: [],\n\t\t};\n\t\t//TODO: require the text field.\n\t\tthis.text = text;\n\t\tthis.html = undefined;\n\t\tthis.aggressive = undefined;\n\t\tthis.addressesHaveLineBreaks = undefined;\n\t\tthis.addressesPerLine = undefined;\n\t}\n}\n\nmodule.exports = Lookup;","const Address = require(\"./Address\");\n\n/**\n * @see <a href=\"https://www.smarty.com/docs/cloud/us-extract-api#http-response-status\">Smarty US Extract API docs</a>\n */\nclass Result {\n\tconstructor({meta, addresses}) {\n\t\tthis.meta = {\n\t\t\tlines: meta.lines,\n\t\t\tunicode: meta.unicode,\n\t\t\taddressCount: meta.address_count,\n\t\t\tverifiedCount: meta.verified_count,\n\t\t\tbytes: meta.bytes,\n\t\t\tcharacterCount: meta.character_count,\n\t\t};\n\n\t\tthis.addresses = addresses.map(rawAddress => new Address(rawAddress));\n\t}\n}\n\nmodule.exports = Result;","const Request = require(\"../Request\");\nconst Response = require(\"./Response\");\nconst buildInputData = require(\"../util/buildInputData\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usReverseGeo;\nconst {UndefinedLookupError} = require(\"../Errors.js\");\n\n/**\n * This client sends lookups to the Smarty US Reverse Geo API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\tsend(lookup) {\n\t\tif (typeof lookup === \"undefined\") throw new UndefinedLookupError();\n\n\t\tlet request = new Request();\n\t\trequest.parameters = buildInputData(lookup, keyTranslationFormat);\n\n\t\treturn new Promise((resolve, reject) => {\n\t\t\tthis.sender.send(request)\n\t\t\t\t.then(response => {\n\t\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\t\tresolve(attachLookupResults(response, lookup));\n\t\t\t\t})\n\t\t\t\t.catch(reject);\n\t\t});\n\n\t\tfunction attachLookupResults(response, lookup) {\n\t\t\tlookup.response = new Response(response.payload);\n\n\t\t\treturn lookup;\n\t\t}\n\t}\n}\n\nmodule.exports = Client;\n","const Response = require(\"./Response\");\n\n/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-street-api#input-fields\"\n */\nclass Lookup {\n\tconstructor(latitude, longitude) {\n\t\tthis.latitude = latitude.toFixed(8);\n\t\tthis.longitude = longitude.toFixed(8);\n\t\tthis.response = new Response();\n\t}\n}\n\nmodule.exports = Lookup;\n","const Result = require(\"./Result\");\n\n/**\n * The SmartyResponse contains the response from a call to the US Reverse Geo API.\n */\nclass Response {\n\tconstructor(responseData) {\n\t\tthis.results = [];\n\n\t\tif (responseData)\n\t\t\tresponseData.results.map(rawResult => {\n\t\t\t\tthis.results.push(new Result(rawResult));\n\t\t\t});\n\t}\n}\n\nmodule.exports = Response;\n","/**\n * A candidate is a possible match for an address that was submitted.<br>\n *     A lookup can have multiple candidates if the address was ambiguous.\n *\n * @see \"https://www.smarty.com/docs/cloud/us-reverse-geo-api#result\"\n */\nclass Result {\n\tconstructor(responseData) {\n\t\tthis.distance = responseData.distance;\n\n\t\tthis.address = {};\n\t\tif (responseData.address) {\n\t\t\tthis.address.street = responseData.address.street;\n\t\t\tthis.address.city = responseData.address.city;\n\t\t\tthis.address.state_abbreviation = responseData.address.state_abbreviation;\n\t\t\tthis.address.zipcode = responseData.address.zipcode;\n\t\t}\n\n\t\tthis.coordinate = {};\n\t\tif (responseData.coordinate) {\n\t\t\tthis.coordinate.latitude = responseData.coordinate.latitude;\n\t\t\tthis.coordinate.longitude = responseData.coordinate.longitude;\n\t\t\tthis.coordinate.accuracy = responseData.coordinate.accuracy;\n\t\t\tswitch (responseData.coordinate.license) {\n\t\t\t\tcase 1:\n\t\t\t\t\tthis.coordinate.license = \"SmartyStreets Proprietary\";\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tthis.coordinate.license = \"SmartyStreets\";\n\t\t\t}\n\t\t}\n\t}\n}\n\nmodule.exports = Result;","/**\n * A candidate is a possible match for an address that was submitted.<br>\n *     A lookup can have multiple candidates if the address was ambiguous, and<br>\n *     the maxCandidates field is set higher than 1.\n *\n * @see \"https://www.smarty.com/docs/cloud/us-street-api#root\"\n */\nclass Candidate {\n\tconstructor(responseData) {\n\t\tthis.inputIndex = responseData.input_index;\n\t\tthis.candidateIndex = responseData.candidate_index;\n\t\tthis.addressee = responseData.addressee;\n\t\tthis.deliveryLine1 = responseData.delivery_line_1;\n\t\tthis.deliveryLine2 = responseData.delivery_line_2;\n\t\tthis.lastLine = responseData.last_line;\n\t\tthis.deliveryPointBarcode = responseData.delivery_point_barcode;\n\n\t\tthis.components = {};\n\t\tif (responseData.components !== undefined) {\n\t\t\tthis.components.urbanization = responseData.components.urbanization;\n\t\t\tthis.components.primaryNumber = responseData.components.primary_number;\n\t\t\tthis.components.streetName = responseData.components.street_name;\n\t\t\tthis.components.streetPredirection = responseData.components.street_predirection;\n\t\t\tthis.components.streetPostdirection = responseData.components.street_postdirection;\n\t\t\tthis.components.streetSuffix = responseData.components.street_suffix;\n\t\t\tthis.components.secondaryNumber = responseData.components.secondary_number;\n\t\t\tthis.components.secondaryDesignator = responseData.components.secondary_designator;\n\t\t\tthis.components.extraSecondaryNumber = responseData.components.extra_secondary_number;\n\t\t\tthis.components.extraSecondaryDesignator = responseData.components.extra_secondary_designator;\n\t\t\tthis.components.pmbDesignator = responseData.components.pmb_designator;\n\t\t\tthis.components.pmbNumber = responseData.components.pmb_number;\n\t\t\tthis.components.cityName = responseData.components.city_name;\n\t\t\tthis.components.defaultCityName = responseData.components.default_city_name;\n\t\t\tthis.components.state = responseData.components.state_abbreviation;\n\t\t\tthis.components.zipCode = responseData.components.zipcode;\n\t\t\tthis.components.plus4Code = responseData.components.plus4_code;\n\t\t\tthis.components.deliveryPoint = responseData.components.delivery_point;\n\t\t\tthis.components.deliveryPointCheckDigit = responseData.components.delivery_point_check_digit;\n\t\t}\n\n\t\tthis.metadata = {};\n\t\tif (responseData.metadata !== undefined) {\n\t\t\tthis.metadata.recordType = responseData.metadata.record_type;\n\t\t\tthis.metadata.zipType = responseData.metadata.zip_type;\n\t\t\tthis.metadata.countyFips = responseData.metadata.county_fips;\n\t\t\tthis.metadata.countyName = responseData.metadata.county_name;\n\t\t\tthis.metadata.carrierRoute = responseData.metadata.carrier_route;\n\t\t\tthis.metadata.congressionalDistrict = responseData.metadata.congressional_district;\n\t\t\tthis.metadata.buildingDefaultIndicator = responseData.metadata.building_default_indicator;\n\t\t\tthis.metadata.rdi = responseData.metadata.rdi;\n\t\t\tthis.metadata.elotSequence = responseData.metadata.elot_sequence;\n\t\t\tthis.metadata.elotSort = responseData.metadata.elot_sort;\n\t\t\tthis.metadata.latitude = responseData.metadata.latitude;\n\t\t\tthis.metadata.longitude = responseData.metadata.longitude;\n\t\t\tswitch (responseData.metadata.coordinate_license)\n\t\t\t{\n\t\t\t\tcase 1:\n\t\t\t\t\tthis.metadata.coordinateLicense = \"SmartyStreets Proprietary\";\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tthis.metadata.coordinateLicense = \"SmartyStreets\";\n\t\t\t}\n\t\t\tthis.metadata.precision = responseData.metadata.precision;\n\t\t\tthis.metadata.timeZone = responseData.metadata.time_zone;\n\t\t\tthis.metadata.utcOffset = responseData.metadata.utc_offset;\n\t\t\tthis.metadata.obeysDst = responseData.metadata.dst;\n\t\t\tthis.metadata.isEwsMatch = responseData.metadata.ews_match;\n\t\t}\n\n\t\tthis.analysis = {};\n\t\tif (responseData.analysis !== undefined) {\n\t\t\tthis.analysis.dpvMatchCode = responseData.analysis.dpv_match_code;\n\t\t\tthis.analysis.dpvFootnotes = responseData.analysis.dpv_footnotes;\n\t\t\tthis.analysis.cmra = responseData.analysis.dpv_cmra;\n\t\t\tthis.analysis.vacant = responseData.analysis.dpv_vacant;\n\t\t\tthis.analysis.noStat = responseData.analysis.dpv_no_stat;\n\t\t\tthis.analysis.active = responseData.analysis.active;\n\t\t\tthis.analysis.isEwsMatch = responseData.analysis.ews_match; // Deprecated, refer to metadata.ews_match\n\t\t\tthis.analysis.footnotes = responseData.analysis.footnotes;\n\t\t\tthis.analysis.lacsLinkCode = responseData.analysis.lacslink_code;\n\t\t\tthis.analysis.lacsLinkIndicator = responseData.analysis.lacslink_indicator;\n\t\t\tthis.analysis.isSuiteLinkMatch = responseData.analysis.suitelink_match;\n\t\t\tthis.analysis.enhancedMatch = responseData.analysis.enhanced_match;\n\t\t}\n\t}\n}\n\nmodule.exports = Candidate;","const Candidate = require(\"./Candidate\");\nconst Lookup = require(\"./Lookup\");\nconst Batch = require(\"../Batch\");\nconst UndefinedLookupError = require(\"../Errors\").UndefinedLookupError;\nconst sendBatch = require(\"../util/sendBatch\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usStreet;\n\n/**\n * This client sends lookups to the Smarty US Street API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\t/**\n\t * Sends up to 100 lookups for validation.\n\t * @param data may be a Lookup object, or a Batch which must contain between 1 and 100 Lookup objects\n\t * @throws SmartyException\n\t */\n\tsend(data) {\n\t\tconst dataIsBatch = data instanceof Batch;\n\t\tconst dataIsLookup = data instanceof Lookup;\n\n\t\tif (!dataIsLookup && !dataIsBatch) throw new UndefinedLookupError;\n\n\t\tlet batch;\n\n\t\tif (dataIsLookup) {\n\t\t\tif (data.maxCandidates == null && data.match == \"enhanced\")\n\t\t\t\tdata.maxCandidates = 5;\n\t\t\tbatch = new Batch();\n\t\t\tbatch.add(data);\n\t\t} else {\n\t\t\tbatch = data;\n\t\t}\n\n\t\treturn sendBatch(batch, this.sender, Candidate, keyTranslationFormat);\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-street-api#input-fields\"\n */\nclass Lookup {\n\tconstructor(street, street2, secondary, city, state, zipCode, lastLine, addressee, urbanization, match, maxCandidates, inputId) {\n\t\tthis.street = street;\n\t\tthis.street2 = street2;\n\t\tthis.secondary = secondary;\n\t\tthis.city = city;\n\t\tthis.state = state;\n\t\tthis.zipCode = zipCode;\n\t\tthis.lastLine = lastLine;\n\t\tthis.addressee = addressee;\n\t\tthis.urbanization = urbanization;\n\t\tthis.match = match;\n\t\tthis.maxCandidates = maxCandidates;\n\t\tthis.inputId = inputId;\n\t\tthis.result = [];\n\t}\n}\n\nmodule.exports = Lookup;\n","const Lookup = require(\"./Lookup\");\nconst Result = require(\"./Result\");\nconst Batch = require(\"../Batch\");\nconst UndefinedLookupError = require(\"../Errors\").UndefinedLookupError;\nconst sendBatch = require(\"../util/sendBatch\");\nconst keyTranslationFormat = require(\"../util/apiToSDKKeyMap\").usZipcode;\n\n/**\n * This client sends lookups to the Smarty US ZIP Code API, <br>\n *     and attaches the results to the appropriate Lookup objects.\n */\nclass Client {\n\tconstructor(sender) {\n\t\tthis.sender = sender;\n\t}\n\n\t/**\n\t * Sends up to 100 lookups for validation.\n\t * @param data May be a Lookup object, or a Batch which must contain between 1 and 100 Lookup objects\n\t * @throws SmartyException\n\t */\n\tsend(data) {\n\t\tconst dataIsBatch = data instanceof Batch;\n\t\tconst dataIsLookup = data instanceof Lookup;\n\n\t\tif (!dataIsLookup && !dataIsBatch) throw new UndefinedLookupError;\n\n\t\tlet batch;\n\n\t\tif (dataIsLookup) {\n\t\t\tbatch = new Batch();\n\t\t\tbatch.add(data);\n\t\t} else batch = data;\n\n\t\treturn sendBatch(batch, this.sender, Result, keyTranslationFormat);\n\t}\n}\n\nmodule.exports = Client;","/**\n * In addition to holding all of the input data for this lookup, this class also<br>\n *     will contain the result of the lookup after it comes back from the API.\n *     @see \"https://www.smarty.com/docs/cloud/us-zipcode-api#http-request-input-fields\"\n */\nclass Lookup {\n\tconstructor(city, state, zipCode, inputId) {\n\t\tthis.city = city;\n\t\tthis.state = state;\n\t\tthis.zipCode = zipCode;\n\t\tthis.inputId = inputId;\n\t\tthis.result = [];\n\t}\n}\n\nmodule.exports = Lookup;","/**\n * @see \"https://www.smarty.com/docs/cloud/us-zipcode-api#root\"\n */\nclass Result {\n\tconstructor(responseData) {\n\t\tthis.inputIndex = responseData.input_index;\n\t\tthis.status = responseData.status;\n\t\tthis.reason = responseData.reason;\n\t\tthis.valid = this.status === undefined && this.reason === undefined;\n\n\t\tthis.cities = !responseData.city_states ? [] : responseData.city_states.map(city => {\n\t\t\treturn {\n\t\t\t\tcity: city.city,\n\t\t\t\tstateAbbreviation: city.state_abbreviation,\n\t\t\t\tstate: city.state,\n\t\t\t\tmailableCity: city.mailable_city,\n\t\t\t};\n\t\t});\n\n\t\tthis.zipcodes = !responseData.zipcodes ? [] : responseData.zipcodes.map(zipcode => {\n\t\t\treturn {\n\t\t\t\tzipcode: zipcode.zipcode,\n\t\t\t\tzipcodeType: zipcode.zipcode_type,\n\t\t\t\tdefaultCity: zipcode.default_city,\n\t\t\t\tcountyFips: zipcode.county_fips,\n\t\t\t\tcountyName: zipcode.county_name,\n\t\t\t\tlatitude: zipcode.latitude,\n\t\t\t\tlongitude: zipcode.longitude,\n\t\t\t\tprecision: zipcode.precision,\n\t\t\t\tstateAbbreviation: zipcode.state_abbreviation,\n\t\t\t\tstate: zipcode.state,\n\t\t\t\talternateCounties: !zipcode.alternate_counties ? [] : zipcode.alternate_counties.map(county => {\n\t\t\t\t\treturn {\n\t\t\t\t\t\tcountyFips: county.county_fips,\n\t\t\t\t\t\tcountyName: county.county_name,\n\t\t\t\t\t\tstateAbbreviation: county.state_abbreviation,\n\t\t\t\t\t\tstate: county.state,\n\t\t\t\t\t}\n\t\t\t\t}),\n\t\t\t};\n\t\t});\n\t}\n}\n\nmodule.exports = Result;","module.exports = {\n\tusStreet: {\n\t\t\"street\": \"street\",\n\t\t\"street2\": \"street2\",\n\t\t\"secondary\": \"secondary\",\n\t\t\"city\": \"city\",\n\t\t\"state\": \"state\",\n\t\t\"zipcode\": \"zipCode\",\n\t\t\"lastline\": \"lastLine\",\n\t\t\"addressee\": \"addressee\",\n\t\t\"urbanization\": \"urbanization\",\n\t\t\"match\": \"match\",\n\t\t\"candidates\": \"maxCandidates\",\n\t},\n\tusZipcode: {\n\t\t\"city\": \"city\",\n\t\t\"state\": \"state\",\n\t\t\"zipcode\": \"zipCode\",\n\t},\n\tinternationalStreet: {\n\t\t\"country\": \"country\",\n\t\t\"freeform\": \"freeform\",\n\t\t\"address1\": \"address1\",\n\t\t\"address2\": \"address2\",\n\t\t\"address3\": \"address3\",\n\t\t\"address4\": \"address4\",\n\t\t\"organization\": \"organization\",\n\t\t\"locality\": \"locality\",\n\t\t\"administrative_area\": \"administrativeArea\",\n\t\t\"postal_code\": \"postalCode\",\n\t\t\"geocode\": \"geocode\",\n\t\t\"language\": \"language\",\n\t},\n\tusReverseGeo: {\n\t\t\"latitude\": \"latitude\",\n\t\t\"longitude\": \"longitude\",\n\t}\n};","const ClientBuilder = require(\"../ClientBuilder\");\n\nfunction instantiateClientBuilder(credentials) {\n\treturn new ClientBuilder(credentials);\n}\n\nfunction buildUsStreetApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsStreetApiClient();\n}\n\nfunction buildUsAutocompleteApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsAutocompleteClient();\n}\n\nfunction buildUsAutocompleteProApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsAutocompleteProClient();\n}\n\nfunction buildUsExtractApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsExtractClient();\n}\n\nfunction buildUsZipcodeApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsZipcodeClient();\n}\n\nfunction buildInternationalStreetApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildInternationalStreetClient();\n}\n\nfunction buildUsReverseGeoApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildUsReverseGeoClient();\n}\n\nfunction buildInternationalAddressAutocompleteApiClient(credentials) {\n\treturn instantiateClientBuilder(credentials).buildInternationalAddressAutocompleteClient();\n}\n\nmodule.exports = {\n\tusStreet: buildUsStreetApiClient,\n\tusAutocomplete: buildUsAutocompleteApiClient,\n\tusAutocompletePro: buildUsAutocompleteProApiClient,\n\tusExtract: buildUsExtractApiClient,\n\tusZipcode: buildUsZipcodeApiClient,\n\tinternationalStreet: buildInternationalStreetApiClient,\n\tusReverseGeo: buildUsReverseGeoApiClient,\n\tinternationalAddressAutocomplete: buildInternationalAddressAutocompleteApiClient,\n};","const InputData = require(\"../InputData\");\n\nmodule.exports = (lookup, keyTranslationFormat) => {\n\tlet inputData = new InputData(lookup);\n\n\tfor (let key in keyTranslationFormat) {\n\t\tinputData.add(key, keyTranslationFormat[key]);\n\t}\n\n\treturn inputData.data;\n};\n","const Request = require(\"../Request\");\nconst Errors = require(\"../Errors\");\nconst buildInputData = require(\"../util/buildInputData\");\n\nmodule.exports = (batch, sender, Result, keyTranslationFormat) => {\n\tif (batch.isEmpty()) throw new Errors.BatchEmptyError;\n\n\tlet request = new Request();\n\n\tif (batch.length() === 1) request.parameters = generateRequestPayload(batch)[0];\n\telse request.payload = generateRequestPayload(batch);\n\n\treturn new Promise((resolve, reject) => {\n\t\tsender.send(request)\n\t\t\t.then(response => {\n\t\t\t\tif (response.error) reject(response.error);\n\n\t\t\t\tresolve(assignResultsToLookups(batch, response));\n\t\t\t})\n\t\t\t.catch(reject);\n\t});\n\n\tfunction generateRequestPayload(batch) {\n\t\treturn batch.lookups.map((lookup) => {\n\t\t\treturn buildInputData(lookup, keyTranslationFormat);\n\t\t});\n\t}\n\n\tfunction assignResultsToLookups(batch, response) {\n\t\tresponse.payload.map(rawResult => {\n\t\t\tlet result = new Result(rawResult);\n\t\t\tlet lookup = batch.getByIndex(result.inputIndex);\n\n\t\t\tlookup.result.push(result);\n\t\t});\n\n\t\treturn batch;\n\t}\n};\n","'use strict'\n\nvar nextTick = nextTickArgs\nprocess.nextTick(upgrade, 42) // pass 42 and see if upgrade is called with it\n\nmodule.exports = thunky\n\nfunction thunky (fn) {\n  var state = run\n  return thunk\n\n  function thunk (callback) {\n    state(callback || noop)\n  }\n\n  function run (callback) {\n    var stack = [callback]\n    state = wait\n    fn(done)\n\n    function wait (callback) {\n      stack.push(callback)\n    }\n\n    function done (err) {\n      var args = arguments\n      state = isError(err) ? run : finished\n      while (stack.length) finished(stack.shift())\n\n      function finished (callback) {\n        nextTick(apply, callback, args)\n      }\n    }\n  }\n}\n\nfunction isError (err) { // inlined from util so this works in the browser\n  return Object.prototype.toString.call(err) === '[object Error]'\n}\n\nfunction noop () {}\n\nfunction apply (callback, args) {\n  callback.apply(null, args)\n}\n\nfunction upgrade (val) {\n  if (val === 42) nextTick = process.nextTick\n}\n\nfunction nextTickArgs (fn, a, b) {\n  process.nextTick(function () {\n    fn(a, b)\n  })\n}\n","var moduleMap = {\n\t\"./models\": () => {\n\t\treturn Promise.all([__webpack_require__.e(777), __webpack_require__.e(867), __webpack_require__.e(334), __webpack_require__.e(732), __webpack_require__.e(829)]).then(() => () => (__webpack_require__(/*! ./src/domain */ \"./src/domain/index.js\")));\n\t},\n\t\"./adapters\": () => {\n\t\treturn Promise.all([__webpack_require__.e(777), __webpack_require__.e(867)]).then(() => () => (__webpack_require__(/*! ./src/adapters */ \"./src/adapters/index.js\")));\n\t},\n\t\"./services\": () => {\n\t\treturn Promise.all([__webpack_require__.e(777), __webpack_require__.e(867), __webpack_require__.e(732), __webpack_require__.e(589)]).then(() => () => (__webpack_require__(/*! ./src/services */ \"./src/services/index.js\")));\n\t},\n\t\"./ports\": () => {\n\t\treturn __webpack_require__.e(334).then(() => () => (__webpack_require__(/*! ./src/domain/ports.js */ \"./src/domain/ports.js\")));\n\t},\n\t\"./event-bus\": () => {\n\t\treturn Promise.all([__webpack_require__.e(777), __webpack_require__.e(867), __webpack_require__.e(857)]).then(() => () => (__webpack_require__(/*! ./src/services/event-bus */ \"./src/services/event-bus.js\")));\n\t}\n};\nvar get = (module) => {\n\treturn (\n\t\t__webpack_require__.o(moduleMap, module)\n\t\t\t? moduleMap[module]()\n\t\t\t: Promise.resolve().then(() => {\n\t\t\t\tthrow new Error('Module \"' + module + '\" does not exist in container.');\n\t\t\t})\n\t);\n};\nvar init = (shareScope) => {\n\tvar oldScope = __webpack_require__.S[\"default\"];\n\tvar name = \"default\"\n\tif(oldScope && oldScope !== shareScope) throw new Error(\"Container initialization failed as it has already been initialized with a different share scope\");\n\t__webpack_require__.S[name] = shareScope;\n\treturn __webpack_require__.I(name);\n};\n\n// This exports getters to disallow modifications\n__webpack_require__.d(exports, {\n\tget: () => get,\n\tinit: () => init\n});","module.exports = require(\"assert\");","module.exports = require(\"buffer\");","module.exports = require(\"crypto\");","module.exports = require(\"dgram\");","module.exports = require(\"events\");","module.exports = require(\"fs\");","module.exports = require(\"http\");","module.exports = require(\"https\");","module.exports = require(\"net\");","module.exports = require(\"os\");","module.exports = require(\"path\");","module.exports = require(\"stream\");","module.exports = require(\"tls\");","module.exports = require(\"tty\");","module.exports = require(\"url\");","module.exports = require(\"util\");","module.exports = require(\"zlib\");","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tif(__webpack_module_cache__[moduleId]) {\n\t\treturn __webpack_module_cache__[moduleId].exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = (module) => {\n\tvar getter = module && module.__esModule ?\n\t\t() => module['default'] :\n\t\t() => module;\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = (chunkId) => {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks\n__webpack_require__.u = (chunkId) => {\n\t// return url for filenames based on template\n\treturn \"\" + chunkId + \".js\";\n};","__webpack_require__.o = (obj, prop) => Object.prototype.hasOwnProperty.call(obj, prop)","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"http://localhost:8000/\";","__webpack_require__.S = {};\nvar initPromises = {};\n__webpack_require__.I = (name) => {\n\t// only runs once\n\tif(initPromises[name]) return initPromises[name];\n\t// handling circular init calls\n\tinitPromises[name] = 1;\n\t// creates a new share scope if needed\n\tif(!__webpack_require__.o(__webpack_require__.S, name)) __webpack_require__.S[name] = {};\n\t// runs all init snippets from all modules reachable\n\tvar scope = __webpack_require__.S[name];\n\tvar warn = (msg) => typeof console !== \"undefined\" && console.warn && console.warn(msg);;\n\tvar uniqueName = \"aegis-app\";\n\tvar register = (name, version, factory) => {\n\t\tvar versions = scope[name] = scope[name] || {};\n\t\tvar activeVersion = versions[version];\n\t\tif(!activeVersion || !activeVersion.loaded && uniqueName > activeVersion.from) versions[version] = { get: factory, from: uniqueName };\n\t};\n\tvar initExternal = (id) => {\n\t\tvar handleError = (err) => warn(\"Initialization of sharing external failed: \" + err);\n\t\ttry {\n\t\t\tvar module = __webpack_require__(id);\n\t\t\tif(!module) return;\n\t\t\tvar initFn = (module) => module && module.init && module.init(__webpack_require__.S[name])\n\t\t\tif(module.then) return promises.push(module.then(initFn, handleError));\n\t\t\tvar initResult = initFn(module);\n\t\t\tif(initResult && initResult.then) return promises.push(initResult.catch(handleError));\n\t\t} catch(err) { handleError(err); }\n\t}\n\tvar promises = [];\n\tswitch(name) {\n\t\tcase \"default\": {\n\t\t\tregister(\"axios\", \"0.21.4\", () => () => __webpack_require__(/*! ./node_modules/axios/index.js */ \"./node_modules/axios/index.js\"));\n\t\t\tregister(\"axios\", \"0.26.1\", () => () => __webpack_require__(/*! ./node_modules/smartystreets-javascript-sdk/node_modules/axios/index.js */ \"./node_modules/smartystreets-javascript-sdk/node_modules/axios/index.js\"));\n\t\t\tregister(\"kafkajs\", \"1.16.0\", () => () => __webpack_require__(/*! ./node_modules/kafkajs/index.js */ \"./node_modules/kafkajs/index.js\"));\n\t\t\tregister(\"multicast-dns\", \"7.2.5\", () => () => __webpack_require__(/*! ./node_modules/multicast-dns/index.js */ \"./node_modules/multicast-dns/index.js\"));\n\t\t\tregister(\"nanoid\", \"3.3.4\", () => () => __webpack_require__(/*! ./node_modules/nanoid/index.js */ \"./node_modules/nanoid/index.js\"));\n\t\t\tregister(\"smartystreets-javascript-sdk\", \"1.13.7\", () => () => __webpack_require__(/*! ./node_modules/smartystreets-javascript-sdk/index.js */ \"./node_modules/smartystreets-javascript-sdk/index.js\"));\n\t\t}\n\t\tbreak;\n\t}\n\treturn promises.length && (initPromises[name] = Promise.all(promises).then(() => initPromises[name] = 1));\n};","var parseVersion = (str) => {\n\t// see webpack/lib/util/semver.js for original code\n\tvar p=p=>{return p.split(\".\").map((p=>{return+p==p?+p:p}))},n=/^([^-+]+)?(?:-([^+]+))?(?:\\+(.+))?$/.exec(str),r=n[1]?p(n[1]):[];return n[2]&&(r.length++,r.push.apply(r,p(n[2]))),n[3]&&(r.push([]),r.push.apply(r,p(n[3]))),r;\n}\nvar versionLt = (a, b) => {\n\t// see webpack/lib/util/semver.js for original code\n\ta=parseVersion(a),b=parseVersion(b);for(var r=0;;){if(r>=a.length)return r<b.length&&\"u\"!=(typeof b[r])[0];var e=a[r],n=(typeof e)[0];if(r>=b.length)return\"u\"==n;var t=b[r],f=(typeof t)[0];if(n!=f)return\"o\"==n&&\"n\"==f||(\"s\"==f||\"u\"==n);if(\"o\"!=n&&\"u\"!=n&&e!=t)return e<t;r++}\n}\nvar rangeToString = (range) => {\n\t// see webpack/lib/util/semver.js for original code\n\tif(1===range.length)return\"*\";if(0 in range){var r=\"\",n=range[0];r+=0==n?\">=\":-1==n?\"<\":1==n?\"^\":2==n?\"~\":n>0?\"=\":\"!=\";for(var e=1,a=1;a<range.length;a++){e--,r+=\"u\"==(typeof(t=range[a]))[0]?\"-\":(e>0?\".\":\"\")+(e=2,t)}return r}var g=[];for(a=1;a<range.length;a++){var t=range[a];g.push(0===t?\"not(\"+o()+\")\":1===t?\"(\"+o()+\" || \"+o()+\")\":2===t?g.pop()+\" \"+g.pop():rangeToString(t))}return o();function o(){return g.pop().replace(/^\\((.+)\\)$/,\"$1\")}\n}\nvar satisfy = (range, version) => {\n\t// see webpack/lib/util/semver.js for original code\n\tif(0 in range){version=parseVersion(version);var e=range[0],r=e<0;r&&(e=-e-1);for(var n=0,i=1,a=!0;;i++,n++){var f,s,g=i<range.length?(typeof range[i])[0]:\"\";if(n>=version.length||\"o\"==(s=(typeof(f=version[n]))[0]))return!a||(\"u\"==g?i>e&&!r:\"\"==g!=r);if(\"u\"==s){if(!a||\"u\"!=g)return!1}else if(a)if(g==s)if(i<=e){if(f!=range[i])return!1}else{if(r?f>range[i]:f<range[i])return!1;f!=range[i]&&(a=!1)}else if(\"s\"!=g&&\"n\"!=g){if(r||i<=e)return!1;a=!1,i--}else{if(i<=e||s<g!=r)return!1;a=!1}else\"s\"!=g&&\"n\"!=g&&(a=!1,i--)}}var t=[],o=t.pop.bind(t);for(n=1;n<range.length;n++){var u=range[n];t.push(1==u?o()|o():2==u?o()&o():u?satisfy(u,version):!o())}return!!o();\n}\nvar ensureExistence = (scopeName, key) => {\n\tvar scope = __webpack_require__.S[scopeName];\n\tif(!scope || !__webpack_require__.o(scope, key)) throw new Error(\"Shared module \" + key + \" doesn't exist in shared scope \" + scopeName);\n\treturn scope;\n};\nvar findVersion = (scope, key) => {\n\tvar versions = scope[key];\n\tvar key = Object.keys(versions).reduce((a, b) => {\n\t\treturn !a || versionLt(a, b) ? b : a;\n\t}, 0);\n\treturn key && versions[key]\n};\nvar findSingletonVersionKey = (scope, key) => {\n\tvar versions = scope[key];\n\treturn Object.keys(versions).reduce((a, b) => {\n\t\treturn !a || (!versions[a].loaded && versionLt(a, b)) ? b : a;\n\t}, 0);\n};\nvar getInvalidSingletonVersionMessage = (key, version, requiredVersion) => {\n\treturn \"Unsatisfied version \" + version + \" of shared singleton module \" + key + \" (required \" + rangeToString(requiredVersion) + \")\"\n};\nvar getSingletonVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar version = findSingletonVersionKey(scope, key);\n\tif (!satisfy(requiredVersion, version)) typeof console !== \"undefined\" && console.warn && console.warn(getInvalidSingletonVersionMessage(key, version, requiredVersion));\n\treturn get(scope[key][version]);\n};\nvar getStrictSingletonVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar version = findSingletonVersionKey(scope, key);\n\tif (!satisfy(requiredVersion, version)) throw new Error(getInvalidSingletonVersionMessage(key, version, requiredVersion));\n\treturn get(scope[key][version]);\n};\nvar findValidVersion = (scope, key, requiredVersion) => {\n\tvar versions = scope[key];\n\tvar key = Object.keys(versions).reduce((a, b) => {\n\t\tif (!satisfy(requiredVersion, b)) return a;\n\t\treturn !a || versionLt(a, b) ? b : a;\n\t}, 0);\n\treturn key && versions[key]\n};\nvar getInvalidVersionMessage = (scope, scopeName, key, requiredVersion) => {\n\tvar versions = scope[key];\n\treturn \"No satisfying version (\" + rangeToString(requiredVersion) + \") of shared module \" + key + \" found in shared scope \" + scopeName + \".\\n\" +\n\t\t\"Available versions: \" + Object.keys(versions).map((key) => {\n\t\treturn key + \" from \" + versions[key].from;\n\t}).join(\", \");\n};\nvar getValidVersion = (scope, scopeName, key, requiredVersion) => {\n\tvar entry = findValidVersion(scope, key, requiredVersion);\n\tif(entry) return get(entry);\n\tthrow new Error(getInvalidVersionMessage(scope, scopeName, key, requiredVersion));\n};\nvar warnInvalidVersion = (scope, scopeName, key, requiredVersion) => {\n\ttypeof console !== \"undefined\" && console.warn && console.warn(getInvalidVersionMessage(scope, scopeName, key, requiredVersion));\n};\nvar get = (entry) => {\n\tentry.loaded = 1;\n\treturn entry.get()\n};\nvar init = (fn) => function(scopeName, a, b, c) {\n\tvar promise = __webpack_require__.I(scopeName);\n\tif (promise.then) return promise.then(fn.bind(fn, scopeName, __webpack_require__.S[scopeName], a, b, c));\n\treturn fn(scopeName, __webpack_require__.S[scopeName], a, b, c);\n};\n\nvar load = /*#__PURE__*/ init((scopeName, scope, key) => {\n\tensureExistence(scopeName, key);\n\treturn get(findVersion(scope, key));\n});\nvar loadFallback = /*#__PURE__*/ init((scopeName, scope, key, fallback) => {\n\treturn scope && __webpack_require__.o(scope, key) ? get(findVersion(scope, key)) : fallback();\n});\nvar loadVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn get(findValidVersion(scope, key, version) || warnInvalidVersion(scope, scopeName, key, version) || findVersion(scope, key));\n});\nvar loadSingletonVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getSingletonVersion(scope, scopeName, key, version);\n});\nvar loadStrictVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getValidVersion(scope, scopeName, key, version);\n});\nvar loadStrictSingletonVersionCheck = /*#__PURE__*/ init((scopeName, scope, key, version) => {\n\tensureExistence(scopeName, key);\n\treturn getStrictSingletonVersion(scope, scopeName, key, version);\n});\nvar loadVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn get(findValidVersion(scope, key, version) || warnInvalidVersion(scope, scopeName, key, version) || findVersion(scope, key));\n});\nvar loadSingletonVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn getSingletonVersion(scope, scopeName, key, version);\n});\nvar loadStrictVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tvar entry = scope && __webpack_require__.o(scope, key) && findValidVersion(scope, key, version);\n\treturn entry ? get(entry) : fallback();\n});\nvar loadStrictSingletonVersionCheckFallback = /*#__PURE__*/ init((scopeName, scope, key, version, fallback) => {\n\tif(!scope || !__webpack_require__.o(scope, key)) return fallback();\n\treturn getStrictSingletonVersion(scope, scopeName, key, version);\n});\nvar installedModules = {};\nvar moduleToHandlerMapping = {\n\t\"webpack/sharing/consume/default/axios/axios?5326\": () => loadStrictVersionCheckFallback(\"default\", \"axios\", [2,0,21,1], () => () => __webpack_require__(/*! axios */ \"./node_modules/axios/index.js\")),\n\t\"webpack/sharing/consume/default/kafkajs/kafkajs\": () => loadStrictVersionCheckFallback(\"default\", \"kafkajs\", [1,1,14,0], () => () => __webpack_require__(/*! kafkajs */ \"./node_modules/kafkajs/index.js\")),\n\t\"webpack/sharing/consume/default/multicast-dns/multicast-dns\": () => loadStrictVersionCheckFallback(\"default\", \"multicast-dns\", [1,7,2,5], () => () => __webpack_require__(/*! multicast-dns */ \"./node_modules/multicast-dns/index.js\")),\n\t\"webpack/sharing/consume/default/nanoid/nanoid\": () => loadStrictVersionCheckFallback(\"default\", \"nanoid\", [1,3,1,12], () => () => __webpack_require__(/*! nanoid */ \"./node_modules/nanoid/index.js\")),\n\t\"webpack/sharing/consume/default/smartystreets-javascript-sdk/smartystreets-javascript-sdk\": () => loadStrictVersionCheckFallback(\"default\", \"smartystreets-javascript-sdk\", [1,1,6,0], () => () => __webpack_require__(/*! smartystreets-javascript-sdk */ \"./node_modules/smartystreets-javascript-sdk/index.js\")),\n\t\"webpack/sharing/consume/default/axios/axios?5c0e\": () => loadStrictVersionCheckFallback(\"default\", \"axios\", [2,0,26,1], () => () => __webpack_require__(/*! axios */ \"./node_modules/smartystreets-javascript-sdk/node_modules/axios/index.js\"))\n};\nvar initialConsumes = [\"webpack/sharing/consume/default/axios/axios?5c0e\"];\ninitialConsumes.forEach((id) => {\n\t__webpack_modules__[id] = (module) => {\n\t\t// Handle case when module is used sync\n\t\tinstalledModules[id] = 0;\n\t\tdelete __webpack_module_cache__[id];\n\t\tvar factory = moduleToHandlerMapping[id]();\n\t\tif(typeof factory !== \"function\") throw new Error(\"Shared module is not available for eager consumption: \" + id);\n\t\tmodule.exports = factory();\n\t}\n});\nvar chunkMapping = {\n\t\"334\": [\n\t\t\"webpack/sharing/consume/default/nanoid/nanoid\"\n\t],\n\t\"589\": [\n\t\t\"webpack/sharing/consume/default/nanoid/nanoid\"\n\t],\n\t\"732\": [\n\t\t\"webpack/sharing/consume/default/smartystreets-javascript-sdk/smartystreets-javascript-sdk\"\n\t],\n\t\"867\": [\n\t\t\"webpack/sharing/consume/default/axios/axios?5326\",\n\t\t\"webpack/sharing/consume/default/kafkajs/kafkajs\",\n\t\t\"webpack/sharing/consume/default/multicast-dns/multicast-dns\"\n\t]\n};\n__webpack_require__.f.consumes = (chunkId, promises) => {\n\tif(__webpack_require__.o(chunkMapping, chunkId)) {\n\t\tchunkMapping[chunkId].forEach((id) => {\n\t\t\tif(__webpack_require__.o(installedModules, id)) return promises.push(installedModules[id]);\n\t\t\tvar onFactory = (factory) => {\n\t\t\t\tinstalledModules[id] = 0;\n\t\t\t\t__webpack_modules__[id] = (module) => {\n\t\t\t\t\tdelete __webpack_module_cache__[id];\n\t\t\t\t\tmodule.exports = factory();\n\t\t\t\t}\n\t\t\t};\n\t\t\tvar onError = (error) => {\n\t\t\t\tdelete installedModules[id];\n\t\t\t\t__webpack_modules__[id] = (module) => {\n\t\t\t\t\tdelete __webpack_module_cache__[id];\n\t\t\t\t\tthrow error;\n\t\t\t\t}\n\t\t\t};\n\t\t\ttry {\n\t\t\t\tvar promise = moduleToHandlerMapping[id]();\n\t\t\t\tif(promise.then) {\n\t\t\t\t\tpromises.push(installedModules[id] = promise.then(onFactory).catch(onError));\n\t\t\t\t} else onFactory(promise);\n\t\t\t} catch(e) { onError(e); }\n\t\t});\n\t}\n}","\nconst { Octokit } = require(\"@octokit/rest\");\nconst fs = require(\"fs\");\nconst path = require(\"path\");\nconst token = process.env.GITHUB_TOKEN;\n\nconst octokit = new Octokit({ auth: token });\n\nfunction githubFetch(url) {\n  console.info(\"github url\", url);\n  const owner = url.searchParams.get(\"owner\");\n  const repo = url.searchParams.get(\"repo\");\n  const filedir = url.searchParams.get(\"filedir\");\n  const branch = url.searchParams.get(\"branch\");\n  return new Promise(function (resolve, reject) {\n    octokit\n      .request(\n        \"GET /repos/{owner}/{repo}/contents/{filedir}?ref={branch}\",\n        {\n          owner,\n          repo,\n          filedir,\n          branch\n        }\n      )\n      .then(function (rest) {\n        const file = rest.data.find(d => \"/\" + d.name === url.pathname);\n        return file.sha;\n      })\n      .then(function (sha) {\n        console.log(sha);\n        return octokit.request(\n          \"GET /repos/{owner}/{repo}/git/blobs/{sha}\",\n          {\n            owner,\n            repo,\n            sha,\n          }\n        );\n      })\n      .then(function (rest) {\n        resolve(Buffer.from(rest.data.content, \"base64\").toString(\"utf-8\"));\n      });\n  });\n}\n\nfunction httpRequest(url) {\n  if (/github/i.test(url.hostname)) \n    return githubFetch(url)\n  return httpGet(url)\n}\n\nfunction httpGet(params) {\n  return new Promise(function(resolve, reject) {\n    var req = require(params.protocol.slice(0, params.protocol.length - 1)).request(params, function(res) {\n      if (res.statusCode < 200 || res.statusCode >= 300) {\n        return reject(new Error('statusCode=' + res.statusCode));\n      }\n      var body = [];\n      res.on('data', function(chunk) {\n        body.push(chunk);\n      });\n      res.on('end', function() {\n        try {\n          body = Buffer.concat(body).toString();\n        } catch(e) {\n          reject(e);\n        }\n        resolve(body);\n      });\n    });\n    req.on('error', function(err) {\n      reject(err);\n    });\n    req.end();\n  });\n}\n\n// object to store loaded chunks\n// \"0\" means \"already loaded\", Promise means loading\nvar installedChunks = {\n\t446: 0\n};\n\nvar installChunk = (chunk) => {\n\tvar moreModules = chunk.modules, chunkIds = chunk.ids, runtime = chunk.runtime;\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\tvar callbacks = [];\n\tfor(var i = 0; i < chunkIds.length; i++) {\n\t\tif(installedChunks[chunkIds[i]])\n\t\t\tcallbacks = callbacks.concat(installedChunks[chunkIds[i]][0]);\n\t\tinstalledChunks[chunkIds[i]] = 0;\n\t}\n\tfor(i = 0; i < callbacks.length; i++)\n\t\tcallbacks[i]();\n};\n\n// ReadFile + VM.run chunk loading for javascript\n__webpack_require__.f.readFileVm = function(chunkId, promises) { console.log(\">>>>>>>>>chunkId\",chunkId);\n\n\tvar installedChunkData = installedChunks[chunkId];\n\tif(installedChunkData !== 0) { // 0 means \"already installed\".\n\t\t// array of [resolve, reject, promise] means \"currently loading\"\n\t\tif(installedChunkData) {\n\t\t\tpromises.push(installedChunkData[2]);\n\t\t} else {\n\t\t\tif(true) { // all chunks have JS\n\t\t\t\t// load the chunk and return promise to it\n\t\t\t\tvar promise = new Promise(function(resolve, reject) {\n\t\t\t\t\tinstalledChunkData = installedChunks[chunkId] = [resolve, reject];\n\t\t\t\t\tvar chunkFileName = \"/\" + __webpack_require__.u(chunkId);\n\t\t\t\t\tvar url = new (require(\"url\").URL)(__webpack_require__.p)\n\t\t\t\t\turl.pathname = chunkFileName;\n\t\t\t\t\thttpRequest(url)\n\t\t\t\t\t\t.then((content) => {\n\t\t\t\t\t\t\tvar chunk = {};\n\t\t\t\t\t\t\trequire('vm').runInThisContext('(function(exports, require, __dirname, __filename) {' + content + '\\n})', chunkFileName)(chunk, require, __dirname, chunkFileName);\n\t\t\t\t\t\t\tinstallChunk(chunk);\n\t\t\t\t\t\t}).catch((err) => {\n\t\t\t\t\t\t\treject(err);\n\t\t\t\t\t\t});\n\t\t\t\t});\n\t\t\t\tpromises.push(installedChunkData[2] = promise);\n\t\t\t} else installedChunks[chunkId] = 0;\n\t\t}\n\t}\n};\n\n// no external install chunk\n\n// no HMR\n\n// no HMR manifest","// module exports must be returned from runtime so entry inlining is disabled\n// startup\n// Load entry module and return exports\nreturn __webpack_require__(\"webpack/container/entry/local\");\n"],"sourceRoot":""}